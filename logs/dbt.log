[0m11:59:43.012353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001964357D610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001964357D6A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001964357C410>]}


============================== 11:59:43.015370 | 187350d9-6e32-45b3-8eff-e6a4d98ea996 ==============================
[0m11:59:43.015370 [info ] [MainThread]: Running with dbt=1.7.4
[0m11:59:43.017384 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m11:59:43.029670 [error] [MainThread]: Encountered an error:
Runtime Error
  Could not find profile named 'testproj'
[0m11:59:43.032687 [debug] [MainThread]: Command `cli run` failed at 11:59:43.031681 after 0.11 seconds
[0m11:59:43.033692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000196431771A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019643533D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000196435310D0>]}
[0m11:59:43.035703 [debug] [MainThread]: Flushing usage events
[0m12:01:33.445874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A70E09DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A70E09E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A70E0BD70>]}


============================== 12:01:33.447886 | 4a4a7bad-f650-4dee-a25f-bbf451a84e42 ==============================
[0m12:01:33.447886 [info ] [MainThread]: Running with dbt=1.7.4
[0m12:01:33.450040 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:01:33.495662 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabricspark", target "dev" invalid: Runtime Error
    Must specify `workspace guid` in profile
[0m12:01:33.498675 [debug] [MainThread]: Command `cli run` failed at 12:01:33.497671 after 0.15 seconds
[0m12:01:33.499679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A70E0B3E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A707A6D50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A70E5AD80>]}
[0m12:01:33.500993 [debug] [MainThread]: Flushing usage events
[0m12:02:34.602822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020644A13290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020644A132F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020644A13260>]}


============================== 12:02:34.604833 | 0d9bcfe4-f5a0-46e1-a5cf-9dd4def54493 ==============================
[0m12:02:34.604833 [info ] [MainThread]: Running with dbt=1.7.4
[0m12:02:34.606844 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m12:02:34.620353 [error] [MainThread]: Encountered an error:
Runtime Error
  
  dbt encountered an error while trying to read your profiles.yml file.
  
  Runtime Error
    Syntax error near line 18
    ------------------------------
    15 |       type: duckdb  
    16 | fabric-spark-test:
    17 |   target: fabricspark-dev
    18 |     fabricspark-dev:
    19 |         authentication: CLI
    20 |         method: livy
    21 |         connect_retries: 0
    
    Raw Error:
    ------------------------------
    mapping values are not allowed in this context
      in "<unicode string>", line 18, column 20
  
[0m12:02:34.623861 [debug] [MainThread]: Command `cli run` failed at 12:02:34.623861 after 0.04 seconds
[0m12:02:34.624878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020644A13590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020644A44380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020644A44260>]}
[0m12:02:34.626889 [debug] [MainThread]: Flushing usage events
[0m12:04:45.635241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6DA603D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6DA603470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6DA6034D0>]}


============================== 12:04:45.637253 | 65a5c88c-1d49-410e-b33d-3fe5e1ba971f ==============================
[0m12:04:45.637253 [info ] [MainThread]: Running with dbt=1.7.4
[0m12:04:45.639266 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:04:45.651354 [error] [MainThread]: Encountered an error:
Runtime Error
  
  dbt encountered an error while trying to read your profiles.yml file.
  
  Runtime Error
    Syntax error near line 19
    ------------------------------
    16 |         
    17 | fabric-spark-test:
    18 |   target: fabricspark-dev
    19 |     fabricspark-dev:
    20 |         authentication: CLI
    21 |         method: livy
    22 |         connect_retries: 0
    
    Raw Error:
    ------------------------------
    mapping values are not allowed in this context
      in "<unicode string>", line 19, column 20
  
[0m12:04:45.654648 [debug] [MainThread]: Command `cli run` failed at 12:04:45.654114 after 0.03 seconds
[0m12:04:45.655873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6DA603980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6DA601310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6DA624320>]}
[0m12:04:45.657900 [debug] [MainThread]: Flushing usage events
[0m12:05:56.636049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7A9B7D8E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7A9683650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7A9B7DAF0>]}


============================== 12:05:56.639064 | 0e693d74-f400-4456-b913-5cd9a739463b ==============================
[0m12:05:56.639064 [info ] [MainThread]: Running with dbt=1.7.4
[0m12:05:56.641078 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m12:05:56.949123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0e693d74-f400-4456-b913-5cd9a739463b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7A9CD6F90>]}
[0m12:05:57.110039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0e693d74-f400-4456-b913-5cd9a739463b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7A9D75BB0>]}
[0m12:05:57.119115 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m12:05:57.149426 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m12:05:57.151447 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m12:05:57.152451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0e693d74-f400-4456-b913-5cd9a739463b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7A9CD7620>]}
[0m12:06:00.078780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0e693d74-f400-4456-b913-5cd9a739463b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7AA148200>]}
[0m12:06:00.094898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0e693d74-f400-4456-b913-5cd9a739463b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7A9F14E60>]}
[0m12:06:00.096911 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m12:06:00.097916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e693d74-f400-4456-b913-5cd9a739463b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7A9DFC7D0>]}
[0m12:06:00.104006 [info ] [MainThread]: 
[0m12:06:00.107410 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m12:06:00.112789 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_schemas'
[0m12:06:00.138557 [debug] [ThreadPool]: Using fabricspark connection "list_schemas"
[0m12:06:00.140569 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:06:00.141573 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:06:00.143583 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Using CLI auth
[0m12:06:02.221797 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: CLI - Fetched Access Token
[0m12:06:02.222807 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6InEtMjNmYWxldlpoaEQzaG05Q1Fia1A1TVF5VSIsImtpZCI6InEtMjNmYWxldlpoaEQzaG05Q1Fia1A1TVF5VSJ9.eyJhdWQiOiJodHRwczovL2FuYWx5c2lzLndpbmRvd3MubmV0L3Bvd2VyYmkvYXBpIiwiaXNzIjoiaHR0cHM6Ly9zdHMud2luZG93cy5uZXQvNmM2Mzc1MTItYzQxNy00ZTc4LTlkNjItYjYxMjU4ZTRiNjE5LyIsImlhdCI6MTcxNDAxNzY2MSwibmJmIjoxNzE0MDE3NjYxLCJleHAiOjE3MTQwMjMyOTQsImFjY3QiOjAsImFjciI6IjEiLCJhaW8iOiJBVlFBcS84V0FBQUF5NUxIeHFnZlhrVlgzQlYvRUYvaEhvZVRrNXBBVlBJK21QZitaa3NvWUh3Q3VoVVBXR3NGRlAxcWNzN1daZllwSGcxZWJFYTVpUUl6KzV1eGNhSStNQk9IM2s5U2hudzd5dU1peFJ5a0ZDYz0iLCJhbXIiOlsicHdkIiwicnNhIiwibWZhIl0sImFwcGlkIjoiMDRiMDc3OTUtOGRkYi00NjFhLWJiZWUtMDJmOWUxYmY3YjQ2IiwiYXBwaWRhY3IiOiIwIiwiZGV2aWNlaWQiOiIwNTQzZDg2ZS05ZTUyLTQ1YTMtODg0My1iMGMzM2I2OWYxNDIiLCJmYW1pbHlfbmFtZSI6IlJhbXBvbm8iLCJnaXZlbl9uYW1lIjoiSm9obiIsImlwYWRkciI6IjE4MC4xNTAuODEuNDYiLCJuYW1lIjoiUmFtcG9ubywgSm9obiIsIm9pZCI6IjE2Mzc1OTBjLWUxNjctNGMzMy05OTU3LTg1ZjZiOWE5OTI3NyIsIm9ucHJlbV9zaWQiOiJTLTEtNS0yMS0xMzQ4MDg0MzM5LTEyNzc0ODA0NS05Mjk3MDEwMDAtMzY3NDc5IiwicHVpZCI6IjEwMDMyMDAzNTk3NEE4MEEiLCJyaCI6IjAuQVNjQUVuVmpiQmZFZUU2ZFlyWVNXT1MyR1FrQUFBQUFBQUFBd0FBQUFBQUFBQUFuQU93LiIsInNjcCI6InVzZXJfaW1wZXJzb25hdGlvbiIsInNpZ25pbl9zdGF0ZSI6WyJrbXNpIl0sInN1YiI6Ik1xTzZyOXdyb3NVLWxjQmRYUGN0ZDhhcXBCT1AxWkY4THNQLW1zNGtHRWMiLCJ0aWQiOiI2YzYzNzUxMi1jNDE3LTRlNzgtOWQ2Mi1iNjEyNThlNGI2MTkiLCJ1bmlxdWVfbmFtZSI6IkpvaG4uUmFtcG9ub0BpbnNpZ2h0LmNvbSIsInVwbiI6IkpvaG4uUmFtcG9ub0BpbnNpZ2h0LmNvbSIsInV0aSI6Ilg1VEJ2dElQLVV1R19MaFdHUFBIQUEiLCJ2ZXIiOiIxLjAiLCJ3aWRzIjpbImI3OWZiZjRkLTNlZjktNDY4OS04MTQzLTc2YjE5NGU4NTUwOSJdLCJ4bXNfY2MiOlsiQ1AxIl19.gZ3kXXbxh4VNg8pa2PMZi2yWTt-3Fq8rHJYhBwtR3a8I2VrG82iMZkyAu8Zi_hHo7axj_U41irMMQ7IvRvOTHhGf7hAyGn3mZ8uv_PATQ70ASr6RnIOlavJ-BOoZmO2I5UU01Wg1iDmcixp4DSgDEacTzKqTTz_ow2m6ilWJGJJBtNDrjyej1QJTxWvw4VegUXjFpxqP74EIpKxtvjC8e7qEU4QVCPGGt4MSIi2ESj_06LkUOj507AGf1oml437S9W-zmzZ_L5Gt3bkRbAZ3z7wpPBEfFlWehgldbujd9qWsFqqN6ySLidbAVLGXXBisiJbepYVEfwyLY8pCJvif6Q
[0m12:06:04.037427 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Connection error: 'id'
[0m12:06:04.041474 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:06:04.043496 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cursor'
[0m12:06:04.045514 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_schemas
[0m12:06:04.047526 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  'NoneType' object has no attribute 'cursor'
[0m12:06:04.051549 [error] [MainThread]: Encountered an error:
can only concatenate str (not "NoneType") to str
[0m12:06:04.075933 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricspark\connections.py", line 80, in exception_handler
    yield
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricspark\connections.py", line 274, in add_query
    cursor = connection.handle.cursor()
             ^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'cursor'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricspark\connections.py", line 80, in exception_handler
    yield
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 1112, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 20, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 28, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 310, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\sql\connections.py", line 138, in execute
    _, cursor = self.add_query(sql, auto_begin)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricspark\connections.py", line 265, in add_query
    with self.exception_handler(sql):
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricspark\connections.py", line 93, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc))
dbt.exceptions.DbtRuntimeError: Runtime Error
  'NoneType' object has no attribute 'cursor'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 434, in execute_with_hooks
    self.before_run(adapter, selected_uids)
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\run.py", line 446, in before_run
    self.create_schemas(adapter, required_schemas)
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 573, in create_schemas
    existing_schemas_lowered.update(ls_future.result())
                                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\utils.py", line 471, in connected
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 550, in list_schemas
    for s in adapter.list_schemas(database_quoted)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\sql\impl.py", line 210, in list_schemas
    results = self.execute_macro(LIST_SCHEMAS_MACRO_NAME, kwargs={"database": database})
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 1111, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricspark\connections.py", line 93, in exception_handler
    raise dbt.exceptions.DbtRuntimeError(str(exc))
dbt.exceptions.DbtRuntimeError: Runtime Error
  Runtime Error
    'NoneType' object has no attribute 'cursor'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 90, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 75, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 197, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 244, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 284, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\main.py", line 625, in run
    results = task.run()
              ^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 474, in run
    result = self.execute_with_hooks(selected_uids)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 438, in execute_with_hooks
    adapter.cleanup_connections()
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 261, in cleanup_connections
    self.connections.cleanup_all()
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricspark\connections.py", line 203, in cleanup_all
    livySession.disconnect()
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricspark\livysession.py", line 489, in disconnect
    if __class__.livy_global_session.is_valid_session():
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricspark\livysession.py", line 195, in is_valid_session
    self.connect_url + "/sessions/" + self.session_id,
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~
TypeError: can only concatenate str (not "NoneType") to str

[0m12:06:04.087013 [debug] [MainThread]: Command `cli run` failed at 12:06:04.085993 after 7.54 seconds
[0m12:06:04.089030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7AA149160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7A9F49DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B7AA0B28D0>]}
[0m12:06:04.090067 [debug] [MainThread]: Flushing usage events
[0m12:08:33.732874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C01E8668D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C01E866660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C01E8675F0>]}


============================== 12:08:33.734924 | fa487f0a-4484-45f6-9127-e89834ec4416 ==============================
[0m12:08:33.734924 [info ] [MainThread]: Running with dbt=1.7.4
[0m12:08:33.736935 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:08:34.047626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fa487f0a-4484-45f6-9127-e89834ec4416', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C01E449940>]}
[0m12:08:34.208784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fa487f0a-4484-45f6-9127-e89834ec4416', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C01E8654C0>]}
[0m12:08:34.211917 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m12:08:34.238135 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m12:08:34.788583 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:08:34.790593 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:08:34.806696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fa487f0a-4484-45f6-9127-e89834ec4416', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C01EAED700>]}
[0m12:08:34.820410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fa487f0a-4484-45f6-9127-e89834ec4416', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C01EB49040>]}
[0m12:08:34.822434 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m12:08:34.823446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fa487f0a-4484-45f6-9127-e89834ec4416', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C01EB48740>]}
[0m12:08:34.829608 [info ] [MainThread]: 
[0m12:08:34.831630 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m12:08:34.836670 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_schemas'
[0m12:08:34.862782 [debug] [ThreadPool]: Using fabricspark connection "list_schemas"
[0m12:08:34.864293 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:08:34.865456 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:08:34.867492 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_schemas"} */

    show databases
  
[0m12:08:34.869532 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'LazyHandle' object has no attribute 'cursor'
[0m12:08:34.871561 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_schemas
[0m12:08:34.872574 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  'LazyHandle' object has no attribute 'cursor'
[0m12:08:34.876851 [info ] [MainThread]: 
[0m12:08:34.879306 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.05 seconds (0.05s).
[0m12:08:34.880311 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    'LazyHandle' object has no attribute 'cursor'
[0m12:08:34.883352 [debug] [MainThread]: Command `cli run` failed at 12:08:34.883352 after 1.25 seconds
[0m12:08:34.885395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C01E864290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C01EB4BB30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C01EB4BB90>]}
[0m12:08:34.886407 [debug] [MainThread]: Flushing usage events
[0m12:11:39.370490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002097AA25430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002097AA249E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002097AA25340>]}


============================== 12:11:39.371496 | 9472cd5d-c77d-4699-a832-04b4b005d00d ==============================
[0m12:11:39.371496 [info ] [MainThread]: Running with dbt=1.7.4
[0m12:11:39.371496 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m12:11:39.472343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9472cd5d-c77d-4699-a832-04b4b005d00d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002097AA69B20>]}
[0m12:11:39.521321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9472cd5d-c77d-4699-a832-04b4b005d00d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002097ABE4E90>]}
[0m12:11:39.522326 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m12:11:39.527365 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m12:11:39.561242 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:11:39.561242 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:11:39.565454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9472cd5d-c77d-4699-a832-04b4b005d00d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002097BCBF500>]}
[0m12:11:39.570592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9472cd5d-c77d-4699-a832-04b4b005d00d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002097BC21AF0>]}
[0m12:11:39.571598 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m12:11:39.571598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9472cd5d-c77d-4699-a832-04b4b005d00d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002097AB3B5F0>]}
[0m12:11:39.572605 [info ] [MainThread]: 
[0m12:11:39.573612 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m12:11:39.574618 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_schemas'
[0m12:11:39.579758 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_schemas
[0m12:11:39.579758 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'description'
[0m12:11:39.580764 [info ] [MainThread]: 
[0m12:11:39.580764 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.01 seconds (0.01s).
[0m12:11:39.581770 [error] [MainThread]: Encountered an error:
Runtime Error
  'NoneType' object has no attribute 'description'
[0m12:11:39.582776 [debug] [MainThread]: Command `cli run` failed at 12:11:39.581770 after 0.23 seconds
[0m12:11:39.582776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002097AA245C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002097AB10770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002097AC30890>]}
[0m12:11:39.582776 [debug] [MainThread]: Flushing usage events
[0m12:12:18.522180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA78B844A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA7B2592B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA7BD860C0>]}


============================== 12:12:18.522180 | 8d00ed3f-31b1-47b2-9cab-0e3c5a0f3656 ==============================
[0m12:12:18.522180 [info ] [MainThread]: Running with dbt=1.7.4
[0m12:12:18.523186 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:12:18.616684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8d00ed3f-31b1-47b2-9cab-0e3c5a0f3656', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA7CE40740>]}
[0m12:12:18.666120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8d00ed3f-31b1-47b2-9cab-0e3c5a0f3656', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA7CF15BB0>]}
[0m12:12:18.667125 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m12:12:18.671167 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m12:12:18.707689 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:12:18.707689 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:12:18.710706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8d00ed3f-31b1-47b2-9cab-0e3c5a0f3656', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA7BD16720>]}
[0m12:12:18.716742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8d00ed3f-31b1-47b2-9cab-0e3c5a0f3656', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA7CF85F70>]}
[0m12:12:18.716742 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m12:12:18.717748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8d00ed3f-31b1-47b2-9cab-0e3c5a0f3656', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA7BD87380>]}
[0m12:12:18.718818 [info ] [MainThread]: 
[0m12:12:18.718818 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m12:12:18.719820 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m12:12:18.725870 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:12:18.725870 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'description'
[0m12:12:18.726875 [debug] [ThreadPool]: fabricspark adapter: Error while retrieving information about test: 'NoneType' object has no attribute 'description'
[0m12:12:18.726875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8d00ed3f-31b1-47b2-9cab-0e3c5a0f3656', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA7CFEA8A0>]}
[0m12:12:18.727881 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-dev')
[0m12:12:18.727881 [info ] [MainThread]: 
[0m12:12:18.729892 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m12:12:18.730898 [debug] [Thread-1 (]: Acquiring new fabricspark connection 'model.testproj.my_first_dbt_model'
[0m12:12:18.730898 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m12:12:18.734918 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m12:12:18.735924 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 12:12:18.730898 => 12:12:18.735924
[0m12:12:18.735924 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m12:12:18.735924 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 12:12:18.735924 => 12:12:18.735924
[0m12:12:18.736930 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m12:12:18.737941 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m12:12:18.737941 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m12:12:18.737941 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m12:12:18.740020 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m12:12:18.741026 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 12:12:18.739018 => 12:12:18.740020
[0m12:12:18.741026 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m12:12:18.741026 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 12:12:18.741026 => 12:12:18.741026
[0m12:12:18.742031 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m12:12:18.742031 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m12:12:18.743037 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m12:12:18.743037 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m12:12:18.750081 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m12:12:18.751086 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 12:12:18.743037 => 12:12:18.750081
[0m12:12:18.751086 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m12:12:18.751086 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 12:12:18.751086 => 12:12:18.751086
[0m12:12:18.752091 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m12:12:18.752091 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m12:12:18.753118 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m12:12:18.753118 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m12:12:18.757137 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m12:12:18.757137 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 12:12:18.753118 => 12:12:18.757137
[0m12:12:18.758141 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m12:12:18.758141 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 12:12:18.758141 => 12:12:18.758141
[0m12:12:18.758141 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m12:12:18.759266 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m12:12:18.759266 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m12:12:18.759266 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m12:12:18.762293 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m12:12:18.763306 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 12:12:18.760270 => 12:12:18.763306
[0m12:12:18.763306 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m12:12:18.763306 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 12:12:18.763306 => 12:12:18.763306
[0m12:12:18.764314 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m12:12:18.764314 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m12:12:18.765322 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m12:12:18.765322 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m12:12:18.767338 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m12:12:18.768358 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 12:12:18.765322 => 12:12:18.768358
[0m12:12:18.768358 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m12:12:18.769363 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 12:12:18.768358 => 12:12:18.768358
[0m12:12:18.769363 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m12:12:18.770368 [debug] [MainThread]: Command end result
[0m12:12:18.776417 [debug] [MainThread]: Command `cli compile` succeeded at 12:12:18.776417 after 0.28 seconds
[0m12:12:18.776417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA7B5A0890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA78B844A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EA7BD4B8F0>]}
[0m12:12:18.776417 [debug] [MainThread]: Flushing usage events
[0m12:55:47.561594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014DE0194B60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014DE01962A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014DE0194EF0>]}


============================== 12:55:47.562606 | 4dce256e-f6e3-4e42-a929-3a2e9bd1b038 ==============================
[0m12:55:47.562606 [info ] [MainThread]: Running with dbt=1.7.4
[0m12:55:47.562606 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m12:55:47.657012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4dce256e-f6e3-4e42-a929-3a2e9bd1b038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014DE11AAF00>]}
[0m12:55:47.706662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4dce256e-f6e3-4e42-a929-3a2e9bd1b038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014DE11D6F30>]}
[0m12:55:47.707674 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m12:55:47.718939 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m12:55:48.223403 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:55:48.223403 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:55:48.228456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4dce256e-f6e3-4e42-a929-3a2e9bd1b038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014DE11A8920>]}
[0m12:55:48.241600 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4dce256e-f6e3-4e42-a929-3a2e9bd1b038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014DE1395F10>]}
[0m12:55:48.242612 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m12:55:48.242612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4dce256e-f6e3-4e42-a929-3a2e9bd1b038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014DE1254E00>]}
[0m12:55:48.244643 [info ] [MainThread]: 
[0m12:55:48.244643 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m12:55:48.245654 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m12:55:48.251705 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:55:48.251705 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: [Errno 2] No such file or directory: 'catalog.json'
[0m12:55:48.252716 [debug] [ThreadPool]: fabricspark adapter: Error while retrieving information about test: [Errno 2] No such file or directory: 'catalog.json'
[0m12:55:48.252716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4dce256e-f6e3-4e42-a929-3a2e9bd1b038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014DE13FA3C0>]}
[0m12:55:48.253727 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-dev')
[0m12:55:48.253727 [info ] [MainThread]: 
[0m12:55:48.255754 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m12:55:48.256781 [debug] [Thread-1 (]: Acquiring new fabricspark connection 'model.testproj.my_first_dbt_model'
[0m12:55:48.256781 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m12:55:48.260810 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m12:55:48.260810 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 12:55:48.256781 => 12:55:48.260810
[0m12:55:48.261816 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m12:55:48.261816 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 12:55:48.261816 => 12:55:48.261816
[0m12:55:48.262824 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m12:55:48.262824 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m12:55:48.263833 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m12:55:48.263833 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m12:55:48.264848 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m12:55:48.265864 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 12:55:48.263833 => 12:55:48.265864
[0m12:55:48.265864 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m12:55:48.266880 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 12:55:48.266880 => 12:55:48.266880
[0m12:55:48.266880 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m12:55:48.267893 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m12:55:48.267893 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m12:55:48.268901 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m12:55:48.274952 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m12:55:48.275958 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 12:55:48.268901 => 12:55:48.275958
[0m12:55:48.275958 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m12:55:48.275958 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 12:55:48.275958 => 12:55:48.275958
[0m12:55:48.276962 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m12:55:48.276962 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m12:55:48.276962 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m12:55:48.278381 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m12:55:48.281402 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m12:55:48.282408 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 12:55:48.278381 => 12:55:48.282408
[0m12:55:48.282408 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m12:55:48.282408 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 12:55:48.282408 => 12:55:48.282408
[0m12:55:48.283422 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m12:55:48.283422 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m12:55:48.284429 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m12:55:48.284429 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m12:55:48.286444 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m12:55:48.287453 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 12:55:48.284429 => 12:55:48.287453
[0m12:55:48.287453 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m12:55:48.288458 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 12:55:48.288458 => 12:55:48.288458
[0m12:55:48.288458 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m12:55:48.289464 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m12:55:48.289464 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m12:55:48.289464 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m12:55:48.292480 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m12:55:48.292480 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 12:55:48.289464 => 12:55:48.292480
[0m12:55:48.292480 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m12:55:48.293487 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 12:55:48.293487 => 12:55:48.293487
[0m12:55:48.293487 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m12:55:48.294497 [debug] [MainThread]: Command end result
[0m12:55:48.299530 [debug] [MainThread]: Command `cli compile` succeeded at 12:55:48.299530 after 0.77 seconds
[0m12:55:48.300544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014DDF6D63C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014DDFFE7A40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014DE142F710>]}
[0m12:55:48.300544 [debug] [MainThread]: Flushing usage events
[0m12:59:04.812993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0BD3A5670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0BD3A5610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0BD3A55B0>]}


============================== 12:59:04.814001 | 9b262e34-76a4-4c6f-88b5-1cff9cc4cad0 ==============================
[0m12:59:04.814001 [info ] [MainThread]: Running with dbt=1.7.4
[0m12:59:04.814001 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m12:59:04.913934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9b262e34-76a4-4c6f-88b5-1cff9cc4cad0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0BE487770>]}
[0m12:59:04.964169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9b262e34-76a4-4c6f-88b5-1cff9cc4cad0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0BC6356A0>]}
[0m12:59:04.965174 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m12:59:04.971230 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m12:59:05.012524 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:59:05.012524 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:59:05.016557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9b262e34-76a4-4c6f-88b5-1cff9cc4cad0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0BE4641A0>]}
[0m12:59:05.021603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9b262e34-76a4-4c6f-88b5-1cff9cc4cad0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0BE5A5EB0>]}
[0m12:59:05.022609 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m12:59:05.022609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b262e34-76a4-4c6f-88b5-1cff9cc4cad0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0BE486E10>]}
[0m12:59:05.023624 [info ] [MainThread]: 
[0m12:59:05.024636 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m12:59:05.025643 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m12:59:05.030690 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m12:59:05.031698 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'RuntimeConfig' object has no attribute 'project_dir'
[0m12:59:05.031698 [debug] [ThreadPool]: fabricspark adapter: Error while retrieving information about test: 'RuntimeConfig' object has no attribute 'project_dir'
[0m12:59:05.031698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b262e34-76a4-4c6f-88b5-1cff9cc4cad0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0BE60A390>]}
[0m12:59:05.032709 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-dev')
[0m12:59:05.032709 [info ] [MainThread]: 
[0m12:59:05.034724 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m12:59:05.035731 [debug] [Thread-1 (]: Acquiring new fabricspark connection 'model.testproj.my_first_dbt_model'
[0m12:59:05.035731 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m12:59:05.039772 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m12:59:05.039772 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 12:59:05.035731 => 12:59:05.039772
[0m12:59:05.040777 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m12:59:05.040777 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 12:59:05.040777 => 12:59:05.040777
[0m12:59:05.041783 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m12:59:05.041783 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m12:59:05.042790 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m12:59:05.042790 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m12:59:05.044810 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m12:59:05.044810 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 12:59:05.042790 => 12:59:05.044810
[0m12:59:05.045816 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m12:59:05.045816 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 12:59:05.045816 => 12:59:05.045816
[0m12:59:05.046823 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m12:59:05.046823 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m12:59:05.046823 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m12:59:05.047829 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m12:59:05.054908 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m12:59:05.054908 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 12:59:05.047829 => 12:59:05.054908
[0m12:59:05.055918 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m12:59:05.055918 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 12:59:05.055918 => 12:59:05.055918
[0m12:59:05.055918 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m12:59:05.056928 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m12:59:05.056928 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m12:59:05.056928 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m12:59:05.060969 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m12:59:05.061974 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 12:59:05.057939 => 12:59:05.061974
[0m12:59:05.061974 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m12:59:05.061974 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 12:59:05.061974 => 12:59:05.061974
[0m12:59:05.062980 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m12:59:05.063486 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m12:59:05.063993 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m12:59:05.063993 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m12:59:05.066007 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m12:59:05.067011 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 12:59:05.063993 => 12:59:05.067011
[0m12:59:05.067011 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m12:59:05.067011 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 12:59:05.067011 => 12:59:05.067011
[0m12:59:05.068015 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m12:59:05.069034 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m12:59:05.069034 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m12:59:05.069034 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m12:59:05.072057 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m12:59:05.072057 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 12:59:05.069034 => 12:59:05.072057
[0m12:59:05.072057 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m12:59:05.073063 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 12:59:05.073063 => 12:59:05.073063
[0m12:59:05.073063 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m12:59:05.074068 [debug] [MainThread]: Command end result
[0m12:59:05.079099 [debug] [MainThread]: Command `cli compile` succeeded at 12:59:05.079099 after 0.29 seconds
[0m12:59:05.080105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0BC7D63C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0BD276BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0BC6D8A10>]}
[0m12:59:05.080105 [debug] [MainThread]: Flushing usage events
[0m13:00:23.970267 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3E9C392B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3E9DE4EF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3E9DE51F0>]}


============================== 13:00:23.971283 | 34135f47-9905-4eed-b86f-b280c36bbfb9 ==============================
[0m13:00:23.971283 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:00:23.971283 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:00:24.065965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '34135f47-9905-4eed-b86f-b280c36bbfb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3E9DE6FF0>]}
[0m13:00:24.115344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '34135f47-9905-4eed-b86f-b280c36bbfb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3E9EFB500>]}
[0m13:00:24.116353 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:00:24.121386 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:00:24.157454 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:00:24.157454 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:00:24.161482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '34135f47-9905-4eed-b86f-b280c36bbfb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3EB07F500>]}
[0m13:00:24.167514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '34135f47-9905-4eed-b86f-b280c36bbfb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3EAFE5C10>]}
[0m13:00:24.167514 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:00:24.167514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '34135f47-9905-4eed-b86f-b280c36bbfb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3E92FD340>]}
[0m13:00:24.168521 [info ] [MainThread]: 
[0m13:00:24.169526 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:00:24.170560 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:00:24.213841 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m13:00:24.213841 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'Table' object has no attribute 'description'
[0m13:00:24.213841 [debug] [ThreadPool]: fabricspark adapter: Error while retrieving information about test: 'Table' object has no attribute 'description'
[0m13:00:24.214847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '34135f47-9905-4eed-b86f-b280c36bbfb9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3EB056300>]}
[0m13:00:24.214847 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-dev')
[0m13:00:24.215856 [info ] [MainThread]: 
[0m13:00:24.217885 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m13:00:24.218895 [debug] [Thread-1 (]: Acquiring new fabricspark connection 'model.testproj.my_first_dbt_model'
[0m13:00:24.218895 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:00:24.222917 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:00:24.222917 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:00:24.218895 => 13:00:24.222917
[0m13:00:24.223921 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:00:24.223921 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 13:00:24.223921 => 13:00:24.223921
[0m13:00:24.224927 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m13:00:24.224927 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m13:00:24.225933 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m13:00:24.225933 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m13:00:24.227946 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m13:00:24.227946 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 13:00:24.225933 => 13:00:24.227946
[0m13:00:24.228954 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m13:00:24.228954 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 13:00:24.228954 => 13:00:24.228954
[0m13:00:24.228954 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m13:00:24.229962 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m13:00:24.229962 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m13:00:24.230967 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m13:00:24.237008 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m13:00:24.238013 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 13:00:24.230967 => 13:00:24.238013
[0m13:00:24.238013 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m13:00:24.238013 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 13:00:24.238013 => 13:00:24.238013
[0m13:00:24.239018 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m13:00:24.239018 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m13:00:24.240024 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m13:00:24.240024 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m13:00:24.244044 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m13:00:24.245051 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 13:00:24.240024 => 13:00:24.244044
[0m13:00:24.245051 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m13:00:24.245051 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 13:00:24.245051 => 13:00:24.245051
[0m13:00:24.246059 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m13:00:24.246059 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m13:00:24.246059 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m13:00:24.247064 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m13:00:24.249076 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m13:00:24.250096 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 13:00:24.247064 => 13:00:24.249076
[0m13:00:24.250096 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m13:00:24.250096 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 13:00:24.250096 => 13:00:24.250096
[0m13:00:24.251102 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m13:00:24.251102 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m13:00:24.251102 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m13:00:24.252107 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m13:00:24.254118 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m13:00:24.255123 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 13:00:24.252107 => 13:00:24.255123
[0m13:00:24.255123 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m13:00:24.256129 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 13:00:24.256129 => 13:00:24.256129
[0m13:00:24.256129 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m13:00:24.257135 [debug] [MainThread]: Command end result
[0m13:00:24.262170 [debug] [MainThread]: Command `cli compile` succeeded at 13:00:24.262170 after 0.31 seconds
[0m13:00:24.262170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3E9B24A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3E9C04D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3EB1C8050>]}
[0m13:00:24.263177 [debug] [MainThread]: Flushing usage events
[0m13:06:11.129586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002247A7F4BF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002247A7F52E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002247A7F41D0>]}


============================== 13:06:11.132602 | e93b5ef0-dc2d-4914-9f97-e19d43e74c2d ==============================
[0m13:06:11.132602 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:06:11.134614 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:06:11.450460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e93b5ef0-dc2d-4914-9f97-e19d43e74c2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002247A94B800>]}
[0m13:06:11.614466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e93b5ef0-dc2d-4914-9f97-e19d43e74c2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002247AA444D0>]}
[0m13:06:11.617489 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:06:11.637171 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:06:11.730376 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:06:11.731382 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:06:11.749574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e93b5ef0-dc2d-4914-9f97-e19d43e74c2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002247A999BE0>]}
[0m13:06:11.763455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e93b5ef0-dc2d-4914-9f97-e19d43e74c2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002247AAC4DA0>]}
[0m13:06:11.764461 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:06:11.766476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e93b5ef0-dc2d-4914-9f97-e19d43e74c2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002247A567A40>]}
[0m13:06:11.772523 [info ] [MainThread]: 
[0m13:06:11.775556 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:06:11.780604 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:07:25.430316 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m13:07:25.431322 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'Table' object has no attribute 'description'
[0m13:09:24.183075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EC8243560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EC8242990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EC8242240>]}


============================== 13:09:24.186092 | 2bb4b478-5f4e-41fa-b907-7cd2058f1d69 ==============================
[0m13:09:24.186092 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:09:24.187108 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:09:24.497966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2bb4b478-5f4e-41fa-b907-7cd2058f1d69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EC723F1A0>]}
[0m13:09:24.663360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2bb4b478-5f4e-41fa-b907-7cd2058f1d69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EC84671A0>]}
[0m13:09:24.665714 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:09:24.690878 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:09:25.250560 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:09:25.251565 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:09:25.268972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2bb4b478-5f4e-41fa-b907-7cd2058f1d69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EC82BF260>]}
[0m13:09:25.282622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2bb4b478-5f4e-41fa-b907-7cd2058f1d69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EC8570B00>]}
[0m13:09:25.283627 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:09:25.285640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2bb4b478-5f4e-41fa-b907-7cd2058f1d69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016EC7CB77A0>]}
[0m13:09:25.290667 [info ] [MainThread]: 
[0m13:09:25.293684 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:09:25.299742 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:09:27.863738 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m13:09:27.865379 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'Table' object has no attribute 'description'
[0m13:10:55.169963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A125C52570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A125C51490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A125C528D0>]}


============================== 13:10:55.173007 | f310fca6-0d3e-47f9-8a5b-3cfadca4f2fc ==============================
[0m13:10:55.173007 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:10:55.175021 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m13:10:55.485180 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f310fca6-0d3e-47f9-8a5b-3cfadca4f2fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A125D6B0E0>]}
[0m13:10:55.647630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f310fca6-0d3e-47f9-8a5b-3cfadca4f2fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A125E6F6E0>]}
[0m13:10:55.649658 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:10:55.670816 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:10:55.762217 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:10:55.763222 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:10:55.780608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f310fca6-0d3e-47f9-8a5b-3cfadca4f2fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A125EA52B0>]}
[0m13:10:55.794160 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f310fca6-0d3e-47f9-8a5b-3cfadca4f2fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A125F74D10>]}
[0m13:10:55.795167 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:10:55.797175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f310fca6-0d3e-47f9-8a5b-3cfadca4f2fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A125DABD40>]}
[0m13:10:55.803616 [info ] [MainThread]: 
[0m13:10:55.806650 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:10:55.810690 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:10:57.984243 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m13:10:57.986258 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'Table' object has no attribute 'description'
[0m13:20:45.110469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C77901490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C77902630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C77900740>]}


============================== 13:20:45.113212 | e2c1fc8c-6b95-45bb-9758-a9838d219b03 ==============================
[0m13:20:45.113212 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:20:45.115252 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m13:20:45.449239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e2c1fc8c-6b95-45bb-9758-a9838d219b03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C77A5B110>]}
[0m13:20:45.617928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e2c1fc8c-6b95-45bb-9758-a9838d219b03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C77A0DEE0>]}
[0m13:20:45.619950 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:20:45.640228 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:20:45.725932 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:20:45.726938 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:20:45.743699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e2c1fc8c-6b95-45bb-9758-a9838d219b03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C77AA86B0>]}
[0m13:20:45.759133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e2c1fc8c-6b95-45bb-9758-a9838d219b03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C77BD4170>]}
[0m13:20:45.761147 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:20:45.763401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e2c1fc8c-6b95-45bb-9758-a9838d219b03', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C77A5ABD0>]}
[0m13:20:45.768469 [info ] [MainThread]: 
[0m13:20:45.771490 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:20:45.775519 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:20:48.936646 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:20:48.937653 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m13:20:48.938663 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:20:48.940719 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:20:48.941728 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:20:48.942734 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'LazyHandle' object has no attribute 'cursor'
[0m13:20:48.944744 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m13:20:48.946108 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  'LazyHandle' object has no attribute 'cursor'
[0m13:22:41.546639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D7F91FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D7F91CA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D7F92C00>]}


============================== 13:22:41.548655 | dc1d2c88-7731-432d-9f7b-112fc3495172 ==============================
[0m13:22:41.548655 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:22:41.550680 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:22:41.869445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dc1d2c88-7731-432d-9f7b-112fc3495172', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D6F8F050>]}
[0m13:22:42.041596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dc1d2c88-7731-432d-9f7b-112fc3495172', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D7FDBC80>]}
[0m13:22:42.043606 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:22:42.063815 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:22:42.150538 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:22:42.152563 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:22:42.170726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dc1d2c88-7731-432d-9f7b-112fc3495172', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D81E58E0>]}
[0m13:22:42.184055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dc1d2c88-7731-432d-9f7b-112fc3495172', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D82B8E60>]}
[0m13:22:42.185573 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:22:42.187317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dc1d2c88-7731-432d-9f7b-112fc3495172', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D82B83E0>]}
[0m13:22:42.192457 [info ] [MainThread]: 
[0m13:22:42.195483 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:22:42.199522 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:22:48.334919 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:23:08.200387 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:23:08.202415 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m13:23:16.652794 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:23:18.349366 [debug] [ThreadPool]: Opening a new connection, currently in state open
[0m13:23:22.103518 [debug] [ThreadPool]: Opening a new connection, currently in state open
[0m13:23:23.164705 [debug] [ThreadPool]: Opening a new connection, currently in state open
[0m13:23:25.141257 [debug] [ThreadPool]: Opening a new connection, currently in state open
[0m13:23:49.317186 [debug] [ThreadPool]: Opening a new connection, currently in state open
[0m13:24:06.108432 [debug] [ThreadPool]: Opening a new connection, currently in state open
[0m13:25:02.062666 [debug] [ThreadPool]: Opening a new connection, currently in state open
[0m13:25:19.980478 [debug] [ThreadPool]: Opening a new connection, currently in state open
[0m13:25:24.267708 [debug] [ThreadPool]: Opening a new connection, currently in state open
[0m13:25:29.962707 [debug] [ThreadPool]: Opening a new connection, currently in state open
[0m13:25:30.793958 [debug] [ThreadPool]: Opening a new connection, currently in state open
[0m13:36:03.827453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001944FF311C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001944FF32E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001944FF32D20>]}


============================== 13:36:03.829817 | 36b3cf29-1103-4123-bdb6-5afdc0d09cbb ==============================
[0m13:36:03.829817 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:36:03.831827 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:36:04.145785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '36b3cf29-1103-4123-bdb6-5afdc0d09cbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001944F831B80>]}
[0m13:36:04.306729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '36b3cf29-1103-4123-bdb6-5afdc0d09cbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001944FE54B60>]}
[0m13:36:04.308739 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:36:04.334546 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:36:04.768165 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:36:04.769170 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:36:04.786264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '36b3cf29-1103-4123-bdb6-5afdc0d09cbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001944FF310A0>]}
[0m13:36:04.799357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '36b3cf29-1103-4123-bdb6-5afdc0d09cbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001945025CE00>]}
[0m13:36:04.800365 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:36:04.802380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '36b3cf29-1103-4123-bdb6-5afdc0d09cbb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001945025CA70>]}
[0m13:36:04.807415 [info ] [MainThread]: 
[0m13:36:04.810974 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:36:04.816152 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:36:07.714007 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:36:07.715889 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m13:36:40.238296 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:36:43.216539 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:37:08.708239 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Using CLI auth
[0m13:37:10.022533 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: CLI - Fetched Access Token
[0m13:37:10.024549 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6InEtMjNmYWxldlpoaEQzaG05Q1Fia1A1TVF5VSIsImtpZCI6InEtMjNmYWxldlpoaEQzaG05Q1Fia1A1TVF5VSJ9.eyJhdWQiOiJodHRwczovL2FuYWx5c2lzLndpbmRvd3MubmV0L3Bvd2VyYmkvYXBpIiwiaXNzIjoiaHR0cHM6Ly9zdHMud2luZG93cy5uZXQvNmM2Mzc1MTItYzQxNy00ZTc4LTlkNjItYjYxMjU4ZTRiNjE5LyIsImlhdCI6MTcxNDAyMzEyOSwibmJmIjoxNzE0MDIzMTI5LCJleHAiOjE3MTQwMjczNjUsImFjY3QiOjAsImFjciI6IjEiLCJhaW8iOiJBVlFBcS84V0FBQUFHcXVKWFVvb0FOdjd2emNVWDhYNkM5L1gwSFhtMEg0d0VyRWJ3aTk4bk1GVk9mMzN4a0dCR3pIejdtVlZYYjFlblc2eFBhSjA3di9ZbENvLzdhR0NPL0ZKKy9DS2RkUTdPb1F1UWkyVVRNRT0iLCJhbXIiOlsicHdkIiwicnNhIiwibWZhIl0sImFwcGlkIjoiMDRiMDc3OTUtOGRkYi00NjFhLWJiZWUtMDJmOWUxYmY3YjQ2IiwiYXBwaWRhY3IiOiIwIiwiZGV2aWNlaWQiOiIwNTQzZDg2ZS05ZTUyLTQ1YTMtODg0My1iMGMzM2I2OWYxNDIiLCJmYW1pbHlfbmFtZSI6IlJhbXBvbm8iLCJnaXZlbl9uYW1lIjoiSm9obiIsImlwYWRkciI6IjE4MC4xNTAuODEuNDYiLCJuYW1lIjoiUmFtcG9ubywgSm9obiIsIm9pZCI6IjE2Mzc1OTBjLWUxNjctNGMzMy05OTU3LTg1ZjZiOWE5OTI3NyIsIm9ucHJlbV9zaWQiOiJTLTEtNS0yMS0xMzQ4MDg0MzM5LTEyNzc0ODA0NS05Mjk3MDEwMDAtMzY3NDc5IiwicHVpZCI6IjEwMDMyMDAzNTk3NEE4MEEiLCJyaCI6IjAuQVNjQUVuVmpiQmZFZUU2ZFlyWVNXT1MyR1FrQUFBQUFBQUFBd0FBQUFBQUFBQUFuQU93LiIsInNjcCI6InVzZXJfaW1wZXJzb25hdGlvbiIsInNpZ25pbl9zdGF0ZSI6WyJrbXNpIl0sInN1YiI6Ik1xTzZyOXdyb3NVLWxjQmRYUGN0ZDhhcXBCT1AxWkY4THNQLW1zNGtHRWMiLCJ0aWQiOiI2YzYzNzUxMi1jNDE3LTRlNzgtOWQ2Mi1iNjEyNThlNGI2MTkiLCJ1bmlxdWVfbmFtZSI6IkpvaG4uUmFtcG9ub0BpbnNpZ2h0LmNvbSIsInVwbiI6IkpvaG4uUmFtcG9ub0BpbnNpZ2h0LmNvbSIsInV0aSI6InRzWE9HR2twZzB1czVrYjlWbGtiQVEiLCJ2ZXIiOiIxLjAiLCJ3aWRzIjpbImI3OWZiZjRkLTNlZjktNDY4OS04MTQzLTc2YjE5NGU4NTUwOSJdLCJ4bXNfY2MiOlsiQ1AxIl19.SVqAJ0UOj4PKE4JTDyAZ80PJk8ZIWL0gcZcD950SAmVsd2yEyq3Z17lKhjS9OPdNOx14ePRcF1cD76uR3Bde7rnj6aTImlW8BybIAPrabilnsfU14IpXzO1KE2TcUVwFlAo3gcgqRRW1feHjTJn6yZoh3FauXBZpfR53DBBbb5tm2U8u1VeIvbyEqMMAg1LziSXJ3hjEE0YeCtunjd85lXwh68zErdDFyEOIzdfyIi5jNxGPSS-m--DZqPPauagxPFh_0rLDJdnY7a0b5saUEWLVmIvXBHCx6s5xLt6LIdGjmYSQzsA-cuYN9uGSzY6naH6ubdcb9Jz51QPqc3DJcA
[0m13:37:11.679678 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Connection error: 'id'
[0m13:38:23.860763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025826AB3DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025826AB1250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025826AB2570>]}


============================== 13:38:23.863792 | 90ee31d9-d71b-4f9a-91a3-5c962c23f61c ==============================
[0m13:38:23.863792 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:38:23.864798 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:38:24.175541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '90ee31d9-d71b-4f9a-91a3-5c962c23f61c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025826B40350>]}
[0m13:38:24.337416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '90ee31d9-d71b-4f9a-91a3-5c962c23f61c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000258261EF230>]}
[0m13:38:24.339429 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:38:24.363387 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:38:24.917241 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:38:24.918248 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:38:24.935358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '90ee31d9-d71b-4f9a-91a3-5c962c23f61c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025826D2D940>]}
[0m13:38:24.948445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '90ee31d9-d71b-4f9a-91a3-5c962c23f61c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025826D88E00>]}
[0m13:38:24.950454 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:38:24.951460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '90ee31d9-d71b-4f9a-91a3-5c962c23f61c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025826D88560>]}
[0m13:38:24.957333 [info ] [MainThread]: 
[0m13:38:24.960262 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:38:24.965535 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:38:28.392907 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:38:28.393927 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m13:38:30.780210 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:38:34.433815 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:38:52.109626 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:38:52.110632 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cursor'
[0m13:38:52.114799 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m13:38:52.116865 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  'NoneType' object has no attribute 'cursor'
[0m13:43:46.342485 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249CBA71460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249CBA739B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249CBA72480>]}


============================== 13:43:46.345498 | 364b521b-7525-4e16-9490-4958a6d15d38 ==============================
[0m13:43:46.345498 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:43:46.347509 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m13:43:46.662022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '364b521b-7525-4e16-9490-4958a6d15d38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249CB673A70>]}
[0m13:43:46.823020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '364b521b-7525-4e16-9490-4958a6d15d38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249CBC7EDB0>]}
[0m13:43:46.825028 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:43:46.843668 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:43:46.932560 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:43:46.934579 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:43:46.952021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '364b521b-7525-4e16-9490-4958a6d15d38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249C8E18710>]}
[0m13:43:46.966701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '364b521b-7525-4e16-9490-4958a6d15d38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249CBD5CCB0>]}
[0m13:43:46.968717 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:43:46.969722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '364b521b-7525-4e16-9490-4958a6d15d38', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000249CBBDA0F0>]}
[0m13:43:46.975982 [info ] [MainThread]: 
[0m13:43:46.978133 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:43:46.983600 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:46:51.078716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002330D6512E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002330D652D50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002330D653290>]}


============================== 13:46:51.081733 | c3395116-55d7-4fd3-bef3-b8bf61921fee ==============================
[0m13:46:51.081733 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:46:51.083746 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:46:51.392483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c3395116-55d7-4fd3-bef3-b8bf61921fee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002330D7E8E90>]}
[0m13:46:51.570466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c3395116-55d7-4fd3-bef3-b8bf61921fee', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002330D6E3290>]}
[0m13:46:51.573485 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:46:51.575497 [error] [MainThread]: Encountered an error:
SparkConnectionManager() takes no arguments
[0m13:46:51.595556 [error] [MainThread]: Traceback (most recent call last):
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 90, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 75, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 197, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 244, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 267, in wrapper
    register_adapter(runtime_config)
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\factory.py", line 176, in register_adapter
    FACTORY.register_adapter(config)
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\factory.py", line 105, in register_adapter
    adapter: Adapter = adapter_type(config)  # type: ignore
                       ^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 248, in __init__
    self.connections = self.ConnectionManager(config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: SparkConnectionManager() takes no arguments

[0m13:46:51.600613 [debug] [MainThread]: Command `cli compile` failed at 13:46:51.599607 after 0.60 seconds
[0m13:46:51.601618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002330D3E58B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002330D85DC40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002330D7BBAD0>]}
[0m13:46:51.602623 [debug] [MainThread]: Flushing usage events
[0m13:50:09.009917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001502C9B2C90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001502C9B3170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001502C9B2CC0>]}


============================== 13:50:09.011928 | 90674639-80b4-4432-925b-50f740622668 ==============================
[0m13:50:09.011928 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:50:09.013940 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m13:50:09.333586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '90674639-80b4-4432-925b-50f740622668', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001502CA587D0>]}
[0m13:50:09.495009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '90674639-80b4-4432-925b-50f740622668', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001502CA08C20>]}
[0m13:50:09.497019 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:50:09.515692 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:50:09.603439 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:50:09.604444 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:50:09.621690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '90674639-80b4-4432-925b-50f740622668', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001502CA480E0>]}
[0m13:50:09.634835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '90674639-80b4-4432-925b-50f740622668', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001502CCECC80>]}
[0m13:50:09.636345 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:50:09.637864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '90674639-80b4-4432-925b-50f740622668', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001502CA09AC0>]}
[0m13:50:09.643485 [info ] [MainThread]: 
[0m13:50:09.646763 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:50:09.652076 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:50:15.091074 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:50:15.092585 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m13:50:16.205184 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:50:17.903518 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:52:24.080580 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:52:24.082596 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cursor'
[0m13:52:24.084616 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m13:52:24.086418 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  'NoneType' object has no attribute 'cursor'
[0m13:53:09.363686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020447065010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020447067620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020447067EC0>]}


============================== 13:53:09.366712 | bb0edbb4-75a2-456e-b40f-0bb722fbf150 ==============================
[0m13:53:09.366712 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:53:09.368675 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m13:53:09.679299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bb0edbb4-75a2-456e-b40f-0bb722fbf150', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204470FA9F0>]}
[0m13:53:09.839413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bb0edbb4-75a2-456e-b40f-0bb722fbf150', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002044727FA70>]}
[0m13:53:09.842435 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:53:09.862453 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:53:09.950820 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:53:09.951891 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:53:09.969316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bb0edbb4-75a2-456e-b40f-0bb722fbf150', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204472DC2F0>]}
[0m13:53:09.982527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bb0edbb4-75a2-456e-b40f-0bb722fbf150', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002044738CBC0>]}
[0m13:53:09.983540 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:53:09.985566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bb0edbb4-75a2-456e-b40f-0bb722fbf150', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000204466509E0>]}
[0m13:53:09.991622 [info ] [MainThread]: 
[0m13:53:09.994123 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:53:09.998468 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:53:13.364073 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:53:13.366096 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m13:53:16.239894 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:53:19.369296 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:56:04.114803 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E6D291970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E6D290860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E6D293B60>]}


============================== 13:56:04.116830 | 1ac0214f-a517-410e-9724-5178841edc9b ==============================
[0m13:56:04.116830 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:56:04.118844 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:56:04.445073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1ac0214f-a517-410e-9724-5178841edc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E6D290A40>]}
[0m13:56:04.612796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1ac0214f-a517-410e-9724-5178841edc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E6C998FB0>]}
[0m13:56:04.614807 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:56:04.639171 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:56:05.104713 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:56:05.105718 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:56:05.122814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1ac0214f-a517-410e-9724-5178841edc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E6D200BC0>]}
[0m13:56:05.137517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1ac0214f-a517-410e-9724-5178841edc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E6D574860>]}
[0m13:56:05.138292 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:56:05.140320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ac0214f-a517-410e-9724-5178841edc9b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016E6D574080>]}
[0m13:56:05.145948 [info ] [MainThread]: 
[0m13:56:05.149094 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:56:05.153120 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:56:32.514290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013199FF1430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013199FF1A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013199FF26C0>]}


============================== 13:56:32.517427 | 1ee6c26d-b319-41b8-9311-9fd5d977b471 ==============================
[0m13:56:32.517427 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:56:32.518433 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:56:32.831778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1ee6c26d-b319-41b8-9311-9fd5d977b471', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001319A084200>]}
[0m13:56:32.993754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1ee6c26d-b319-41b8-9311-9fd5d977b471', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001319A2217C0>]}
[0m13:56:32.996769 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:56:33.016243 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:56:33.101469 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:56:33.102474 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:56:33.119081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1ee6c26d-b319-41b8-9311-9fd5d977b471', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001319A1F0B90>]}
[0m13:56:33.132168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1ee6c26d-b319-41b8-9311-9fd5d977b471', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001319A2C8DA0>]}
[0m13:56:33.134189 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:56:33.135195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ee6c26d-b319-41b8-9311-9fd5d977b471', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001319A2C8500>]}
[0m13:56:33.141228 [info ] [MainThread]: 
[0m13:56:33.144290 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:56:33.148970 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:56:35.405872 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:56:35.407899 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m13:56:36.251076 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:56:38.674324 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:56:43.039565 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:56:43.040574 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cursor'
[0m13:56:43.042598 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m13:56:43.044102 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  'NoneType' object has no attribute 'cursor'
[0m13:58:15.283474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E064011400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E064012150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E064010B00>]}


============================== 13:58:15.286500 | c362608d-07e3-4d8b-88e3-56581f9a08cf ==============================
[0m13:58:15.286500 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:58:15.288036 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:58:15.601148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c362608d-07e3-4d8b-88e3-56581f9a08cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E0641A4D10>]}
[0m13:58:15.764224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c362608d-07e3-4d8b-88e3-56581f9a08cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E064249610>]}
[0m13:58:15.766234 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:58:15.784375 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:58:15.872421 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:58:15.873640 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:58:15.891162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c362608d-07e3-4d8b-88e3-56581f9a08cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E063FAA000>]}
[0m13:58:15.904254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c362608d-07e3-4d8b-88e3-56581f9a08cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E06434CD10>]}
[0m13:58:15.905904 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:58:15.907480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c362608d-07e3-4d8b-88e3-56581f9a08cf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E063F41A60>]}
[0m13:58:15.912511 [info ] [MainThread]: 
[0m13:58:15.915499 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:58:15.919038 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:58:18.377141 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:58:18.379176 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m13:58:19.042954 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:58:19.547263 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:58:24.248677 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:58:24.250708 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cursor'
[0m13:58:24.252739 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m13:58:24.254870 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  'NoneType' object has no attribute 'cursor'
[0m13:58:42.338763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239D7B48CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239D7B4A180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239D7B4AB70>]}


============================== 13:58:42.340793 | 686bebf2-7c1e-4667-bcb4-d1ca36ead8c9 ==============================
[0m13:58:42.340793 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:58:42.342806 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:58:42.669749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '686bebf2-7c1e-4667-bcb4-d1ca36ead8c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239D7ADB830>]}
[0m13:58:42.831220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '686bebf2-7c1e-4667-bcb4-d1ca36ead8c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239D7D73830>]}
[0m13:58:42.833241 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:58:42.853428 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:58:42.938466 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:58:42.939471 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:58:42.956687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '686bebf2-7c1e-4667-bcb4-d1ca36ead8c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239D7CF9610>]}
[0m13:58:42.969809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '686bebf2-7c1e-4667-bcb4-d1ca36ead8c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239D7E7C620>]}
[0m13:58:42.970815 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:58:42.972826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '686bebf2-7c1e-4667-bcb4-d1ca36ead8c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239D7DCFEF0>]}
[0m13:58:42.978220 [info ] [MainThread]: 
[0m13:58:42.981099 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:58:42.986249 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:59:24.611688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F23E7B4D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F23E7A2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F23E79A60>]}


============================== 13:59:24.614706 | 328d8908-c856-49ab-9e4e-4a2fa5c4a815 ==============================
[0m13:59:24.614706 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:59:24.616719 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:59:24.931071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '328d8908-c856-49ab-9e4e-4a2fa5c4a815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F234D2570>]}
[0m13:59:25.094089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '328d8908-c856-49ab-9e4e-4a2fa5c4a815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F240AB170>]}
[0m13:59:25.096188 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m13:59:25.115951 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:59:25.230519 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:59:25.231578 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:59:25.248870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '328d8908-c856-49ab-9e4e-4a2fa5c4a815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F23FDB470>]}
[0m13:59:25.261968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '328d8908-c856-49ab-9e4e-4a2fa5c4a815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F2415CCE0>]}
[0m13:59:25.262975 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:59:25.264986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '328d8908-c856-49ab-9e4e-4a2fa5c4a815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F23F16120>]}
[0m13:59:25.270967 [info ] [MainThread]: 
[0m13:59:25.273704 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m13:59:25.278775 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m13:59:27.451659 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:59:27.452672 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m13:59:28.970844 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:59:29.758487 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:59:32.936549 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m13:59:32.937573 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cursor'
[0m13:59:32.940079 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m13:59:32.942100 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  'NoneType' object has no attribute 'cursor'
[0m13:59:34.266888 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    'NoneType' object has no attribute 'cursor'
[0m13:59:34.269941 [debug] [MainThread]: Command `cli compile` failed at 13:59:34.269941 after 9.74 seconds
[0m13:59:34.271960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F23DE7F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F242EF020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021F242EEE40>]}
[0m13:59:34.273982 [debug] [MainThread]: Flushing usage events
[0m14:01:30.965275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD7A178DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD7A17A0F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD7A17B3E0>]}


============================== 14:01:30.968298 | 811d9ca4-c100-465a-ae7f-e84f33234dca ==============================
[0m14:01:30.968298 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:01:30.969304 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m14:01:31.280848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '811d9ca4-c100-465a-ae7f-e84f33234dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD7A2DC350>]}
[0m14:01:31.442129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '811d9ca4-c100-465a-ae7f-e84f33234dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD7A3AB6E0>]}
[0m14:01:31.444141 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m14:01:31.462978 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:01:31.552969 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:01:31.553974 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:01:31.571087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '811d9ca4-c100-465a-ae7f-e84f33234dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD7A207050>]}
[0m14:01:31.585184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '811d9ca4-c100-465a-ae7f-e84f33234dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD7A450FB0>]}
[0m14:01:31.586188 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:01:31.588197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '811d9ca4-c100-465a-ae7f-e84f33234dca', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DD7A450770>]}
[0m14:01:31.593240 [info ] [MainThread]: 
[0m14:01:31.596550 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m14:01:31.601654 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m14:01:37.518262 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:01:37.520326 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m14:01:38.920889 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m14:01:39.525790 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:06:06.122232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE6201640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE6201820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE6201940>]}


============================== 14:06:06.124262 | 526a3400-78ff-45e4-a4f6-52388ff705cb ==============================
[0m14:06:06.124262 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:06:06.125686 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:06:06.437837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '526a3400-78ff-45e4-a4f6-52388ff705cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE625B830>]}
[0m14:06:06.601287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '526a3400-78ff-45e4-a4f6-52388ff705cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE5B6C3B0>]}
[0m14:06:06.603296 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m14:06:06.621445 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:06:06.708820 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:06:06.709950 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:06:06.727516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '526a3400-78ff-45e4-a4f6-52388ff705cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE6395A30>]}
[0m14:06:06.740584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '526a3400-78ff-45e4-a4f6-52388ff705cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE64DCDD0>]}
[0m14:06:06.741590 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:06:06.743666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '526a3400-78ff-45e4-a4f6-52388ff705cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019CE64DC590>]}
[0m14:06:06.748691 [info ] [MainThread]: 
[0m14:06:06.752209 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m14:06:06.756549 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m14:06:13.490116 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:06:25.965550 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:06:25.967561 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m14:06:28.197244 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:16:48.049078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002547365D4F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002547365D6D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002547365D340>]}


============================== 15:16:48.049078 | 42391e65-d8c7-4c05-8b6f-6fce6c8f363d ==============================
[0m15:16:48.049078 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:16:48.050188 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:16:48.147231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '42391e65-d8c7-4c05-8b6f-6fce6c8f363d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025474711340>]}
[0m15:16:48.195772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '42391e65-d8c7-4c05-8b6f-6fce6c8f363d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025474767C80>]}
[0m15:16:48.196782 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:16:48.215051 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:16:48.774177 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:16:48.774177 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:16:48.778222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '42391e65-d8c7-4c05-8b6f-6fce6c8f363d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254746ADA90>]}
[0m15:16:48.788441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '42391e65-d8c7-4c05-8b6f-6fce6c8f363d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254748BAC60>]}
[0m15:16:48.789448 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:16:48.789448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '42391e65-d8c7-4c05-8b6f-6fce6c8f363d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002547365D340>]}
[0m15:16:48.790457 [info ] [MainThread]: 
[0m15:16:48.791466 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:16:48.792471 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:16:48.797505 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:16:48.797505 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:16:48.798510 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:16:48.798510 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:16:48.798510 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:16:48.799516 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: cursor.execute() takes 2 positional arguments but 3 were given
[0m15:16:48.799516 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:16:48.799516 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  cursor.execute() takes 2 positional arguments but 3 were given
[0m15:16:48.800523 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    cursor.execute() takes 2 positional arguments but 3 were given
[0m15:16:48.801528 [debug] [MainThread]: Command `cli compile` failed at 15:16:48.800523 after 0.79 seconds
[0m15:16:48.801637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025472B74A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025472F20410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002547365D400>]}
[0m15:16:48.801637 [debug] [MainThread]: Flushing usage events
[0m15:17:38.281336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221231D9010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221231D9160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221231D8D10>]}


============================== 15:17:38.282338 | f4b9741b-cb80-4fbd-b5b8-325310b28545 ==============================
[0m15:17:38.282338 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:17:38.282841 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:17:38.375270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f4b9741b-cb80-4fbd-b5b8-325310b28545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221231DB170>]}
[0m15:17:38.424123 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f4b9741b-cb80-4fbd-b5b8-325310b28545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221231685C0>]}
[0m15:17:38.425129 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:17:38.430158 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:17:38.464699 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:17:38.464699 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:17:38.467719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f4b9741b-cb80-4fbd-b5b8-325310b28545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022124219310>]}
[0m15:17:38.473903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f4b9741b-cb80-4fbd-b5b8-325310b28545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022124436F30>]}
[0m15:17:38.473903 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:17:38.474916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4b9741b-cb80-4fbd-b5b8-325310b28545', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221231DA180>]}
[0m15:17:38.475925 [info ] [MainThread]: 
[0m15:17:38.475925 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:17:38.476933 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:17:38.482989 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:17:38.482989 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:17:38.483494 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:17:38.484000 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:17:38.484000 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:17:38.484506 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:17:38.484506 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Compilation Error
  'method' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:17:38.485519 [error] [MainThread]: Encountered an error:
Compilation Error
  'method' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:17:38.486027 [debug] [MainThread]: Command `cli compile` failed at 15:17:38.486027 after 0.23 seconds
[0m15:17:38.486539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221231DA2D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221231D9A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221242EBDD0>]}
[0m15:17:38.486539 [debug] [MainThread]: Flushing usage events
[0m15:21:11.453426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F0E59850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F0E59BB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F0E597F0>]}


============================== 15:21:11.454431 | 936778d4-f673-42b9-9beb-1a3c837738aa ==============================
[0m15:21:11.454431 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:21:11.454431 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:21:11.554821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '936778d4-f673-42b9-9beb-1a3c837738aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F0697E30>]}
[0m15:21:11.602427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '936778d4-f673-42b9-9beb-1a3c837738aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F2007FE0>]}
[0m15:21:11.604435 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:21:11.608454 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:21:11.642887 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:21:11.642887 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:21:11.646904 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '936778d4-f673-42b9-9beb-1a3c837738aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F1F91B20>]}
[0m15:21:11.651928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '936778d4-f673-42b9-9beb-1a3c837738aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F20BACF0>]}
[0m15:21:11.652936 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:21:11.652936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '936778d4-f673-42b9-9beb-1a3c837738aa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F0C81550>]}
[0m15:21:11.654089 [info ] [MainThread]: 
[0m15:21:11.655092 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:21:11.656097 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:21:11.662136 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:21:11.662136 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:21:11.662136 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:21:11.663142 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:21:11.663142 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:21:11.664149 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:21:11.664149 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Compilation Error
  'method' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:21:11.665157 [error] [MainThread]: Encountered an error:
Compilation Error
  'method' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:21:11.666172 [debug] [MainThread]: Command `cli compile` failed at 15:21:11.666172 after 0.24 seconds
[0m15:21:11.666172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F0E5A6F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F1F13290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280F211DAC0>]}
[0m15:21:11.666172 [debug] [MainThread]: Flushing usage events
[0m15:22:09.324741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3FFC18E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3FFC1A3F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3FFC18DA0>]}


============================== 15:22:09.324741 | 2e8860f8-9c7d-4552-8922-55fc842352b0 ==============================
[0m15:22:09.324741 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:22:09.325749 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m15:22:09.419945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2e8860f8-9c7d-4552-8922-55fc842352b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3FFC1BDA0>]}
[0m15:22:09.469007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2e8860f8-9c7d-4552-8922-55fc842352b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3FFDF6990>]}
[0m15:22:09.470012 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:22:09.475040 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:22:09.508469 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:22:09.508469 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:22:09.512487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2e8860f8-9c7d-4552-8922-55fc842352b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3FFDF7320>]}
[0m15:22:09.517544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2e8860f8-9c7d-4552-8922-55fc842352b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3FFEAB290>]}
[0m15:22:09.518551 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:22:09.518551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2e8860f8-9c7d-4552-8922-55fc842352b0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3FFD2B950>]}
[0m15:22:09.519561 [info ] [MainThread]: 
[0m15:22:09.520679 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:22:09.521682 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:22:09.528237 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:22:09.528237 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:22:09.528743 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:22:09.528743 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:22:09.529249 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:22:09.529755 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:22:09.529755 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Compilation Error
  'method' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:22:09.530759 [error] [MainThread]: Encountered an error:
Compilation Error
  'method' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:22:09.531766 [debug] [MainThread]: Command `cli compile` failed at 15:22:09.531766 after 0.23 seconds
[0m15:22:09.531766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3FFC19340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3FFD04DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F3FFF0D880>]}
[0m15:22:09.532774 [debug] [MainThread]: Flushing usage events
[0m15:25:12.319644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223EA4E8170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223EB8F98B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223EB8F9850>]}


============================== 15:25:12.319644 | 303426fa-5b9f-4591-8476-ab86f084b818 ==============================
[0m15:25:12.319644 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:25:12.320650 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:25:12.422975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '303426fa-5b9f-4591-8476-ab86f084b818', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223EB8FB320>]}
[0m15:25:12.471794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '303426fa-5b9f-4591-8476-ab86f084b818', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223ECAA4500>]}
[0m15:25:12.473811 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:25:12.482875 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:25:12.957266 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:25:12.958271 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:25:12.961293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '303426fa-5b9f-4591-8476-ab86f084b818', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223ECACEA80>]}
[0m15:25:12.972438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '303426fa-5b9f-4591-8476-ab86f084b818', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223ECB5ABA0>]}
[0m15:25:12.972438 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:25:12.973441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '303426fa-5b9f-4591-8476-ab86f084b818', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223ECA044D0>]}
[0m15:25:12.974470 [info ] [MainThread]: 
[0m15:25:12.974470 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:25:12.975476 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:25:12.981509 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:25:12.981509 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:25:12.981509 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:25:12.982514 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:25:12.982514 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:25:12.982514 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:25:12.983520 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:25:12.983520 [error] [MainThread]: Encountered an error:
Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:25:12.984535 [debug] [MainThread]: Command `cli compile` failed at 15:25:12.984535 after 0.70 seconds
[0m15:25:12.985543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223EB8F8CE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223EB856DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223ECBBDE50>]}
[0m15:25:12.985543 [debug] [MainThread]: Flushing usage events
[0m15:26:39.116805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B42DA88080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B42DA895E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B42DA89700>]}


============================== 15:26:39.118816 | ca74e951-92c0-469b-b6b4-c2cee4985c64 ==============================
[0m15:26:39.118816 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:26:39.120827 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:26:39.434534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ca74e951-92c0-469b-b6b4-c2cee4985c64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B42DA8A390>]}
[0m15:26:39.595722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ca74e951-92c0-469b-b6b4-c2cee4985c64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B42DCB1A60>]}
[0m15:26:39.597780 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:26:39.617018 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:26:39.701385 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:26:39.702391 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:26:39.719524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ca74e951-92c0-469b-b6b4-c2cee4985c64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B42DCE7200>]}
[0m15:26:39.732625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ca74e951-92c0-469b-b6b4-c2cee4985c64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B42DD55190>]}
[0m15:26:39.733633 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:26:39.735657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ca74e951-92c0-469b-b6b4-c2cee4985c64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B42D9EEAE0>]}
[0m15:26:39.741214 [info ] [MainThread]: 
[0m15:26:39.743955 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:26:39.749001 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:26:45.426214 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:26:45.429312 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:26:46.504759 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:26:49.052021 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:27:33.455287 [debug] [ThreadPool]: SQL status: OK in 46.95000076293945 seconds
[0m15:27:52.536047 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:27:52.538062 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:28:14.566247 [error] [MainThread]: Encountered an error:
Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:28:14.569324 [debug] [MainThread]: Command `cli compile` failed at 15:28:14.569324 after 95.54 seconds
[0m15:28:14.571334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B42DA8BB00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B42DF11640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B42DF11700>]}
[0m15:28:14.573355 [debug] [MainThread]: Flushing usage events
[0m15:31:42.558235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D9145C080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D9145D760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D9145D6A0>]}


============================== 15:31:42.560246 | 24e80fee-5268-4c37-bb5c-9b2c755205a2 ==============================
[0m15:31:42.560246 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:31:42.562316 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:31:42.869231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '24e80fee-5268-4c37-bb5c-9b2c755205a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D9145C320>]}
[0m15:31:43.029284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '24e80fee-5268-4c37-bb5c-9b2c755205a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D91687950>]}
[0m15:31:43.032300 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:31:43.049480 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:31:43.135402 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:31:43.136408 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:31:43.153509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '24e80fee-5268-4c37-bb5c-9b2c755205a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D91608800>]}
[0m15:31:43.166613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '24e80fee-5268-4c37-bb5c-9b2c755205a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D917290A0>]}
[0m15:31:43.168624 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:31:43.169629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '24e80fee-5268-4c37-bb5c-9b2c755205a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D8E7EB170>]}
[0m15:31:43.175846 [info ] [MainThread]: 
[0m15:31:43.179096 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:31:43.184140 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:31:47.459483 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:31:47.461503 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:31:48.114861 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:31:48.724969 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:32:04.934461 [debug] [ThreadPool]: SQL status: OK in 16.81999969482422 seconds
[0m15:34:09.959775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184448A8B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184448A8860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184448A8800>]}


============================== 15:34:09.962806 | 83e64011-36e0-4031-b86a-a637c4a8e7c5 ==============================
[0m15:34:09.962806 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:34:09.963817 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:34:10.279134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '83e64011-36e0-4031-b86a-a637c4a8e7c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184446E35F0>]}
[0m15:34:10.447401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '83e64011-36e0-4031-b86a-a637c4a8e7c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018444AD6CC0>]}
[0m15:34:10.450816 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:34:10.469967 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:34:10.567331 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:34:10.568336 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:34:10.585508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '83e64011-36e0-4031-b86a-a637c4a8e7c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018444AD5280>]}
[0m15:34:10.598621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '83e64011-36e0-4031-b86a-a637c4a8e7c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018444B78D40>]}
[0m15:34:10.600638 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:34:10.601647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '83e64011-36e0-4031-b86a-a637c4a8e7c5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000184448FFCB0>]}
[0m15:34:10.607705 [info ] [MainThread]: 
[0m15:34:10.610023 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:34:10.615074 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:34:19.440586 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:34:19.442614 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:34:27.644120 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:34:27.646187 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:34:37.962466 [debug] [ThreadPool]: SQL status: OK in 10.319999694824219 seconds
[0m15:36:12.565739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283B07A1A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283B07A1B20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283B07A1C70>]}


============================== 15:36:12.568778 | f2df2c84-de35-46ef-8ed7-48b538ecc660 ==============================
[0m15:36:12.568778 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:36:12.569789 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:36:12.885013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f2df2c84-de35-46ef-8ed7-48b538ecc660', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283B0831B50>]}
[0m15:36:13.051887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f2df2c84-de35-46ef-8ed7-48b538ecc660', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283B04A6D80>]}
[0m15:36:13.053896 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:36:13.073096 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:36:13.157385 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:36:13.159396 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:36:13.177664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f2df2c84-de35-46ef-8ed7-48b538ecc660', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283B07FFB00>]}
[0m15:36:13.190798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f2df2c84-de35-46ef-8ed7-48b538ecc660', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283B0A74E60>]}
[0m15:36:13.192851 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:36:13.193862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f2df2c84-de35-46ef-8ed7-48b538ecc660', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283B0959760>]}
[0m15:36:13.200061 [info ] [MainThread]: 
[0m15:36:13.202093 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:36:13.207132 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:36:13.234475 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:36:13.235485 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:36:13.236491 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:36:13.238531 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:36:17.869072 [debug] [ThreadPool]: SQL status: OK in 4.630000114440918 seconds
[0m15:36:24.674532 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:36:24.676554 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:36:24.682639 [error] [MainThread]: Encountered an error:
Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:36:24.685680 [debug] [MainThread]: Command `cli compile` failed at 15:36:24.685680 after 12.21 seconds
[0m15:36:24.688714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283B07A2AE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283B0537500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283B08C3740>]}
[0m15:36:24.689725 [debug] [MainThread]: Flushing usage events
[0m15:43:15.438459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002131A1C0320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002131A1C02C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002131A1C1580>]}


============================== 15:43:15.441513 | f16879cc-67d9-49c8-a766-0e8847d24d63 ==============================
[0m15:43:15.441513 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:43:15.442519 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m15:43:15.755629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f16879cc-67d9-49c8-a766-0e8847d24d63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002131A1C23C0>]}
[0m15:43:15.917196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f16879cc-67d9-49c8-a766-0e8847d24d63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002131A42A1E0>]}
[0m15:43:15.920211 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:43:15.944593 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:43:16.503532 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:43:16.505551 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:43:16.523035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f16879cc-67d9-49c8-a766-0e8847d24d63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002131A1C3A70>]}
[0m15:43:16.537150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f16879cc-67d9-49c8-a766-0e8847d24d63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002131A4A0F80>]}
[0m15:43:16.538213 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:43:16.540258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f16879cc-67d9-49c8-a766-0e8847d24d63', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002131A4A0680>]}
[0m15:43:16.545287 [info ] [MainThread]: 
[0m15:43:16.548304 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:43:16.553365 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:43:16.580692 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:43:16.582723 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:43:16.583731 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:43:16.585743 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:46:36.358496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002674CBFC1D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002674CBFD6D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002674CBFD7F0>]}


============================== 15:46:36.361546 | 4e975244-d5ee-4d56-a0b4-34f49b833e08 ==============================
[0m15:46:36.361546 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:46:36.362558 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:46:36.678076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4e975244-d5ee-4d56-a0b4-34f49b833e08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002674CBFCB00>]}
[0m15:46:36.839066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4e975244-d5ee-4d56-a0b4-34f49b833e08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002674CE5B350>]}
[0m15:46:36.842130 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:46:36.861537 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:46:36.946669 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:46:36.947675 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:46:36.964778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4e975244-d5ee-4d56-a0b4-34f49b833e08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002674CE22180>]}
[0m15:46:36.977952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4e975244-d5ee-4d56-a0b4-34f49b833e08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002674CEC50D0>]}
[0m15:46:36.979967 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:46:36.980976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e975244-d5ee-4d56-a0b4-34f49b833e08', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002674CEC4890>]}
[0m15:46:36.987015 [info ] [MainThread]: 
[0m15:46:36.990049 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:46:36.994091 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:46:37.021419 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:46:37.022426 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:46:37.023493 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:46:37.025502 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:50:01.388049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD8D1C3B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD8D1DB80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD8D1DC10>]}


============================== 15:50:01.396628 | 942c5efc-d4b4-4d24-a20f-d8df95eaec2f ==============================
[0m15:50:01.396628 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:50:01.397639 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m15:50:01.705930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '942c5efc-d4b4-4d24-a20f-d8df95eaec2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD8E33C80>]}
[0m15:50:01.867739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '942c5efc-d4b4-4d24-a20f-d8df95eaec2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD8F76E40>]}
[0m15:50:01.869754 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:50:01.887848 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:50:01.976064 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:50:01.977143 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:50:01.995322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '942c5efc-d4b4-4d24-a20f-d8df95eaec2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD8DA0B00>]}
[0m15:50:02.009984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '942c5efc-d4b4-4d24-a20f-d8df95eaec2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD8FF8F80>]}
[0m15:50:02.011503 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:50:02.013019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '942c5efc-d4b4-4d24-a20f-d8df95eaec2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD8FF8680>]}
[0m15:50:02.018559 [info ] [MainThread]: 
[0m15:50:02.021926 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:50:02.026987 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:50:02.055540 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:50:02.056545 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:50:02.057556 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:50:02.059580 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:50:10.846668 [debug] [ThreadPool]: SQL status: OK in 8.789999961853027 seconds
[0m15:50:10.848683 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:50:10.850696 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:50:10.855728 [error] [MainThread]: Encountered an error:
Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:50:10.859773 [debug] [MainThread]: Command `cli compile` failed at 15:50:10.858767 after 9.55 seconds
[0m15:50:10.860778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD8D1EF60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD918DEB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD918DE20>]}
[0m15:50:10.862792 [debug] [MainThread]: Flushing usage events
[0m15:50:34.054183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEC5EBD3A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEC5E6FC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEC5EBD520>]}


============================== 15:50:34.057218 | 9ea759d6-e086-4c85-a128-a2e66ea5e436 ==============================
[0m15:50:34.057218 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:50:34.059234 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:50:34.386460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9ea759d6-e086-4c85-a128-a2e66ea5e436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEC60467B0>]}
[0m15:50:34.555003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9ea759d6-e086-4c85-a128-a2e66ea5e436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEC600ED80>]}
[0m15:50:34.559042 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:50:34.578305 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:50:34.669412 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:50:34.671422 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:50:34.690601 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9ea759d6-e086-4c85-a128-a2e66ea5e436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEC60E7260>]}
[0m15:50:34.704150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9ea759d6-e086-4c85-a128-a2e66ea5e436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEC618CE90>]}
[0m15:50:34.706268 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:50:34.708333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9ea759d6-e086-4c85-a128-a2e66ea5e436', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEC621D070>]}
[0m15:50:34.713872 [info ] [MainThread]: 
[0m15:50:34.716461 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:50:34.720994 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:50:34.746268 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:50:34.748282 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:50:34.749288 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:50:34.750335 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:50:44.513036 [debug] [ThreadPool]: SQL status: OK in 9.760000228881836 seconds
[0m15:51:30.824213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001710BB6CF20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001710BB6DE80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001710BB6E2D0>]}


============================== 15:51:30.827231 | ccf122db-ca9c-4854-be3c-0edfeb1b43bb ==============================
[0m15:51:30.827231 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:51:30.828239 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:51:31.141279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ccf122db-ca9c-4854-be3c-0edfeb1b43bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001710BBC26F0>]}
[0m15:51:31.301252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ccf122db-ca9c-4854-be3c-0edfeb1b43bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001710BD76D80>]}
[0m15:51:31.303262 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:51:31.322685 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:51:31.408119 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:51:31.409125 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:51:31.426251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ccf122db-ca9c-4854-be3c-0edfeb1b43bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001710BDA9B80>]}
[0m15:51:31.440361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ccf122db-ca9c-4854-be3c-0edfeb1b43bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001710BE3A9F0>]}
[0m15:51:31.442371 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:51:31.443375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ccf122db-ca9c-4854-be3c-0edfeb1b43bb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001710BE3A2A0>]}
[0m15:51:31.450024 [info ] [MainThread]: 
[0m15:51:31.452562 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:51:31.457600 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:51:31.484917 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:51:31.485925 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:51:31.487948 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:51:31.488956 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:51:35.552517 [debug] [ThreadPool]: SQL status: OK in 4.059999942779541 seconds
[0m15:51:35.555559 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:51:35.556563 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:51:35.562600 [error] [MainThread]: Encountered an error:
Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:51:35.565621 [debug] [MainThread]: Command `cli compile` failed at 15:51:35.565621 after 4.83 seconds
[0m15:51:35.567632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001710BB6D430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001710BFD4830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001710BFD4860>]}
[0m15:51:35.568639 [debug] [MainThread]: Flushing usage events
[0m15:52:25.965928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002765ED7C080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002765EBD2960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002765EBD0F20>]}


============================== 15:52:25.968960 | be1e3a67-2d8f-47f5-91fd-679055120745 ==============================
[0m15:52:25.968960 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:52:25.971995 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:52:26.300867 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'be1e3a67-2d8f-47f5-91fd-679055120745', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002765EA26870>]}
[0m15:52:26.464582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'be1e3a67-2d8f-47f5-91fd-679055120745', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002765EF5AF60>]}
[0m15:52:26.467604 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:52:26.486979 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:52:26.580523 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:52:26.581544 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:52:26.598700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'be1e3a67-2d8f-47f5-91fd-679055120745', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002765EEB57F0>]}
[0m15:52:26.611802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'be1e3a67-2d8f-47f5-91fd-679055120745', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002765F042C00>]}
[0m15:52:26.613830 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:52:26.614836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be1e3a67-2d8f-47f5-91fd-679055120745', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002765EF59910>]}
[0m15:52:26.620888 [info ] [MainThread]: 
[0m15:52:26.623535 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:52:26.628090 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:52:26.654370 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:52:26.656380 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:52:26.657454 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:52:26.658457 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:52:31.026963 [debug] [ThreadPool]: SQL status: OK in 4.369999885559082 seconds
[0m15:52:31.029058 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:52:31.031092 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:52:31.037464 [error] [MainThread]: Encountered an error:
Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:52:31.040490 [debug] [MainThread]: Command `cli compile` failed at 15:52:31.040490 after 5.16 seconds
[0m15:52:31.042511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002765EC47530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002765F1E4AA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002765F1E4AD0>]}
[0m15:52:31.044533 [debug] [MainThread]: Flushing usage events
[0m15:55:10.305508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20292E4B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20292D880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20292C8C0>]}


============================== 15:55:10.307520 | 115ec583-69b9-4a24-aed4-8ae8f31eb951 ==============================
[0m15:55:10.307520 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:55:10.309535 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:55:10.627358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '115ec583-69b9-4a24-aed4-8ae8f31eb951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20295A4B0>]}
[0m15:55:10.789383 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '115ec583-69b9-4a24-aed4-8ae8f31eb951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A202B3CCE0>]}
[0m15:55:10.791394 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:55:10.817603 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:55:11.270474 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:55:11.272494 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:55:11.291654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '115ec583-69b9-4a24-aed4-8ae8f31eb951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A202B126C0>]}
[0m15:55:11.305748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '115ec583-69b9-4a24-aed4-8ae8f31eb951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A202B859D0>]}
[0m15:55:11.307760 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:55:11.309782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '115ec583-69b9-4a24-aed4-8ae8f31eb951', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20263B170>]}
[0m15:55:11.315814 [info ] [MainThread]: 
[0m15:55:11.318097 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:55:11.324155 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:55:11.352482 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:55:11.354525 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:55:11.356042 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:55:11.357568 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:55:13.370738 [debug] [ThreadPool]: SQL status: OK in 2.009999990463257 seconds
[0m15:55:13.373039 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m15:55:13.375053 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:55:13.381096 [error] [MainThread]: Encountered an error:
Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m15:55:13.384113 [debug] [MainThread]: Command `cli compile` failed at 15:55:13.384113 after 3.18 seconds
[0m15:55:13.387130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20292C3B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A202DA02F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A202DA0320>]}
[0m15:55:13.388153 [debug] [MainThread]: Flushing usage events
[0m15:55:32.945027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A2AB60B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A2AB60830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A2AB60140>]}


============================== 15:55:32.948042 | 7f36e791-be9a-4ee5-bd25-952285975583 ==============================
[0m15:55:32.948042 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:55:32.949047 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:55:33.262187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7f36e791-be9a-4ee5-bd25-952285975583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A2A9B3B90>]}
[0m15:55:33.440201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7f36e791-be9a-4ee5-bd25-952285975583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A2AD6FFB0>]}
[0m15:55:33.443217 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:55:33.463428 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:55:33.556401 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:55:33.558414 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:55:33.576100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7f36e791-be9a-4ee5-bd25-952285975583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A2ABB1610>]}
[0m15:55:33.590607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7f36e791-be9a-4ee5-bd25-952285975583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A2ADB5EE0>]}
[0m15:55:33.592624 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:55:33.593633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7f36e791-be9a-4ee5-bd25-952285975583', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A29BA1F70>]}
[0m15:55:33.599903 [info ] [MainThread]: 
[0m15:55:33.602949 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:55:33.609029 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:57:40.399862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029ECCC8C1A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029ECCC8D820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029ECCC8C230>]}


============================== 15:57:40.402948 | 39976341-6b93-4a74-b3f5-ffa176fcb7fc ==============================
[0m15:57:40.402948 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:57:40.404961 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:57:40.725564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '39976341-6b93-4a74-b3f5-ffa176fcb7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029ECCD61910>]}
[0m15:57:40.893721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '39976341-6b93-4a74-b3f5-ffa176fcb7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029ECCEC8E90>]}
[0m15:57:40.895733 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:57:40.916074 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:57:41.012820 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:57:41.013827 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:57:41.031996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '39976341-6b93-4a74-b3f5-ffa176fcb7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029ECD073FB0>]}
[0m15:57:41.045140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '39976341-6b93-4a74-b3f5-ffa176fcb7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029ECCF57500>]}
[0m15:57:41.047153 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:57:41.048160 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '39976341-6b93-4a74-b3f5-ffa176fcb7fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029ECCDCEC60>]}
[0m15:57:41.054207 [info ] [MainThread]: 
[0m15:57:41.056735 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:57:41.061275 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:58:12.627356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199A3F6CA70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199A3F6D8B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199A3F6D850>]}


============================== 15:58:12.630367 | 0fa4a20b-f07f-4cf6-a6c7-f5410db12b79 ==============================
[0m15:58:12.630367 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:58:12.632388 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:58:12.941817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0fa4a20b-f07f-4cf6-a6c7-f5410db12b79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199A3F6D250>]}
[0m15:58:13.103873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0fa4a20b-f07f-4cf6-a6c7-f5410db12b79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199A41C7EC0>]}
[0m15:58:13.105887 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:58:13.125053 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:58:13.210145 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:58:13.211151 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:58:13.228291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0fa4a20b-f07f-4cf6-a6c7-f5410db12b79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199A40F5A30>]}
[0m15:58:13.241403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0fa4a20b-f07f-4cf6-a6c7-f5410db12b79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199A4244B00>]}
[0m15:58:13.243414 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:58:13.244419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0fa4a20b-f07f-4cf6-a6c7-f5410db12b79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000199A3FE4B60>]}
[0m15:58:13.250448 [info ] [MainThread]: 
[0m15:58:13.252933 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:58:13.257982 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:58:16.071950 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:58:16.072994 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:58:16.075022 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:58:16.077038 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:04:43.744494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029A938DC350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029A938DC650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029A938DD520>]}


============================== 16:04:43.746511 | 5248696e-26d6-4caa-9590-c544bf54f281 ==============================
[0m16:04:43.746511 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:04:43.748526 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:04:44.061107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5248696e-26d6-4caa-9590-c544bf54f281', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029A9392BCE0>]}
[0m16:04:44.228913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5248696e-26d6-4caa-9590-c544bf54f281', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029A9309EC60>]}
[0m16:04:44.230923 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m16:04:44.249045 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:04:44.340915 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:04:44.342927 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:04:44.360041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5248696e-26d6-4caa-9590-c544bf54f281', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029A939775F0>]}
[0m16:04:44.373161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5248696e-26d6-4caa-9590-c544bf54f281', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029A93BA5040>]}
[0m16:04:44.375176 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:04:44.376183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5248696e-26d6-4caa-9590-c544bf54f281', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029A93B03D40>]}
[0m16:04:44.382214 [info ] [MainThread]: 
[0m16:04:44.384433 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m16:04:44.390719 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m16:04:47.355508 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:04:47.357522 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m16:04:47.358620 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:04:47.360632 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:04:53.015599 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:04:53.016605 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: No module named 'SimpleNamespace'
[0m16:04:53.020691 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m16:04:53.021698 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  No module named 'SimpleNamespace'
[0m16:04:53.027745 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    No module named 'SimpleNamespace'
[0m16:04:53.031775 [debug] [MainThread]: Command `cli compile` failed at 16:04:53.030770 after 9.37 seconds
[0m16:04:53.033788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029A938DDA30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029A93DA80B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029A93DA8260>]}
[0m16:04:53.034797 [debug] [MainThread]: Flushing usage events
[0m16:06:26.742901 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E906F5CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E906F5190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E906F5C70>]}


============================== 16:06:26.742901 | d5a7fb4c-6c37-416f-b7f5-c3ffd828737d ==============================
[0m16:06:26.742901 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:06:26.743917 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:06:26.845013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd5a7fb4c-6c37-416f-b7f5-c3ffd828737d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E917B7050>]}
[0m16:06:26.893907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd5a7fb4c-6c37-416f-b7f5-c3ffd828737d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E9051D460>]}
[0m16:06:26.894909 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m16:06:26.899944 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:06:26.936599 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:06:26.937606 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:06:26.940624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd5a7fb4c-6c37-416f-b7f5-c3ffd828737d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E91880B90>]}
[0m16:06:26.946679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd5a7fb4c-6c37-416f-b7f5-c3ffd828737d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E918F6300>]}
[0m16:06:26.947693 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:06:26.947693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd5a7fb4c-6c37-416f-b7f5-c3ffd828737d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E91817440>]}
[0m16:06:26.948725 [info ] [MainThread]: 
[0m16:06:26.949733 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m16:06:26.950739 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m16:06:26.956907 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:06:26.956907 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m16:06:26.956907 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:06:26.956907 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:06:26.990317 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:06:26.991326 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: type object 'types.SimpleNamespace' has no attribute 'SimpleNamespace'
[0m16:06:26.991326 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m16:06:26.991326 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  type object 'types.SimpleNamespace' has no attribute 'SimpleNamespace'
[0m16:06:26.992332 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    type object 'types.SimpleNamespace' has no attribute 'SimpleNamespace'
[0m16:06:26.993340 [debug] [MainThread]: Command `cli compile` failed at 16:06:26.993340 after 0.27 seconds
[0m16:06:26.993340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E895BCA40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E91817830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E906F5190>]}
[0m16:06:26.994364 [debug] [MainThread]: Flushing usage events
[0m16:06:48.186717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019F248212B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019F24F28BC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019F24F2BE00>]}


============================== 16:06:48.187721 | 63f74790-9be1-4ef9-95fe-6643d6c88f3f ==============================
[0m16:06:48.187721 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:06:48.187721 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:06:48.288956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '63f74790-9be1-4ef9-95fe-6643d6c88f3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019F24084590>]}
[0m16:06:48.338876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '63f74790-9be1-4ef9-95fe-6643d6c88f3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019F250CD400>]}
[0m16:06:48.339882 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m16:06:48.343924 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:06:48.379425 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:06:48.380430 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:06:48.383447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '63f74790-9be1-4ef9-95fe-6643d6c88f3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019F2506FF20>]}
[0m16:06:48.389655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '63f74790-9be1-4ef9-95fe-6643d6c88f3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019F2618B4A0>]}
[0m16:06:48.389655 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:06:48.390659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '63f74790-9be1-4ef9-95fe-6643d6c88f3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019F25011CA0>]}
[0m16:06:48.391672 [info ] [MainThread]: 
[0m16:06:48.392686 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m16:06:48.392686 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m16:06:48.398744 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:06:48.399249 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m16:06:48.399761 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:06:48.399761 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:06:48.432828 [debug] [ThreadPool]: SQL status: OK in 0.029999999329447746 seconds
[0m16:06:48.432828 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m16:06:48.433833 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'types.SimpleNamespace' object has no attribute 'column_names'
[0m16:06:48.433833 [error] [MainThread]: Encountered an error:
Runtime Error
  'types.SimpleNamespace' object has no attribute 'column_names'
[0m16:06:48.434851 [debug] [MainThread]: Command `cli compile` failed at 16:06:48.434851 after 0.27 seconds
[0m16:06:48.435860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019F24F29490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019F25011850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019F26160590>]}
[0m16:06:48.435860 [debug] [MainThread]: Flushing usage events
[0m16:14:33.770860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245D28C200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245D28D970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245D28DAC0>]}


============================== 16:14:33.772879 | 2680f7bf-924b-4ef6-85fb-e5f549a33864 ==============================
[0m16:14:33.772879 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:14:33.774901 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:14:34.082418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2680f7bf-924b-4ef6-85fb-e5f549a33864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245D415880>]}
[0m16:14:34.244929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2680f7bf-924b-4ef6-85fb-e5f549a33864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245D417C80>]}
[0m16:14:34.247949 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m16:14:34.273634 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:14:34.710366 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:14:34.711372 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:14:34.730554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2680f7bf-924b-4ef6-85fb-e5f549a33864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245D4E7B90>]}
[0m16:14:34.744671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2680f7bf-924b-4ef6-85fb-e5f549a33864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245D554E60>]}
[0m16:14:34.746683 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:14:34.747691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2680f7bf-924b-4ef6-85fb-e5f549a33864', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002245D4B8D10>]}
[0m16:14:34.753729 [info ] [MainThread]: 
[0m16:14:34.756754 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m16:14:34.761285 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m16:14:38.203531 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:14:38.205597 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m16:14:38.207651 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:14:38.208661 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:25:27.245870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8DF5AD8E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8DF5ADAF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8DF5AFEF0>]}


============================== 16:25:27.248891 | 8a380472-a0f6-4b5b-a835-068596f1c32b ==============================
[0m16:25:27.248891 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:25:27.249900 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:25:27.584746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8a380472-a0f6-4b5b-a835-068596f1c32b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8DEFF9AC0>]}
[0m16:25:27.752185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8a380472-a0f6-4b5b-a835-068596f1c32b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8DF7DF560>]}
[0m16:25:27.755202 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m16:25:27.779356 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:25:28.353746 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:25:28.354751 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:25:28.371864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8a380472-a0f6-4b5b-a835-068596f1c32b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8DF7A6B70>]}
[0m16:25:28.384953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8a380472-a0f6-4b5b-a835-068596f1c32b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8DF874D70>]}
[0m16:25:28.386967 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:25:28.387977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8a380472-a0f6-4b5b-a835-068596f1c32b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8DF700F20>]}
[0m16:25:28.394021 [info ] [MainThread]: 
[0m16:25:28.397037 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m16:25:28.401061 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m16:25:30.978312 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:25:30.980323 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m16:25:30.981831 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:25:30.982841 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:25:44.359434 [debug] [ThreadPool]: SQL status: OK in 13.380000114440918 seconds
[0m16:25:49.278229 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m16:25:49.280245 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'dict' object has no attribute 'column_names'
[0m16:25:49.287319 [error] [MainThread]: Encountered an error:
Runtime Error
  'dict' object has no attribute 'column_names'
[0m16:25:49.290352 [debug] [MainThread]: Command `cli compile` failed at 16:25:49.290352 after 22.15 seconds
[0m16:25:49.292362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8DF5AE840>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8DFA80E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8DFA81280>]}
[0m16:25:49.293367 [debug] [MainThread]: Flushing usage events
[0m16:27:17.329771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDEFE1D820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDEFE1DA60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDEFE1DBE0>]}


============================== 16:27:17.332836 | 57ea4b5a-0f41-41bf-b478-dedcb4c1d44a ==============================
[0m16:27:17.332836 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:27:17.334848 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:27:17.650411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '57ea4b5a-0f41-41bf-b478-dedcb4c1d44a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDEFC52C60>]}
[0m16:27:17.810446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '57ea4b5a-0f41-41bf-b478-dedcb4c1d44a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDF007F5C0>]}
[0m16:27:17.813582 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m16:27:17.832714 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:27:17.925040 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:27:17.926047 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:27:17.944163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '57ea4b5a-0f41-41bf-b478-dedcb4c1d44a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDEFEB4140>]}
[0m16:27:17.957272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '57ea4b5a-0f41-41bf-b478-dedcb4c1d44a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDF00DD700>]}
[0m16:27:17.959286 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:27:17.960294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '57ea4b5a-0f41-41bf-b478-dedcb4c1d44a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDF00DCCB0>]}
[0m16:27:17.966849 [info ] [MainThread]: 
[0m16:27:17.969362 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m16:27:17.973904 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m16:27:22.196273 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:27:22.198285 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m16:27:22.199292 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:27:22.200298 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:27:27.230920 [debug] [ThreadPool]: SQL status: OK in 5.03000020980835 seconds
[0m16:27:31.566360 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m16:27:31.568369 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m16:27:31.573421 [error] [MainThread]: Encountered an error:
Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m16:27:31.577454 [debug] [MainThread]: Command `cli compile` failed at 16:27:31.576442 after 14.33 seconds
[0m16:27:31.579470 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDEFE1F620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDF02F0290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDEFF35BE0>]}
[0m16:27:31.581480 [debug] [MainThread]: Flushing usage events
[0m16:32:52.887388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A380FFD790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A380FFD7F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A380FFDA60>]}


============================== 16:32:52.889413 | 6b81883b-a175-47d5-adaa-90731b7ab223 ==============================
[0m16:32:52.889413 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:32:52.891429 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:32:53.192992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6b81883b-a175-47d5-adaa-90731b7ab223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A380FB3830>]}
[0m16:32:53.359698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6b81883b-a175-47d5-adaa-90731b7ab223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A38125E540>]}
[0m16:32:53.361722 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m16:32:53.383023 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:32:53.473840 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:32:53.475862 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:32:53.493110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6b81883b-a175-47d5-adaa-90731b7ab223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A380FFDF40>]}
[0m16:32:53.506194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6b81883b-a175-47d5-adaa-90731b7ab223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A3812C50A0>]}
[0m16:32:53.507203 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:32:53.509229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6b81883b-a175-47d5-adaa-90731b7ab223', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A380F5D070>]}
[0m16:32:53.514268 [info ] [MainThread]: 
[0m16:32:53.517295 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m16:32:53.521325 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m16:32:56.960919 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:32:56.962932 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m16:32:56.963938 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:32:56.964948 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:33:00.470597 [debug] [ThreadPool]: SQL status: OK in 3.509999990463257 seconds
[0m16:46:07.488395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7AB48C800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7AB48DA90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7AB48DBE0>]}


============================== 16:46:07.491413 | 5ec87ed3-6c8d-4597-b33b-78c4001a7c75 ==============================
[0m16:46:07.491413 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:46:07.492428 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:46:07.810219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5ec87ed3-6c8d-4597-b33b-78c4001a7c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7AB5A2900>]}
[0m16:46:07.974766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5ec87ed3-6c8d-4597-b33b-78c4001a7c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7AB6BF5F0>]}
[0m16:46:07.977781 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m16:46:08.004132 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:46:08.534417 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:46:08.535440 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:46:08.553566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5ec87ed3-6c8d-4597-b33b-78c4001a7c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7AB7F5370>]}
[0m16:46:08.567653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5ec87ed3-6c8d-4597-b33b-78c4001a7c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7AB760AD0>]}
[0m16:46:08.568659 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:46:08.570669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5ec87ed3-6c8d-4597-b33b-78c4001a7c75', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7AB5E39E0>]}
[0m16:46:08.575695 [info ] [MainThread]: 
[0m16:46:08.578858 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m16:46:08.583924 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m16:46:16.219416 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:46:16.221425 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m16:46:16.222432 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:46:16.223439 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:46:16.225459 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Using CLI auth
[0m16:46:17.422028 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: CLI - Fetched Access Token
[0m16:46:17.424048 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6InEtMjNmYWxldlpoaEQzaG05Q1Fia1A1TVF5VSIsImtpZCI6InEtMjNmYWxldlpoaEQzaG05Q1Fia1A1TVF5VSJ9.eyJhdWQiOiJodHRwczovL2FuYWx5c2lzLndpbmRvd3MubmV0L3Bvd2VyYmkvYXBpIiwiaXNzIjoiaHR0cHM6Ly9zdHMud2luZG93cy5uZXQvNmM2Mzc1MTItYzQxNy00ZTc4LTlkNjItYjYxMjU4ZTRiNjE5LyIsImlhdCI6MTcxNDAzNDQ3NywibmJmIjoxNzE0MDM0NDc3LCJleHAiOjE3MTQwMzkxNDAsImFjY3QiOjAsImFjciI6IjEiLCJhaW8iOiJBVlFBcS84V0FBQUFBUVExSWRTazFyZ2M0QmFqRzZjRU9SNDJSb2VtMThUZGgxL3FCT0ZrVFpPbEpJUUNNYWszSkRhTEdQK1d6bkxzYlN6ck9vbDNhNFE3MTZZUDlSVVJCNnZuTWVtaUw1WVpNZjhXdHA3ZzM4Yz0iLCJhbXIiOlsicHdkIiwicnNhIiwibWZhIl0sImFwcGlkIjoiMDRiMDc3OTUtOGRkYi00NjFhLWJiZWUtMDJmOWUxYmY3YjQ2IiwiYXBwaWRhY3IiOiIwIiwiZGV2aWNlaWQiOiIwNTQzZDg2ZS05ZTUyLTQ1YTMtODg0My1iMGMzM2I2OWYxNDIiLCJmYW1pbHlfbmFtZSI6IlJhbXBvbm8iLCJnaXZlbl9uYW1lIjoiSm9obiIsImlwYWRkciI6IjE4MC4xNTAuODEuNDYiLCJuYW1lIjoiUmFtcG9ubywgSm9obiIsIm9pZCI6IjE2Mzc1OTBjLWUxNjctNGMzMy05OTU3LTg1ZjZiOWE5OTI3NyIsIm9ucHJlbV9zaWQiOiJTLTEtNS0yMS0xMzQ4MDg0MzM5LTEyNzc0ODA0NS05Mjk3MDEwMDAtMzY3NDc5IiwicHVpZCI6IjEwMDMyMDAzNTk3NEE4MEEiLCJyaCI6IjAuQVNjQUVuVmpiQmZFZUU2ZFlyWVNXT1MyR1FrQUFBQUFBQUFBd0FBQUFBQUFBQUFuQU93LiIsInNjcCI6InVzZXJfaW1wZXJzb25hdGlvbiIsInNpZ25pbl9zdGF0ZSI6WyJrbXNpIl0sInN1YiI6Ik1xTzZyOXdyb3NVLWxjQmRYUGN0ZDhhcXBCT1AxWkY4THNQLW1zNGtHRWMiLCJ0aWQiOiI2YzYzNzUxMi1jNDE3LTRlNzgtOWQ2Mi1iNjEyNThlNGI2MTkiLCJ1bmlxdWVfbmFtZSI6IkpvaG4uUmFtcG9ub0BpbnNpZ2h0LmNvbSIsInVwbiI6IkpvaG4uUmFtcG9ub0BpbnNpZ2h0LmNvbSIsInV0aSI6Im83V1JHR0o4UWtlR1BnbVgyaU96QUEiLCJ2ZXIiOiIxLjAiLCJ3aWRzIjpbImI3OWZiZjRkLTNlZjktNDY4OS04MTQzLTc2YjE5NGU4NTUwOSJdLCJ4bXNfY2MiOlsiQ1AxIl19.xHTIrxN0gDYxl8b7MzX83GiEgV0-JCOoO29Gt-6ZCsQWZLfOxEyg4Qj6zinLfhvGxNS5XgallbmTnsmcpyjTUoGQ2KOLLpL0v8sAOiFdbRyCmYYDL_mMRrUVWmrWd87CoO34GibD6SElQfWCdIhYdAM6hONspw4ttTXRiW2WcvPlY9iVYlyF0p9fEanaYi5_r8gkckHzQLgEktj3PrCbke9uI8Fs2bhzIk3c2tSpUotVrXuL2KVPFRSCFUyh1qnv8KmsQL3AdV69fHDw3Hej7EoKOH1ptzHb4k70fBkKZZ5ebxwsMbtjdMwomnL807tK2eXMgZYPAZsfoER7k0awKA
[0m16:46:18.869648 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Connection error: 'id'
[0m16:49:38.127624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002703F3C23C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002703F3C2540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002703F3C25A0>]}


============================== 16:49:38.130650 | 20b32ca4-b095-436b-a2aa-f8a5b99b3c9e ==============================
[0m16:49:38.130650 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:49:38.132667 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:49:38.456399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '20b32ca4-b095-436b-a2aa-f8a5b99b3c9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002703F543140>]}
[0m16:49:38.619755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '20b32ca4-b095-436b-a2aa-f8a5b99b3c9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002703F5E2F00>]}
[0m16:49:38.621762 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m16:49:38.639864 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:49:38.725501 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:49:38.726506 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:49:38.743637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '20b32ca4-b095-436b-a2aa-f8a5b99b3c9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002703F3C1370>]}
[0m16:49:38.757723 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '20b32ca4-b095-436b-a2aa-f8a5b99b3c9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002703F671F40>]}
[0m16:49:38.758728 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:49:38.760739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '20b32ca4-b095-436b-a2aa-f8a5b99b3c9e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002703F510F50>]}
[0m16:49:38.765777 [info ] [MainThread]: 
[0m16:49:38.769260 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m16:49:38.774311 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m16:49:42.358933 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:49:42.360945 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m16:49:42.361952 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:49:42.363968 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:53:23.192938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DDCFBC800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DDCFBCB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DDCFBC680>]}


============================== 16:53:23.195454 | 5276a6cf-b6ac-41e9-a5c9-65f1bae8c9c1 ==============================
[0m16:53:23.195454 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:53:23.196965 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:53:23.507508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5276a6cf-b6ac-41e9-a5c9-65f1bae8c9c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DDD0D6570>]}
[0m16:53:23.669748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5276a6cf-b6ac-41e9-a5c9-65f1bae8c9c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DDD1EAC60>]}
[0m16:53:23.671757 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m16:53:23.689875 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:53:23.775505 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:53:23.777567 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:53:23.794703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5276a6cf-b6ac-41e9-a5c9-65f1bae8c9c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DDD321310>]}
[0m16:53:23.807997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5276a6cf-b6ac-41e9-a5c9-65f1bae8c9c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DDD290CE0>]}
[0m16:53:23.809508 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:53:23.811024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5276a6cf-b6ac-41e9-a5c9-65f1bae8c9c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DDCF93CE0>]}
[0m16:53:23.816670 [info ] [MainThread]: 
[0m16:53:23.819197 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m16:53:23.824254 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m16:53:27.314973 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:53:27.316989 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m16:53:27.317994 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:53:27.320021 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:53:36.800494 [debug] [ThreadPool]: SQL status: OK in 9.479999542236328 seconds
[0m16:53:40.258386 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m16:53:40.260396 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m16:53:40.265435 [error] [MainThread]: Encountered an error:
can only concatenate str (not "int") to str
[0m16:53:40.318967 [error] [MainThread]: Traceback (most recent call last):
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 302, in exception_handler
    yield
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 310, in execute
    return self.connections.execute(sql=sql, auto_begin=auto_begin, fetch=fetch, limit=limit)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\sql\connections.py", line 141, in execute
    table = self.get_result_from_cursor(cursor, limit)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\sql\connections.py", line 130, in get_result_from_cursor
    data = cls.process_results(column_names, rows)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\sql\connections.py", line 117, in process_results
    return [dict(zip(column_names, row)) for row in rows]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'NoneType' object is not iterable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 434, in execute_with_hooks
    self.before_run(adapter, selected_uids)
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 421, in before_run
    self.populate_adapter_cache(adapter)
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 412, in populate_adapter_cache
    adapter.set_relations_cache(self.manifest)
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 519, in set_relations_cache
    self._relations_cache_for_schemas(manifest, required_schemas)
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 495, in _relations_cache_for_schemas
    for relation in future.result():
                    ^^^^^^^^^^^^^^^
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\utils.py", line 471, in connected
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricspark\impl.py", line 213, in list_relations_without_caching
    raise(e)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricspark\impl.py", line 207, in list_relations_without_caching
    show_table_extended_rows = self.execute_macro(LIST_RELATIONS_MACRO_NAME, kwargs=kwargs)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 1112, in execute_macro
    result = macro_function(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 21, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 33, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 255, in call_macro
    with self.exception_handler():
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 304, in exception_handler
    raise CaughtMacroErrorWithNodeError(exc=e, node=self.macro)
dbt.exceptions.CaughtMacroErrorWithNodeError: Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 90, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 75, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 197, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 244, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 284, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\main.py", line 371, in compile
    results = task.run()
              ^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 474, in run
    result = self.execute_with_hooks(selected_uids)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 438, in execute_with_hooks
    adapter.cleanup_connections()
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 261, in cleanup_connections
    self.connections.cleanup_all()
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricspark\connections.py", line 207, in cleanup_all
    livySession.disconnect()
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricspark\livysession.py", line 492, in disconnect
    if __class__.livy_global_session.is_valid_session():
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricspark\livysession.py", line 198, in is_valid_session
    self.connect_url + "/sessions/" + self.session_id,
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~
TypeError: can only concatenate str (not "int") to str

[0m16:53:40.327021 [debug] [MainThread]: Command `cli compile` failed at 16:53:40.326015 after 17.22 seconds
[0m16:53:40.329042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DDD489B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DDD489A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022DDD489B80>]}
[0m16:53:40.331064 [debug] [MainThread]: Flushing usage events
[0m16:54:40.363654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018779D9C560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018779D9C1A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018779D9C050>]}


============================== 16:54:40.365671 | 5b8d5185-dab7-472d-9c2d-869a5701960d ==============================
[0m16:54:40.365671 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:54:40.367682 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:54:40.678482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5b8d5185-dab7-472d-9c2d-869a5701960d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018779E28380>]}
[0m16:54:40.840881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5b8d5185-dab7-472d-9c2d-869a5701960d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018779EB6DE0>]}
[0m16:54:40.842895 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m16:54:40.867538 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:54:41.387393 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:54:41.388449 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:54:41.407086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5b8d5185-dab7-472d-9c2d-869a5701960d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018779F4D850>]}
[0m16:54:41.419691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5b8d5185-dab7-472d-9c2d-869a5701960d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001877A0652B0>]}
[0m16:54:41.420738 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:54:41.422759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5b8d5185-dab7-472d-9c2d-869a5701960d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018778FA0E00>]}
[0m16:54:41.428823 [info ] [MainThread]: 
[0m16:54:41.430883 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m16:54:41.435930 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m16:54:45.118911 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:54:45.119920 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m16:54:45.121941 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:54:45.122949 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:54:52.008568 [debug] [ThreadPool]: SQL status: OK in 6.889999866485596 seconds
[0m16:54:54.711765 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m16:54:54.713779 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m16:54:54.717809 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Closing the livy session: 0
[0m16:54:54.719822 [error] [MainThread]: Microsoft Fabric-Spark adapter: Unable to close the livy session 0, error: can only concatenate str (not "int") to str
[0m16:54:59.406282 [error] [MainThread]: Encountered an error:
Compilation Error
  'NoneType' object is not iterable
  
  > in macro fabricspark__list_relations_without_caching (macros\adapters\relation.sql)
  > called by macro list_relations_without_caching (macros\adapters\metadata.sql)
  > called by <Unknown>
[0m16:54:59.411337 [debug] [MainThread]: Command `cli compile` failed at 16:54:59.410331 after 19.14 seconds
[0m16:54:59.413346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018779D9DB50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001877A2699A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001877A2699D0>]}
[0m16:54:59.414351 [debug] [MainThread]: Flushing usage events
[0m16:55:29.592497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B24CC680E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B24CC69BB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B24CC69CD0>]}


============================== 16:55:29.594505 | c68b6f4c-78c6-4c83-bd37-e22f9d6e50e4 ==============================
[0m16:55:29.594505 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:55:29.596819 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:55:29.905838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c68b6f4c-78c6-4c83-bd37-e22f9d6e50e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B24CC69C40>]}
[0m16:55:30.070479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c68b6f4c-78c6-4c83-bd37-e22f9d6e50e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B24CEC2C90>]}
[0m16:55:30.073499 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m16:55:30.093247 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:55:30.183139 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:55:30.184144 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:55:30.201274 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c68b6f4c-78c6-4c83-bd37-e22f9d6e50e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B24CEE0AA0>]}
[0m16:55:30.214436 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c68b6f4c-78c6-4c83-bd37-e22f9d6e50e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B24CF34DD0>]}
[0m16:55:30.216454 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:55:30.218496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c68b6f4c-78c6-4c83-bd37-e22f9d6e50e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B24CD7E690>]}
[0m16:55:30.223547 [info ] [MainThread]: 
[0m16:55:30.226661 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m16:55:30.230695 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m16:55:33.050363 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:55:33.052376 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m16:55:33.053386 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:55:33.055413 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:55:59.795879 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:55:59.797895 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'LivySessionConnectionWrapper' object has no attribute 'SetProfile'
[0m16:55:59.800920 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m16:55:59.801931 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  'LivySessionConnectionWrapper' object has no attribute 'SetProfile'
[0m16:55:59.806965 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Closing the livy session: 0
[0m16:55:59.807972 [error] [MainThread]: Microsoft Fabric-Spark adapter: Unable to close the livy session 0, error: can only concatenate str (not "int") to str
[0m16:56:01.793714 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    'LivySessionConnectionWrapper' object has no attribute 'SetProfile'
[0m16:56:01.797757 [debug] [MainThread]: Command `cli compile` failed at 16:56:01.796748 after 32.29 seconds
[0m16:56:01.800800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B24CC6B380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B24D0EDFA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B24D0EDFD0>]}
[0m16:56:01.802814 [debug] [MainThread]: Flushing usage events
[0m16:56:48.665205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002626D751850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002626D7519D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002626D751A30>]}


============================== 16:56:48.667218 | bc36989b-f074-4cdd-a686-e0f8e4e3e88f ==============================
[0m16:56:48.667218 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:56:48.669231 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:56:48.969882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bc36989b-f074-4cdd-a686-e0f8e4e3e88f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002626D7525A0>]}
[0m16:56:49.129900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bc36989b-f074-4cdd-a686-e0f8e4e3e88f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002626D9BFF80>]}
[0m16:56:49.131910 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m16:56:49.151062 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:56:49.232913 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:56:49.233919 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:56:49.251081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bc36989b-f074-4cdd-a686-e0f8e4e3e88f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002626D750590>]}
[0m16:56:49.266421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bc36989b-f074-4cdd-a686-e0f8e4e3e88f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002626DA251F0>]}
[0m16:56:49.267430 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:56:49.269442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bc36989b-f074-4cdd-a686-e0f8e4e3e88f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002626D733AA0>]}
[0m16:56:49.275478 [info ] [MainThread]: 
[0m16:56:49.277511 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m16:56:49.282562 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m16:56:51.977826 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:56:51.979838 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m16:56:51.980844 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:56:51.981906 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:58:41.929140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002693871C2F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002693871C0E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002693871D340>]}


============================== 16:58:41.932169 | 0ac9a818-d1e6-460a-81db-5cd8db33b93d ==============================
[0m16:58:41.932169 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:58:41.933178 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:58:42.240372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0ac9a818-d1e6-460a-81db-5cd8db33b93d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269388468D0>]}
[0m16:58:42.406781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0ac9a818-d1e6-460a-81db-5cd8db33b93d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002693895F200>]}
[0m16:58:42.408793 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m16:58:42.426980 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:58:42.508743 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:58:42.510753 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:58:42.527922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0ac9a818-d1e6-460a-81db-5cd8db33b93d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002693895E210>]}
[0m16:58:42.541045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0ac9a818-d1e6-460a-81db-5cd8db33b93d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026938A08BF0>]}
[0m16:58:42.543086 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:58:42.546147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ac9a818-d1e6-460a-81db-5cd8db33b93d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000269377234D0>]}
[0m16:58:42.552697 [info ] [MainThread]: 
[0m16:58:42.555722 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m16:58:42.560763 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m16:58:45.506394 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:58:45.508529 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m16:58:45.510561 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:58:45.511569 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:59:07.573162 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:59:07.575787 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: can only concatenate str (not "int") to str
[0m16:59:07.581338 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m16:59:07.583378 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  can only concatenate str (not "int") to str
[0m16:59:07.589434 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Closing the livy session: 0
[0m16:59:07.591449 [error] [MainThread]: Microsoft Fabric-Spark adapter: Unable to close the livy session 0, error: can only concatenate str (not "int") to str
[0m16:59:09.829811 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    can only concatenate str (not "int") to str
[0m16:59:09.833844 [debug] [MainThread]: Command `cli compile` failed at 16:59:09.832836 after 27.98 seconds
[0m16:59:09.836868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002693871D970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026938C38680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026938C39A30>]}
[0m16:59:09.837873 [debug] [MainThread]: Flushing usage events
[0m17:00:13.732605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001635513C200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001635513D820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001635513D790>]}


============================== 17:00:13.734619 | aaaf2160-1e99-4f02-9109-7d296010fd8a ==============================
[0m17:00:13.734619 [info ] [MainThread]: Running with dbt=1.7.4
[0m17:00:13.736640 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:00:14.042971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'aaaf2160-1e99-4f02-9109-7d296010fd8a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163552C5670>]}
[0m17:00:14.211134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'aaaf2160-1e99-4f02-9109-7d296010fd8a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001635539BB60>]}
[0m17:00:14.214161 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m17:00:14.233399 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m17:00:14.319241 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:00:14.320248 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:00:14.339616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aaaf2160-1e99-4f02-9109-7d296010fd8a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163552C7410>]}
[0m17:00:14.355938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aaaf2160-1e99-4f02-9109-7d296010fd8a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001635540CE90>]}
[0m17:00:14.356973 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:00:14.358992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aaaf2160-1e99-4f02-9109-7d296010fd8a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001635540C5F0>]}
[0m17:00:14.366241 [info ] [MainThread]: 
[0m17:00:14.369282 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m17:00:14.373306 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m17:00:16.568493 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:00:16.569497 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m17:00:16.570547 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m17:00:16.572561 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:00:19.238722 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Submitted: {'code': "show table extended in test like '*'", 'kind': 'sql'} https://api.fabric.microsoft.com/v1/workspaces/bab084ca-748d-438e-94ad-405428bd5694/lakehouses/ccb45a7d-60fc-447b-b1d3-713e05f55e9a/livyapi/versions/2023-12-01/sessions/0/statements
[0m17:00:19.240740 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Using CLI auth
[0m17:00:20.030105 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: CLI - Fetched Access Token
[0m17:00:21.258790 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m17:00:21.259797 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'id'
[0m17:00:21.261810 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_relations_without_caching
[0m17:00:21.262819 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  'id'
[0m17:00:21.267943 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Closing the livy session: 0
[0m17:00:22.532298 [error] [MainThread]: Microsoft Fabric-Spark adapter: Unable to close the livy session 0, error: module 'urllib.response' has no attribute 'raise_for_status'
[0m17:00:24.689821 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    'id'
[0m17:00:24.693857 [debug] [MainThread]: Command `cli compile` failed at 17:00:24.693857 after 11.05 seconds
[0m17:00:24.696908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001635513FF20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163555CD790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163555CE5A0>]}
[0m17:00:24.698923 [debug] [MainThread]: Flushing usage events
[0m17:01:39.072797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D6A90650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D6A901D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D6A900E0>]}


============================== 17:01:39.075830 | 4e6f6e8d-3a53-417e-98b9-05ae29c54a69 ==============================
[0m17:01:39.075830 [info ] [MainThread]: Running with dbt=1.7.4
[0m17:01:39.076837 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m17:01:39.392092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4e6f6e8d-3a53-417e-98b9-05ae29c54a69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D6BF5A30>]}
[0m17:01:39.557153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4e6f6e8d-3a53-417e-98b9-05ae29c54a69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D6CFF890>]}
[0m17:01:39.560175 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m17:01:39.580756 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m17:01:39.673869 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:01:39.674872 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:01:39.693553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4e6f6e8d-3a53-417e-98b9-05ae29c54a69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D6C94AA0>]}
[0m17:01:39.707674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4e6f6e8d-3a53-417e-98b9-05ae29c54a69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D6D55AF0>]}
[0m17:01:39.709754 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:01:39.711783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4e6f6e8d-3a53-417e-98b9-05ae29c54a69', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D6D550A0>]}
[0m17:01:39.717883 [info ] [MainThread]: 
[0m17:01:39.719912 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m17:01:39.724947 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m17:01:43.653297 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:01:43.655321 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m17:01:43.656330 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m17:01:43.657337 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:01:49.256971 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Submitted: {'code': "show table extended in test like '*'", 'kind': 'sql'} https://api.fabric.microsoft.com/v1/workspaces/bab084ca-748d-438e-94ad-405428bd5694/lakehouses/ccb45a7d-60fc-447b-b1d3-713e05f55e9a/livyapi/versions/2023-12-01/sessions/0/statements
[0m17:01:49.258987 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Using CLI auth
[0m17:01:49.821502 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: CLI - Fetched Access Token
[0m17:04:07.084937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B7A995D00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B7A994F20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B79F681D0>]}


============================== 17:04:07.086948 | 5675d013-b5a6-45c8-b7a1-f4a8f0e2837c ==============================
[0m17:04:07.086948 [info ] [MainThread]: Running with dbt=1.7.4
[0m17:04:07.089020 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m17:04:07.398605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5675d013-b5a6-45c8-b7a1-f4a8f0e2837c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B7A77AC60>]}
[0m17:04:07.560472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5675d013-b5a6-45c8-b7a1-f4a8f0e2837c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B7A944440>]}
[0m17:04:07.562488 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m17:04:07.580745 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m17:04:07.668012 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:04:07.670026 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:04:07.687110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5675d013-b5a6-45c8-b7a1-f4a8f0e2837c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B7AB9C560>]}
[0m17:04:07.700187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5675d013-b5a6-45c8-b7a1-f4a8f0e2837c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B7ABFDAF0>]}
[0m17:04:07.701192 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:04:07.703201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5675d013-b5a6-45c8-b7a1-f4a8f0e2837c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B7A6C14C0>]}
[0m17:04:07.708226 [info ] [MainThread]: 
[0m17:04:07.711241 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m17:04:07.717319 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m17:04:11.442450 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:04:11.443455 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m17:04:11.444461 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m17:04:11.446554 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:04:35.762792 [debug] [ThreadPool]: SQL status: OK in 24.31999969482422 seconds
[0m17:04:35.773899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5675d013-b5a6-45c8-b7a1-f4a8f0e2837c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019B7ABFDF70>]}
[0m17:04:35.776920 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-dev')
[0m17:04:35.777970 [info ] [MainThread]: 
[0m17:04:35.785032 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m17:04:35.787045 [debug] [Thread-7 (]: Acquiring new fabricspark connection 'model.testproj.my_first_dbt_model'
[0m17:04:35.789057 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:04:35.823430 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:04:35.825454 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:04:35.790071 => 17:04:35.825454
[0m17:04:35.827477 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:04:35.828542 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:04:35.828542 => 17:04:35.828542
[0m17:04:35.831765 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:04:35.833786 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m17:04:35.835800 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m17:04:35.837824 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:04:35.847055 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:04:35.850085 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:04:35.838833 => 17:04:35.849076
[0m17:04:35.851095 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:04:35.853112 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:04:35.852105 => 17:04:35.852105
[0m17:04:35.855127 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:04:35.857138 [debug] [Thread-7 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:04:35.859191 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:04:35.861201 [debug] [Thread-7 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:04:35.889458 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:04:35.891466 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:04:35.862205 => 17:04:35.891466
[0m17:04:35.893475 [debug] [Thread-7 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:04:35.894480 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:04:35.894480 => 17:04:35.894480
[0m17:04:35.897493 [debug] [Thread-7 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:04:35.898557 [debug] [Thread-7 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:04:35.900566 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:04:35.901570 [debug] [Thread-7 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:04:35.918844 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:04:35.920350 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:04:35.902573 => 17:04:35.920350
[0m17:04:35.922425 [debug] [Thread-7 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:04:35.923431 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:04:35.923431 => 17:04:35.923431
[0m17:04:35.926450 [debug] [Thread-7 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:04:35.927455 [debug] [Thread-7 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:04:35.930473 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m17:04:35.931479 [debug] [Thread-7 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:04:35.943636 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:04:35.945649 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 17:04:35.932485 => 17:04:35.945649
[0m17:04:35.947664 [debug] [Thread-7 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:04:35.948673 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 17:04:35.948673 => 17:04:35.948673
[0m17:04:35.951722 [debug] [Thread-7 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:04:35.952730 [debug] [Thread-7 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:04:35.955751 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m17:04:35.956779 [debug] [Thread-7 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:04:35.968312 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:04:35.971333 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 17:04:35.957786 => 17:04:35.971333
[0m17:04:35.973349 [debug] [Thread-7 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:04:35.974357 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 17:04:35.973349 => 17:04:35.973349
[0m17:04:35.977377 [debug] [Thread-7 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:04:35.979390 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Closing the livy session: 0
[0m17:04:35.981414 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Using CLI auth
[0m17:04:36.483995 [debug] [MainThread]: Microsoft Fabric-Spark adapter: CLI - Fetched Access Token
[0m17:04:37.772701 [error] [MainThread]: Microsoft Fabric-Spark adapter: Unable to close the livy session 0, error: module 'urllib.response' has no attribute 'raise_for_status'
[0m17:05:12.707328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D89A9DC1A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D89A9DD6D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D89A9DD760>]}


============================== 17:05:12.709339 | dc531a1a-e4c9-47a0-9d1a-dc5f9574f0a2 ==============================
[0m17:05:12.709339 [info ] [MainThread]: Running with dbt=1.7.4
[0m17:05:12.711353 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:05:13.012883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dc531a1a-e4c9-47a0-9d1a-dc5f9574f0a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D89AA7B7D0>]}
[0m17:05:13.173188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dc531a1a-e4c9-47a0-9d1a-dc5f9574f0a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D89AC0A600>]}
[0m17:05:13.176205 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m17:05:13.196383 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m17:05:13.293482 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:05:13.295493 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:05:13.313597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dc531a1a-e4c9-47a0-9d1a-dc5f9574f0a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D89AC3E8D0>]}
[0m17:05:13.327691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dc531a1a-e4c9-47a0-9d1a-dc5f9574f0a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D89ACB8CB0>]}
[0m17:05:13.329206 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:05:13.330722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dc531a1a-e4c9-47a0-9d1a-dc5f9574f0a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D89A3775C0>]}
[0m17:05:13.336794 [info ] [MainThread]: 
[0m17:05:13.339905 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m17:05:13.344949 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m17:05:15.876392 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:05:15.877396 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m17:05:15.878404 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m17:05:15.880416 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:05:21.714634 [debug] [ThreadPool]: SQL status: OK in 5.829999923706055 seconds
[0m17:05:21.765642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dc531a1a-e4c9-47a0-9d1a-dc5f9574f0a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D89AE5EB10>]}
[0m17:05:21.769704 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-dev')
[0m17:05:21.771735 [info ] [MainThread]: 
[0m17:05:21.780021 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m17:05:21.782042 [debug] [Thread-7 (]: Acquiring new fabricspark connection 'model.testproj.my_first_dbt_model'
[0m17:05:21.784070 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:05:21.817675 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:05:21.819705 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:05:21.785084 => 17:05:21.819705
[0m17:05:21.821717 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:05:21.823730 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:05:21.822724 => 17:05:21.822724
[0m17:05:21.825745 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:05:21.827760 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m17:05:21.830784 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m17:05:21.831868 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:05:21.840938 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:05:21.842956 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:05:21.832871 => 17:05:21.841946
[0m17:05:21.844977 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:05:21.845992 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:05:21.845992 => 17:05:21.845992
[0m17:05:21.850124 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:05:21.852173 [debug] [Thread-7 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:05:21.854195 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:05:21.856212 [debug] [Thread-7 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:05:21.886739 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:05:21.888757 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:05:21.857220 => 17:05:21.888757
[0m17:05:21.890766 [debug] [Thread-7 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:05:21.891775 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:05:21.891775 => 17:05:21.891775
[0m17:05:21.894834 [debug] [Thread-7 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:05:21.896846 [debug] [Thread-7 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:05:21.898858 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:05:21.900949 [debug] [Thread-7 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:05:21.918080 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:05:21.920160 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:05:21.901955 => 17:05:21.919154
[0m17:05:21.921165 [debug] [Thread-7 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:05:21.923175 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:05:21.922170 => 17:05:21.922170
[0m17:05:21.926226 [debug] [Thread-7 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:05:21.927748 [debug] [Thread-7 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:05:21.929778 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m17:05:21.931297 [debug] [Thread-7 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:05:21.943844 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:05:21.946371 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 17:05:21.932310 => 17:05:21.945360
[0m17:05:21.947887 [debug] [Thread-7 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:05:21.949402 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 17:05:21.948392 => 17:05:21.948897
[0m17:05:21.953008 [debug] [Thread-7 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:05:21.955057 [debug] [Thread-7 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:05:21.957111 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m17:05:21.958631 [debug] [Thread-7 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:05:21.970851 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:05:21.972879 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 17:05:21.959643 => 17:05:21.972370
[0m17:05:21.974403 [debug] [Thread-7 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:05:21.975922 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 17:05:21.975415 => 17:05:21.975415
[0m17:05:21.978457 [debug] [Thread-7 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:05:24.848351 [debug] [MainThread]: Command end result
[0m17:05:24.861466 [debug] [MainThread]: Command `cli compile` succeeded at 17:05:24.861466 after 12.23 seconds
[0m17:05:24.863483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D89A6C7140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D89AF31850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D89AF30F80>]}
[0m17:05:24.865496 [debug] [MainThread]: Flushing usage events
[0m17:06:08.451579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE5929C920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE5929C650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE5929C5F0>]}


============================== 17:06:08.453590 | 8eddb369-7024-4035-a0b6-d9d29e07160a ==============================
[0m17:06:08.453590 [info ] [MainThread]: Running with dbt=1.7.4
[0m17:06:08.455601 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:06:08.799431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8eddb369-7024-4035-a0b6-d9d29e07160a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE58FDAC60>]}
[0m17:06:08.963730 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8eddb369-7024-4035-a0b6-d9d29e07160a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE594CC560>]}
[0m17:06:08.965743 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m17:06:08.983981 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m17:06:09.066530 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:06:09.067547 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:06:09.085658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8eddb369-7024-4035-a0b6-d9d29e07160a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE594497C0>]}
[0m17:06:09.098746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8eddb369-7024-4035-a0b6-d9d29e07160a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE5956CF20>]}
[0m17:06:09.099770 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:06:09.101781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8eddb369-7024-4035-a0b6-d9d29e07160a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE59314770>]}
[0m17:06:09.106827 [info ] [MainThread]: 
[0m17:06:09.109887 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m17:06:09.114953 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_schemas'
[0m17:06:09.140244 [debug] [ThreadPool]: Using fabricspark connection "list_schemas"
[0m17:06:09.142254 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:06:09.143333 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:06:15.118156 [debug] [ThreadPool]: SQL status: OK in 5.96999979019165 seconds
[0m17:06:15.127310 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m17:06:15.130322 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m17:06:15.141399 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:06:15.142407 [debug] [ThreadPool]: Using fabricspark connection "create__test"
[0m17:06:15.143415 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "create__test"} */

    select 1
  
[0m17:06:16.658763 [debug] [ThreadPool]: SQL status: OK in 1.5099999904632568 seconds
[0m17:06:16.661798 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:06:16.668891 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m17:06:17.432190 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:06:17.433194 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m17:06:17.435206 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m17:06:17.436211 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:06:17.437217 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m17:06:19.509741 [debug] [ThreadPool]: SQL status: OK in 2.069999933242798 seconds
[0m17:06:19.517879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8eddb369-7024-4035-a0b6-d9d29e07160a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE59742AE0>]}
[0m17:06:19.519891 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:06:19.520898 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:06:19.523926 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-dev')
[0m17:06:19.524934 [info ] [MainThread]: 
[0m17:06:19.532000 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m17:06:19.533006 [info ] [Thread-7 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m17:06:19.536079 [debug] [Thread-7 (]: Acquiring new fabricspark connection 'model.testproj.my_first_dbt_model'
[0m17:06:19.537090 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:06:19.580657 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:06:19.583728 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:06:19.539115 => 17:06:19.582721
[0m17:06:19.585740 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:06:19.640106 [debug] [Thread-7 (]: Using fabricspark connection "model.testproj.my_first_dbt_model"
[0m17:06:19.642119 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m17:06:19.643129 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m17:06:19.645144 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m17:06:23.007171 [debug] [Thread-7 (]: SQL status: OK in 3.359999895095825 seconds
[0m17:06:23.139077 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:06:23.146261 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:06:23.149302 [debug] [Thread-7 (]: Using fabricspark connection "model.testproj.my_first_dbt_model"
[0m17:06:23.150352 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:06:35.755308 [debug] [Thread-7 (]: SQL status: OK in 12.600000381469727 seconds
[0m17:06:35.801971 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:06:19.586745 => 17:06:35.800964
[0m17:06:35.804989 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8eddb369-7024-4035-a0b6-d9d29e07160a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE598259D0>]}
[0m17:06:35.805995 [info ] [Thread-7 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 16.27s]
[0m17:06:35.809015 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:06:35.812141 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m17:06:35.814160 [info ] [Thread-7 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m17:06:35.816188 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m17:06:35.817193 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:06:35.827314 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:06:35.829337 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:06:35.818198 => 17:06:35.829337
[0m17:06:35.831351 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:06:35.872969 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:06:35.876016 [debug] [Thread-7 (]: Using fabricspark connection "model.testproj.my_second_dbt_model"
[0m17:06:35.879045 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m17:06:35.880051 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m17:06:35.887119 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:06:35.832358 => 17:06:35.886112
[0m17:06:35.889132 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8eddb369-7024-4035-a0b6-d9d29e07160a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE59827B90>]}
[0m17:06:35.891148 [info ] [Thread-7 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.07s]
[0m17:06:35.894177 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:06:35.898221 [debug] [MainThread]: On master: ROLLBACK
[0m17:06:35.900246 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:06:35.902273 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m17:06:39.844054 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: rollback
[0m17:06:39.846066 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:06:39.848079 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:06:47.330422 [info ] [MainThread]: 
[0m17:06:47.332440 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 38.22 seconds (38.22s).
[0m17:06:47.336470 [debug] [MainThread]: Command end result
[0m17:06:47.347601 [info ] [MainThread]: 
[0m17:06:47.349614 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:06:47.352166 [info ] [MainThread]: 
[0m17:06:47.353696 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m17:06:47.357266 [debug] [MainThread]: Command `cli run` succeeded at 17:06:47.357266 after 38.99 seconds
[0m17:06:47.359288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE59824170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE59826AE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE59824BC0>]}
[0m17:06:47.360801 [debug] [MainThread]: Flushing usage events
[0m17:07:06.931484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A8C291130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A8C291190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A8C2914F0>]}


============================== 17:07:06.931484 | f557359f-6668-4f8e-b8ce-e015a80c8c29 ==============================
[0m17:07:06.931484 [info ] [MainThread]: Running with dbt=1.7.4
[0m17:07:06.932492 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:07:07.027762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f557359f-6668-4f8e-b8ce-e015a80c8c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A8C3079B0>]}
[0m17:07:07.078727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f557359f-6668-4f8e-b8ce-e015a80c8c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A8C4702F0>]}
[0m17:07:07.078727 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m17:07:07.084918 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m17:07:07.120843 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:07:07.120843 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:07:07.124864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f557359f-6668-4f8e-b8ce-e015a80c8c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A8C473D70>]}
[0m17:07:07.130915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f557359f-6668-4f8e-b8ce-e015a80c8c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A8D4964E0>]}
[0m17:07:07.130915 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:07:07.131923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f557359f-6668-4f8e-b8ce-e015a80c8c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A8C415AC0>]}
[0m17:07:07.132933 [info ] [MainThread]: 
[0m17:07:07.132933 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m17:07:07.133944 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_schemas'
[0m17:07:07.140017 [debug] [ThreadPool]: Using fabricspark connection "list_schemas"
[0m17:07:07.140017 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_schemas"} */

    show databases
  
[0m17:07:07.141022 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:07.141022 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:07:07.142024 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m17:07:07.142024 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m17:07:07.145423 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:07:07.145423 [debug] [ThreadPool]: Using fabricspark connection "create__test"
[0m17:07:07.145423 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "create__test"} */

    select 1
  
[0m17:07:07.146433 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:07:07.146433 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:07:07.148453 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m17:07:07.151497 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:07:07.152524 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m17:07:07.152524 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m17:07:07.152524 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:07:07.152524 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m17:07:07.153531 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:07:07.154538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f557359f-6668-4f8e-b8ce-e015a80c8c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A8B302840>]}
[0m17:07:07.154538 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:07:07.154538 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:07:07.155547 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-dev')
[0m17:07:07.155547 [info ] [MainThread]: 
[0m17:07:07.157559 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:07:07.157559 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m17:07:07.158566 [debug] [Thread-1 (]: Acquiring new fabricspark connection 'model.testproj.my_first_dbt_model'
[0m17:07:07.158566 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:07:07.162595 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:07:07.163601 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:07:07.158566 => 17:07:07.163601
[0m17:07:07.164608 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:07:07.175837 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_first_dbt_model"
[0m17:07:07.175837 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m17:07:07.175837 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:07:07.176839 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m17:07:07.176839 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:07:07.199141 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:07:07.201152 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:07:07.201152 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_first_dbt_model"
[0m17:07:07.201152 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:07:07.201152 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:07:07.211208 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:07:07.164608 => 17:07:07.211208
[0m17:07:07.212225 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f557359f-6668-4f8e-b8ce-e015a80c8c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A8D652A80>]}
[0m17:07:07.213230 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.05s]
[0m17:07:07.213230 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:07:07.214236 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:07:07.214236 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m17:07:07.215245 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m17:07:07.215245 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:07:07.217433 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:07:07.217433 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:07:07.215245 => 17:07:07.217433
[0m17:07:07.218439 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:07:07.227510 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:07:07.227510 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_second_dbt_model"
[0m17:07:07.228517 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-test", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m17:07:07.228517 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:07:07.229524 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:07:07.218439 => 17:07:07.229524
[0m17:07:07.230530 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f557359f-6668-4f8e-b8ce-e015a80c8c29', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A8D651760>]}
[0m17:07:07.230530 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.02s]
[0m17:07:07.231553 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:07:07.232563 [debug] [MainThread]: On master: ROLLBACK
[0m17:07:07.232563 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:07:07.233574 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m17:07:07.233574 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: rollback
[0m17:07:07.233574 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:07:07.234582 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:07:07.234582 [info ] [MainThread]: 
[0m17:07:07.234582 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.10 seconds (0.10s).
[0m17:07:07.235588 [debug] [MainThread]: Command end result
[0m17:07:07.239766 [info ] [MainThread]: 
[0m17:07:07.240771 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:07:07.240771 [info ] [MainThread]: 
[0m17:07:07.241778 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m17:07:07.242295 [debug] [MainThread]: Command `cli run` succeeded at 17:07:07.242295 after 0.33 seconds
[0m17:07:07.242801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A84EF8350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A8B302810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012A8B3E8410>]}
[0m17:07:07.242801 [debug] [MainThread]: Flushing usage events
[0m06:51:38.191926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE07D72B70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE07D72C30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE07D726F0>]}


============================== 06:51:38.192932 | a37f18ed-61a9-4caf-88e5-05d3c0ede896 ==============================
[0m06:51:38.192932 [info ] [MainThread]: Running with dbt=1.7.4
[0m06:51:38.193940 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m06:51:38.308021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a37f18ed-61a9-4caf-88e5-05d3c0ede896', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE07BFD4F0>]}
[0m06:51:38.363359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a37f18ed-61a9-4caf-88e5-05d3c0ede896', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE00C5CB00>]}
[0m06:51:38.364373 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m06:51:38.374859 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m06:51:38.390100 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m06:51:38.390100 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m06:51:38.391107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a37f18ed-61a9-4caf-88e5-05d3c0ede896', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE08F4EF60>]}
[0m06:51:39.623918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a37f18ed-61a9-4caf-88e5-05d3c0ede896', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE08F70AA0>]}
[0m06:51:39.639157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a37f18ed-61a9-4caf-88e5-05d3c0ede896', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE091F3170>]}
[0m06:51:39.640165 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m06:51:39.640165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a37f18ed-61a9-4caf-88e5-05d3c0ede896', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE08FC80E0>]}
[0m06:51:39.641173 [info ] [MainThread]: 
[0m06:51:39.643188 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m06:51:39.645225 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_schemas'
[0m06:51:39.652820 [debug] [ThreadPool]: Using fabricspark connection "list_schemas"
[0m06:51:39.652820 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "connection_name": "list_schemas"} */

    show databases
  
[0m06:51:39.652820 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:51:39.653828 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m06:51:39.654835 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m06:51:39.655842 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m06:51:39.657856 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:51:39.658863 [debug] [ThreadPool]: Using fabricspark connection "create__test"
[0m06:51:39.658863 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "connection_name": "create__test"} */

    select 1
  
[0m06:51:39.658863 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m06:51:39.660888 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:51:39.661904 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m06:51:39.665990 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:51:39.666999 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m06:51:39.666999 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m06:51:39.666999 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m06:51:39.668009 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m06:51:39.668009 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m06:51:39.669024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a37f18ed-61a9-4caf-88e5-05d3c0ede896', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE09039670>]}
[0m06:51:39.670034 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:51:39.670034 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:51:39.670034 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-dev')
[0m06:51:39.671042 [info ] [MainThread]: 
[0m06:51:39.674063 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m06:51:39.674063 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m06:51:39.675070 [debug] [Thread-1 (]: Acquiring new fabricspark connection 'model.testproj.my_first_dbt_model'
[0m06:51:39.675070 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m06:51:39.679106 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m06:51:39.681151 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 06:51:39.675070 => 06:51:39.680120
[0m06:51:39.681151 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m06:51:39.692232 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_first_dbt_model"
[0m06:51:39.693240 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m06:51:39.693240 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m06:51:39.694254 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m06:51:39.694254 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m06:51:39.720678 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m06:51:39.721685 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:51:39.721685 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_first_dbt_model"
[0m06:51:39.721685 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m06:51:39.722694 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m06:51:39.733863 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 06:51:39.681151 => 06:51:39.732852
[0m06:51:39.734873 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a37f18ed-61a9-4caf-88e5-05d3c0ede896', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE093348C0>]}
[0m06:51:39.734873 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.06s]
[0m06:51:39.735884 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m06:51:39.736892 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m06:51:39.736892 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m06:51:39.737900 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m06:51:39.737900 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m06:51:39.739915 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m06:51:39.740923 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 06:51:39.737900 => 06:51:39.740923
[0m06:51:39.740923 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m06:51:39.752117 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m06:51:39.753131 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_second_dbt_model"
[0m06:51:39.753131 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m06:51:39.754144 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m06:51:39.755157 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 06:51:39.741971 => 06:51:39.755157
[0m06:51:39.756170 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a37f18ed-61a9-4caf-88e5-05d3c0ede896', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE092F94F0>]}
[0m06:51:39.756170 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.02s]
[0m06:51:39.757175 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m06:51:39.758186 [debug] [MainThread]: On master: ROLLBACK
[0m06:51:39.758698 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:51:39.759204 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m06:51:39.759711 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: rollback
[0m06:51:39.760220 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:51:39.760729 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:51:39.761744 [info ] [MainThread]: 
[0m06:51:39.761744 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.12 seconds (0.12s).
[0m06:51:39.762760 [debug] [MainThread]: Command end result
[0m06:51:39.768845 [info ] [MainThread]: 
[0m06:51:39.769856 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:51:39.769856 [info ] [MainThread]: 
[0m06:51:39.770867 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m06:51:39.771879 [debug] [MainThread]: Command `cli run` succeeded at 06:51:39.771879 after 1.63 seconds
[0m06:51:39.772911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE07B93290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE0637E600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AE091BA4B0>]}
[0m06:51:39.772911 [debug] [MainThread]: Flushing usage events
[0m06:52:03.532227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBB7EF19A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBB7EF1AF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBB7EF1940>]}


============================== 06:52:03.533237 | 42b17093-f977-4387-8033-3dd9fbbd9b19 ==============================
[0m06:52:03.533237 [info ] [MainThread]: Running with dbt=1.7.4
[0m06:52:03.533237 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m06:52:03.538325 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-dev" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m06:52:03.539332 [debug] [MainThread]: Command `cli run` failed at 06:52:03.538325 after 0.04 seconds
[0m06:52:03.539332 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBB7CE6270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBB77E0350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBB7EF0B00>]}
[0m06:52:03.539332 [debug] [MainThread]: Flushing usage events
[0m06:57:03.828373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167D1DE7B60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167D1DE41A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167D1DE6EA0>]}


============================== 06:57:03.829380 | 6d51e2d0-4571-4e3b-a4b2-d6329f80c516 ==============================
[0m06:57:03.829380 [info ] [MainThread]: Running with dbt=1.7.4
[0m06:57:03.829380 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m06:57:03.833436 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-dev" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m06:57:03.834448 [debug] [MainThread]: Command `cli run` failed at 06:57:03.834448 after 0.03 seconds
[0m06:57:03.834448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167D1DE63F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167D0BC57C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000167D0BC7FB0>]}
[0m06:57:03.835457 [debug] [MainThread]: Flushing usage events
[0m15:11:33.533524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB1614D0D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB1614D490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB1614D070>]}


============================== 15:11:33.533524 | 5c28ccaf-2108-4cb4-809e-dbf3a7a4e250 ==============================
[0m15:11:33.533524 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:11:33.534532 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:11:33.538081 [info ] [MainThread]: Error importing adapter: No module named 'dbt.adapters.postgres'
[0m15:11:33.538587 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-dev" invalid: Runtime Error
    Could not find adapter type postgres!
[0m15:11:33.539607 [debug] [MainThread]: Command `cli run` failed at 15:11:33.539607 after 0.05 seconds
[0m15:11:33.539607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB1605F7D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB15EDCFB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB1608FB00>]}
[0m15:11:33.539607 [debug] [MainThread]: Flushing usage events
[0m15:13:48.955498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B3A9A990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B3A9B290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B3A9A870>]}


============================== 15:13:48.956508 | 0eb9a078-2c20-4212-afd8-ad3eace0548f ==============================
[0m15:13:48.956508 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:13:48.956508 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:13:49.060250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0eb9a078-2c20-4212-afd8-ad3eace0548f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B4B48740>]}
[0m15:13:49.109592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0eb9a078-2c20-4212-afd8-ad3eace0548f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B370B0E0>]}
[0m15:13:49.110598 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:13:49.122713 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:13:49.135810 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m15:13:49.136817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0eb9a078-2c20-4212-afd8-ad3eace0548f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B4BAEF60>]}
[0m15:13:50.119070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0eb9a078-2c20-4212-afd8-ad3eace0548f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B4FD4080>]}
[0m15:13:50.124106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0eb9a078-2c20-4212-afd8-ad3eace0548f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B4EBA2A0>]}
[0m15:13:50.124106 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:13:50.124106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0eb9a078-2c20-4212-afd8-ad3eace0548f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B38FE720>]}
[0m15:13:50.126130 [info ] [MainThread]: 
[0m15:13:50.126130 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:13:50.127138 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_schemas'
[0m15:13:50.133181 [debug] [ThreadPool]: Using fabricspark connection "list_schemas"
[0m15:13:50.133181 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:13:50.133181 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:13:50.133181 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:13:50.135195 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m15:13:50.135195 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m15:13:50.137209 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:13:50.138216 [debug] [ThreadPool]: Using fabricspark connection "create__test"
[0m15:13:50.138216 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "connection_name": "create__test"} */

    select 1
  
[0m15:13:50.138216 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:13:50.139223 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:13:50.140229 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:13:50.143269 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:13:50.144279 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:13:50.144279 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:13:50.144279 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:13:50.145287 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m15:13:50.145287 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:13:50.146290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0eb9a078-2c20-4212-afd8-ad3eace0548f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B4CD4BC0>]}
[0m15:13:50.146793 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:13:50.146793 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:13:50.147801 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-dev')
[0m15:13:50.147801 [info ] [MainThread]: 
[0m15:13:50.149816 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:13:50.150823 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m15:13:50.150823 [debug] [Thread-1 (]: Acquiring new fabricspark connection 'model.testproj.my_first_dbt_model'
[0m15:13:50.151831 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:13:50.155857 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:13:50.156863 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:13:50.151831 => 15:13:50.156863
[0m15:13:50.156863 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:13:50.167960 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_first_dbt_model"
[0m15:13:50.168967 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m15:13:50.168967 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:13:50.168967 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m15:13:50.169974 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:13:50.191161 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:13:50.193177 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:13:50.193177 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_first_dbt_model"
[0m15:13:50.193177 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:13:50.194186 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:13:50.204283 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:13:50.157883 => 15:13:50.204283
[0m15:13:50.205290 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0eb9a078-2c20-4212-afd8-ad3eace0548f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B4FD3D40>]}
[0m15:13:50.205290 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.05s]
[0m15:13:50.206297 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:13:50.206297 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:13:50.207303 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m15:13:50.207303 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:13:50.208310 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:13:50.210327 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:13:50.211342 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:13:50.208310 => 15:13:50.211342
[0m15:13:50.211342 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:13:50.220457 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:13:50.221468 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_second_dbt_model"
[0m15:13:50.221468 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m15:13:50.222481 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:13:50.223537 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:13:50.211342 => 15:13:50.223537
[0m15:13:50.223537 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0eb9a078-2c20-4212-afd8-ad3eace0548f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B50A0740>]}
[0m15:13:50.224600 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.02s]
[0m15:13:50.224600 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:13:50.226679 [debug] [MainThread]: On master: ROLLBACK
[0m15:13:50.226679 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:13:50.226679 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m15:13:50.227691 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: rollback
[0m15:13:50.227691 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:13:50.227691 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:13:50.228698 [info ] [MainThread]: 
[0m15:13:50.228698 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.10 seconds (0.10s).
[0m15:13:50.229707 [debug] [MainThread]: Command end result
[0m15:13:50.234762 [info ] [MainThread]: 
[0m15:13:50.235769 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:13:50.235769 [info ] [MainThread]: 
[0m15:13:50.236776 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:13:50.237785 [debug] [MainThread]: Command `cli run` succeeded at 15:13:50.237785 after 1.31 seconds
[0m15:13:50.237785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B2A9E510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B39263C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3B3855B80>]}
[0m15:13:50.238790 [debug] [MainThread]: Flushing usage events
[0m15:34:48.625127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC01DEFC80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC01DEE840>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC01ECCD40>]}


============================== 15:34:48.626134 | 853c344d-e6d7-4d52-8429-6664b86dd864 ==============================
[0m15:34:48.626134 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:34:48.627141 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:34:48.631198 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-dev" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type NotImplemented
[0m15:34:48.632212 [debug] [MainThread]: Command `cli run` failed at 15:34:48.632212 after 0.05 seconds
[0m15:34:48.633223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC01E1E510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC01C6CA40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC004ADF70>]}
[0m15:34:48.633223 [debug] [MainThread]: Flushing usage events
[0m15:39:42.155459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1B224D550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1B224D820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1B224D2B0>]}


============================== 15:39:42.155459 | b223f9e0-665f-4db4-9d85-2d97280d7b45 ==============================
[0m15:39:42.155459 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:39:42.156472 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:39:42.160507 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-dev" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type NotImplemented
[0m15:39:42.162531 [debug] [MainThread]: Command `cli run` failed at 15:39:42.161515 after 0.03 seconds
[0m15:39:42.162531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1B1BCEDB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1B1BF8F20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B1B216D6D0>]}
[0m15:39:42.162531 [debug] [MainThread]: Flushing usage events
[0m15:41:12.365124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A33AB1A030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A33AB1A330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A33AB19FD0>]}


============================== 15:41:12.366142 | 40ce537c-186a-41ff-a944-b41e59644df3 ==============================
[0m15:41:12.366142 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:41:12.367155 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m15:41:12.371186 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-dev" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type NotImplemented
[0m15:41:12.372193 [debug] [MainThread]: Command `cli run` failed at 15:41:12.371186 after 0.03 seconds
[0m15:41:12.372193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A33AB19520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A33A4263C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A33A3346B0>]}
[0m15:41:12.372193 [debug] [MainThread]: Flushing usage events
[0m15:41:26.438358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF255AA1E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF255AA4E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF255AA180>]}


============================== 15:41:26.439366 | 246adcc3-afaa-46d5-877d-6ae9f2e8ca72 ==============================
[0m15:41:26.439366 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:41:26.439366 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:41:26.443398 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-dev" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type NotImplemented
[0m15:41:26.444405 [debug] [MainThread]: Command `cli run` failed at 15:41:26.444405 after 0.03 seconds
[0m15:41:26.445412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF255A9DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF246B8560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AF2522BDA0>]}
[0m15:41:26.445412 [debug] [MainThread]: Flushing usage events
[0m15:41:50.201854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FF2AA6AE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FF2AA55E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FF2AA6A50>]}


============================== 15:41:50.201854 | b7efd454-57c6-4677-ad6a-5fdeb375e8e3 ==============================
[0m15:41:50.201854 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:41:50.202858 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m15:41:50.206908 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-dev" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type NotImplemented
[0m15:41:50.207926 [debug] [MainThread]: Command `cli run` failed at 15:41:50.207926 after 0.03 seconds
[0m15:41:50.208942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FF2A33200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FF1F617C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FF2AA6AE0>]}
[0m15:41:50.208942 [debug] [MainThread]: Flushing usage events
[0m15:42:33.174100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A11F9E60F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A11F9E5820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A11F9E4D10>]}


============================== 15:42:33.174100 | cdaf650b-0cf8-4b16-a243-8927361e0643 ==============================
[0m15:42:33.174100 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:42:33.175106 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:42:33.277899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cdaf650b-0cf8-4b16-a243-8927361e0643', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A11EFA88C0>]}
[0m15:42:33.330861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cdaf650b-0cf8-4b16-a243-8927361e0643', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A120B03B30>]}
[0m15:42:33.332879 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:42:33.346065 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:42:33.886380 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:42:33.886380 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:42:33.890407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cdaf650b-0cf8-4b16-a243-8927361e0643', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A11FA73140>]}
[0m15:42:33.901505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cdaf650b-0cf8-4b16-a243-8927361e0643', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A120BF61E0>]}
[0m15:42:33.902514 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:42:33.902514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cdaf650b-0cf8-4b16-a243-8927361e0643', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A120B44860>]}
[0m15:42:33.903524 [info ] [MainThread]: 
[0m15:42:33.904534 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:42:33.905541 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_schemas'
[0m15:42:33.910571 [debug] [ThreadPool]: Using fabricspark connection "list_schemas"
[0m15:42:33.910571 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:42:33.911577 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:42:33.911577 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:42:33.912599 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m15:42:33.913607 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m15:42:33.915623 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:42:33.915623 [debug] [ThreadPool]: Using fabricspark connection "create__test"
[0m15:42:33.916632 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "connection_name": "create__test"} */

    select 1
  
[0m15:42:33.916632 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:42:33.917640 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:42:33.918648 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:42:33.921667 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:42:33.921667 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:42:33.921667 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:42:33.921667 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:42:33.922674 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m15:42:33.922674 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:42:33.923682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cdaf650b-0cf8-4b16-a243-8927361e0643', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A120B44470>]}
[0m15:42:33.923682 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:42:33.923682 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:42:33.924689 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-dev')
[0m15:42:33.924689 [info ] [MainThread]: 
[0m15:42:33.926705 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:42:33.926705 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m15:42:33.927730 [debug] [Thread-1 (]: Acquiring new fabricspark connection 'model.testproj.my_first_dbt_model'
[0m15:42:33.927730 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:42:33.931776 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:42:33.932791 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:42:33.927730 => 15:42:33.932791
[0m15:42:33.933808 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:42:33.945947 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_first_dbt_model"
[0m15:42:33.945947 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m15:42:33.945947 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:42:33.945947 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m15:42:33.946956 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:42:33.969163 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:42:33.970169 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:42:33.971177 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_first_dbt_model"
[0m15:42:33.971177 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:42:33.971177 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:42:33.981262 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:42:33.933808 => 15:42:33.981262
[0m15:42:33.981772 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdaf650b-0cf8-4b16-a243-8927361e0643', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A120DE2150>]}
[0m15:42:33.982280 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.05s]
[0m15:42:33.982280 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:42:33.983295 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:42:33.983295 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m15:42:33.984306 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:42:33.984306 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:42:33.986327 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:42:33.987341 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:42:33.984306 => 15:42:33.987341
[0m15:42:33.987341 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:42:33.996455 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:42:33.997467 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_second_dbt_model"
[0m15:42:33.997467 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m15:42:33.997467 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:42:33.999493 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:42:33.987341 => 15:42:33.999493
[0m15:42:34.000508 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdaf650b-0cf8-4b16-a243-8927361e0643', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A120DE2990>]}
[0m15:42:34.001529 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.02s]
[0m15:42:34.002576 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:42:34.003607 [debug] [MainThread]: On master: ROLLBACK
[0m15:42:34.003607 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:42:34.003607 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m15:42:34.003607 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: rollback
[0m15:42:34.004619 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:42:34.004619 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:42:34.004619 [info ] [MainThread]: 
[0m15:42:34.005628 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.10 seconds (0.10s).
[0m15:42:34.005628 [debug] [MainThread]: Command end result
[0m15:42:34.010667 [info ] [MainThread]: 
[0m15:42:34.010667 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:42:34.011676 [info ] [MainThread]: 
[0m15:42:34.011676 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:42:34.012686 [debug] [MainThread]: Command `cli run` succeeded at 15:42:34.012686 after 0.86 seconds
[0m15:42:34.012686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A120C2E360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A120D70A40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A120CBF3E0>]}
[0m15:42:34.013696 [debug] [MainThread]: Flushing usage events
[0m15:43:37.597572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000158F9CA1AC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000158F9CA1A30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000158F9CA0950>]}


============================== 15:43:37.601624 | b32b9230-f7f4-42a8-931d-03169ce7509d ==============================
[0m15:43:37.601624 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:43:37.602631 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:43:37.936959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b32b9230-f7f4-42a8-931d-03169ce7509d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000158F9CEA840>]}
[0m15:43:38.102350 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b32b9230-f7f4-42a8-931d-03169ce7509d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000158F9ED0AA0>]}
[0m15:43:38.105370 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:43:38.125535 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:43:38.215322 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:43:38.217335 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:43:38.234469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b32b9230-f7f4-42a8-931d-03169ce7509d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000158F9F240E0>]}
[0m15:43:38.247582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b32b9230-f7f4-42a8-931d-03169ce7509d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000158F9F976B0>]}
[0m15:43:38.249597 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:43:38.251621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b32b9230-f7f4-42a8-931d-03169ce7509d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000158F9C6E750>]}
[0m15:43:38.256656 [info ] [MainThread]: 
[0m15:43:38.259674 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:43:38.263705 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_schemas'
[0m15:43:38.289939 [debug] [ThreadPool]: Using fabricspark connection "list_schemas"
[0m15:43:38.291960 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "connection_name": "list_schemas"} */

    show databases
  
[0m15:43:38.292968 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:43:38.293973 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:43:38.303062 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m15:43:38.305070 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m15:43:38.315662 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:43:38.317675 [debug] [ThreadPool]: Using fabricspark connection "create__test"
[0m15:43:38.318683 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "connection_name": "create__test"} */

    select 1
  
[0m15:43:38.319688 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:43:38.323060 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:43:38.328110 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:43:44.877939 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:43:44.878947 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:43:44.881023 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:43:44.882036 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:43:44.884129 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m15:43:44.885145 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:43:44.891209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b32b9230-f7f4-42a8-931d-03169ce7509d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000158F9F95AC0>]}
[0m15:43:44.893274 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:43:44.895304 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:43:44.898340 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-dev')
[0m15:43:44.899357 [info ] [MainThread]: 
[0m15:43:44.909481 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m15:43:44.912550 [info ] [Thread-7 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m15:43:44.915583 [debug] [Thread-7 (]: Acquiring new fabricspark connection 'model.testproj.my_first_dbt_model'
[0m15:43:44.917614 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:43:44.952171 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:43:44.955210 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:43:44.918645 => 15:43:44.954197
[0m15:43:44.956245 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:43:45.012867 [debug] [Thread-7 (]: Using fabricspark connection "model.testproj.my_first_dbt_model"
[0m15:43:45.014885 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m15:43:45.016957 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m15:43:45.017970 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m15:43:45.019986 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m15:43:45.120128 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:43:45.122141 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:43:45.123148 [debug] [Thread-7 (]: Using fabricspark connection "model.testproj.my_first_dbt_model"
[0m15:43:45.124653 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:43:45.126157 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m15:43:45.168552 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:43:44.957254 => 15:43:45.167545
[0m15:43:45.170580 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b32b9230-f7f4-42a8-931d-03169ce7509d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000158FA198890>]}
[0m15:43:45.172593 [info ] [Thread-7 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.26s]
[0m15:43:45.174607 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:43:45.175614 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m15:43:45.177629 [info ] [Thread-7 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m15:43:45.179643 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:43:45.181657 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:43:45.189729 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:43:45.190736 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:43:45.181657 => 15:43:45.190736
[0m15:43:45.192751 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:43:45.232616 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:43:45.235637 [debug] [Thread-7 (]: Using fabricspark connection "model.testproj.my_second_dbt_model"
[0m15:43:45.236644 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-dev", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m15:43:45.237650 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m15:43:45.242684 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:43:45.193759 => 15:43:45.242684
[0m15:43:45.245706 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b32b9230-f7f4-42a8-931d-03169ce7509d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000158FA1ADDC0>]}
[0m15:43:45.246722 [info ] [Thread-7 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.07s]
[0m15:43:45.249751 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:43:45.252773 [debug] [MainThread]: On master: ROLLBACK
[0m15:43:45.253781 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:43:45.255797 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m15:43:45.256804 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: rollback
[0m15:43:45.257812 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:43:45.258818 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:43:45.261858 [info ] [MainThread]: 
[0m15:43:45.262865 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 7.00 seconds (7.00s).
[0m15:43:45.265894 [debug] [MainThread]: Command end result
[0m15:43:45.277014 [info ] [MainThread]: 
[0m15:43:45.278021 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:43:45.280035 [info ] [MainThread]: 
[0m15:43:45.282053 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:43:45.286100 [debug] [MainThread]: Command `cli run` succeeded at 15:43:45.285092 after 7.77 seconds
[0m15:43:45.288119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000158FA17A960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000158FA17B650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000158FA1787D0>]}
[0m15:43:45.290135 [debug] [MainThread]: Flushing usage events
[0m15:45:03.388855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F4C708E090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F4C708D2E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F4C708E390>]}


============================== 15:45:03.388855 | 3ecc78fa-df36-4158-a254-30e6cfd2050d ==============================
[0m15:45:03.388855 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:45:03.389861 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m15:45:03.393420 [error] [MainThread]: Encountered an error:
Runtime Error
  The profile 'fabric-spark-testnb' does not have a target named 'fabricspark-dev'. The valid target names for this profile are:
   - fabricspark-devnb
[0m15:45:03.394439 [debug] [MainThread]: Command `cli run` failed at 15:45:03.394439 after 0.03 seconds
[0m15:45:03.394439 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F4C7046690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F4C6460560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F4C708F2F0>]}
[0m15:45:03.394947 [debug] [MainThread]: Flushing usage events
[0m15:45:49.161149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193963267E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019396327440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193963267B0>]}


============================== 15:45:49.161149 | 59f636dd-c911-4024-8801-69728e3bdb4d ==============================
[0m15:45:49.161149 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:45:49.162157 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:45:49.256308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '59f636dd-c911-4024-8801-69728e3bdb4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019396373A70>]}
[0m15:45:49.304661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '59f636dd-c911-4024-8801-69728e3bdb4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001939651B260>]}
[0m15:45:49.306673 [info ] [MainThread]: Registered adapter: fabricspark=1.7.0
[0m15:45:49.311711 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:45:49.318262 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m15:45:49.318262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '59f636dd-c911-4024-8801-69728e3bdb4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019396326180>]}
[0m15:45:49.933464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '59f636dd-c911-4024-8801-69728e3bdb4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001939771A7B0>]}
[0m15:45:49.939516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '59f636dd-c911-4024-8801-69728e3bdb4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193977F8980>]}
[0m15:45:49.940525 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:45:49.940525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '59f636dd-c911-4024-8801-69728e3bdb4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019395C97B90>]}
[0m15:45:49.942041 [info ] [MainThread]: 
[0m15:45:49.943055 [debug] [MainThread]: Acquiring new fabricspark connection 'master'
[0m15:45:49.944067 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_schemas'
[0m15:45:49.949139 [debug] [ThreadPool]: Using fabricspark connection "list_schemas"
[0m15:45:49.949643 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m15:45:49.949643 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:45:49.950149 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:45:49.950655 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m15:45:49.951661 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m15:45:49.953674 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:45:49.954681 [debug] [ThreadPool]: Using fabricspark connection "create__test"
[0m15:45:49.954681 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m15:45:49.954681 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:45:49.955688 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:45:49.956694 [debug] [ThreadPool]: Acquiring new fabricspark connection 'list_None_test'
[0m15:45:49.959715 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:45:49.959715 [debug] [ThreadPool]: Using fabricspark connection "list_None_test"
[0m15:45:49.959715 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m15:45:49.960736 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:45:49.960736 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m15:45:49.960736 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m15:45:49.961743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '59f636dd-c911-4024-8801-69728e3bdb4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193976325A0>]}
[0m15:45:49.961743 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:45:49.962750 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:45:49.962750 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:45:49.962750 [info ] [MainThread]: 
[0m15:45:49.964764 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:45:49.965771 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m15:45:49.965771 [debug] [Thread-1 (]: Acquiring new fabricspark connection 'model.testproj.my_first_dbt_model'
[0m15:45:49.965771 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:45:49.969818 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:45:49.970829 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:45:49.966785 => 15:45:49.970829
[0m15:45:49.970829 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:45:49.981961 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_first_dbt_model"
[0m15:45:49.982968 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m15:45:49.982968 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:45:49.982968 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m15:45:49.983975 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:45:50.006302 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:45:50.007308 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:45:50.007308 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_first_dbt_model"
[0m15:45:50.008314 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:45:50.008314 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:45:50.017379 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:45:49.970829 => 15:45:50.017379
[0m15:45:50.018386 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59f636dd-c911-4024-8801-69728e3bdb4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193978E47D0>]}
[0m15:45:50.018386 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.05s]
[0m15:45:50.019394 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:45:50.019394 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:45:50.020402 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m15:45:50.020402 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:45:50.021423 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:45:50.022434 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:45:50.023447 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:45:50.021423 => 15:45:50.023447
[0m15:45:50.023447 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:45:50.032551 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:45:50.033561 [debug] [Thread-1 (]: Using fabricspark connection "model.testproj.my_second_dbt_model"
[0m15:45:50.033561 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m15:45:50.033561 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:45:50.035587 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:45:50.024461 => 15:45:50.034574
[0m15:45:50.035587 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59f636dd-c911-4024-8801-69728e3bdb4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193978E47D0>]}
[0m15:45:50.036633 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.02s]
[0m15:45:50.036633 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:45:50.037662 [debug] [MainThread]: On master: ROLLBACK
[0m15:45:50.037662 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:45:50.038721 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m15:45:50.038721 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: rollback
[0m15:45:50.038721 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:45:50.038721 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:45:50.039735 [info ] [MainThread]: 
[0m15:45:50.039735 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.10 seconds (0.10s).
[0m15:45:50.040743 [debug] [MainThread]: Command end result
[0m15:45:50.044774 [info ] [MainThread]: 
[0m15:45:50.045780 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:45:50.045780 [info ] [MainThread]: 
[0m15:45:50.045780 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:45:50.046787 [debug] [MainThread]: Command `cli run` succeeded at 15:45:50.046787 after 0.91 seconds
[0m15:45:50.046787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193964F9B50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001939751BA70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019397595D90>]}
[0m15:45:50.047795 [debug] [MainThread]: Flushing usage events
[0m15:57:17.860766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BCDE145EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BCDE145EE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BCDE145C70>]}


============================== 15:57:17.861771 | e1a4f13a-96ea-4fec-b4c6-c038879283d0 ==============================
[0m15:57:17.861771 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:57:17.861771 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:57:17.867831 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m15:57:17.868839 [debug] [MainThread]: Command `cli run` failed at 15:57:17.868839 after 0.05 seconds
[0m15:57:17.868839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BCDE145550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BCDD54A300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BCDD9D4BF0>]}
[0m15:57:17.869879 [debug] [MainThread]: Flushing usage events
[0m16:07:03.318176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229ECC131D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229F2890170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229F2BC4B90>]}


============================== 16:07:03.319183 | 7239d297-9c9c-4409-91ac-ddcb7ebae8f6 ==============================
[0m16:07:03.319183 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:07:03.320191 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:07:03.325246 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m16:07:03.326258 [debug] [MainThread]: Command `cli run` failed at 16:07:03.326258 after 0.05 seconds
[0m16:07:03.326258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229F2AED820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229F2AECFB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229F23F71D0>]}
[0m16:07:03.327267 [debug] [MainThread]: Flushing usage events
[0m16:09:02.194444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021CADC26360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021CADC267B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021CADC26270>]}


============================== 16:09:02.194444 | 181321c1-1bc2-47f1-9858-0704b2c74750 ==============================
[0m16:09:02.194444 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:09:02.195451 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:09:02.199498 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m16:09:02.200507 [debug] [MainThread]: Command `cli run` failed at 16:09:02.200507 after 0.03 seconds
[0m16:09:02.200507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021CADC25DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021CAD934110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021CAD8F1070>]}
[0m16:09:02.200507 [debug] [MainThread]: Flushing usage events
[0m16:11:16.350908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D92A7DE9C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D92A7DF170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D92A7DEB10>]}


============================== 16:11:16.351915 | b5a1ae9d-ad88-4e51-a690-62090fdad057 ==============================
[0m16:11:16.351915 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:11:16.351915 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:11:16.354969 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m16:11:16.355978 [debug] [MainThread]: Command `cli run` failed at 16:11:16.355978 after 0.03 seconds
[0m16:11:16.356986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D92A733D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D928C75100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D92A14BB00>]}
[0m16:11:16.356986 [debug] [MainThread]: Flushing usage events
[0m16:11:38.442090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002088947CE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002088947D190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002088947D1F0>]}


============================== 16:11:38.444106 | d5a9cb02-31eb-49f2-89e6-977fe1831ba3 ==============================
[0m16:11:38.444106 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:11:38.446123 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:11:38.463795 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m16:11:38.467852 [debug] [MainThread]: Command `cli run` failed at 16:11:38.466827 after 0.11 seconds
[0m16:11:38.468860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020889177590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208894CBB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208894CBE00>]}
[0m16:11:38.470874 [debug] [MainThread]: Flushing usage events
[0m16:12:50.624076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B9C6ACE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B9C6AE450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B9C6AF260>]}


============================== 16:12:50.627109 | b4035fb0-7ffb-4eda-b76b-fe753c16ed0c ==============================
[0m16:12:50.627109 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:12:50.629134 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:12:50.650474 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m16:12:50.656549 [debug] [MainThread]: Command `cli run` failed at 16:12:50.656549 after 0.14 seconds
[0m16:12:50.658567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B9C6AED50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B9C6FFB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011B9C6FFA10>]}
[0m16:12:50.661612 [debug] [MainThread]: Flushing usage events
[0m16:20:38.665830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F988236270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F988236690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F988236210>]}


============================== 16:20:38.665830 | 7b4e9881-385d-4c56-a322-9d963f518b8f ==============================
[0m16:20:38.665830 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:20:38.666837 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:20:38.670358 [error] [MainThread]: Encountered an error:
type object 'SparkConnectionManager' has no attribute 'TYPE'
[0m16:20:38.704652 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 90, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 75, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 165, in wrapper
    profile = load_profile(flags.PROJECT_DIR, flags.VARS, flags.PROFILE, flags.TARGET, threads)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\config\runtime.py", line 70, in load_profile
    profile = Profile.render(
              ^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\config\profile.py", line 436, in render
    return cls.from_raw_profiles(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\config\profile.py", line 401, in from_raw_profiles
    return cls.from_raw_profile_info(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\config\profile.py", line 355, in from_raw_profile_info
    credentials: Credentials = cls._credentials_from_profile(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\config\profile.py", line 165, in _credentials_from_profile
    cls = load_plugin(typename)
          ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\factory.py", line 212, in load_plugin
    return FACTORY.load_plugin(name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\factory.py", line 71, in load_plugin
    plugin_type = plugin.adapter.type()
                  ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 365, in type
    return cls.ConnectionManager.TYPE
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: type object 'SparkConnectionManager' has no attribute 'TYPE'

[0m16:20:38.705659 [debug] [MainThread]: Command `cli run` failed at 16:20:38.705659 after 0.07 seconds
[0m16:20:38.705659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9878D8C80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F987BAEF90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F9873B0350>]}
[0m16:20:38.705659 [debug] [MainThread]: Flushing usage events
[0m16:23:04.699098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026D23947530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026D239446E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026D23944800>]}


============================== 16:23:04.703137 | d7ebf9ad-e35c-49c5-a9a1-cd41f8a8b740 ==============================
[0m16:23:04.703137 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:23:04.705155 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:23:04.723325 [error] [MainThread]: Encountered an error:
type object 'SparkConnectionManager' has no attribute 'TYPE'
[0m16:23:04.728377 [error] [MainThread]: Traceback (most recent call last):
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 90, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 75, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 165, in wrapper
    profile = load_profile(flags.PROJECT_DIR, flags.VARS, flags.PROFILE, flags.TARGET, threads)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\config\runtime.py", line 70, in load_profile
    profile = Profile.render(
              ^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\config\profile.py", line 436, in render
    return cls.from_raw_profiles(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\config\profile.py", line 401, in from_raw_profiles
    return cls.from_raw_profile_info(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\config\profile.py", line 355, in from_raw_profile_info
    credentials: Credentials = cls._credentials_from_profile(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\config\profile.py", line 165, in _credentials_from_profile
    cls = load_plugin(typename)
          ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\factory.py", line 212, in load_plugin
    return FACTORY.load_plugin(name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\factory.py", line 71, in load_plugin
    plugin_type = plugin.adapter.type()
                  ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 365, in type
    return cls.ConnectionManager.TYPE
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: type object 'SparkConnectionManager' has no attribute 'TYPE'

[0m16:23:04.734433 [debug] [MainThread]: Command `cli run` failed at 16:23:04.734433 after 0.13 seconds
[0m16:23:04.736451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026D2382BAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026D239A1280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026D239A3800>]}
[0m16:23:04.738489 [debug] [MainThread]: Flushing usage events
[0m16:26:05.494775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020751C85D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020751C86210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020751C85D60>]}


============================== 16:26:05.495788 | 24d16f6f-4164-4bab-aa32-f4c57a63ff27 ==============================
[0m16:26:05.495788 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:26:05.495788 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:26:05.598548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '24d16f6f-4164-4bab-aa32-f4c57a63ff27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020751D17890>]}
[0m16:26:05.649437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '24d16f6f-4164-4bab-aa32-f4c57a63ff27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020752E6E6C0>]}
[0m16:26:05.656491 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:26:05.670616 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:26:05.680687 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:26:05.681694 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m16:26:05.681694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '24d16f6f-4164-4bab-aa32-f4c57a63ff27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020752E6EF60>]}
[0m16:26:06.471496 [error] [MainThread]: Encountered an error:
Compilation Error
  In dispatch: No macro named 'create_temporary_view' found within namespace: 'dbt'
      Searched for: 'testproj.fabricsparknb__create_temporary_view', 'testproj.default__create_temporary_view', 'dbt.fabricsparknb__create_temporary_view', 'dbt.default__create_temporary_view'
[0m16:26:06.472503 [debug] [MainThread]: Command `cli run` failed at 16:26:06.472503 after 1.00 seconds
[0m16:26:06.472503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020751554230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020752E6D790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020752D739E0>]}
[0m16:26:06.473509 [debug] [MainThread]: Flushing usage events
[0m16:27:17.200056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9F90A6B40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9F90A6480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9F90A6A80>]}


============================== 16:27:17.201071 | cf9dcf66-5f41-4e4a-b018-54a9210f8918 ==============================
[0m16:27:17.201071 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:27:17.201071 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:27:17.296236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cf9dcf66-5f41-4e4a-b018-54a9210f8918', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9F53D5940>]}
[0m16:27:17.346169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cf9dcf66-5f41-4e4a-b018-54a9210f8918', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FA249B20>]}
[0m16:27:17.347175 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:27:17.354745 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:27:17.359782 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m16:27:17.360800 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m16:27:17.360800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cf9dcf66-5f41-4e4a-b018-54a9210f8918', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9F9116480>]}
[0m16:27:18.132705 [error] [MainThread]: Encountered an error:
'SparkConnectionManager' object has no attribute 'profile'
[0m16:27:18.142785 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 90, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 75, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 197, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 244, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 271, in wrapper
    manifest = ManifestLoader.get_full_manifest(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\parser\manifest.py", line 325, in get_full_manifest
    loader.save_macros_to_adapter(adapter)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\parser\manifest.py", line 1004, in save_macros_to_adapter
    self.macro_hook(macro_manifest)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\connections.py", line 82, in set_query_header
    self.query_header = MacroQueryStringSetter(self.profile, manifest)
                                               ^^^^^^^^^^^^
AttributeError: 'SparkConnectionManager' object has no attribute 'profile'

[0m16:27:18.143791 [debug] [MainThread]: Command `cli run` failed at 16:27:18.143791 after 0.97 seconds
[0m16:27:18.144797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9F8FBCD70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FA27C1D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E9FA366AB0>]}
[0m16:27:18.144797 [debug] [MainThread]: Flushing usage events
[0m16:27:55.733743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001749DD65D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001749DD66540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001749DD661B0>]}


============================== 16:27:55.733743 | 377ffa3d-2947-443e-8d30-b8dab424e57e ==============================
[0m16:27:55.733743 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:27:55.734752 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:27:55.831538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '377ffa3d-2947-443e-8d30-b8dab424e57e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001749DC8EA20>]}
[0m16:27:55.880974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '377ffa3d-2947-443e-8d30-b8dab424e57e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001749EF4DF70>]}
[0m16:27:55.883001 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:27:55.889082 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:27:55.944569 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:27:55.945580 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:27:55.948604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '377ffa3d-2947-443e-8d30-b8dab424e57e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001749DEB4DA0>]}
[0m16:27:55.954645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '377ffa3d-2947-443e-8d30-b8dab424e57e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001749EFCF470>]}
[0m16:27:55.955653 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m16:27:55.955653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '377ffa3d-2947-443e-8d30-b8dab424e57e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001749DE441D0>]}
[0m16:27:55.956664 [info ] [MainThread]: 
[0m16:27:55.957676 [info ] [MainThread]: 
[0m16:27:55.958695 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.00 seconds (0.00s).
[0m16:27:55.958695 [error] [MainThread]: Encountered an error:
'SparkConnectionManager' object has no attribute 'lock'
[0m16:27:55.988963 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 90, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 75, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 197, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 244, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 284, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\main.py", line 625, in run
    results = task.run()
              ^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 474, in run
    result = self.execute_with_hooks(selected_uids)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 434, in execute_with_hooks
    self.before_run(adapter, selected_uids)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\run.py", line 444, in before_run
    with adapter.connection_named("master"):
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 283, in connection_named
    self.acquire_connection(name)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 255, in acquire_connection
    return self.connections.set_connection_name(name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\connections.py", line 153, in set_connection_name
    conn = self.get_if_exists()
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\connections.py", line 107, in get_if_exists
    with self.lock:
         ^^^^^^^^^
AttributeError: 'SparkConnectionManager' object has no attribute 'lock'

[0m16:27:55.989969 [debug] [MainThread]: Command `cli run` failed at 16:27:55.989969 after 0.28 seconds
[0m16:27:55.989969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001749DD66E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001749DC59B80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001749F051F40>]}
[0m16:27:55.990977 [debug] [MainThread]: Flushing usage events
[0m16:30:50.607138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3F2956420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3F29563C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3F29560C0>]}


============================== 16:30:50.608149 | e0dadc68-3175-41fa-8393-c32915c5da54 ==============================
[0m16:30:50.608149 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:30:50.608149 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:30:50.718247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e0dadc68-3175-41fa-8393-c32915c5da54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3F3A40AD0>]}
[0m16:30:50.772996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e0dadc68-3175-41fa-8393-c32915c5da54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3F1F03EC0>]}
[0m16:30:50.774002 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:30:50.782160 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:30:50.833016 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:30:50.833016 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:30:50.837069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e0dadc68-3175-41fa-8393-c32915c5da54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3F28AF950>]}
[0m16:30:50.845170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e0dadc68-3175-41fa-8393-c32915c5da54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3F3BBB470>]}
[0m16:30:50.845170 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m16:30:50.846176 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e0dadc68-3175-41fa-8393-c32915c5da54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3F2A3DAC0>]}
[0m16:30:50.848214 [info ] [MainThread]: 
[0m16:30:50.848214 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:30:50.850236 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:30:50.852392 [info ] [MainThread]: 
[0m16:30:50.852895 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.00 seconds (0.00s).
[0m16:30:50.853404 [error] [MainThread]: Encountered an error:
'_GeneratorContextManager' object is not an iterator
[0m16:30:50.896970 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 90, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 75, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 197, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 244, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 284, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\main.py", line 625, in run
    results = task.run()
              ^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 474, in run
    result = self.execute_with_hooks(selected_uids)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 434, in execute_with_hooks
    self.before_run(adapter, selected_uids)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\run.py", line 446, in before_run
    self.create_schemas(adapter, required_schemas)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 573, in create_schemas
    existing_schemas_lowered.update(ls_future.result())
                                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\utils.py", line 471, in connected
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 550, in list_schemas
    for s in adapter.list_schemas(database_quoted)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\sql\impl.py", line 210, in list_schemas
    results = self.execute_macro(LIST_SCHEMAS_MACRO_NAME, kwargs={"database": database})
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 1111, in execute_macro
    with self.connections.exception_handler(f"macro {macro_name}"):
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\contextlib.py", line 137, in __enter__
    return next(self.gen)
           ^^^^^^^^^^^^^^
TypeError: '_GeneratorContextManager' object is not an iterator

[0m16:30:50.900024 [debug] [MainThread]: Command `cli run` failed at 16:30:50.899010 after 0.32 seconds
[0m16:30:50.900537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3F25F8920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3F26C5DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C3F2A3F8F0>]}
[0m16:30:50.901044 [debug] [MainThread]: Flushing usage events
[0m16:32:03.576157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018599FE8170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001859ADE57C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001859ADE73B0>]}


============================== 16:32:03.577165 | 9544c236-d213-464c-ba98-c92b828e0319 ==============================
[0m16:32:03.577165 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:32:03.577165 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:32:03.670939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9544c236-d213-464c-ba98-c92b828e0319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001859A2E4860>]}
[0m16:32:03.721325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9544c236-d213-464c-ba98-c92b828e0319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001859BFCDD60>]}
[0m16:32:03.722333 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:32:03.737954 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:32:04.270038 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:32:04.270038 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:32:04.275088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9544c236-d213-464c-ba98-c92b828e0319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001859AF59430>]}
[0m16:32:04.289330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9544c236-d213-464c-ba98-c92b828e0319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001859C047380>]}
[0m16:32:04.290338 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m16:32:04.291352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9544c236-d213-464c-ba98-c92b828e0319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001859AE20DD0>]}
[0m16:32:04.292371 [info ] [MainThread]: 
[0m16:32:04.293393 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:32:04.294429 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:32:04.301495 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m16:32:04.301495 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m16:32:04.302502 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:32:04.302502 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m16:32:04.303510 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m16:32:04.304517 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m16:32:04.306532 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:32:04.306532 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m16:32:04.306532 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m16:32:04.307539 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m16:32:04.307539 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:32:04.308546 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m16:32:04.311591 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:32:04.312599 [debug] [ThreadPool]: Using fabricsparknb connection "list_None_test"
[0m16:32:04.312599 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:32:04.312599 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:32:04.312599 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m16:32:04.313608 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m16:32:04.314616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9544c236-d213-464c-ba98-c92b828e0319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001859BFCF590>]}
[0m16:32:04.314616 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:32:04.314616 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:32:04.315624 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:32:04.315624 [info ] [MainThread]: 
[0m16:32:04.317639 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:32:04.317639 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m16:32:04.318646 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:32:04.318646 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:32:04.322685 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:32:04.322685 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:32:04.318646 => 16:32:04.322685
[0m16:32:04.323699 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:32:04.334818 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:32:04.334818 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m16:32:04.335825 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:32:04.335825 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m16:32:04.335825 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:32:04.358274 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m16:32:04.359291 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:32:04.359291 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:32:04.359291 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:32:04.360305 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:32:04.369372 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:32:04.323699 => 16:32:04.369372
[0m16:32:04.370409 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9544c236-d213-464c-ba98-c92b828e0319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001859C1BD070>]}
[0m16:32:04.370409 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.05s]
[0m16:32:04.371418 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:32:04.371418 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:32:04.372427 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m16:32:04.372427 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m16:32:04.373440 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m16:32:04.374451 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m16:32:04.375463 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 16:32:04.373440 => 16:32:04.375463
[0m16:32:04.375463 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m16:32:04.384573 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m16:32:04.385599 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:32:04.385599 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m16:32:04.386611 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:32:04.387623 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 16:32:04.375463 => 16:32:04.387623
[0m16:32:04.387623 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9544c236-d213-464c-ba98-c92b828e0319', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001859AF98830>]}
[0m16:32:04.388636 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.02s]
[0m16:32:04.388636 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:32:04.389648 [debug] [MainThread]: On master: ROLLBACK
[0m16:32:04.389648 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:32:04.390660 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m16:32:04.390660 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: rollback
[0m16:32:04.390660 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:32:04.391672 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:32:04.391672 [info ] [MainThread]: 
[0m16:32:04.391672 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.10 seconds (0.10s).
[0m16:32:04.392683 [debug] [MainThread]: Command end result
[0m16:32:04.397721 [info ] [MainThread]: 
[0m16:32:04.397721 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:32:04.397721 [info ] [MainThread]: 
[0m16:32:04.398729 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m16:32:04.399735 [debug] [MainThread]: Command `cli run` succeeded at 16:32:04.398729 after 0.86 seconds
[0m16:32:04.399735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018599FE8170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001859AB14920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001859C050A40>]}
[0m16:32:04.399735 [debug] [MainThread]: Flushing usage events
[0m16:33:46.305881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CAFF4A12B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CAFF4A1040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CAFF4A13D0>]}


============================== 16:33:46.306894 | fe3eb74d-0f63-45d7-9cfb-a504672b1a5d ==============================
[0m16:33:46.306894 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:33:46.306894 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:33:46.418200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fe3eb74d-0f63-45d7-9cfb-a504672b1a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CAFEDD4110>]}
[0m16:33:46.471052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fe3eb74d-0f63-45d7-9cfb-a504672b1a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CAFF3CD970>]}
[0m16:33:46.473065 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:33:46.481186 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:33:46.528689 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:33:46.528689 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:33:46.532732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fe3eb74d-0f63-45d7-9cfb-a504672b1a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CAFF4DAD20>]}
[0m16:33:46.538893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fe3eb74d-0f63-45d7-9cfb-a504672b1a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CAFF6E5BE0>]}
[0m16:33:46.538893 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m16:33:46.539905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fe3eb74d-0f63-45d7-9cfb-a504672b1a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CAFF4A3710>]}
[0m16:33:46.540917 [info ] [MainThread]: 
[0m16:33:46.541930 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:33:46.541930 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:33:46.547969 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m16:33:46.547969 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m16:33:46.548977 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:33:46.548977 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m16:33:46.549983 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m16:33:46.549983 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m16:33:46.553028 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:33:46.553028 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m16:33:46.554036 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m16:33:46.554036 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m16:33:46.554036 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:33:46.556054 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m16:33:46.558068 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:33:46.559075 [debug] [ThreadPool]: Using fabricsparknb connection "list_None_test"
[0m16:33:46.559075 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:33:46.559075 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:33:46.559075 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m16:33:46.560082 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m16:33:46.561089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fe3eb74d-0f63-45d7-9cfb-a504672b1a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CAFF620CE0>]}
[0m16:33:46.561089 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:33:46.561089 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:33:46.562097 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:33:46.562097 [info ] [MainThread]: 
[0m16:33:46.564111 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:33:46.564111 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m16:33:46.565120 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:33:46.565120 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:33:46.569174 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:33:46.570184 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:33:46.565120 => 16:33:46.570184
[0m16:33:46.570184 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:33:46.581268 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:33:46.582276 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m16:33:46.582276 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:33:46.582276 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m16:33:46.583308 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:33:46.604461 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m16:33:46.605468 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:33:46.605468 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:33:46.606475 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:33:46.606475 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:33:46.615565 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:33:46.570184 => 16:33:46.615565
[0m16:33:46.616572 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe3eb74d-0f63-45d7-9cfb-a504672b1a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CAFF88BD40>]}
[0m16:33:46.616572 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.05s]
[0m16:33:46.617579 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:33:46.617579 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:33:46.618589 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m16:33:46.618589 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m16:33:46.619599 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m16:33:46.620609 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m16:33:46.621622 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 16:33:46.619599 => 16:33:46.621622
[0m16:33:46.621622 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m16:33:46.630738 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m16:33:46.631745 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:33:46.631745 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m16:33:46.632751 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:33:46.633761 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 16:33:46.622633 => 16:33:46.633761
[0m16:33:46.633761 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe3eb74d-0f63-45d7-9cfb-a504672b1a5d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CAFF804D40>]}
[0m16:33:46.634768 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.02s]
[0m16:33:46.634768 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:33:46.635776 [debug] [MainThread]: On master: ROLLBACK
[0m16:33:46.635776 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:33:46.636786 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m16:33:46.636786 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: rollback
[0m16:33:46.636786 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:33:46.636786 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:33:46.637796 [info ] [MainThread]: 
[0m16:33:46.637796 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.10 seconds (0.10s).
[0m16:33:46.638804 [debug] [MainThread]: Command end result
[0m16:33:46.643841 [info ] [MainThread]: 
[0m16:33:46.643841 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:33:46.643841 [info ] [MainThread]: 
[0m16:33:46.644866 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m16:33:46.644866 [debug] [MainThread]: Command `cli run` succeeded at 16:33:46.644866 after 0.36 seconds
[0m16:33:46.645875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CAFF4E6690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CAFF2C3650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001CAFE8B6480>]}
[0m16:33:46.645875 [debug] [MainThread]: Flushing usage events
[0m16:37:47.362099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BFC667440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BFC6674A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BFC6675F0>]}


============================== 16:37:47.362099 | 76e4904b-4e51-46ac-84b9-89218ae55c08 ==============================
[0m16:37:47.362099 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:37:47.363107 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:37:47.367138 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m16:37:47.368145 [debug] [MainThread]: Command `cli run` failed at 16:37:47.368145 after 0.03 seconds
[0m16:37:47.368145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BFC415790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BFC3AF1D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020BFC5CD7C0>]}
[0m16:37:47.369148 [debug] [MainThread]: Flushing usage events
[0m16:47:47.929329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AA0CE7560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AA0CE4A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AA0CE7F20>]}


============================== 16:47:47.930336 | ca8071ac-f45e-4554-b0ac-e37e72bfbd6e ==============================
[0m16:47:47.930336 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:47:47.930336 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:47:47.934397 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m16:47:47.935406 [debug] [MainThread]: Command `cli run` failed at 16:47:47.935406 after 0.05 seconds
[0m16:47:47.935406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016A9D29FEC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AA0C3E0F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AA0CE6510>]}
[0m16:47:47.936416 [debug] [MainThread]: Flushing usage events
[0m16:48:20.650657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000297B4DD73B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000297B4DD74A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000297B4DD7470>]}


============================== 16:48:20.650657 | 6cfd472d-243a-4c9e-a834-dce9160d5286 ==============================
[0m16:48:20.650657 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:48:20.650657 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:48:20.654706 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m16:48:20.655713 [debug] [MainThread]: Command `cli run` failed at 16:48:20.655713 after 0.03 seconds
[0m16:48:20.655713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000297B43064E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000297B4B862A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000297B42D06B0>]}
[0m16:48:20.655713 [debug] [MainThread]: Flushing usage events
[0m16:48:31.275981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202A01E1010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202A01E0AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202A01E0D70>]}


============================== 16:48:31.275981 | f2afe9a9-78dd-4212-ba0d-cefbe47aac62 ==============================
[0m16:48:31.275981 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:48:31.276989 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:48:31.280010 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m16:48:31.281020 [debug] [MainThread]: Command `cli run` failed at 16:48:31.281020 after 0.03 seconds
[0m16:48:31.282045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000202A01E0AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002029F13FBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002029F13FAD0>]}
[0m16:48:31.282045 [debug] [MainThread]: Flushing usage events
[0m16:52:24.584052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E80EE229C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E80EE20A40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E80EE23860>]}


============================== 16:52:24.585060 | 3a2db06c-00c0-4f87-aac4-ee804ece10be ==============================
[0m16:52:24.585060 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:52:24.585060 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:52:24.589184 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m16:52:24.590217 [debug] [MainThread]: Command `cli run` failed at 16:52:24.590217 after 0.03 seconds
[0m16:52:24.591226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E80EE229C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E80EE21A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E80EE21580>]}
[0m16:52:24.591226 [debug] [MainThread]: Flushing usage events
[0m16:55:48.726759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D396CC260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D396CCC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D396CC500>]}


============================== 16:55:48.730791 | 10730a97-0fd6-4902-b067-70ebb391167e ==============================
[0m16:55:48.730791 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:55:48.732813 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:55:48.752018 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m16:55:48.756575 [debug] [MainThread]: Command `cli run` failed at 16:55:48.756065 after 0.13 seconds
[0m16:55:48.758094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D392D6480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D3970D2B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022D3970D7F0>]}
[0m16:55:48.759619 [debug] [MainThread]: Flushing usage events
[0m16:56:56.919293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C83A3770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C83A2510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C83A25A0>]}


============================== 16:56:56.921306 | 0bb20382-57c9-48c5-b8a1-746d4a8f51af ==============================
[0m16:56:56.921306 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:56:56.923326 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:56:56.943639 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Expected to find adapter with type named fabricsparknb, got adapter with type fabricspark
[0m16:56:56.948753 [debug] [MainThread]: Command `cli run` failed at 16:56:56.947746 after 0.12 seconds
[0m16:56:56.949761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C83A1790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C8415760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000219C8415AF0>]}
[0m16:56:56.951786 [debug] [MainThread]: Flushing usage events
[0m16:59:07.628708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027743C1BA10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027743C19190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027743C1BB60>]}


============================== 16:59:07.629714 | 0835b34c-cddf-49d8-9386-ec6a0bb13a3b ==============================
[0m16:59:07.629714 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:59:07.629714 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:59:07.725527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0835b34c-cddf-49d8-9386-ec6a0bb13a3b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027743C7E000>]}
[0m16:59:07.775908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0835b34c-cddf-49d8-9386-ec6a0bb13a3b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277434E4170>]}
[0m16:59:07.776913 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:59:07.793085 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:59:08.436269 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:59:08.437279 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:59:08.441317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0835b34c-cddf-49d8-9386-ec6a0bb13a3b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027744D23230>]}
[0m16:59:08.449423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0835b34c-cddf-49d8-9386-ec6a0bb13a3b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027744E85B50>]}
[0m16:59:08.449423 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m16:59:08.450431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0835b34c-cddf-49d8-9386-ec6a0bb13a3b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027743CF6AB0>]}
[0m16:59:08.451443 [info ] [MainThread]: 
[0m16:59:08.452452 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:59:08.453460 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:59:08.460560 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m16:59:08.460560 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m16:59:08.460560 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:59:08.461573 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m16:59:08.462584 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m16:59:08.462584 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m16:59:08.465606 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:59:08.465606 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m16:59:08.466613 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m16:59:08.466613 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m16:59:08.466613 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:59:08.468628 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m16:59:08.472665 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:59:08.472665 [debug] [ThreadPool]: Using fabricsparknb connection "list_None_test"
[0m16:59:08.472665 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m16:59:08.473675 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:59:08.473675 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m16:59:08.473675 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m16:59:08.474681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0835b34c-cddf-49d8-9386-ec6a0bb13a3b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027743CF4B30>]}
[0m16:59:08.475727 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:59:08.475727 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:59:08.475727 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:59:08.476737 [info ] [MainThread]: 
[0m16:59:08.478753 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:59:08.478753 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m16:59:08.479760 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:59:08.479760 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:59:08.483802 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:59:08.484815 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:59:08.479760 => 16:59:08.484815
[0m16:59:08.484815 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:59:08.498995 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:59:08.498995 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m16:59:08.500001 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:59:08.500001 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m16:59:08.500001 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:59:08.523288 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m16:59:08.524300 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:59:08.525309 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:59:08.525309 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:59:08.525309 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:59:08.535438 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:59:08.485828 => 16:59:08.535438
[0m16:59:08.536465 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0835b34c-cddf-49d8-9386-ec6a0bb13a3b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027744ED0170>]}
[0m16:59:08.537484 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.06s]
[0m16:59:08.538522 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:59:08.539541 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:59:08.540555 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m16:59:08.541568 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m16:59:08.541568 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m16:59:08.543588 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m16:59:08.544597 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 16:59:08.541568 => 16:59:08.543588
[0m16:59:08.544597 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m16:59:08.554709 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m16:59:08.555719 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:59:08.556732 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m16:59:08.556732 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:59:08.557741 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 16:59:08.544597 => 16:59:08.557741
[0m16:59:08.558750 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0835b34c-cddf-49d8-9386-ec6a0bb13a3b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027744FCC0B0>]}
[0m16:59:08.558750 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.02s]
[0m16:59:08.559755 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:59:08.560764 [debug] [MainThread]: On master: ROLLBACK
[0m16:59:08.560764 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:59:08.560764 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m16:59:08.561775 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: rollback
[0m16:59:08.561775 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:59:08.561775 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:59:08.562782 [info ] [MainThread]: 
[0m16:59:08.562782 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.11 seconds (0.11s).
[0m16:59:08.563791 [debug] [MainThread]: Command end result
[0m16:59:08.569880 [info ] [MainThread]: 
[0m16:59:08.569880 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:59:08.570891 [info ] [MainThread]: 
[0m16:59:08.570891 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m16:59:08.571906 [debug] [MainThread]: Command `cli run` succeeded at 16:59:08.571906 after 0.97 seconds
[0m16:59:08.572916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027743B3CF50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027744FD3260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027744DBA540>]}
[0m16:59:08.572916 [debug] [MainThread]: Flushing usage events
[0m17:01:35.429924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002745D246780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002745D2445F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002745D245F40>]}


============================== 17:01:35.432948 | 7f317aa8-da7d-442c-9d2a-352d96f30736 ==============================
[0m17:01:35.432948 [info ] [MainThread]: Running with dbt=1.7.4
[0m17:01:35.434962 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:01:48.848013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7f317aa8-da7d-442c-9d2a-352d96f30736', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002745CEAEC00>]}
[0m17:01:49.026398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7f317aa8-da7d-442c-9d2a-352d96f30736', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002745D46B2F0>]}
[0m17:01:49.029442 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:01:49.060837 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m17:01:49.225140 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:01:49.228178 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:01:49.247421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7f317aa8-da7d-442c-9d2a-352d96f30736', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002745D46AD50>]}
[0m17:01:49.263573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7f317aa8-da7d-442c-9d2a-352d96f30736', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002745D52F590>]}
[0m17:01:49.265090 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m17:01:49.266614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7f317aa8-da7d-442c-9d2a-352d96f30736', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002745CDB17C0>]}
[0m17:01:49.272187 [info ] [MainThread]: 
[0m17:01:49.274725 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:01:49.278758 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:01:49.310217 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m17:01:49.313255 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m17:01:49.314264 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:01:49.316278 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:01:49.324400 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m17:01:49.328459 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m17:01:49.344202 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:01:49.346229 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m17:01:49.348251 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m17:01:49.349258 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:01:49.352302 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:01:49.359404 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m17:01:49.374543 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:01:49.375552 [debug] [ThreadPool]: Using fabricsparknb connection "list_None_test"
[0m17:01:49.377564 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m17:01:49.378570 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:01:49.379577 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m17:01:49.380583 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:01:49.386636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7f317aa8-da7d-442c-9d2a-352d96f30736', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002745D3C4EF0>]}
[0m17:01:49.387644 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:01:49.389670 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:01:49.393747 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:01:49.395774 [info ] [MainThread]: 
[0m17:01:49.401859 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m17:01:49.403872 [info ] [Thread-7 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m17:01:49.407935 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:01:49.409972 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:01:49.451744 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:01:49.454781 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:01:49.412020 => 17:01:49.453771
[0m17:01:49.460039 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:01:49.521961 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:01:49.523977 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m17:01:49.524988 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m17:01:49.527003 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m17:01:49.528010 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m17:01:49.631119 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:01:49.633133 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:01:49.634140 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:01:49.635152 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:01:49.637168 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m17:01:49.680522 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:01:49.461051 => 17:01:49.680522
[0m17:01:49.682534 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f317aa8-da7d-442c-9d2a-352d96f30736', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002745D6F9100>]}
[0m17:01:49.684547 [info ] [Thread-7 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.28s]
[0m17:01:49.686560 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:01:49.689611 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m17:01:49.691633 [info ] [Thread-7 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m17:01:49.694659 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m17:01:49.695668 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:01:49.705768 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:01:49.708793 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:01:49.696678 => 17:01:49.707782
[0m17:01:49.709803 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:01:49.758881 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:01:49.760898 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:01:49.762912 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m17:01:49.763933 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m17:01:49.768967 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:01:49.710811 => 17:01:49.768967
[0m17:01:49.771991 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7f317aa8-da7d-442c-9d2a-352d96f30736', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002745D698560>]}
[0m17:01:49.775050 [info ] [Thread-7 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.08s]
[0m17:01:49.777083 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:01:49.781126 [debug] [MainThread]: On master: ROLLBACK
[0m17:01:49.782134 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:01:49.783141 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m17:01:49.785154 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: rollback
[0m17:01:49.786161 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:01:49.787167 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:01:49.790212 [info ] [MainThread]: 
[0m17:01:49.792237 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.52 seconds (0.52s).
[0m17:01:49.794266 [debug] [MainThread]: Command end result
[0m17:01:49.808394 [info ] [MainThread]: 
[0m17:01:49.810447 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:01:49.812469 [info ] [MainThread]: 
[0m17:01:49.813477 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m17:01:49.816502 [debug] [MainThread]: Command `cli run` succeeded at 17:01:49.816502 after 14.48 seconds
[0m17:01:49.818529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002745D244050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002745D731670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002745D6ED8B0>]}
[0m17:01:49.819539 [debug] [MainThread]: Flushing usage events
[0m17:04:30.432908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001639FE5ADE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001639FE59EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001639FE59670>]}


============================== 17:04:30.436996 | 4068f524-5702-47a3-b24b-2bc49570e022 ==============================
[0m17:04:30.436996 [info ] [MainThread]: Running with dbt=1.7.4
[0m17:04:30.439010 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m17:04:30.787772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4068f524-5702-47a3-b24b-2bc49570e022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001639FFC19A0>]}
[0m17:04:30.952674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4068f524-5702-47a3-b24b-2bc49570e022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001639FECEE70>]}
[0m17:04:30.955693 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:04:30.985013 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m17:04:31.106039 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:04:31.108054 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:04:31.125189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4068f524-5702-47a3-b24b-2bc49570e022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001639FECEED0>]}
[0m17:04:31.147460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4068f524-5702-47a3-b24b-2bc49570e022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163A0156450>]}
[0m17:04:31.151040 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m17:04:31.153587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4068f524-5702-47a3-b24b-2bc49570e022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001639FFE69C0>]}
[0m17:04:31.163187 [info ] [MainThread]: 
[0m17:04:31.168326 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:04:31.173878 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:04:31.201368 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m17:04:31.203383 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m17:04:31.204391 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:04:31.205398 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:04:31.215721 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m17:04:31.219836 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m17:04:31.235058 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:04:31.236067 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m17:04:31.237074 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m17:04:31.238081 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:04:31.242126 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:04:31.250258 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m17:04:31.268467 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:04:31.269478 [debug] [ThreadPool]: Using fabricsparknb connection "list_None_test"
[0m17:04:31.270984 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m17:04:31.271987 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:04:31.274044 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m17:04:31.276068 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:04:31.283157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4068f524-5702-47a3-b24b-2bc49570e022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001639FFE5EB0>]}
[0m17:04:31.286188 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:04:31.288211 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:04:31.290250 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:04:31.292270 [info ] [MainThread]: 
[0m17:04:31.302390 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m17:04:31.304424 [info ] [Thread-7 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m17:04:31.307452 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:04:31.309478 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:04:31.351282 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:04:31.354331 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:04:31.310497 => 17:04:31.354331
[0m17:04:31.356349 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:04:31.419333 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:04:31.421349 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m17:04:31.422360 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m17:04:31.424381 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m17:04:31.425464 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m17:04:31.536874 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:04:31.538887 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:04:31.539895 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:04:31.540904 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:04:31.541913 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m17:04:31.585284 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:04:31.357357 => 17:04:31.584272
[0m17:04:31.588308 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4068f524-5702-47a3-b24b-2bc49570e022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163A036FAA0>]}
[0m17:04:31.589315 [info ] [Thread-7 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.28s]
[0m17:04:31.592341 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:04:31.594374 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m17:04:31.595384 [info ] [Thread-7 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m17:04:31.598417 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m17:04:31.599423 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:04:31.608497 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:04:31.610524 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:04:31.600430 => 17:04:31.610524
[0m17:04:31.612544 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:04:31.658064 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:04:31.663151 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:04:31.665171 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m17:04:31.667196 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m17:04:31.673278 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:04:31.613553 => 17:04:31.672272
[0m17:04:31.675297 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4068f524-5702-47a3-b24b-2bc49570e022', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163A036C470>]}
[0m17:04:31.678325 [info ] [Thread-7 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.08s]
[0m17:04:31.680341 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:04:31.684371 [debug] [MainThread]: On master: ROLLBACK
[0m17:04:31.685378 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:04:31.686410 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Reusing session: 0
[0m17:04:31.688427 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: rollback
[0m17:04:31.689434 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:04:31.691451 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:04:31.694484 [info ] [MainThread]: 
[0m17:04:31.696519 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.53 seconds (0.53s).
[0m17:04:31.699551 [debug] [MainThread]: Command end result
[0m17:04:31.715783 [info ] [MainThread]: 
[0m17:04:31.717836 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:04:31.719856 [info ] [MainThread]: 
[0m17:04:31.720863 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m17:04:31.723885 [debug] [MainThread]: Command `cli run` succeeded at 17:04:31.723885 after 1.40 seconds
[0m17:04:31.725912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001639EFA6FF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163A03811F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000163A0381220>]}
[0m17:04:31.726923 [debug] [MainThread]: Flushing usage events
[0m17:05:45.938574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187C329F170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187C329DDF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187C329EC60>]}


============================== 17:05:45.941622 | 42323e7d-f073-48fc-a29d-cd907752e939 ==============================
[0m17:05:45.941622 [info ] [MainThread]: Running with dbt=1.7.4
[0m17:05:45.945741 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:05:46.307195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '42323e7d-f073-48fc-a29d-cd907752e939', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187C329F680>]}
[0m17:05:46.470765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '42323e7d-f073-48fc-a29d-cd907752e939', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187C3499640>]}
[0m17:05:46.473821 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:05:46.504256 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m17:05:46.624790 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:05:46.625799 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:05:46.643974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '42323e7d-f073-48fc-a29d-cd907752e939', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187C35151C0>]}
[0m17:05:46.660150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '42323e7d-f073-48fc-a29d-cd907752e939', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187C358AE70>]}
[0m17:05:46.661157 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m17:05:46.663184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '42323e7d-f073-48fc-a29d-cd907752e939', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187C33F7B00>]}
[0m17:05:46.668263 [info ] [MainThread]: 
[0m17:05:46.671356 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:05:46.676417 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:05:46.703802 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m17:05:46.705818 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m17:05:46.706834 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:11:14.254386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D45BF24980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D45BF249E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D45BF249B0>]}


============================== 17:11:14.255393 | 1cbb5740-8d31-41a4-b3f8-96c005de7baa ==============================
[0m17:11:14.255393 [info ] [MainThread]: Running with dbt=1.7.4
[0m17:11:14.255393 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m17:11:14.353269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1cbb5740-8d31-41a4-b3f8-96c005de7baa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D45B1F7B00>]}
[0m17:11:14.406799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1cbb5740-8d31-41a4-b3f8-96c005de7baa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D45BC67C80>]}
[0m17:11:14.407810 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:11:14.417925 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m17:11:14.468423 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:11:14.468423 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:11:14.472498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1cbb5740-8d31-41a4-b3f8-96c005de7baa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D45D037B00>]}
[0m17:11:14.480574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1cbb5740-8d31-41a4-b3f8-96c005de7baa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D45D192300>]}
[0m17:11:14.481585 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m17:11:14.481585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1cbb5740-8d31-41a4-b3f8-96c005de7baa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D45D223800>]}
[0m17:11:14.482593 [info ] [MainThread]: 
[0m17:11:14.483599 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:11:14.484609 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:11:14.490687 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m17:11:14.490687 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m17:11:14.491697 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:11:14.491697 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:11:14.492706 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m17:11:14.493713 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m17:11:14.495732 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:11:14.496742 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m17:11:14.496742 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m17:11:14.496742 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:11:14.497753 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:11:14.498760 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m17:11:14.502812 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:11:14.502812 [debug] [ThreadPool]: Using fabricsparknb connection "list_None_test"
[0m17:11:14.502812 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m17:11:14.502812 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:11:14.503818 [debug] [ThreadPool]: fabricsparknb adapter: Reusing session: 0
[0m17:11:14.503818 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:11:14.504826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1cbb5740-8d31-41a4-b3f8-96c005de7baa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D45D24C320>]}
[0m17:11:14.504826 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:11:14.505836 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:11:14.505836 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:11:14.505836 [info ] [MainThread]: 
[0m17:11:14.507854 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:11:14.508862 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m17:11:14.508862 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:11:14.509873 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:11:14.514932 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:11:14.516969 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:11:14.509873 => 17:11:14.515942
[0m17:11:14.516969 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:11:14.530073 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:11:14.531084 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m17:11:14.531084 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:11:14.532115 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m17:11:14.532115 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:11:14.555354 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:11:14.556363 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:11:14.556363 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:11:14.557373 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:11:14.557373 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:11:14.568527 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:11:14.516969 => 17:11:14.567519
[0m17:11:14.569537 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1cbb5740-8d31-41a4-b3f8-96c005de7baa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D45D1445C0>]}
[0m17:11:14.569537 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.06s]
[0m17:11:14.570545 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:11:14.571555 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:11:14.571555 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m17:11:14.572566 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m17:11:14.572566 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:11:14.574592 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:11:14.575602 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:11:14.572566 => 17:11:14.575602
[0m17:11:14.575602 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:11:14.586722 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:11:14.587731 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:11:14.587731 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m17:11:14.587731 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:11:14.589752 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:11:14.576618 => 17:11:14.589752
[0m17:11:14.589752 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1cbb5740-8d31-41a4-b3f8-96c005de7baa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D45D1445C0>]}
[0m17:11:14.590781 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.02s]
[0m17:11:14.591790 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:11:14.592823 [debug] [MainThread]: On master: ROLLBACK
[0m17:11:14.592823 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:11:14.592823 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:11:14.593831 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:11:14.593831 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:11:14.593831 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:11:14.594841 [info ] [MainThread]: 
[0m17:11:14.594841 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.11 seconds (0.11s).
[0m17:11:14.595851 [debug] [MainThread]: Command end result
[0m17:11:14.603953 [info ] [MainThread]: 
[0m17:11:14.604962 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:11:14.604962 [info ] [MainThread]: 
[0m17:11:14.605971 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m17:11:14.607033 [debug] [MainThread]: Command `cli run` succeeded at 17:11:14.607033 after 0.37 seconds
[0m17:11:14.607033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D45BEE7980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D45BEE7E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D45D24C3E0>]}
[0m17:11:14.608035 [debug] [MainThread]: Flushing usage events
[0m17:11:46.749310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191CDC165A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191CDC14EC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191CDC14260>]}


============================== 17:11:46.752353 | 168ec0d3-24e8-4577-ac4e-d50b4c057003 ==============================
[0m17:11:46.752353 [info ] [MainThread]: Running with dbt=1.7.4
[0m17:11:46.755401 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:11:47.122202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '168ec0d3-24e8-4577-ac4e-d50b4c057003', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191CDD9B920>]}
[0m17:11:47.294059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '168ec0d3-24e8-4577-ac4e-d50b4c057003', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191CDD6CCE0>]}
[0m17:11:47.297133 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:11:47.325429 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m17:11:47.434313 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:11:47.435320 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:11:47.453475 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '168ec0d3-24e8-4577-ac4e-d50b4c057003', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191CDE3D3A0>]}
[0m17:11:47.468617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '168ec0d3-24e8-4577-ac4e-d50b4c057003', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191CDF0AE10>]}
[0m17:11:47.470637 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 534 macros, 0 groups, 0 semantic models
[0m17:11:47.471647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '168ec0d3-24e8-4577-ac4e-d50b4c057003', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191CD87E570>]}
[0m17:11:47.477739 [info ] [MainThread]: 
[0m17:11:47.479760 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:11:47.484807 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:11:47.511176 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m17:11:47.513192 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m17:11:47.514199 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:11:56.669398 [debug] [ThreadPool]: SQL status: OK in 9.15999984741211 seconds
[0m17:11:56.679555 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m17:11:56.682582 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m17:11:56.697865 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:11:56.699887 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m17:11:56.700898 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m17:12:08.855596 [debug] [ThreadPool]: SQL status: OK in 12.149999618530273 seconds
[0m17:12:08.859682 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:12:08.866751 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m17:12:08.883956 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:12:08.885977 [debug] [ThreadPool]: Using fabricsparknb connection "list_None_test"
[0m17:12:08.887039 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m17:12:08.888041 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:12:08.890057 [debug] [ThreadPool]: fabricsparknb adapter: Reusing session: 0
[0m17:12:10.669503 [debug] [ThreadPool]: SQL status: OK in 1.7799999713897705 seconds
[0m17:12:10.679757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '168ec0d3-24e8-4577-ac4e-d50b4c057003', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191CE09F770>]}
[0m17:12:10.683336 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:12:10.684854 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:12:10.687892 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:12:10.690449 [info ] [MainThread]: 
[0m17:12:10.701222 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m17:12:10.703244 [info ] [Thread-7 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m17:12:10.705778 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:12:10.707298 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:12:10.748240 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:12:10.751287 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:12:10.708312 => 17:12:10.750274
[0m17:12:10.753312 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:12:10.823368 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:12:10.826410 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m17:12:10.830480 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m17:12:10.832550 [debug] [Thread-7 (]: fabricsparknb adapter: Reusing session: 0
[0m17:12:14.469077 [debug] [Thread-7 (]: SQL status: OK in 3.640000104904175 seconds
[0m17:12:14.586235 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:12:14.589320 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:12:14.590371 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:12:14.592392 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:12:20.513100 [debug] [Thread-7 (]: SQL status: OK in 5.920000076293945 seconds
[0m17:12:20.569889 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:12:10.754320 => 17:12:20.568877
[0m17:12:20.572926 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '168ec0d3-24e8-4577-ac4e-d50b4c057003', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191CE145850>]}
[0m17:12:20.574947 [info ] [Thread-7 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 9.87s]
[0m17:12:20.577985 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:12:20.582083 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m17:12:20.584114 [info ] [Thread-7 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m17:12:20.587151 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m17:12:20.589173 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:12:20.600358 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:12:20.603421 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:12:20.590186 => 17:12:20.602407
[0m17:12:20.605447 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:12:20.658786 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:12:20.661837 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:12:20.664923 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m17:12:20.665937 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m17:12:20.673026 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:12:20.606455 => 17:12:20.673026
[0m17:12:20.676087 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '168ec0d3-24e8-4577-ac4e-d50b4c057003', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191CDE3D760>]}
[0m17:12:20.678109 [info ] [Thread-7 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.09s]
[0m17:12:20.681192 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:12:20.685235 [debug] [MainThread]: On master: ROLLBACK
[0m17:12:20.686242 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:12:20.688260 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:12:20.689303 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:12:20.690311 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:12:20.692325 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:12:20.694344 [info ] [MainThread]: 
[0m17:12:20.697398 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 33.21 seconds (33.21s).
[0m17:12:20.700542 [debug] [MainThread]: Command end result
[0m17:12:20.722371 [info ] [MainThread]: 
[0m17:12:20.724414 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:12:20.726440 [info ] [MainThread]: 
[0m17:12:20.730518 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m17:12:20.737719 [debug] [MainThread]: Command `cli run` succeeded at 17:12:20.736704 after 34.09 seconds
[0m17:12:20.742812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191CD9914F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191CDE3D940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000191CDE3FD10>]}
[0m17:12:20.745860 [debug] [MainThread]: Flushing usage events
[0m17:19:39.184479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224431455B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022443145340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022443144530>]}


============================== 17:19:39.184479 | 20259d82-5927-45fd-9afc-d555f602b2db ==============================
[0m17:19:39.184479 [info ] [MainThread]: Running with dbt=1.7.4
[0m17:19:39.185492 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:19:39.299625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '20259d82-5927-45fd-9afc-d555f602b2db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224430F7B00>]}
[0m17:19:39.349021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '20259d82-5927-45fd-9afc-d555f602b2db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224417DFB00>]}
[0m17:19:39.350032 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:19:39.357106 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m17:19:39.395454 [debug] [MainThread]: Partial parsing enabled: 30 files deleted, 0 files added, 0 files changed.
[0m17:19:39.396461 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\utils\assert_not_null.sql
[0m17:19:39.397473 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\adapters\columns.sql
[0m17:19:39.397473 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\materializations\models\table\create_table_as.sql
[0m17:19:39.398485 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\materializations\snapshots\snapshot.sql
[0m17:19:39.398485 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\utils\array_concat.sql
[0m17:19:39.398485 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\utils\array_construct.sql
[0m17:19:39.399498 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\utils\dateadd.sql
[0m17:19:39.399498 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\materializations\test.sql
[0m17:19:39.399498 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\materializations\models\incremental\validate.sql
[0m17:19:39.400509 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\materializations\models\table\table.sql
[0m17:19:39.400509 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\materializations\models\table\clone.sql
[0m17:19:39.400509 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\utils\bool_or.sql
[0m17:19:39.401518 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\utils\concat.sql
[0m17:19:39.401518 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\utils\array_append.sql
[0m17:19:39.401518 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\utils\escape_single_quotes.sql
[0m17:19:39.401518 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\adapters\schema.sql
[0m17:19:39.402525 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\adapters\relation.sql
[0m17:19:39.402525 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\utils\datediff.sql
[0m17:19:39.402525 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\materializations\models\incremental\strategies.sql
[0m17:19:39.403531 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\materializations\models\view\create_view_as.sql
[0m17:19:39.403531 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\materializations\models\incremental\incremental.sql
[0m17:19:39.403531 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\materializations\models\incremental\column_helpers.sql
[0m17:19:39.403531 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\utils\listagg.sql
[0m17:19:39.404538 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\materializations\seeds\seed.sql
[0m17:19:39.404538 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\materializations\models\view\mv.sql
[0m17:19:39.404538 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\utils\any_value.sql
[0m17:19:39.404538 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\adapters.sql
[0m17:19:39.405545 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\utils\split_part.sql
[0m17:19:39.405545 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\apply_grants.sql
[0m17:19:39.405545 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\utils\timestamps.sql
[0m17:19:39.683880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '20259d82-5927-45fd-9afc-d555f602b2db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224432733E0>]}
[0m17:19:39.689940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '20259d82-5927-45fd-9afc-d555f602b2db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224443A4710>]}
[0m17:19:39.689940 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:19:39.690951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '20259d82-5927-45fd-9afc-d555f602b2db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022442539D00>]}
[0m17:19:39.691962 [info ] [MainThread]: 
[0m17:19:39.692971 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:19:39.692971 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:19:39.699024 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m17:19:39.700032 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m17:19:39.700032 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:19:39.701040 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:19:39.704087 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m17:19:39.705096 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m17:19:39.709141 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:19:39.710152 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m17:19:39.710152 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m17:19:39.710152 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:19:39.711159 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:19:39.712167 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m17:19:39.715188 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:19:39.715188 [debug] [ThreadPool]: Using fabricsparknb connection "list_None_test"
[0m17:19:39.715188 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m17:19:39.716195 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:19:39.716195 [debug] [ThreadPool]: fabricsparknb adapter: Reusing session: 0
[0m17:19:39.716195 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:19:39.717226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '20259d82-5927-45fd-9afc-d555f602b2db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022444485940>]}
[0m17:19:39.717226 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:19:39.718234 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:19:39.718234 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:19:39.718234 [info ] [MainThread]: 
[0m17:19:39.720249 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:19:39.721258 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m17:19:39.721258 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:19:39.722269 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:19:39.726310 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:19:39.727317 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:19:39.722269 => 17:19:39.726310
[0m17:19:39.727317 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:19:39.737408 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:19:39.738415 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m17:19:39.738415 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:19:39.738415 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m17:19:39.738415 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:19:39.760612 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:19:39.761619 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:19:39.761619 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:19:39.762632 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:19:39.762632 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:19:39.771697 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:19:39.727317 => 17:19:39.771697
[0m17:19:39.772704 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '20259d82-5927-45fd-9afc-d555f602b2db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000224432B7860>]}
[0m17:19:39.772704 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.05s]
[0m17:19:39.773712 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:19:39.773712 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:19:39.774721 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m17:19:39.774721 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m17:19:39.775732 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:19:39.777776 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:19:39.778787 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:19:39.775732 => 17:19:39.778787
[0m17:19:39.778787 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:19:39.823443 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:19:39.824453 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:19:39.825460 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m17:19:39.825460 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:19:39.826470 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:19:39.779800 => 17:19:39.826470
[0m17:19:39.827477 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '20259d82-5927-45fd-9afc-d555f602b2db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002244452AF00>]}
[0m17:19:39.827477 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.05s]
[0m17:19:39.828484 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:19:39.829492 [debug] [MainThread]: On master: ROLLBACK
[0m17:19:39.829492 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:19:39.830498 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:19:39.830498 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:19:39.830498 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:19:39.831506 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:19:39.831506 [info ] [MainThread]: 
[0m17:19:39.832513 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m17:19:39.832513 [debug] [MainThread]: Command end result
[0m17:19:39.838603 [info ] [MainThread]: 
[0m17:19:39.838603 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:19:39.839612 [info ] [MainThread]: 
[0m17:19:39.839612 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m17:19:39.841628 [debug] [MainThread]: Command `cli run` succeeded at 17:19:39.840619 after 0.68 seconds
[0m17:19:39.841628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022442951580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022444504110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022444504920>]}
[0m17:19:39.841628 [debug] [MainThread]: Flushing usage events
[0m17:21:52.456226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027918D09100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027918D08E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027918D09160>]}


============================== 17:21:52.456226 | 1c2aeb68-c7ea-4a27-bc82-1ffa3b8acbd6 ==============================
[0m17:21:52.456226 [info ] [MainThread]: Running with dbt=1.7.4
[0m17:21:52.457239 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:21:52.556713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1c2aeb68-c7ea-4a27-bc82-1ffa3b8acbd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027918B23770>]}
[0m17:21:52.607137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1c2aeb68-c7ea-4a27-bc82-1ffa3b8acbd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027919DF2450>]}
[0m17:21:52.608144 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:21:52.614197 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m17:21:52.658564 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:21:52.658564 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:21:52.662591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1c2aeb68-c7ea-4a27-bc82-1ffa3b8acbd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027918A74DA0>]}
[0m17:21:52.667626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1c2aeb68-c7ea-4a27-bc82-1ffa3b8acbd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027919F16C00>]}
[0m17:21:52.668635 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:21:52.668635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1c2aeb68-c7ea-4a27-bc82-1ffa3b8acbd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027918D08080>]}
[0m17:21:52.670660 [info ] [MainThread]: 
[0m17:21:52.670660 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:21:52.671673 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:21:52.677740 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m17:21:52.677740 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m17:21:52.677740 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:21:52.677740 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:21:52.679756 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m17:21:52.679756 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m17:21:52.682789 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:21:52.682789 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m17:21:52.682789 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m17:21:52.682789 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:21:52.683802 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:21:52.684813 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m17:21:52.688920 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:21:52.688920 [debug] [ThreadPool]: Using fabricsparknb connection "list_None_test"
[0m17:21:52.688920 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m17:21:52.689927 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:21:52.689927 [debug] [ThreadPool]: fabricsparknb adapter: Reusing session: 0
[0m17:21:52.689927 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m17:21:52.690934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1c2aeb68-c7ea-4a27-bc82-1ffa3b8acbd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002791A07B530>]}
[0m17:21:52.690934 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:21:52.691940 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:21:52.691940 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:21:52.691940 [info ] [MainThread]: 
[0m17:21:52.693954 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:21:52.693954 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m17:21:52.694961 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:21:52.694961 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:21:52.699004 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:21:52.700015 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:21:52.695974 => 17:21:52.700015
[0m17:21:52.700015 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:21:52.711117 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:21:52.712124 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m17:21:52.712124 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:21:52.712124 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m17:21:52.712124 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:21:52.735324 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:21:52.736331 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:21:52.736331 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:21:52.737339 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:21:52.737339 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:21:52.747447 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:21:52.700015 => 17:21:52.747447
[0m17:21:52.747447 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1c2aeb68-c7ea-4a27-bc82-1ffa3b8acbd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002791A0D7380>]}
[0m17:21:52.748459 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.05s]
[0m17:21:52.749485 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:21:52.749994 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:21:52.750502 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m17:21:52.751511 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m17:21:52.751511 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:21:52.753527 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:21:52.754536 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:21:52.751511 => 17:21:52.753527
[0m17:21:52.754536 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:21:52.763603 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:21:52.764624 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:21:52.764624 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m17:21:52.764624 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:21:52.766639 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:21:52.754536 => 17:21:52.765632
[0m17:21:52.766639 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1c2aeb68-c7ea-4a27-bc82-1ffa3b8acbd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002791A0D4380>]}
[0m17:21:52.767648 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.02s]
[0m17:21:52.767648 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:21:52.768655 [debug] [MainThread]: On master: ROLLBACK
[0m17:21:52.769664 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:21:52.769664 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:21:52.770671 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:21:52.770671 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:21:52.770671 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:21:52.771680 [info ] [MainThread]: 
[0m17:21:52.771680 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.10 seconds (0.10s).
[0m17:21:52.772693 [debug] [MainThread]: Command end result
[0m17:21:52.777740 [info ] [MainThread]: 
[0m17:21:52.778747 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:21:52.778747 [info ] [MainThread]: 
[0m17:21:52.779769 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m17:21:52.780778 [debug] [MainThread]: Command `cli run` succeeded at 17:21:52.780778 after 0.35 seconds
[0m17:21:52.780778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027918C5FB00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027918ABCFB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027918C2F110>]}
[0m17:21:52.781786 [debug] [MainThread]: Flushing usage events
[0m19:14:54.826863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0DB8F4260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0DB8F4440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0DB8F42C0>]}


============================== 19:14:54.827875 | b2eabe62-facf-425e-835e-526f2be5862b ==============================
[0m19:14:54.827875 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:14:54.828884 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m19:14:54.925858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b2eabe62-facf-425e-835e-526f2be5862b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0DB95E150>]}
[0m19:14:54.977398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b2eabe62-facf-425e-835e-526f2be5862b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0DB9366C0>]}
[0m19:14:54.979431 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:14:54.992543 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:14:55.589334 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:14:55.589839 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:14:55.593887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b2eabe62-facf-425e-835e-526f2be5862b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0DC9E3E60>]}
[0m19:14:55.598965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b2eabe62-facf-425e-835e-526f2be5862b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0DCB06540>]}
[0m19:14:55.599471 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:14:55.599977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b2eabe62-facf-425e-835e-526f2be5862b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0DB629430>]}
[0m19:14:55.600990 [info ] [MainThread]: 
[0m19:14:55.601497 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:14:55.602509 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:14:55.608079 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:14:55.608587 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:14:55.608587 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:14:55.609092 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:14:55.610116 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m19:14:55.610623 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m19:14:55.613153 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:14:55.613659 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m19:14:55.613659 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m19:14:55.614163 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:14:55.614674 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:14:55.615698 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m19:14:55.618735 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:14:55.619240 [debug] [ThreadPool]: Using fabricsparknb connection "list_None_test"
[0m19:14:55.619240 [debug] [ThreadPool]: On list_None_test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_None_test"} */
show table extended in test like '*'
  
[0m19:14:55.619746 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:14:55.619746 [debug] [ThreadPool]: fabricsparknb adapter: Reusing session: 0
[0m19:14:55.620252 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:14:55.620757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b2eabe62-facf-425e-835e-526f2be5862b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0DCA9F590>]}
[0m19:14:55.621263 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:14:55.621769 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:14:55.622275 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:14:55.622275 [info ] [MainThread]: 
[0m19:14:55.624315 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m19:14:55.625354 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m19:14:55.625862 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:14:55.626370 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:14:55.629910 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:14:55.630924 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:14:55.626370 => 19:14:55.630924
[0m19:14:55.630924 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:14:55.642066 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:14:55.642572 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m19:14:55.642572 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:14:55.643077 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m19:14:55.643077 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:14:55.665833 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:14:55.666843 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:14:55.667855 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:14:55.667855 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:14:55.667855 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:14:55.677960 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:14:55.631431 => 19:14:55.677960
[0m19:14:55.678966 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b2eabe62-facf-425e-835e-526f2be5862b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0DCD06C90>]}
[0m19:14:55.678966 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.05s]
[0m19:14:55.679972 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:14:55.680981 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m19:14:55.680981 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m19:14:55.681993 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:14:55.681993 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:14:55.684016 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:14:55.685029 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:14:55.681993 => 19:14:55.684016
[0m19:14:55.685029 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:14:55.694112 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:14:55.695118 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:14:55.696129 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m19:14:55.696129 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:14:55.697136 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:14:55.685029 => 19:14:55.697136
[0m19:14:55.698144 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b2eabe62-facf-425e-835e-526f2be5862b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0DCD05190>]}
[0m19:14:55.698144 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.02s]
[0m19:14:55.699154 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:14:55.700164 [debug] [MainThread]: On master: ROLLBACK
[0m19:14:55.700164 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:14:55.700164 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:14:55.701187 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:14:55.701187 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:14:55.701187 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:14:55.702196 [info ] [MainThread]: 
[0m19:14:55.702196 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.10 seconds (0.10s).
[0m19:14:55.703203 [debug] [MainThread]: Command end result
[0m19:14:55.709257 [info ] [MainThread]: 
[0m19:14:55.709257 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:14:55.710266 [info ] [MainThread]: 
[0m19:14:55.710266 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:14:55.711277 [debug] [MainThread]: Command `cli run` succeeded at 19:14:55.711277 after 0.92 seconds
[0m19:14:55.712286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0DCC9A390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0DCD097F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0DB8B7B30>]}
[0m19:14:55.712286 [debug] [MainThread]: Flushing usage events
[0m19:15:45.845713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFEA731970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFEA7320F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFEA732A20>]}


============================== 19:15:45.847745 | a083133d-7566-484c-90a4-67054137f52d ==============================
[0m19:15:45.847745 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:15:45.849761 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:15:46.179799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a083133d-7566-484c-90a4-67054137f52d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFEA8ABFB0>]}
[0m19:15:46.350733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a083133d-7566-484c-90a4-67054137f52d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFEA927110>]}
[0m19:15:46.353766 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:15:46.382157 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:15:46.478249 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:15:46.479257 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:15:46.498480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a083133d-7566-484c-90a4-67054137f52d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFEA926C30>]}
[0m19:15:46.513634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a083133d-7566-484c-90a4-67054137f52d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFEA9FC4D0>]}
[0m19:15:46.514645 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:15:46.516657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a083133d-7566-484c-90a4-67054137f52d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFEA9FC050>]}
[0m19:15:46.522736 [info ] [MainThread]: 
[0m19:15:46.525270 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:15:46.530840 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:15:46.559204 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:15:46.561219 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:15:46.563251 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:15:48.550643 [debug] [ThreadPool]: SQL status: OK in 1.9900000095367432 seconds
[0m19:15:48.558726 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m19:15:48.561746 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m19:15:48.574403 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:15:48.576435 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m19:15:48.577448 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m19:15:50.068628 [debug] [ThreadPool]: SQL status: OK in 1.4900000095367432 seconds
[0m19:15:50.073702 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:15:50.079241 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m19:18:29.174220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F54990A210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F54990A270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F549909E50>]}


============================== 19:18:29.177254 | a252aa0f-619c-4ea7-b4db-08deff106af8 ==============================
[0m19:18:29.177254 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:18:29.179266 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:18:29.501647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a252aa0f-619c-4ea7-b4db-08deff106af8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F549A41CD0>]}
[0m19:18:29.669013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a252aa0f-619c-4ea7-b4db-08deff106af8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F546CB7B60>]}
[0m19:18:29.672097 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:18:29.711604 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:18:30.382045 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:18:30.384090 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:18:30.401220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a252aa0f-619c-4ea7-b4db-08deff106af8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F549D5FFE0>]}
[0m19:18:30.415337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a252aa0f-619c-4ea7-b4db-08deff106af8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F549C2F890>]}
[0m19:18:30.417350 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:18:30.418357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a252aa0f-619c-4ea7-b4db-08deff106af8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F549C2F140>]}
[0m19:18:30.424398 [info ] [MainThread]: 
[0m19:18:30.427420 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:18:30.431461 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:18:30.457694 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:18:30.459708 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:18:30.460720 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:18:33.249766 [debug] [ThreadPool]: SQL status: OK in 2.7899999618530273 seconds
[0m19:18:33.256830 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m19:18:33.259877 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m19:18:33.271980 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:18:33.272989 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m19:18:33.275037 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m19:18:34.890544 [debug] [ThreadPool]: SQL status: OK in 1.6100000143051147 seconds
[0m19:18:34.894584 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:18:34.900660 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m19:19:04.651894 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about test: Invalid value from "show tables extended ...", got 523 values, expected 4
[0m19:19:04.656976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a252aa0f-619c-4ea7-b4db-08deff106af8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F549DBF7D0>]}
[0m19:19:04.660039 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:19:04.661046 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:19:04.664078 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:19:04.665090 [info ] [MainThread]: 
[0m19:19:04.673195 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m19:19:04.675212 [info ] [Thread-7 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m19:19:04.677229 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:19:04.679249 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:19:04.712736 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:19:04.715783 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:19:04.680261 => 19:19:04.714767
[0m19:19:04.717842 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:19:04.774039 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:19:04.775046 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m19:19:04.777060 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m19:19:04.778067 [debug] [Thread-7 (]: fabricsparknb adapter: Reusing session: 0
[0m19:19:07.260748 [debug] [Thread-7 (]: SQL status: OK in 2.4800000190734863 seconds
[0m19:19:07.361873 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:19:07.363887 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:19:07.364897 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:19:07.366932 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:19:07.883372 [debug] [Thread-7 (]: SQL status: OK in 0.5199999809265137 seconds
[0m19:19:07.926903 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:19:04.718857 => 19:19:07.926395
[0m19:19:07.929944 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a252aa0f-619c-4ea7-b4db-08deff106af8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F549E686B0>]}
[0m19:19:07.931962 [info ] [Thread-7 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 3.25s]
[0m19:19:07.933982 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:19:07.937013 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m19:19:07.939033 [info ] [Thread-7 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m19:19:07.941052 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:19:07.943070 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:19:07.952200 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:19:07.954219 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:19:07.944109 => 19:19:07.953208
[0m19:19:07.955225 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:19:08.005120 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:19:08.008166 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:19:08.009177 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m19:19:11.073498 [debug] [Thread-7 (]: SQL status: OK in 3.059999942779541 seconds
[0m19:19:11.079570 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:19:07.956235 => 19:19:11.078562
[0m19:19:11.081593 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a252aa0f-619c-4ea7-b4db-08deff106af8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F549E771D0>]}
[0m19:19:11.083608 [info ] [Thread-7 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 3.14s]
[0m19:19:11.085624 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:19:11.089663 [debug] [MainThread]: On master: ROLLBACK
[0m19:19:11.091713 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:19:11.092724 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:19:11.093735 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:19:11.095760 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:19:11.096774 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:19:11.099820 [info ] [MainThread]: 
[0m19:19:11.101840 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 40.67 seconds (40.67s).
[0m19:19:11.103857 [debug] [MainThread]: Command end result
[0m19:19:11.120089 [info ] [MainThread]: 
[0m19:19:11.122159 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:19:11.124203 [info ] [MainThread]: 
[0m19:19:11.125216 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:19:11.130279 [debug] [MainThread]: Command `cli run` succeeded at 19:19:11.129263 after 42.07 seconds
[0m19:19:11.135416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F549908470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F549E682F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F549E6B4A0>]}
[0m19:19:11.140615 [debug] [MainThread]: Flushing usage events
[0m19:21:01.613134 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F704671430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F704670FB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F704671490>]}


============================== 19:21:01.614148 | daa66a17-7e7b-4e03-8211-bb48d7a3dfd6 ==============================
[0m19:21:01.614148 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:21:01.614660 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m19:21:01.708987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'daa66a17-7e7b-4e03-8211-bb48d7a3dfd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7046CA1B0>]}
[0m19:21:01.758353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'daa66a17-7e7b-4e03-8211-bb48d7a3dfd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7057E67E0>]}
[0m19:21:01.759361 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:21:01.765403 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:21:01.802682 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:21:01.802682 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:21:01.806729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'daa66a17-7e7b-4e03-8211-bb48d7a3dfd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F705776660>]}
[0m19:21:01.812771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'daa66a17-7e7b-4e03-8211-bb48d7a3dfd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F705876AB0>]}
[0m19:21:01.812771 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:21:01.812771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'daa66a17-7e7b-4e03-8211-bb48d7a3dfd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7046C83E0>]}
[0m19:21:01.813781 [info ] [MainThread]: 
[0m19:21:01.814784 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:21:01.815904 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:21:01.821993 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:21:01.821993 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:21:01.823004 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:21:01.823004 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:01.824010 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m19:21:01.824010 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m19:21:01.827030 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:21:01.827030 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m19:21:01.827030 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m19:21:01.828035 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:21:01.828035 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:21:01.829042 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m19:21:01.868379 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about test: Invalid value from "show tables extended ...", got 523 values, expected 4
[0m19:21:01.868379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'daa66a17-7e7b-4e03-8211-bb48d7a3dfd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7059DF9B0>]}
[0m19:21:01.869389 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:21:01.869389 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:21:01.869389 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:21:01.870395 [info ] [MainThread]: 
[0m19:21:01.872413 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m19:21:01.872413 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m19:21:01.873423 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:21:01.873423 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:21:01.877468 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:21:01.877468 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:21:01.873423 => 19:21:01.877468
[0m19:21:01.878480 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:21:01.889604 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:21:01.889604 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m19:21:01.889604 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:21:01.890617 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m19:21:01.890617 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:21:01.912821 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:21:01.913837 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:21:01.913837 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:21:01.914840 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:21:01.914840 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:21:01.924439 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:21:01.878480 => 19:21:01.924439
[0m19:21:01.924946 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'daa66a17-7e7b-4e03-8211-bb48d7a3dfd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F705A0F380>]}
[0m19:21:01.925452 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.05s]
[0m19:21:01.925962 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:21:01.926472 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m19:21:01.926472 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m19:21:01.927480 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:21:01.927480 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:21:01.929510 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:21:01.930524 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:21:01.927480 => 19:21:01.929510
[0m19:21:01.930524 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:21:01.939609 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:21:01.940617 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:21:01.940617 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m19:21:01.941626 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:21:01.942651 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:21:01.930524 => 19:21:01.942651
[0m19:21:01.942651 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'daa66a17-7e7b-4e03-8211-bb48d7a3dfd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F705A0DAC0>]}
[0m19:21:01.943659 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.02s]
[0m19:21:01.943659 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:21:01.944704 [debug] [MainThread]: On master: ROLLBACK
[0m19:21:01.945710 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:21:01.945710 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:21:01.945710 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:21:01.946716 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:21:01.946716 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:21:01.946716 [info ] [MainThread]: 
[0m19:21:01.947723 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m19:21:01.948730 [debug] [MainThread]: Command end result
[0m19:21:01.953768 [info ] [MainThread]: 
[0m19:21:01.953768 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:21:01.954777 [info ] [MainThread]: 
[0m19:21:01.954777 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:21:01.955785 [debug] [MainThread]: Command `cli run` succeeded at 19:21:01.955785 after 0.37 seconds
[0m19:21:01.956796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F704627B30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7059CA570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F70471C380>]}
[0m19:21:01.956796 [debug] [MainThread]: Flushing usage events
[0m19:22:33.149596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AE002C080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AE002C710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AE002C050>]}


============================== 19:22:33.150602 | 59b8f772-1d2e-4dfc-80d3-7d682d9be393 ==============================
[0m19:22:33.150602 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:22:33.150602 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:22:33.246763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '59b8f772-1d2e-4dfc-80d3-7d682d9be393', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026ADFF7FC20>]}
[0m19:22:33.296197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '59b8f772-1d2e-4dfc-80d3-7d682d9be393', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AE01D7CE0>]}
[0m19:22:33.297205 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:22:33.303258 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:22:33.339583 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:22:33.340593 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:22:33.344625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '59b8f772-1d2e-4dfc-80d3-7d682d9be393', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AE00FEDE0>]}
[0m19:22:33.349676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '59b8f772-1d2e-4dfc-80d3-7d682d9be393', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AE1236690>]}
[0m19:22:33.349676 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:22:33.350685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '59b8f772-1d2e-4dfc-80d3-7d682d9be393', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AE00DFC20>]}
[0m19:22:33.351698 [info ] [MainThread]: 
[0m19:22:33.351698 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:22:33.352706 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:22:33.358754 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:22:33.358754 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:22:33.359762 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:22:33.359762 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:33.360769 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m19:22:33.361777 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m19:22:33.364323 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:22:33.364829 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m19:22:33.364829 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m19:22:33.364829 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:22:33.365837 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:22:33.366847 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m19:22:33.401132 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about test: Invalid value from "show tables extended ...", got 523 values, expected 4
[0m19:22:33.402139 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '59b8f772-1d2e-4dfc-80d3-7d682d9be393', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AE01D7E90>]}
[0m19:22:33.402139 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:22:33.402139 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:22:33.403146 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:22:33.403146 [info ] [MainThread]: 
[0m19:22:33.405163 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m19:22:33.405163 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m19:22:33.406173 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:22:33.407183 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:22:33.411242 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:22:33.411242 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:22:33.407183 => 19:22:33.411242
[0m19:22:33.412254 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:22:33.422322 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:22:33.423328 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m19:22:33.423328 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:22:33.423328 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m19:22:33.424338 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:22:33.446557 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:22:33.447564 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:22:33.447564 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:22:33.447564 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:22:33.447564 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:22:33.457651 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:22:33.412254 => 19:22:33.457651
[0m19:22:33.457651 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59b8f772-1d2e-4dfc-80d3-7d682d9be393', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AE13AC560>]}
[0m19:22:33.458662 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.05s]
[0m19:22:33.458662 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:22:33.459674 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m19:22:33.459674 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m19:22:33.460682 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:22:33.460682 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:22:33.462700 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:22:33.462700 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:22:33.460682 => 19:22:33.462700
[0m19:22:33.463707 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:22:33.472807 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:22:33.472807 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:22:33.473814 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m19:22:33.473814 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:22:33.474825 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:22:33.463707 => 19:22:33.474825
[0m19:22:33.475837 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '59b8f772-1d2e-4dfc-80d3-7d682d9be393', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AE13ACB00>]}
[0m19:22:33.475837 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.02s]
[0m19:22:33.476846 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:22:33.477858 [debug] [MainThread]: On master: ROLLBACK
[0m19:22:33.477858 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:22:33.477858 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:22:33.477858 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:22:33.478868 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:22:33.478868 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:22:33.478868 [info ] [MainThread]: 
[0m19:22:33.479875 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m19:22:33.479875 [debug] [MainThread]: Command end result
[0m19:22:33.483901 [info ] [MainThread]: 
[0m19:22:33.484909 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:22:33.484909 [info ] [MainThread]: 
[0m19:22:33.484909 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:22:33.485940 [debug] [MainThread]: Command `cli run` succeeded at 19:22:33.485940 after 0.36 seconds
[0m19:22:33.485940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026ADFCD95E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AE13B4A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AE13B70B0>]}
[0m19:22:33.486948 [debug] [MainThread]: Flushing usage events
[0m19:23:07.772233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D0848470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D084A690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D0849AF0>]}


============================== 19:23:07.775257 | 8ea6f457-5c0c-4a56-aa9f-9bba17805d87 ==============================
[0m19:23:07.775257 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:23:07.777272 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m19:23:08.091820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8ea6f457-5c0c-4a56-aa9f-9bba17805d87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D08C3920>]}
[0m19:23:08.262349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8ea6f457-5c0c-4a56-aa9f-9bba17805d87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D0A68590>]}
[0m19:23:08.265374 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:23:08.293651 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:23:08.385507 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:23:08.386519 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:23:08.404662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8ea6f457-5c0c-4a56-aa9f-9bba17805d87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D08C0590>]}
[0m19:23:08.417773 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8ea6f457-5c0c-4a56-aa9f-9bba17805d87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D0B02180>]}
[0m19:23:08.419788 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:23:08.421829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8ea6f457-5c0c-4a56-aa9f-9bba17805d87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D0B01A00>]}
[0m19:23:08.426892 [info ] [MainThread]: 
[0m19:23:08.429927 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:23:08.433965 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:23:08.463300 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:23:08.465322 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:23:08.466357 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:23:08.468376 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:08.475438 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m19:23:08.477457 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m19:23:08.490586 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:23:08.491595 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m19:23:08.492603 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m19:23:08.494622 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:23:08.497785 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:23:08.503847 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m19:23:55.253013 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about test: Invalid value from "show tables extended ...", got 523 values, expected 4
[0m19:23:55.257064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8ea6f457-5c0c-4a56-aa9f-9bba17805d87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D0B03050>]}
[0m19:23:55.259090 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:23:55.261110 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:23:55.263123 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:23:55.264130 [info ] [MainThread]: 
[0m19:23:55.271227 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m19:23:55.273244 [info ] [Thread-7 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m19:23:55.276281 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:23:55.277292 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:23:55.310668 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:23:55.314735 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:23:55.278301 => 19:23:55.313721
[0m19:23:55.315743 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:23:55.371374 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:23:55.373389 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m19:23:55.375449 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m19:23:55.376458 [debug] [Thread-7 (]: fabricsparknb adapter: Reusing session: 0
[0m19:23:55.378474 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m19:23:55.477559 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:23:55.480583 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:23:55.481608 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:23:55.482618 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:23:55.484642 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m19:23:55.529141 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:23:55.317781 => 19:23:55.528134
[0m19:23:55.531156 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8ea6f457-5c0c-4a56-aa9f-9bba17805d87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D0D34F50>]}
[0m19:23:55.533171 [info ] [Thread-7 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.26s]
[0m19:23:55.535188 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:23:55.538211 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m19:23:55.540227 [info ] [Thread-7 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m19:23:55.542248 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:23:55.543275 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:23:55.552372 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:23:55.555403 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:23:55.544287 => 19:23:55.554394
[0m19:23:55.556420 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:23:55.607467 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:23:55.611537 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:23:55.613576 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m19:23:55.614583 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m19:23:55.621168 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:23:55.558510 => 19:23:55.621168
[0m19:23:55.624193 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8ea6f457-5c0c-4a56-aa9f-9bba17805d87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D0D629C0>]}
[0m19:23:55.626218 [info ] [Thread-7 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.08s]
[0m19:23:55.629268 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:23:55.633311 [debug] [MainThread]: On master: ROLLBACK
[0m19:23:55.635348 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:23:55.636360 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:23:55.637371 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:23:55.639393 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:23:55.640402 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:23:55.643446 [info ] [MainThread]: 
[0m19:23:55.645478 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 47.21 seconds (47.21s).
[0m19:23:55.648539 [debug] [MainThread]: Command end result
[0m19:23:55.665888 [info ] [MainThread]: 
[0m19:23:55.669036 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:23:55.672143 [info ] [MainThread]: 
[0m19:23:55.674178 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:23:55.678230 [debug] [MainThread]: Command `cli run` succeeded at 19:23:55.678230 after 47.99 seconds
[0m19:23:55.681287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D08481D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D0D30E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A4D0D31EE0>]}
[0m19:23:55.682306 [debug] [MainThread]: Flushing usage events
[0m19:24:33.391260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAC20ACF80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAC20AC110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAC20AD0D0>]}


============================== 19:24:33.392268 | d4af214f-9b4b-468c-9147-834bf31401c2 ==============================
[0m19:24:33.392268 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:24:33.392268 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m19:24:33.488064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd4af214f-9b4b-468c-9147-834bf31401c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAC20E66C0>]}
[0m19:24:33.537432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd4af214f-9b4b-468c-9147-834bf31401c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAC20D5FD0>]}
[0m19:24:33.538440 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:24:33.544979 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:24:33.581277 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:24:33.582283 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:24:33.585316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd4af214f-9b4b-468c-9147-834bf31401c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAC31CE7B0>]}
[0m19:24:33.590351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd4af214f-9b4b-468c-9147-834bf31401c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAC32CA870>]}
[0m19:24:33.591358 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:24:33.591358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd4af214f-9b4b-468c-9147-834bf31401c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAC19844A0>]}
[0m19:24:33.592366 [info ] [MainThread]: 
[0m19:24:33.593374 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:24:33.594383 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:24:33.599432 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:24:33.600437 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:24:33.600437 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:24:33.601445 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:33.602454 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__test)
[0m19:24:33.602454 [debug] [ThreadPool]: Creating schema "schema: "test"
"
[0m19:24:33.605474 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:24:33.605474 [debug] [ThreadPool]: Using fabricsparknb connection "create__test"
[0m19:24:33.605474 [debug] [ThreadPool]: On create__test: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__test"} */

    select 1
  
[0m19:24:33.606481 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:24:33.606481 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:24:33.607488 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_test'
[0m19:24:33.642265 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about test: Invalid value from "show tables extended ...", got 523 values, expected 4
[0m19:24:33.643276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd4af214f-9b4b-468c-9147-834bf31401c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAC342BAA0>]}
[0m19:24:33.643276 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:24:33.643276 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:24:33.644287 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:24:33.644287 [info ] [MainThread]: 
[0m19:24:33.647325 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m19:24:33.647325 [info ] [Thread-1 (]: 1 of 2 START sql table model test.my_first_dbt_model ........................... [RUN]
[0m19:24:33.648333 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:24:33.648333 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:24:33.652362 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:24:33.653369 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:24:33.648333 => 19:24:33.653369
[0m19:24:33.654376 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:24:33.665487 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:24:33.665487 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists test.my_first_dbt_model
[0m19:24:33.666501 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:24:33.666501 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m19:24:33.667510 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:24:33.689688 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:24:33.690695 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:24:33.690695 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:24:33.691703 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table test.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:24:33.691703 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:24:33.701792 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:24:33.654376 => 19:24:33.701792
[0m19:24:33.702799 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4af214f-9b4b-468c-9147-834bf31401c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAC3467650>]}
[0m19:24:33.703806 [info ] [Thread-1 (]: 1 of 2 OK created sql table model test.my_first_dbt_model ...................... [[32mOK[0m in 0.06s]
[0m19:24:33.703806 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:24:33.704813 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m19:24:33.704813 [info ] [Thread-1 (]: 2 of 2 START sql view model test.my_second_dbt_model ........................... [RUN]
[0m19:24:33.705819 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:24:33.705819 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:24:33.707834 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:24:33.708841 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:24:33.706827 => 19:24:33.708841
[0m19:24:33.708841 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:24:33.717924 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:24:33.718931 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:24:33.719938 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view test.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from test.my_first_dbt_model
where id = 1

[0m19:24:33.719938 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:24:33.721957 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:24:33.709863 => 19:24:33.721957
[0m19:24:33.722965 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd4af214f-9b4b-468c-9147-834bf31401c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAC3464E60>]}
[0m19:24:33.723973 [info ] [Thread-1 (]: 2 of 2 OK created sql view model test.my_second_dbt_model ...................... [[32mOK[0m in 0.02s]
[0m19:24:33.723973 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:24:33.726032 [debug] [MainThread]: On master: ROLLBACK
[0m19:24:33.726032 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:24:33.727035 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:24:33.727035 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:24:33.727537 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:24:33.728045 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:24:33.728552 [info ] [MainThread]: 
[0m19:24:33.729059 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m19:24:33.729564 [debug] [MainThread]: Command end result
[0m19:24:33.735651 [info ] [MainThread]: 
[0m19:24:33.735651 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:24:33.736661 [info ] [MainThread]: 
[0m19:24:33.736661 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:24:33.738679 [debug] [MainThread]: Command `cli run` succeeded at 19:24:33.738679 after 0.37 seconds
[0m19:24:33.738679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAC3426270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAC20E6AE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAC1DFB0B0>]}
[0m19:24:33.739696 [debug] [MainThread]: Flushing usage events
[0m19:26:38.544550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB093901A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB093902C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB09391070>]}


============================== 19:26:38.544550 | a911f183-be66-4329-a61f-068c70694c56 ==============================
[0m19:26:38.544550 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:26:38.545557 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:26:38.641373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a911f183-be66-4329-a61f-068c70694c56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB0A472B10>]}
[0m19:26:38.690744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a911f183-be66-4329-a61f-068c70694c56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB0A5266F0>]}
[0m19:26:38.691753 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:26:38.697805 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:26:38.702853 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m19:26:38.702853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a911f183-be66-4329-a61f-068c70694c56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB0A506480>]}
[0m19:26:39.347069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a911f183-be66-4329-a61f-068c70694c56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB0A714710>]}
[0m19:26:39.354138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a911f183-be66-4329-a61f-068c70694c56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB0A59E480>]}
[0m19:26:39.354138 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:26:39.355150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a911f183-be66-4329-a61f-068c70694c56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB0A623710>]}
[0m19:26:39.356679 [info ] [MainThread]: 
[0m19:26:39.357699 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:26:39.358712 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:26:39.364301 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:26:39.364809 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:26:39.365314 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:26:39.365314 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:26:39.367334 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m19:26:39.367334 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m19:26:39.370373 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:26:39.370373 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m19:26:39.371381 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m19:26:39.371381 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:26:39.372389 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:26:39.373396 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:26:39.412729 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about datalake: Invalid value from "show tables extended ...", got 523 values, expected 4
[0m19:26:39.413736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a911f183-be66-4329-a61f-068c70694c56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB0A717830>]}
[0m19:26:39.413736 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:26:39.414743 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:26:39.414743 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:26:39.415757 [info ] [MainThread]: 
[0m19:26:39.417773 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m19:26:39.417773 [info ] [Thread-1 (]: 1 of 2 START sql table model datalake.my_first_dbt_model ....................... [RUN]
[0m19:26:39.418782 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:26:39.418782 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:26:39.422826 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:26:39.423838 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:26:39.419794 => 19:26:39.423838
[0m19:26:39.424852 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:26:39.436004 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:26:39.436004 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists datalake.my_first_dbt_model
[0m19:26:39.436004 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:26:39.437011 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m19:26:39.437011 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:26:39.459220 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:26:39.460226 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:26:39.461235 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:26:39.461235 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table datalake.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:26:39.462258 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:26:39.472353 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:26:39.424852 => 19:26:39.472353
[0m19:26:39.473360 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a911f183-be66-4329-a61f-068c70694c56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB0A8D6570>]}
[0m19:26:39.474370 [info ] [Thread-1 (]: 1 of 2 OK created sql table model datalake.my_first_dbt_model .................. [[32mOK[0m in 0.05s]
[0m19:26:39.474370 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:26:39.475379 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m19:26:39.476390 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m19:26:39.476390 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:26:39.477427 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:26:39.479448 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:26:39.480455 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:26:39.477427 => 19:26:39.480455
[0m19:26:39.481464 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:26:39.492602 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:26:39.493609 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:26:39.493609 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m19:26:39.494617 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:26:39.495623 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:26:39.481464 => 19:26:39.495623
[0m19:26:39.496631 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a911f183-be66-4329-a61f-068c70694c56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB0A8D6600>]}
[0m19:26:39.497638 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m19:26:39.498654 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:26:39.499665 [debug] [MainThread]: On master: ROLLBACK
[0m19:26:39.500673 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:26:39.500673 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:26:39.501685 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:26:39.501685 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:26:39.501685 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:26:39.502694 [info ] [MainThread]: 
[0m19:26:39.502694 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m19:26:39.503701 [debug] [MainThread]: Command end result
[0m19:26:39.510775 [info ] [MainThread]: 
[0m19:26:39.510775 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:26:39.511784 [info ] [MainThread]: 
[0m19:26:39.512293 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:26:39.512803 [debug] [MainThread]: Command `cli run` succeeded at 19:26:39.512803 after 1.00 seconds
[0m19:26:39.513812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB090CAF30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB0A49A660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002EB09391880>]}
[0m19:26:39.513812 [debug] [MainThread]: Flushing usage events
[0m19:27:35.847108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E55FC080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E55FC4A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E55FC320>]}


============================== 19:27:35.847108 | 9d9eafcb-bce3-4f21-a992-512acd46f733 ==============================
[0m19:27:35.847108 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:27:35.848116 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m19:27:35.942849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9d9eafcb-bce3-4f21-a992-512acd46f733', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E55FE510>]}
[0m19:27:35.992715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9d9eafcb-bce3-4f21-a992-512acd46f733', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E67773B0>]}
[0m19:27:35.993722 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:27:36.000793 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:27:36.011880 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m19:27:36.011880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '9d9eafcb-bce3-4f21-a992-512acd46f733', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E670E3C0>]}
[0m19:27:36.623171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9d9eafcb-bce3-4f21-a992-512acd46f733', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E6864110>]}
[0m19:27:36.630240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9d9eafcb-bce3-4f21-a992-512acd46f733', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E696CF20>]}
[0m19:27:36.631252 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:27:36.631252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9d9eafcb-bce3-4f21-a992-512acd46f733', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E691EAB0>]}
[0m19:27:36.633266 [info ] [MainThread]: 
[0m19:27:36.633768 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:27:36.634782 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:27:36.640352 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:27:36.640858 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:27:36.641367 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:27:36.641367 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:27:36.643382 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m19:27:36.643382 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m19:27:36.646418 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:27:36.646930 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m19:27:36.646930 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m19:27:36.647436 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:27:36.648443 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:27:36.649452 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:27:36.683806 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about datalake: Invalid value from "show tables extended ...", got 523 values, expected 4
[0m19:27:36.684813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9d9eafcb-bce3-4f21-a992-512acd46f733', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E696F1A0>]}
[0m19:27:36.684813 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:27:36.685820 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:27:36.685820 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:27:36.686830 [info ] [MainThread]: 
[0m19:27:36.688851 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m19:27:36.688851 [info ] [Thread-1 (]: 1 of 2 START sql table model datalake.my_first_dbt_model ....................... [RUN]
[0m19:27:36.689861 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:27:36.690871 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:27:36.694901 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:27:36.695908 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:27:36.690871 => 19:27:36.694901
[0m19:27:36.695908 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:27:36.706997 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:27:36.706997 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists datalake.my_first_dbt_model
[0m19:27:36.708008 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:27:36.708008 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m19:27:36.709017 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:27:36.734488 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:27:36.736526 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:27:36.737537 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:27:36.737537 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table datalake.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:27:36.738543 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:27:36.750735 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:27:36.695908 => 19:27:36.750735
[0m19:27:36.751743 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9d9eafcb-bce3-4f21-a992-512acd46f733', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E6B0BE00>]}
[0m19:27:36.752749 [info ] [Thread-1 (]: 1 of 2 OK created sql table model datalake.my_first_dbt_model .................. [[32mOK[0m in 0.06s]
[0m19:27:36.753755 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:27:36.753755 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m19:27:36.754763 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m19:27:36.754763 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:27:36.755773 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:27:36.757799 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:27:36.758809 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:27:36.755773 => 19:27:36.758809
[0m19:27:36.759834 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:27:36.770914 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:27:36.771922 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:27:36.771922 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m19:27:36.772929 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:27:36.774962 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:27:36.759834 => 19:27:36.773937
[0m19:27:36.774962 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9d9eafcb-bce3-4f21-a992-512acd46f733', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E6A71FA0>]}
[0m19:27:36.775970 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m19:27:36.776979 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:27:36.777986 [debug] [MainThread]: On master: ROLLBACK
[0m19:27:36.778994 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:27:36.778994 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:27:36.780005 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:27:36.780005 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:27:36.780005 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:27:36.781014 [info ] [MainThread]: 
[0m19:27:36.781014 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m19:27:36.782021 [debug] [MainThread]: Command end result
[0m19:27:36.789064 [info ] [MainThread]: 
[0m19:27:36.790081 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:27:36.790081 [info ] [MainThread]: 
[0m19:27:36.790081 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:27:36.791094 [debug] [MainThread]: Command `cli run` succeeded at 19:27:36.791094 after 0.97 seconds
[0m19:27:36.791094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E53E5AC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E6A44080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E7E6A445F0>]}
[0m19:27:36.792107 [debug] [MainThread]: Flushing usage events
[0m19:28:19.487836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002736EF1EA80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002736EF1D5E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002736EF1C620>]}


============================== 19:28:19.490873 | 3f8eb947-5d07-46a5-9a0f-2c2c74835707 ==============================
[0m19:28:19.490873 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:28:19.492889 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m19:28:19.805826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3f8eb947-5d07-46a5-9a0f-2c2c74835707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002736F0A03B0>]}
[0m19:28:19.973095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3f8eb947-5d07-46a5-9a0f-2c2c74835707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002736F148AD0>]}
[0m19:28:19.977123 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:28:20.005390 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:28:20.112493 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:28:20.113499 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:28:20.130620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3f8eb947-5d07-46a5-9a0f-2c2c74835707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002736EFA0590>]}
[0m19:28:20.148834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3f8eb947-5d07-46a5-9a0f-2c2c74835707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002736F1F4E60>]}
[0m19:28:20.150862 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:28:20.153388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f8eb947-5d07-46a5-9a0f-2c2c74835707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002736EBCAE40>]}
[0m19:28:20.159536 [info ] [MainThread]: 
[0m19:28:20.161558 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:28:20.166609 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:28:20.194392 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:28:20.195420 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:28:20.197441 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:28:20.198452 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:28:20.206569 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m19:28:20.209616 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m19:28:20.221770 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:28:20.223786 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m19:28:20.224795 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m19:28:20.225802 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:28:20.229827 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:28:20.234898 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:28:20.538618 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about datalake: Invalid value from "show tables extended ...", got 523 values, expected 4
[0m19:28:20.540630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3f8eb947-5d07-46a5-9a0f-2c2c74835707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002736EFA04D0>]}
[0m19:28:20.542651 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:28:20.544667 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:28:20.546687 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:28:20.548704 [info ] [MainThread]: 
[0m19:28:20.555765 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m19:28:20.557795 [info ] [Thread-6 (]: 1 of 2 START sql table model datalake.my_first_dbt_model ....................... [RUN]
[0m19:28:20.560822 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:28:20.561835 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:28:20.599781 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:28:20.602829 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:28:20.562841 => 19:28:20.601800
[0m19:28:20.603841 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:28:20.659768 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:28:20.660779 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists datalake.my_first_dbt_model
[0m19:28:20.662801 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m19:28:20.663825 [debug] [Thread-6 (]: fabricsparknb adapter: Reusing session: 0
[0m19:28:20.665844 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m19:28:20.768375 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:28:20.770911 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:28:20.771417 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:28:20.773432 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table datalake.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:28:20.775446 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m19:28:20.821957 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:28:20.604851 => 19:28:20.821957
[0m19:28:20.824989 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3f8eb947-5d07-46a5-9a0f-2c2c74835707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002736F3E0E00>]}
[0m19:28:20.827014 [info ] [Thread-6 (]: 1 of 2 OK created sql table model datalake.my_first_dbt_model .................. [[32mOK[0m in 0.27s]
[0m19:28:20.829034 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:28:20.831047 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m19:28:20.833077 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m19:28:20.835096 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:28:20.837112 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:28:20.847260 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:28:20.849282 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:28:20.838118 => 19:28:20.848272
[0m19:28:20.850292 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:28:20.898018 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:28:20.901056 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:28:20.902064 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m19:28:20.904080 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m19:28:20.910184 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:28:20.851300 => 19:28:20.909173
[0m19:28:20.913232 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3f8eb947-5d07-46a5-9a0f-2c2c74835707', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002736F44DF70>]}
[0m19:28:20.915270 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m19:28:20.918315 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:28:20.921338 [debug] [MainThread]: On master: ROLLBACK
[0m19:28:20.923367 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:28:20.924377 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:28:20.926398 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:28:20.927406 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:28:20.928417 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:28:20.931446 [info ] [MainThread]: 
[0m19:28:20.934510 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.77 seconds (0.77s).
[0m19:28:20.938644 [debug] [MainThread]: Command end result
[0m19:28:20.954923 [info ] [MainThread]: 
[0m19:28:20.957953 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:28:20.958961 [info ] [MainThread]: 
[0m19:28:20.960994 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:28:20.965081 [debug] [MainThread]: Command `cli run` succeeded at 19:28:20.965081 after 1.56 seconds
[0m19:28:20.967111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002736F0A30B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002736E70CE00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002736F42D8B0>]}
[0m19:28:20.969155 [debug] [MainThread]: Flushing usage events
[0m19:28:43.223725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195CC9473E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195CC947170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195CC946B40>]}


============================== 19:28:43.226749 | 4bb03d2c-3317-4f7b-a952-403e6a5679fc ==============================
[0m19:28:43.226749 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:28:43.228763 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m19:28:43.551769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4bb03d2c-3317-4f7b-a952-403e6a5679fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195CC0CA570>]}
[0m19:28:43.718634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4bb03d2c-3317-4f7b-a952-403e6a5679fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195CC9C07D0>]}
[0m19:28:43.720651 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:28:43.747872 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:28:43.835006 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:28:43.836013 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:28:43.854152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4bb03d2c-3317-4f7b-a952-403e6a5679fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195CCB8FA10>]}
[0m19:28:43.868262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4bb03d2c-3317-4f7b-a952-403e6a5679fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195CCC093D0>]}
[0m19:28:43.870277 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:28:43.871285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4bb03d2c-3317-4f7b-a952-403e6a5679fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195CCC08860>]}
[0m19:28:43.877361 [info ] [MainThread]: 
[0m19:28:43.879380 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:28:43.884431 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:28:43.911776 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:28:43.913794 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:28:43.914804 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:28:43.916820 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:28:43.924948 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m19:28:43.927983 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m19:28:43.941153 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:28:43.942161 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m19:28:43.944176 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m19:28:43.945182 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:28:43.949213 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:28:43.954279 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:29:34.473233 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about datalake: Invalid value from "show tables extended ...", got 523 values, expected 4
[0m19:29:34.478291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4bb03d2c-3317-4f7b-a952-403e6a5679fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195CCCA5640>]}
[0m19:29:34.480316 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:29:34.482355 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:29:34.485386 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:29:34.486395 [info ] [MainThread]: 
[0m19:29:34.494506 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m19:29:34.495515 [info ] [Thread-7 (]: 1 of 2 START sql table model datalake.my_first_dbt_model ....................... [RUN]
[0m19:29:34.498560 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:29:34.499569 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:29:34.534498 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:29:34.538541 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:29:34.500578 => 19:29:34.537531
[0m19:29:34.539573 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:29:34.632017 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:29:34.635091 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists datalake.my_first_dbt_model
[0m19:29:34.637147 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m19:29:34.639186 [debug] [Thread-7 (]: fabricsparknb adapter: Reusing session: 0
[0m19:29:34.641223 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m19:29:34.749464 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:29:34.751492 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:29:34.752505 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:29:34.754529 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table datalake.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:29:34.756556 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m19:29:34.806876 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:29:34.541687 => 19:29:34.806367
[0m19:29:34.808893 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4bb03d2c-3317-4f7b-a952-403e6a5679fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195CCE8BB30>]}
[0m19:29:34.810909 [info ] [Thread-7 (]: 1 of 2 OK created sql table model datalake.my_first_dbt_model .................. [[32mOK[0m in 0.31s]
[0m19:29:34.813942 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:29:34.815965 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m19:29:34.817986 [info ] [Thread-7 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m19:29:34.821093 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:29:34.823112 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:29:34.831182 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:29:34.835346 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:29:34.824120 => 19:29:34.834220
[0m19:29:34.838408 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:29:34.884219 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:29:34.889301 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:29:34.891324 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m19:29:34.893348 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m19:29:34.899972 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:29:34.839420 => 19:29:34.899462
[0m19:29:34.904114 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4bb03d2c-3317-4f7b-a952-403e6a5679fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195CCEA6A20>]}
[0m19:29:34.906640 [info ] [Thread-7 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m19:29:34.908151 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:29:34.911177 [debug] [MainThread]: On master: ROLLBACK
[0m19:29:34.913231 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:29:34.914246 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:29:34.916269 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:29:34.917279 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:29:34.918287 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:29:34.922390 [info ] [MainThread]: 
[0m19:29:34.925459 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 51.04 seconds (51.04s).
[0m19:29:34.929572 [debug] [MainThread]: Command end result
[0m19:29:34.948063 [info ] [MainThread]: 
[0m19:29:34.950097 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:29:34.951121 [info ] [MainThread]: 
[0m19:29:34.953154 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:29:34.961393 [debug] [MainThread]: Command `cli run` succeeded at 19:29:34.961393 after 51.84 seconds
[0m19:29:34.964443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195CC945640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195CCE918E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195CCE90B60>]}
[0m19:29:34.965452 [debug] [MainThread]: Flushing usage events
[0m19:32:14.406748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280060D47D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028005FBB410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280060D6BD0>]}


============================== 19:32:14.408762 | 3e99d567-0b07-434e-8d2b-37955f658228 ==============================
[0m19:32:14.408762 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:32:14.410775 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:32:14.724345 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3e99d567-0b07-434e-8d2b-37955f658228', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028005FFAF00>]}
[0m19:32:14.890081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3e99d567-0b07-434e-8d2b-37955f658228', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028006252F60>]}
[0m19:32:14.893124 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:32:14.921610 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:32:15.021205 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:32:15.023224 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:32:15.040373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3e99d567-0b07-434e-8d2b-37955f658228', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002800631FD70>]}
[0m19:32:15.055581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3e99d567-0b07-434e-8d2b-37955f658228', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280063A4AD0>]}
[0m19:32:15.057614 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:32:15.059630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3e99d567-0b07-434e-8d2b-37955f658228', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028005E22C30>]}
[0m19:32:15.065699 [info ] [MainThread]: 
[0m19:32:15.068734 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:32:15.074866 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:32:15.102197 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:32:15.103223 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:32:15.105238 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:32:15.107258 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:32:15.115440 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m19:32:15.117459 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m19:32:15.130624 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:32:15.131630 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m19:32:15.132635 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m19:32:15.135692 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:32:15.141835 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:32:15.148966 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:33:29.169164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3e99d567-0b07-434e-8d2b-37955f658228', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280064F7800>]}
[0m19:33:29.172226 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:33:29.173236 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:33:29.175253 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:33:29.177267 [info ] [MainThread]: 
[0m19:33:29.184344 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m19:33:29.186396 [info ] [Thread-7 (]: 1 of 2 START sql table model datalake.my_first_dbt_model ....................... [RUN]
[0m19:33:29.189423 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:33:29.191437 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:33:29.224930 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:33:29.227955 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:33:29.192445 => 19:33:29.226947
[0m19:33:29.228962 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:33:29.332679 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:33:29.334696 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists datalake.my_first_dbt_model
[0m19:33:29.335705 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m19:33:29.337726 [debug] [Thread-7 (]: fabricsparknb adapter: Reusing session: 0
[0m19:33:29.339777 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m19:33:29.447395 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:33:29.450421 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:33:29.451928 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:33:29.452940 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table datalake.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:33:29.455968 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m19:33:29.512365 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:33:29.240226 => 19:33:29.512365
[0m19:33:29.515384 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3e99d567-0b07-434e-8d2b-37955f658228', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280065C6930>]}
[0m19:33:29.516392 [info ] [Thread-7 (]: 1 of 2 OK created sql table model datalake.my_first_dbt_model .................. [[32mOK[0m in 0.33s]
[0m19:33:29.519418 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:33:29.521439 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m19:33:29.523500 [info ] [Thread-7 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m19:33:29.526545 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:33:29.528563 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:33:29.537725 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:33:29.541795 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:33:29.529570 => 19:33:29.540775
[0m19:33:29.542804 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:33:29.588443 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:33:29.590467 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:33:29.592486 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m19:33:29.594506 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m19:33:29.602709 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:33:29.543813 => 19:33:29.601697
[0m19:33:29.606759 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3e99d567-0b07-434e-8d2b-37955f658228', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280061E26C0>]}
[0m19:33:29.608788 [info ] [Thread-7 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m19:33:29.612894 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:33:29.617004 [debug] [MainThread]: On master: ROLLBACK
[0m19:33:29.619022 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:33:29.620033 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:33:29.622048 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:33:29.624069 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:33:29.625077 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:33:29.627098 [info ] [MainThread]: 
[0m19:33:29.629127 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 1 minutes and 14.56 seconds (74.56s).
[0m19:33:29.634225 [debug] [MainThread]: Command end result
[0m19:33:29.673452 [info ] [MainThread]: 
[0m19:33:29.676552 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:33:29.679597 [info ] [MainThread]: 
[0m19:33:29.680608 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:33:29.687730 [debug] [MainThread]: Command `cli run` succeeded at 19:33:29.686712 after 75.36 seconds
[0m19:33:29.691878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028006047350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280065EB1A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000280065E9160>]}
[0m19:33:29.693991 [debug] [MainThread]: Flushing usage events
[0m19:34:56.861117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214EA747C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214EA747950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214EA7474A0>]}


============================== 19:34:56.863142 | 47acf34a-d20a-4a64-96ce-f8068756af21 ==============================
[0m19:34:56.863142 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:34:56.865157 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:34:57.176777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '47acf34a-d20a-4a64-96ce-f8068756af21', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214EA88F710>]}
[0m19:34:57.337624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '47acf34a-d20a-4a64-96ce-f8068756af21', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214EA9933E0>]}
[0m19:34:57.340650 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:34:57.372946 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:34:57.961921 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:34:57.962927 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:34:57.981072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '47acf34a-d20a-4a64-96ce-f8068756af21', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214EA7458B0>]}
[0m19:34:57.994193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '47acf34a-d20a-4a64-96ce-f8068756af21', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214EA9F19A0>]}
[0m19:34:57.995705 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:34:57.997218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47acf34a-d20a-4a64-96ce-f8068756af21', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214EA5018E0>]}
[0m19:34:58.002280 [info ] [MainThread]: 
[0m19:34:58.005306 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:34:58.010356 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:34:58.036683 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:34:58.038697 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:34:58.039704 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:34:58.041720 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:34:58.049840 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m19:34:58.051863 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m19:34:58.065005 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:34:58.067022 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m19:34:58.068033 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m19:34:58.070047 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:34:58.073068 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:34:58.079139 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:35:50.371076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '47acf34a-d20a-4a64-96ce-f8068756af21', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214EAB66E10>]}
[0m19:35:50.373102 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:35:50.374119 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:35:50.377154 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:35:50.378162 [info ] [MainThread]: 
[0m19:35:50.386285 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m19:35:50.387294 [info ] [Thread-7 (]: 1 of 2 START sql table model datalake.my_first_dbt_model ....................... [RUN]
[0m19:35:50.390318 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:35:50.391327 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:35:50.428453 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:35:50.431493 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:35:50.392336 => 19:35:50.430480
[0m19:35:50.433525 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:41:03.656118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016870445310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016870447B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016870447440>]}


============================== 19:41:03.659157 | ffa28a01-9efa-4a47-87f7-5d974c9fb7dd ==============================
[0m19:41:03.659157 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:41:03.661175 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m19:41:03.981106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ffa28a01-9efa-4a47-87f7-5d974c9fb7dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000168703FFE60>]}
[0m19:41:04.145947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ffa28a01-9efa-4a47-87f7-5d974c9fb7dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001687048A900>]}
[0m19:41:04.147961 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:41:04.177301 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:41:04.262142 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:41:04.264168 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:41:04.281317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ffa28a01-9efa-4a47-87f7-5d974c9fb7dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001687058EB10>]}
[0m19:41:04.295423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ffa28a01-9efa-4a47-87f7-5d974c9fb7dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000168706F1A00>]}
[0m19:41:04.296940 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:41:04.298465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ffa28a01-9efa-4a47-87f7-5d974c9fb7dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016870182ED0>]}
[0m19:41:04.304026 [info ] [MainThread]: 
[0m19:41:04.307050 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:41:04.311090 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:41:04.338335 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:41:04.339395 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:41:04.341410 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:41:04.343430 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:41:04.350495 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m19:41:04.353532 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m19:41:04.365655 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:41:04.366661 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m19:41:04.368682 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m19:41:04.370720 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:41:04.374790 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:41:04.380853 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:41:04.464328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ffa28a01-9efa-4a47-87f7-5d974c9fb7dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000168700E97F0>]}
[0m19:41:04.466343 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:41:04.467354 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:41:04.470382 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:41:04.471390 [info ] [MainThread]: 
[0m19:41:04.479484 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m19:41:04.480490 [info ] [Thread-6 (]: 1 of 2 START sql table model datalake.my_first_dbt_model ....................... [RUN]
[0m19:41:04.483515 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:41:04.484522 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:41:04.522113 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:41:04.525152 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:41:04.485530 => 19:41:04.524142
[0m19:41:04.526166 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:42:49.488839 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:42:49.489846 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists datalake.my_first_dbt_model
[0m19:42:49.491869 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m19:42:49.493899 [debug] [Thread-6 (]: fabricsparknb adapter: Reusing session: 0
[0m19:42:49.495958 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m19:42:49.609144 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:42:49.612176 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:42:49.614196 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:42:49.615203 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table datalake.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:42:49.618240 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m19:42:49.669870 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:41:04.528209 => 19:42:49.668857
[0m19:42:49.673970 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffa28a01-9efa-4a47-87f7-5d974c9fb7dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000168708EC950>]}
[0m19:42:49.677006 [info ] [Thread-6 (]: 1 of 2 OK created sql table model datalake.my_first_dbt_model .................. [[32mOK[0m in 105.19s]
[0m19:42:49.679048 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:42:49.682071 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m19:42:49.684087 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m19:42:49.687117 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:42:49.689140 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:42:49.698248 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:42:49.701273 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:42:49.690151 => 19:42:49.700264
[0m19:42:49.702281 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:42:51.707758 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:42:51.709771 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:42:51.710784 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m19:42:51.712811 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m19:42:51.718894 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:42:49.703292 => 19:42:51.717883
[0m19:42:51.720913 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ffa28a01-9efa-4a47-87f7-5d974c9fb7dd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000168709C0620>]}
[0m19:42:51.722930 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 2.04s]
[0m19:42:51.724943 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:42:51.729986 [debug] [MainThread]: On master: ROLLBACK
[0m19:42:51.730994 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:42:51.733021 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:42:51.734033 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:42:51.735042 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:42:51.737066 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:42:51.740099 [info ] [MainThread]: 
[0m19:42:51.742136 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 1 minutes and 47.43 seconds (107.43s).
[0m19:42:51.745169 [debug] [MainThread]: Command end result
[0m19:42:51.760390 [info ] [MainThread]: 
[0m19:42:51.761896 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:42:51.763937 [info ] [MainThread]: 
[0m19:42:51.765954 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:42:51.769987 [debug] [MainThread]: Command `cli run` succeeded at 19:42:51.768979 after 108.20 seconds
[0m19:42:51.772010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001687032B680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016870942F60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016870943230>]}
[0m19:42:51.774029 [debug] [MainThread]: Flushing usage events
[0m19:43:19.904323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234126C7B00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234126C7170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234126C5820>]}


============================== 19:43:19.907369 | b045deb6-d4dc-49db-a6b9-d34dd15e4ccc ==============================
[0m19:43:19.907369 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:43:19.909392 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:43:20.223164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b045deb6-d4dc-49db-a6b9-d34dd15e4ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002341270A030>]}
[0m19:43:20.385966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b045deb6-d4dc-49db-a6b9-d34dd15e4ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234128E8920>]}
[0m19:43:20.388987 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:43:20.416219 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:43:20.510154 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:43:20.511162 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:43:20.529342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b045deb6-d4dc-49db-a6b9-d34dd15e4ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023412906ED0>]}
[0m19:43:20.542462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b045deb6-d4dc-49db-a6b9-d34dd15e4ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023412988BF0>]}
[0m19:43:20.543470 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:43:20.545482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b045deb6-d4dc-49db-a6b9-d34dd15e4ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023412842FC0>]}
[0m19:43:20.551053 [info ] [MainThread]: 
[0m19:43:20.554083 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:43:20.558150 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:43:20.584891 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:43:20.586905 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:43:20.587913 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:43:20.589934 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:43:20.598054 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m19:43:20.601076 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m19:43:20.612164 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:43:20.614177 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m19:43:20.615183 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m19:43:20.617204 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:43:20.620246 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:43:20.625284 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:43:20.710235 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b045deb6-d4dc-49db-a6b9-d34dd15e4ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023412752A20>]}
[0m19:43:20.712267 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:43:20.713275 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:43:20.715291 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:43:20.717312 [info ] [MainThread]: 
[0m19:43:20.724373 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m19:43:20.725380 [info ] [Thread-6 (]: 1 of 2 START sql table model datalake.my_first_dbt_model ....................... [RUN]
[0m19:43:20.728411 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:43:20.729419 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:43:20.763772 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:43:20.767842 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:43:20.730426 => 19:43:20.766831
[0m19:43:20.769862 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:43:23.808817 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:43:23.809824 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists datalake.my_first_dbt_model
[0m19:43:23.811843 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m19:43:23.812859 [debug] [Thread-6 (]: fabricsparknb adapter: Reusing session: 0
[0m19:43:23.814888 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m19:43:23.929147 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:43:23.932168 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:43:23.933176 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:43:23.934183 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table datalake.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:43:23.937206 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m19:43:23.993068 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:43:20.770866 => 19:43:23.993068
[0m19:43:23.996112 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b045deb6-d4dc-49db-a6b9-d34dd15e4ccc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023412BC3710>]}
[0m19:43:24.000278 [info ] [Thread-6 (]: 1 of 2 OK created sql table model datalake.my_first_dbt_model .................. [[32mOK[0m in 3.27s]
[0m19:43:24.004359 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:43:24.008424 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m19:43:24.011478 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m19:43:24.014521 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:43:24.016545 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:43:24.029881 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:43:24.032924 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:43:24.017559 => 19:43:24.032924
[0m19:43:24.034976 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:49:29.792799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216A0B09F40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216A0B09250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216A0B090A0>]}


============================== 19:49:29.795836 | 94a4c940-2394-40f5-bc9e-0fad31d29ca4 ==============================
[0m19:49:29.795836 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:49:29.797871 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:49:30.116436 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '94a4c940-2394-40f5-bc9e-0fad31d29ca4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216A0ACE9C0>]}
[0m19:49:30.285930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '94a4c940-2394-40f5-bc9e-0fad31d29ca4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216A0D0C7A0>]}
[0m19:49:30.288951 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:49:30.316220 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:49:30.410083 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:49:30.411090 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:49:30.429228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '94a4c940-2394-40f5-bc9e-0fad31d29ca4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216A0D0CC80>]}
[0m19:49:30.442863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '94a4c940-2394-40f5-bc9e-0fad31d29ca4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216A0DCD490>]}
[0m19:49:30.444386 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:49:30.445911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '94a4c940-2394-40f5-bc9e-0fad31d29ca4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216A0B490A0>]}
[0m19:49:30.451498 [info ] [MainThread]: 
[0m19:49:30.454524 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:49:30.459582 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:49:30.486907 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:49:30.487916 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:49:30.489933 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:49:30.491951 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:49:30.502176 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m19:49:30.505207 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m19:49:30.518372 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:49:30.520387 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m19:49:30.521397 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m19:49:30.523411 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:49:30.526441 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:49:30.534565 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:49:30.622127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '94a4c940-2394-40f5-bc9e-0fad31d29ca4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216A0950EF0>]}
[0m19:49:30.623134 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:49:30.625150 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:49:30.627165 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:49:30.629183 [info ] [MainThread]: 
[0m19:49:30.638281 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m19:49:30.639291 [info ] [Thread-6 (]: 1 of 2 START sql table model datalake.my_first_dbt_model ....................... [RUN]
[0m19:49:30.642316 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:49:30.643325 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:49:30.683052 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:49:30.686109 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:49:30.644332 => 19:49:30.685095
[0m19:49:30.688163 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:50:30.976476 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:50:30.978491 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists datalake.my_first_dbt_model
[0m19:50:30.980505 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m19:50:30.981521 [debug] [Thread-6 (]: fabricsparknb adapter: Reusing session: 0
[0m19:50:30.983542 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m19:50:31.102888 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:50:31.104906 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:50:31.105918 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:50:31.107933 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table datalake.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:50:31.110960 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m19:50:31.183312 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:49:30.690223 => 19:50:31.181272
[0m19:50:31.189501 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '94a4c940-2394-40f5-bc9e-0fad31d29ca4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216A108A930>]}
[0m19:50:31.192555 [info ] [Thread-6 (]: 1 of 2 OK created sql table model datalake.my_first_dbt_model .................. [[32mOK[0m in 60.55s]
[0m19:50:31.197117 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:50:31.206402 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m19:50:31.210483 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m19:50:31.215581 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:50:31.219731 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:50:31.235029 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:50:31.241179 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:50:31.221766 => 19:50:31.240151
[0m19:50:31.244252 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:50:31.309523 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:50:31.314739 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:50:31.317799 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m19:50:31.321908 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m19:50:31.331074 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:50:31.246285 => 19:50:31.330057
[0m19:50:31.335117 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '94a4c940-2394-40f5-bc9e-0fad31d29ca4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216A1088B00>]}
[0m19:50:31.338175 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.12s]
[0m19:50:31.340198 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:50:31.345287 [debug] [MainThread]: On master: ROLLBACK
[0m19:50:31.348321 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:50:31.349331 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:50:31.352381 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:50:31.355474 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:50:31.356508 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:50:31.359617 [info ] [MainThread]: 
[0m19:50:31.362739 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 1 minutes and 0.91 seconds (60.91s).
[0m19:50:31.365763 [debug] [MainThread]: Command end result
[0m19:50:31.383186 [info ] [MainThread]: 
[0m19:50:31.385703 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:50:31.389375 [info ] [MainThread]: 
[0m19:50:31.392433 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:50:31.399544 [debug] [MainThread]: Command `cli run` succeeded at 19:50:31.398535 after 61.69 seconds
[0m19:50:31.401562 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216A03BC380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216A106D370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000216A106EF00>]}
[0m19:50:31.403588 [debug] [MainThread]: Flushing usage events
[0m19:52:02.599597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024666A75100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024664114770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024666A76A80>]}


============================== 19:52:02.602634 | 0ef0c011-182d-40c4-89e1-9c0adefeba87 ==============================
[0m19:52:02.602634 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:52:02.604656 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:52:02.925955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0ef0c011-182d-40c4-89e1-9c0adefeba87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024666B86000>]}
[0m19:52:03.090741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0ef0c011-182d-40c4-89e1-9c0adefeba87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024666A77980>]}
[0m19:52:03.092754 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:52:03.118976 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:52:03.218329 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:52:03.219348 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:52:03.237481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0ef0c011-182d-40c4-89e1-9c0adefeba87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024666176E40>]}
[0m19:52:03.251618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0ef0c011-182d-40c4-89e1-9c0adefeba87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024666D21880>]}
[0m19:52:03.252627 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:52:03.254674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ef0c011-182d-40c4-89e1-9c0adefeba87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024666ABFEF0>]}
[0m19:52:03.260243 [info ] [MainThread]: 
[0m19:52:03.262779 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:52:03.267867 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:52:03.293068 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:52:03.295081 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:52:03.296105 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:52:03.299138 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:52:03.307260 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m19:52:03.309296 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m19:52:03.323483 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:52:03.324496 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m19:52:03.326530 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m19:52:03.327541 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:52:03.331580 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:52:03.337646 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:52:03.422615 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ef0c011-182d-40c4-89e1-9c0adefeba87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024666B850D0>]}
[0m19:52:03.423623 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:52:03.425638 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:52:03.427652 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:52:03.428658 [info ] [MainThread]: 
[0m19:52:03.436782 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m19:52:03.438798 [info ] [Thread-6 (]: 1 of 2 START sql table model datalake.my_first_dbt_model ....................... [RUN]
[0m19:52:03.441822 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:52:03.442829 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:52:03.477243 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:52:03.480300 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:52:03.443837 => 19:52:03.479290
[0m19:52:03.481310 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:52:03.534798 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:52:03.536810 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
drop table if exists datalake.my_first_dbt_model
[0m19:52:03.537818 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m19:52:03.539852 [debug] [Thread-6 (]: fabricsparknb adapter: Reusing session: 0
[0m19:52:03.540860 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m19:52:03.641814 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:52:03.643829 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:52:03.644840 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:52:03.645885 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create table datalake.my_first_dbt_model
      
      
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:52:03.648911 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m19:52:03.704670 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:52:03.482323 => 19:52:03.703662
[0m19:52:03.708222 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ef0c011-182d-40c4-89e1-9c0adefeba87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024666F73B90>]}
[0m19:52:03.717839 [info ] [Thread-6 (]: 1 of 2 OK created sql table model datalake.my_first_dbt_model .................. [[32mOK[0m in 0.27s]
[0m19:52:03.719862 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:52:03.722913 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m19:52:03.723924 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m19:52:03.726954 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:52:03.727963 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:52:03.738126 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:52:03.741152 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:52:03.728970 => 19:52:03.740142
[0m19:52:03.742158 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:52:03.789808 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:52:03.791838 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:52:03.793856 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m19:52:03.795877 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m19:52:03.800925 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:52:03.743165 => 19:52:03.800925
[0m19:52:03.803956 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ef0c011-182d-40c4-89e1-9c0adefeba87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024667003830>]}
[0m19:52:03.805978 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m19:52:03.807999 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:52:03.811024 [debug] [MainThread]: On master: ROLLBACK
[0m19:52:03.813046 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:52:03.814073 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:52:03.816098 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:52:03.817109 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:52:03.818120 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:52:03.821146 [info ] [MainThread]: 
[0m19:52:03.822156 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.56 seconds (0.56s).
[0m19:52:03.826229 [debug] [MainThread]: Command end result
[0m19:52:03.838362 [info ] [MainThread]: 
[0m19:52:03.840386 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:52:03.843432 [info ] [MainThread]: 
[0m19:52:03.844473 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:52:03.848534 [debug] [MainThread]: Command `cli run` succeeded at 19:52:03.848534 after 1.33 seconds
[0m19:52:03.850548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000246661FF320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024666DCC5F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024666DCFF80>]}
[0m19:52:03.851556 [debug] [MainThread]: Flushing usage events
[0m19:58:24.076379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218DAF6D3A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218DAF6D100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218DAF6D670>]}


============================== 19:58:24.076379 | 4b4537de-c8fc-44d5-a51d-4f79eed1d305 ==============================
[0m19:58:24.076379 [info ] [MainThread]: Running with dbt=1.7.4
[0m19:58:24.077392 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m19:58:24.180947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4b4537de-c8fc-44d5-a51d-4f79eed1d305', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218DB01FE00>]}
[0m19:58:24.230811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4b4537de-c8fc-44d5-a51d-4f79eed1d305', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218DB03CE30>]}
[0m19:58:24.232841 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:58:24.238885 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m19:58:24.281254 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:58:24.281254 [debug] [MainThread]: Partial parsing: updated file: testproj://models\example\my_first_dbt_model.sql
[0m19:58:24.436505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4b4537de-c8fc-44d5-a51d-4f79eed1d305', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218DB13EA20>]}
[0m19:58:24.443565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4b4537de-c8fc-44d5-a51d-4f79eed1d305', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218DB1A5CA0>]}
[0m19:58:24.443565 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m19:58:24.444575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4b4537de-c8fc-44d5-a51d-4f79eed1d305', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218D96473E0>]}
[0m19:58:24.445580 [info ] [MainThread]: 
[0m19:58:24.445580 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:58:24.446588 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:58:24.452640 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m19:58:24.452640 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m19:58:24.453646 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:58:24.453646 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:58:24.454656 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m19:58:24.455664 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m19:58:24.458704 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:58:24.458704 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m19:58:24.459716 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m19:58:24.459716 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m19:58:24.460727 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:58:24.461736 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:58:24.472839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4b4537de-c8fc-44d5-a51d-4f79eed1d305', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218DB546180>]}
[0m19:58:24.473848 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:58:24.473848 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:58:24.474860 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:58:24.474860 [info ] [MainThread]: 
[0m19:58:24.477889 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m19:58:24.477889 [info ] [Thread-1 (]: 1 of 2 START sql table model datalake.my_first_dbt_model ....................... [RUN]
[0m19:58:24.478899 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:58:24.478899 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:58:24.482946 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:58:24.484965 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:58:24.478899 => 19:58:24.483957
[0m19:58:24.484965 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:58:24.521456 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:58:24.523468 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:58:24.523468 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:58:24.524506 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    
        create or replace table datalake.my_first_dbt_model
      
      
    using delta
      
      
      
      
      
      

      as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:58:24.524506 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:58:24.524506 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m19:58:24.525534 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:58:24.537647 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:58:24.484965 => 19:58:24.537647
[0m19:58:24.538653 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b4537de-c8fc-44d5-a51d-4f79eed1d305', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218DB35A570>]}
[0m19:58:24.538653 [info ] [Thread-1 (]: 1 of 2 OK created sql table model datalake.my_first_dbt_model .................. [[32mOK[0m in 0.06s]
[0m19:58:24.539658 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:58:24.540680 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m19:58:24.540680 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m19:58:24.541689 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m19:58:24.541689 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:58:24.543713 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:58:24.544721 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:58:24.542699 => 19:58:24.544721
[0m19:58:24.544721 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:58:24.555857 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:58:24.556877 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:58:24.557889 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m19:58:24.557889 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:58:24.559908 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:58:24.545728 => 19:58:24.559908
[0m19:58:24.560920 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b4537de-c8fc-44d5-a51d-4f79eed1d305', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218DB35A570>]}
[0m19:58:24.560920 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m19:58:24.561931 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:58:24.562941 [debug] [MainThread]: On master: ROLLBACK
[0m19:58:24.562941 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:58:24.562941 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:58:24.563950 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:58:24.563950 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:58:24.563950 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:58:24.564958 [info ] [MainThread]: 
[0m19:58:24.564958 [info ] [MainThread]: Finished running 1 table model, 1 view model in 0 hours 0 minutes and 0.12 seconds (0.12s).
[0m19:58:24.565966 [debug] [MainThread]: Command end result
[0m19:58:24.580115 [info ] [MainThread]: 
[0m19:58:24.580115 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:58:24.581125 [info ] [MainThread]: 
[0m19:58:24.581125 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m19:58:24.583151 [debug] [MainThread]: Command `cli run` succeeded at 19:58:24.583151 after 0.53 seconds
[0m19:58:24.583151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218DAF6D7C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218DAF6F860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000218DAF6D730>]}
[0m19:58:24.584164 [debug] [MainThread]: Flushing usage events
[0m20:01:45.598402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FC13A02C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FC13A0410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FC13A0200>]}


============================== 20:01:45.599409 | 12bdeca8-4064-4899-9d6c-867e9c28e872 ==============================
[0m20:01:45.599409 [info ] [MainThread]: Running with dbt=1.7.4
[0m20:01:45.599409 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m20:01:45.699691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '12bdeca8-4064-4899-9d6c-867e9c28e872', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FC2482A50>]}
[0m20:01:45.750334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '12bdeca8-4064-4899-9d6c-867e9c28e872', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FC2537740>]}
[0m20:01:45.751341 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:01:45.759452 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m20:01:45.805834 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:01:45.805834 [debug] [MainThread]: Partial parsing: updated file: testproj://models\example\my_first_dbt_model.sql
[0m20:01:45.960179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '12bdeca8-4064-4899-9d6c-867e9c28e872', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FC27E6180>]}
[0m20:01:45.966252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '12bdeca8-4064-4899-9d6c-867e9c28e872', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FC25F6750>]}
[0m20:01:45.966252 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m20:01:45.967261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '12bdeca8-4064-4899-9d6c-867e9c28e872', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FC2899400>]}
[0m20:01:45.968269 [info ] [MainThread]: 
[0m20:01:45.968269 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:01:45.969278 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:01:45.975348 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m20:01:45.975348 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m20:01:45.975348 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:01:45.976357 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m20:01:45.977364 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m20:01:45.977364 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m20:01:45.980386 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:01:45.981394 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m20:01:45.981394 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m20:01:45.981394 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m20:01:45.982402 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:01:45.983411 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:01:45.994519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '12bdeca8-4064-4899-9d6c-867e9c28e872', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FC1357470>]}
[0m20:01:45.994519 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:01:45.995527 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:01:45.995527 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:01:45.995527 [info ] [MainThread]: 
[0m20:01:45.997543 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m20:01:45.998549 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:01:45.998549 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:01:45.998549 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:01:46.002593 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:01:46.003598 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:01:45.999556 => 20:01:46.003598
[0m20:01:46.003598 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:01:46.029795 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:01:46.030802 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:01:46.030802 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:01:46.030802 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:01:46.030802 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m20:01:46.031810 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:01:46.039887 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:01:46.039887 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

      describe table extended datalake.my_first_dbt_model
  
[0m20:01:46.040902 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:01:46.041912 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m20:01:46.042920 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:01:46.042920 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select  from my_first_dbt_model__dbt_tmp


[0m20:01:46.042920 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:01:46.051005 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:01:46.004606 => 20:01:46.051005
[0m20:01:46.051005 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '12bdeca8-4064-4899-9d6c-867e9c28e872', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FC289BB90>]}
[0m20:01:46.052018 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.05s]
[0m20:01:46.052018 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:01:46.053031 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m20:01:46.053031 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m20:01:46.054042 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m20:01:46.054042 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m20:01:46.056058 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m20:01:46.056058 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 20:01:46.054042 => 20:01:46.056058
[0m20:01:46.057066 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m20:01:46.070206 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m20:01:46.071217 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m20:01:46.071217 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m20:01:46.072227 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:01:46.073240 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 20:01:46.057066 => 20:01:46.073240
[0m20:01:46.074255 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '12bdeca8-4064-4899-9d6c-867e9c28e872', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FC2966EA0>]}
[0m20:01:46.074255 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m20:01:46.075269 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:01:46.076279 [debug] [MainThread]: On master: ROLLBACK
[0m20:01:46.076279 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:01:46.077290 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:01:46.077290 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:01:46.077290 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:01:46.077290 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:01:46.078299 [info ] [MainThread]: 
[0m20:01:46.079319 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.11 seconds (0.11s).
[0m20:01:46.079319 [debug] [MainThread]: Command end result
[0m20:01:46.160923 [info ] [MainThread]: 
[0m20:01:46.161939 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:01:46.162446 [info ] [MainThread]: 
[0m20:01:46.162954 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m20:01:46.164477 [debug] [MainThread]: Command `cli run` succeeded at 20:01:46.164477 after 0.59 seconds
[0m20:01:46.164986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FC0970E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FC24A8E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022FC13FA330>]}
[0m20:01:46.165490 [debug] [MainThread]: Flushing usage events
[0m20:13:42.349590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DB854A030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DB8548A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DB8518710>]}


============================== 20:13:42.351607 | 92eca1f7-c0b3-4334-a86c-60d7728f10a8 ==============================
[0m20:13:42.351607 [info ] [MainThread]: Running with dbt=1.7.4
[0m20:13:42.353643 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:13:42.671728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '92eca1f7-c0b3-4334-a86c-60d7728f10a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DB854B2C0>]}
[0m20:13:42.841349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '92eca1f7-c0b3-4334-a86c-60d7728f10a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DB86A24E0>]}
[0m20:13:42.844431 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:13:42.877768 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m20:13:43.366457 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:13:43.367464 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:13:43.385639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '92eca1f7-c0b3-4334-a86c-60d7728f10a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DB879CD40>]}
[0m20:13:43.400287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '92eca1f7-c0b3-4334-a86c-60d7728f10a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DB8801DC0>]}
[0m20:13:43.401807 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m20:13:43.402817 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '92eca1f7-c0b3-4334-a86c-60d7728f10a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DB88013D0>]}
[0m20:13:43.408868 [info ] [MainThread]: 
[0m20:13:43.410884 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:13:43.415961 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:13:43.447360 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m20:13:43.449389 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m20:13:43.451409 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:13:43.453427 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m20:13:43.462565 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m20:13:43.465602 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m20:13:43.479816 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:13:43.480825 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m20:13:43.482847 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m20:13:43.484868 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m20:13:43.489951 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:13:43.498104 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:13:43.602930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '92eca1f7-c0b3-4334-a86c-60d7728f10a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DB85E0A40>]}
[0m20:13:43.604950 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:13:43.606967 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:13:43.608982 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:13:43.610009 [info ] [MainThread]: 
[0m20:13:43.617071 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m20:13:43.619091 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:13:43.622123 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:13:43.624144 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:13:43.660663 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:13:43.664803 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:13:43.625172 => 20:13:43.663763
[0m20:13:43.667866 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:13:43.792632 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:13:43.794652 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:13:43.795659 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:13:43.797673 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:13:43.798681 [debug] [Thread-6 (]: fabricsparknb adapter: Reusing session: 0
[0m20:13:43.801722 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m20:14:04.612727 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m20:14:04.614760 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:14:04.616799 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select  from my_first_dbt_model__dbt_tmp


[0m20:14:04.618830 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m20:14:04.653185 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:13:43.668875 => 20:14:04.653185
[0m20:14:04.656212 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92eca1f7-c0b3-4334-a86c-60d7728f10a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DB8A53EC0>]}
[0m20:14:04.658228 [info ] [Thread-6 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 21.03s]
[0m20:14:04.660243 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:14:04.662256 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m20:14:04.664271 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m20:14:04.666288 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m20:14:04.668320 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m20:14:04.679464 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m20:14:04.681491 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 20:14:04.669331 => 20:14:04.681491
[0m20:14:04.683536 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m20:14:04.751880 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m20:14:04.755918 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m20:14:04.759057 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m20:14:04.762117 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m20:14:04.768195 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 20:14:04.684545 => 20:14:04.768195
[0m20:14:04.773284 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92eca1f7-c0b3-4334-a86c-60d7728f10a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DB8AFCBC0>]}
[0m20:14:04.775338 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.11s]
[0m20:14:04.779406 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:14:04.783471 [debug] [MainThread]: On master: ROLLBACK
[0m20:14:04.784484 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:14:04.786504 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:14:04.788528 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:14:04.790593 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:14:04.792628 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:14:04.794651 [info ] [MainThread]: 
[0m20:14:04.796671 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 21.38 seconds (21.38s).
[0m20:14:04.799704 [debug] [MainThread]: Command end result
[0m20:14:04.817098 [info ] [MainThread]: 
[0m20:14:04.819116 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:14:04.821160 [info ] [MainThread]: 
[0m20:14:04.825261 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m20:14:04.833388 [debug] [MainThread]: Command `cli run` succeeded at 20:14:04.832376 after 22.58 seconds
[0m20:14:04.836502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DB8AFD010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DB8AFDA30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017DB8AFD7F0>]}
[0m20:14:04.841600 [debug] [MainThread]: Flushing usage events
[0m09:59:00.364875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E04B84230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E04B84920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E04B842C0>]}


============================== 09:59:00.365691 | c65af516-64e1-4139-982e-4b794894ef0b ==============================
[0m09:59:00.365691 [info ] [MainThread]: Running with dbt=1.7.4
[0m09:59:00.366198 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m09:59:00.472843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c65af516-64e1-4139-982e-4b794894ef0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E04B57E00>]}
[0m09:59:00.522339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c65af516-64e1-4139-982e-4b794894ef0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E04B57C50>]}
[0m09:59:00.524349 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:59:00.537064 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m09:59:01.037213 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m09:59:01.037213 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m09:59:01.041237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c65af516-64e1-4139-982e-4b794894ef0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E04CB18E0>]}
[0m09:59:01.052892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c65af516-64e1-4139-982e-4b794894ef0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E05DA5760>]}
[0m09:59:01.053901 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m09:59:01.053901 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c65af516-64e1-4139-982e-4b794894ef0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E05DA5A90>]}
[0m09:59:01.054908 [info ] [MainThread]: 
[0m09:59:01.055914 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:59:01.056920 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:59:01.061946 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m09:59:01.061946 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m09:59:01.062951 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:59:01.062951 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m09:59:01.063957 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m09:59:01.065159 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m09:59:01.067250 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:59:01.068258 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m09:59:01.068258 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m09:59:01.068258 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m09:59:01.069264 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:59:01.070270 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:59:01.087042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c65af516-64e1-4139-982e-4b794894ef0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E05D77E90>]}
[0m09:59:01.088047 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:59:01.088047 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:59:01.089052 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:59:01.089052 [info ] [MainThread]: 
[0m09:59:01.092068 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m09:59:01.092068 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m09:59:01.093073 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m09:59:01.093073 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m09:59:01.097095 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m09:59:01.098829 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 09:59:01.094079 => 09:59:01.098829
[0m09:59:01.099403 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m09:59:01.124965 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:59:01.124965 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m09:59:01.125970 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m09:59:01.125970 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:59:01.125970 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m09:59:01.126975 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m09:59:01.133041 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m09:59:01.134273 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m09:59:01.134273 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select  from my_first_dbt_model__dbt_tmp


[0m09:59:01.134273 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m09:59:01.142316 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 09:59:01.099403 => 09:59:01.142316
[0m09:59:01.143321 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c65af516-64e1-4139-982e-4b794894ef0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E05F21220>]}
[0m09:59:01.143321 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.05s]
[0m09:59:01.144326 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m09:59:01.144326 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m09:59:01.145331 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m09:59:01.145331 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m09:59:01.146337 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m09:59:01.148943 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m09:59:01.149452 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 09:59:01.146337 => 09:59:01.149452
[0m09:59:01.150464 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m09:59:01.162746 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m09:59:01.165432 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m09:59:01.165944 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m09:59:01.165944 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m09:59:01.168222 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 09:59:01.150595 => 09:59:01.167717
[0m09:59:01.168222 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c65af516-64e1-4139-982e-4b794894ef0b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E05F22A50>]}
[0m09:59:01.169233 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m09:59:01.169233 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m09:59:01.170243 [debug] [MainThread]: On master: ROLLBACK
[0m09:59:01.171252 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:59:01.171252 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m09:59:01.171252 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m09:59:01.172259 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:59:01.172259 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:59:01.173266 [info ] [MainThread]: 
[0m09:59:01.173266 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.12 seconds (0.12s).
[0m09:59:01.174273 [debug] [MainThread]: Command end result
[0m09:59:01.180345 [info ] [MainThread]: 
[0m09:59:01.180345 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:59:01.181348 [info ] [MainThread]: 
[0m09:59:01.181872 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m09:59:01.182494 [debug] [MainThread]: Command `cli run` succeeded at 09:59:01.182494 after 0.86 seconds
[0m09:59:01.183505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E041FE8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E041A11C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015E05F1CC20>]}
[0m09:59:01.183505 [debug] [MainThread]: Flushing usage events
[0m10:57:14.940973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5C23FBE30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5C23FB530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5C23FB0B0>]}


============================== 10:57:14.941978 | f4c5da09-3c10-4f7f-81ed-debfd6bde66c ==============================
[0m10:57:14.941978 [info ] [MainThread]: Running with dbt=1.7.4
[0m10:57:14.941978 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m10:57:15.042604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f4c5da09-3c10-4f7f-81ed-debfd6bde66c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5C098FFE0>]}
[0m10:57:15.090887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f4c5da09-3c10-4f7f-81ed-debfd6bde66c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5C1A191F0>]}
[0m10:57:15.091896 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m10:57:15.102964 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m10:57:15.566751 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:57:15.566751 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:57:15.570770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f4c5da09-3c10-4f7f-81ed-debfd6bde66c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5C3576270>]}
[0m10:57:15.580825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f4c5da09-3c10-4f7f-81ed-debfd6bde66c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5C3652CF0>]}
[0m10:57:15.580825 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m10:57:15.581829 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4c5da09-3c10-4f7f-81ed-debfd6bde66c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5C23B5250>]}
[0m10:57:15.582834 [info ] [MainThread]: 
[0m10:57:15.582834 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m10:57:15.583840 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m10:57:15.589878 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m10:57:15.589878 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m10:57:15.589878 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:57:15.590883 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m10:57:15.591888 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m10:57:15.591888 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m10:57:15.594903 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m10:57:15.594903 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m10:57:15.594903 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m10:57:15.595908 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m10:57:15.595908 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m10:57:15.596913 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m10:57:15.597917 [info ] [MainThread]: 
[0m10:57:15.598923 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m10:57:15.598923 [error] [MainThread]: Encountered an error:
[Errno 2] No such file or directory: 'testproj./mataextracts/ListRelations.json'
[0m10:57:15.645209 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 90, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 75, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 197, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 244, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 284, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\main.py", line 625, in run
    results = task.run()
              ^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 474, in run
    result = self.execute_with_hooks(selected_uids)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 434, in execute_with_hooks
    self.before_run(adapter, selected_uids)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\run.py", line 447, in before_run
    self.populate_adapter_cache(adapter, required_schemas)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\runnable.py", line 412, in populate_adapter_cache
    adapter.set_relations_cache(self.manifest)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 519, in set_relations_cache
    self._relations_cache_for_schemas(manifest, required_schemas)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\base\impl.py", line 495, in _relations_cache_for_schemas
    for relation in future.result():
                    ^^^^^^^^^^^^^^^
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\utils.py", line 471, in connected
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricsparknb\impl.py", line 209, in list_relations_without_caching
    show_table_extended_rows = catalog.ListRelations(self.config) #self.execute_macro(LIST_RELATIONS_MACRO_NAME, kwargs=kwargs)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricsparknb\catalog.py", line 9, in ListRelations
    with open(profile.project_root + './mataextracts/ListRelations.json', 'r') as file:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'testproj./mataextracts/ListRelations.json'

[0m10:57:15.646215 [debug] [MainThread]: Command `cli run` failed at 10:57:15.646215 after 0.74 seconds
[0m10:57:15.647220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5C21B7740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5C3576090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F5C362F860>]}
[0m10:57:15.647220 [debug] [MainThread]: Flushing usage events
[0m10:57:26.620409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E51A806E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E51A807A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E51A80BC0>]}


============================== 10:57:26.621413 | d81f027e-8697-4f4b-ab27-32cda0df8037 ==============================
[0m10:57:26.621413 [info ] [MainThread]: Running with dbt=1.7.4
[0m10:57:26.621413 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m10:57:26.720725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd81f027e-8697-4f4b-ab27-32cda0df8037', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E5199F860>]}
[0m10:57:26.769041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd81f027e-8697-4f4b-ab27-32cda0df8037', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E51387110>]}
[0m10:57:26.769041 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m10:57:26.776086 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m10:57:26.811308 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:57:26.811308 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:57:26.815340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd81f027e-8697-4f4b-ab27-32cda0df8037', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E51C47B00>]}
[0m10:57:26.820571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd81f027e-8697-4f4b-ab27-32cda0df8037', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E52C85820>]}
[0m10:57:26.820571 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m10:57:26.821626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd81f027e-8697-4f4b-ab27-32cda0df8037', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E51B2D460>]}
[0m10:57:26.822674 [info ] [MainThread]: 
[0m10:57:26.823723 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m10:57:26.823723 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m10:57:26.830016 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m10:57:26.830016 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m10:57:26.830016 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:57:26.830016 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m10:57:26.832130 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m10:57:26.833140 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m10:57:26.836190 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m10:57:26.837201 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m10:57:26.837201 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m10:57:26.838210 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m10:57:26.838210 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m10:57:26.840229 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m10:57:26.855404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd81f027e-8697-4f4b-ab27-32cda0df8037', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E52CAF2C0>]}
[0m10:57:26.856413 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m10:57:26.856413 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m10:57:26.856413 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m10:57:26.857422 [info ] [MainThread]: 
[0m10:57:26.859438 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m10:57:26.860443 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m10:57:26.860443 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m10:57:26.861450 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m10:57:26.866540 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m10:57:26.867553 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 10:57:26.861450 => 10:57:26.867553
[0m10:57:26.867553 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m10:57:26.895147 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m10:57:26.896153 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m10:57:26.896153 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m10:57:26.897157 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:57:26.897157 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m10:57:26.897157 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m10:57:26.903186 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m10:57:26.904190 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m10:57:26.904190 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select  from my_first_dbt_model__dbt_tmp


[0m10:57:26.904190 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m10:57:26.912254 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 10:57:26.868565 => 10:57:26.912254
[0m10:57:26.912254 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd81f027e-8697-4f4b-ab27-32cda0df8037', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E51C47380>]}
[0m10:57:26.913263 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.05s]
[0m10:57:26.913263 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m10:57:26.914273 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m10:57:26.914273 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m10:57:26.915279 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m10:57:26.915279 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m10:57:26.917303 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m10:57:26.918311 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 10:57:26.916285 => 10:57:26.918311
[0m10:57:26.918311 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m10:57:26.929420 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m10:57:26.930429 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m10:57:26.930429 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m10:57:26.931438 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m10:57:26.932447 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 10:57:26.919321 => 10:57:26.932447
[0m10:57:26.932447 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd81f027e-8697-4f4b-ab27-32cda0df8037', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E52D60D10>]}
[0m10:57:26.933498 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m10:57:26.933498 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m10:57:26.934509 [debug] [MainThread]: On master: ROLLBACK
[0m10:57:26.935517 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:57:26.935517 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m10:57:26.935517 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m10:57:26.935517 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m10:57:26.936522 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m10:57:26.936522 [info ] [MainThread]: 
[0m10:57:26.936522 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.11 seconds (0.11s).
[0m10:57:26.937526 [debug] [MainThread]: Command end result
[0m10:57:26.941555 [info ] [MainThread]: 
[0m10:57:26.942560 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:57:26.942560 [info ] [MainThread]: 
[0m10:57:26.942560 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m10:57:26.943565 [debug] [MainThread]: Command `cli run` succeeded at 10:57:26.943565 after 0.35 seconds
[0m10:57:26.943565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E52CAF2C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E52D10B30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E51B51580>]}
[0m10:57:26.943565 [debug] [MainThread]: Flushing usage events
[0m11:10:00.209301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC1824E240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC1824CB60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC182E21E0>]}


============================== 11:10:00.211324 | ea619ac2-fc4e-4baf-b84d-6d01826adfe4 ==============================
[0m11:10:00.211324 [info ] [MainThread]: Running with dbt=1.7.4
[0m11:10:00.213334 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:10:00.525969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ea619ac2-fc4e-4baf-b84d-6d01826adfe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC17E75CD0>]}
[0m11:10:00.690049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ea619ac2-fc4e-4baf-b84d-6d01826adfe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC1822E960>]}
[0m11:10:00.692063 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m11:10:00.722582 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m11:10:01.262987 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:10:01.263994 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:10:01.281154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ea619ac2-fc4e-4baf-b84d-6d01826adfe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC18881190>]}
[0m11:10:01.294751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ea619ac2-fc4e-4baf-b84d-6d01826adfe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC18874CE0>]}
[0m11:10:01.295805 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m11:10:01.297907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ea619ac2-fc4e-4baf-b84d-6d01826adfe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC1874CB90>]}
[0m11:10:01.303065 [info ] [MainThread]: 
[0m11:10:01.305075 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m11:10:01.310125 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m11:10:01.336821 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m11:10:01.337826 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m11:10:01.338831 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:10:01.340841 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:10:01.347186 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m11:10:01.350202 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m11:10:01.361284 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:10:01.362290 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m11:10:01.364003 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m11:10:01.365532 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:10:01.368545 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:10:01.372997 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m11:10:01.481874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ea619ac2-fc4e-4baf-b84d-6d01826adfe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC18814530>]}
[0m11:10:01.484383 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:10:01.484912 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:10:01.487938 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m11:10:01.488944 [info ] [MainThread]: 
[0m11:10:01.495007 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m11:10:01.497191 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m11:10:01.500277 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m11:10:01.501283 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m11:10:01.535753 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m11:10:01.538780 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 11:10:01.502288 => 11:10:01.537772
[0m11:10:01.539786 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m11:10:01.652769 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:10:01.653777 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m11:10:01.655903 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m11:10:01.657085 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m11:10:01.658096 [debug] [Thread-6 (]: fabricsparknb adapter: Reusing session: 0
[0m11:10:01.660114 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m11:10:18.778314 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 11:10:01.540790 => 11:10:18.777302
[0m11:10:18.781347 [error] [Thread-6 (]: [31mUnhandled error while executing [0m
'function' object has no attribute 'GetColumnsInRelation'
[0m11:10:18.820854 [debug] [Thread-6 (]: Traceback (most recent call last):
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 372, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 323, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 419, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\run.py", line 291, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 137, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 42, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 133, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 36, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 33, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricsparknb\impl.py", line 310, in get_columns_in_relation
    rows: AttrDict = Catalog.GetColumnsInRelation(self.config, relation.schema, relation.identifier)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'function' object has no attribute 'GetColumnsInRelation'

[0m11:10:18.823362 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ea619ac2-fc4e-4baf-b84d-6d01826adfe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC18AB4830>]}
[0m11:10:18.826393 [error] [Thread-6 (]: 1 of 2 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 17.32s]
[0m11:10:18.829436 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m11:10:18.833489 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m11:10:18.835895 [info ] [Thread-6 (]: 2 of 2 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m11:10:18.837914 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m11:10:18.840943 [debug] [MainThread]: On master: ROLLBACK
[0m11:10:18.842773 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:10:18.844286 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m11:10:18.846307 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m11:10:18.847332 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:10:18.849352 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:10:18.853669 [info ] [MainThread]: 
[0m11:10:18.855688 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 17.55 seconds (17.55s).
[0m11:10:18.858726 [debug] [MainThread]: Command end result
[0m11:10:18.877099 [info ] [MainThread]: 
[0m11:10:18.883256 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:10:18.887744 [info ] [MainThread]: 
[0m11:10:18.890782 [error] [MainThread]:   'function' object has no attribute 'GetColumnsInRelation'
[0m11:10:18.892816 [info ] [MainThread]: 
[0m11:10:18.895849 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m11:10:18.907630 [debug] [MainThread]: Command `cli run` failed at 11:10:18.906557 after 18.79 seconds
[0m11:10:18.911747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC18420E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC18AA2F60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC18AA2B40>]}
[0m11:10:18.912806 [debug] [MainThread]: Flushing usage events
[0m11:10:46.474349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A06440BC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A064409250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A06440B980>]}


============================== 11:10:46.476362 | dca36439-aba5-4776-8292-9e3ca8f3d8d2 ==============================
[0m11:10:46.476362 [info ] [MainThread]: Running with dbt=1.7.4
[0m11:10:46.478374 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m11:10:46.792551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dca36439-aba5-4776-8292-9e3ca8f3d8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0640AFA40>]}
[0m11:10:46.956165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dca36439-aba5-4776-8292-9e3ca8f3d8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0646290D0>]}
[0m11:10:46.959184 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m11:10:46.987195 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m11:10:47.078798 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:10:47.080806 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:10:47.097885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dca36439-aba5-4776-8292-9e3ca8f3d8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A064581550>]}
[0m11:10:47.110997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dca36439-aba5-4776-8292-9e3ca8f3d8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0646C57F0>]}
[0m11:10:47.113015 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m11:10:47.114024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dca36439-aba5-4776-8292-9e3ca8f3d8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0646C4D10>]}
[0m11:10:47.120099 [info ] [MainThread]: 
[0m11:10:47.122643 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m11:10:47.128154 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m11:10:47.155554 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m11:10:47.157286 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m11:10:47.158800 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:10:47.160825 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:10:47.166875 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m11:10:47.170092 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m11:10:47.182529 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:10:47.184545 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m11:10:47.185879 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m11:10:47.187892 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:10:47.190907 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:10:47.196274 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m11:10:47.301278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dca36439-aba5-4776-8292-9e3ca8f3d8d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A0646DE5A0>]}
[0m11:10:47.302284 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:10:47.303721 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:10:47.305764 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m11:10:47.307633 [info ] [MainThread]: 
[0m11:10:47.314190 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m11:10:47.315197 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m11:10:47.318218 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m11:10:47.319225 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m11:10:47.358940 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m11:10:47.364082 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 11:10:47.320232 => 11:10:47.363066
[0m11:10:47.366098 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m11:10:47.483711 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:10:47.485723 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m11:10:47.487339 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m11:10:47.489267 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m11:10:47.490625 [debug] [Thread-6 (]: fabricsparknb adapter: Reusing session: 0
[0m11:10:47.493687 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m11:13:51.266299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241E4D3B3E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241E4D3AE10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241E4D3BC80>]}


============================== 11:13:51.268312 | 482c24dc-f644-4191-b88e-c1af224fead9 ==============================
[0m11:13:51.268312 [info ] [MainThread]: Running with dbt=1.7.4
[0m11:13:51.270325 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:13:51.586627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '482c24dc-f644-4191-b88e-c1af224fead9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241E4D39BE0>]}
[0m11:13:51.750196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '482c24dc-f644-4191-b88e-c1af224fead9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241E4DB0470>]}
[0m11:13:51.752739 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m11:13:51.779167 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m11:13:51.864019 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:13:51.866032 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:13:51.882672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '482c24dc-f644-4191-b88e-c1af224fead9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241E4F59550>]}
[0m11:13:51.896062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '482c24dc-f644-4191-b88e-c1af224fead9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241E4FE5D90>]}
[0m11:13:51.897724 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m11:13:51.898412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '482c24dc-f644-4191-b88e-c1af224fead9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241E4FE5550>]}
[0m11:13:51.903894 [info ] [MainThread]: 
[0m11:13:51.906910 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m11:13:51.912016 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m11:13:51.940300 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m11:13:51.942321 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m11:13:51.943327 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:13:51.945356 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:13:51.952805 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m11:13:51.955830 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m11:13:51.967917 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:13:51.969957 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m11:13:51.971996 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m11:13:51.974013 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:13:51.978051 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:13:51.983133 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m11:13:52.089750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '482c24dc-f644-4191-b88e-c1af224fead9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000241E4741D60>]}
[0m11:13:52.091761 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:13:52.092766 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:13:52.094777 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m11:13:52.096788 [info ] [MainThread]: 
[0m11:13:52.104287 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m11:13:52.106297 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m11:13:52.108309 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m11:13:52.109312 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m11:13:52.146031 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m11:13:52.149048 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 11:13:52.110624 => 11:13:52.148042
[0m11:13:52.151068 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m11:13:52.266789 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:13:52.267804 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m11:13:52.268818 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m11:13:52.270828 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m11:13:52.271833 [debug] [Thread-6 (]: fabricsparknb adapter: Reusing session: 0
[0m11:13:52.274849 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m11:14:34.014151 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013892E55A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013892E57860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013892E57410>]}


============================== 11:14:34.016160 | 950e7ae4-7b10-4bd2-a1b8-7f5524efed79 ==============================
[0m11:14:34.016160 [info ] [MainThread]: Running with dbt=1.7.4
[0m11:14:34.018169 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m11:14:34.336488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '950e7ae4-7b10-4bd2-a1b8-7f5524efed79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013892EC94C0>]}
[0m11:14:34.498788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '950e7ae4-7b10-4bd2-a1b8-7f5524efed79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001389306BB90>]}
[0m11:14:34.500798 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m11:14:34.526535 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m11:14:34.614596 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:14:34.615601 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:14:34.632925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '950e7ae4-7b10-4bd2-a1b8-7f5524efed79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013893040950>]}
[0m11:14:34.646583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '950e7ae4-7b10-4bd2-a1b8-7f5524efed79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013893118D70>]}
[0m11:14:34.647589 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m11:14:34.649713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '950e7ae4-7b10-4bd2-a1b8-7f5524efed79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013893118650>]}
[0m11:14:34.655277 [info ] [MainThread]: 
[0m11:14:34.657812 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m11:14:34.661851 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m11:14:34.687535 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m11:14:34.689546 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m11:14:34.691562 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:14:34.693592 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:14:34.703027 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m11:14:34.705038 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m11:14:34.718368 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:14:34.720376 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m11:14:34.721382 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m11:14:34.723391 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:14:34.727652 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:14:34.733725 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m11:14:34.841021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '950e7ae4-7b10-4bd2-a1b8-7f5524efed79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013892E8A540>]}
[0m11:14:34.843046 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:14:34.844056 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:14:34.847079 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m11:14:34.848084 [info ] [MainThread]: 
[0m11:14:34.855180 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m11:14:34.858206 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m11:14:34.861233 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m11:14:34.863249 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m11:14:34.902833 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m11:14:34.904847 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 11:14:34.866326 => 11:14:34.904847
[0m11:14:34.906866 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m11:14:35.028710 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:14:35.030722 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m11:14:35.031727 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m11:14:35.032734 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m11:14:35.034744 [debug] [Thread-6 (]: fabricsparknb adapter: Reusing session: 0
[0m11:14:35.036763 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m11:14:37.533415 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 11:14:34.908390 => 11:14:37.532909
[0m11:14:37.535425 [error] [Thread-6 (]: [31mUnhandled error while executing [0m
'function' object has no attribute 'GetColumnsInRelation'
[0m11:14:37.542486 [debug] [Thread-6 (]: Traceback (most recent call last):
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 372, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 323, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 419, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\run.py", line 291, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 137, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 42, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 133, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 36, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 33, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricsparknb\impl.py", line 310, in get_columns_in_relation
    rows: AttrDict = Catalog.GetColumnsInRelation(self.config, relation.schema, relation.identifier)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'function' object has no attribute 'GetColumnsInRelation'

[0m11:14:37.544504 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '950e7ae4-7b10-4bd2-a1b8-7f5524efed79', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013893357890>]}
[0m11:14:37.545510 [error] [Thread-6 (]: 1 of 2 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 2.68s]
[0m11:14:37.547528 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m11:14:37.551564 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m11:14:37.553582 [info ] [Thread-6 (]: 2 of 2 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m11:14:37.554590 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m11:14:37.557692 [debug] [MainThread]: On master: ROLLBACK
[0m11:14:37.559702 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:14:37.560710 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m11:14:37.561715 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m11:14:37.563734 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:14:37.564747 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:14:37.567776 [info ] [MainThread]: 
[0m11:14:37.569788 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 2.91 seconds (2.91s).
[0m11:14:37.571817 [debug] [MainThread]: Command end result
[0m11:14:37.584931 [info ] [MainThread]: 
[0m11:14:37.586956 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:14:37.588971 [info ] [MainThread]: 
[0m11:14:37.589974 [error] [MainThread]:   'function' object has no attribute 'GetColumnsInRelation'
[0m11:14:37.591992 [info ] [MainThread]: 
[0m11:14:37.593001 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m11:14:37.597034 [debug] [MainThread]: Command `cli run` failed at 11:14:37.596025 after 3.67 seconds
[0m11:14:37.598044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013892D6D940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013893343FE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013893340890>]}
[0m11:14:37.600061 [debug] [MainThread]: Flushing usage events
[0m11:15:35.575620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288F569AB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288F569B050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288F569B3B0>]}


============================== 11:15:35.578644 | 45d18b5c-3951-47c6-a287-274621ee0d4e ==============================
[0m11:15:35.578644 [info ] [MainThread]: Running with dbt=1.7.4
[0m11:15:35.580656 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:15:35.893539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '45d18b5c-3951-47c6-a287-274621ee0d4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288F57601D0>]}
[0m11:15:36.057593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '45d18b5c-3951-47c6-a287-274621ee0d4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288F58F2E70>]}
[0m11:15:36.060615 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m11:15:36.085785 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m11:15:36.173277 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:15:36.175290 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:15:36.191818 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '45d18b5c-3951-47c6-a287-274621ee0d4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288F587DDF0>]}
[0m11:15:36.205225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '45d18b5c-3951-47c6-a287-274621ee0d4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288F5989820>]}
[0m11:15:36.206624 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m11:15:36.208652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '45d18b5c-3951-47c6-a287-274621ee0d4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288F57EE900>]}
[0m11:15:36.214212 [info ] [MainThread]: 
[0m11:15:36.216745 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m11:15:36.221801 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m11:15:36.248504 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m11:15:36.250516 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m11:15:36.251522 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:15:36.253543 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:15:36.261636 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m11:15:36.263659 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m11:15:36.278838 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:15:36.280348 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m11:15:36.281355 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m11:15:36.283366 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:15:36.287397 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:15:36.295552 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m11:15:36.400900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '45d18b5c-3951-47c6-a287-274621ee0d4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288F59A63F0>]}
[0m11:15:36.402913 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:15:36.404932 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:15:36.407150 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m11:15:36.408157 [info ] [MainThread]: 
[0m11:15:36.415206 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m11:15:36.417222 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m11:15:36.419240 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m11:15:36.421250 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m11:15:36.456683 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m11:15:36.460757 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 11:15:36.422259 => 11:15:36.459731
[0m11:15:36.462816 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m11:15:36.580756 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:15:36.581761 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m11:15:36.582772 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m11:15:36.584785 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m11:15:36.585792 [debug] [Thread-6 (]: fabricsparknb adapter: Reusing session: 0
[0m11:15:36.587806 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m11:15:43.542527 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 11:15:36.463829 => 11:15:43.541520
[0m11:15:43.544539 [error] [Thread-6 (]: [31mUnhandled error while executing [0m
'schema'
[0m11:15:43.578127 [debug] [Thread-6 (]: Traceback (most recent call last):
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 372, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 323, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 419, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\run.py", line 291, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 137, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 42, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 133, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 36, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 33, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricsparknb\impl.py", line 310, in get_columns_in_relation
    rows: AttrDict = catalog.GetColumnsInRelation(self.config, relation.schema, relation.identifier)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricsparknb\catalog.py", line 56, in GetColumnsInRelation
    filtered_table = table.where(lambda row: row['schema'] == schema and row['identifier'] == identifier)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\agate\table\where.py", line 25, in where
    if test(row):
       ^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricsparknb\catalog.py", line 56, in <lambda>
    filtered_table = table.where(lambda row: row['schema'] == schema and row['identifier'] == identifier)
                                             ~~~^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\agate\mapped_sequence.py", line 90, in __getitem__
    return self.dict()[key]
           ~~~~~~~~~~~^^^^^
KeyError: 'schema'

[0m11:15:43.581154 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '45d18b5c-3951-47c6-a287-274621ee0d4e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288F5C340B0>]}
[0m11:15:43.583206 [error] [Thread-6 (]: 1 of 2 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 7.16s]
[0m11:15:43.585220 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m11:15:43.589250 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m11:15:43.591270 [info ] [Thread-6 (]: 2 of 2 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m11:15:43.593288 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m11:15:43.599382 [debug] [MainThread]: On master: ROLLBACK
[0m11:15:43.600390 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:15:43.602404 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m11:15:43.603410 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m11:15:43.604415 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:15:43.606432 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:15:43.608445 [info ] [MainThread]: 
[0m11:15:43.610459 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 7.39 seconds (7.39s).
[0m11:15:43.612476 [debug] [MainThread]: Command end result
[0m11:15:43.627688 [info ] [MainThread]: 
[0m11:15:43.629722 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m11:15:43.632777 [info ] [MainThread]: 
[0m11:15:43.634798 [error] [MainThread]:   'schema'
[0m11:15:43.636820 [info ] [MainThread]: 
[0m11:15:43.638832 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m11:15:43.642864 [debug] [MainThread]: Command `cli run` failed at 11:15:43.641857 after 8.15 seconds
[0m11:15:43.646978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288F569B830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288F5C35160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000288F5C34740>]}
[0m11:15:43.648999 [debug] [MainThread]: Flushing usage events
[0m11:16:24.794497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032850E4B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032850F3B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032850DE50>]}


============================== 11:16:24.797522 | 79ef2075-c0c6-462d-84d8-1544fcfc6abf ==============================
[0m11:16:24.797522 [info ] [MainThread]: Running with dbt=1.7.4
[0m11:16:24.799555 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m11:16:25.138712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '79ef2075-c0c6-462d-84d8-1544fcfc6abf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032861A960>]}
[0m11:16:25.301375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '79ef2075-c0c6-462d-84d8-1544fcfc6abf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032850D310>]}
[0m11:16:25.304392 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m11:16:25.332443 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m11:16:25.426481 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:16:25.428505 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:16:25.444843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '79ef2075-c0c6-462d-84d8-1544fcfc6abf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020328159370>]}
[0m11:16:25.458771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '79ef2075-c0c6-462d-84d8-1544fcfc6abf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203287C9430>]}
[0m11:16:25.459778 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m11:16:25.461795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '79ef2075-c0c6-462d-84d8-1544fcfc6abf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032854EDB0>]}
[0m11:16:25.467105 [info ] [MainThread]: 
[0m11:16:25.470203 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m11:16:25.474236 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m11:16:25.501494 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m11:16:25.503506 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m11:16:25.505564 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:16:25.507596 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:16:25.517248 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m11:16:25.520487 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m11:16:25.532582 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:16:25.534591 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m11:16:25.535597 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m11:16:25.536604 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:16:25.541689 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:16:25.546765 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m11:16:25.652209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '79ef2075-c0c6-462d-84d8-1544fcfc6abf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203287E2CC0>]}
[0m11:16:25.654221 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:16:25.655229 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:16:25.657244 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m11:16:25.659258 [info ] [MainThread]: 
[0m11:16:25.665994 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m11:16:25.667519 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m11:16:25.670551 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m11:16:25.672010 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m11:16:25.708306 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m11:16:25.711328 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 11:16:25.672669 => 11:16:25.710321
[0m11:16:25.713357 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m11:16:25.833522 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:16:25.835532 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m11:16:25.836536 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m11:16:25.837541 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m11:16:25.839549 [debug] [Thread-6 (]: fabricsparknb adapter: Reusing session: 0
[0m11:16:25.841560 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m11:17:02.916263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F39834E60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F3947B650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F398367E0>]}


============================== 11:17:02.919283 | 5995e8f6-e4a1-4b0c-964d-eeb60c6df9a8 ==============================
[0m11:17:02.919283 [info ] [MainThread]: Running with dbt=1.7.4
[0m11:17:02.920289 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:17:03.237060 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5995e8f6-e4a1-4b0c-964d-eeb60c6df9a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F3974E4E0>]}
[0m11:17:03.399651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5995e8f6-e4a1-4b0c-964d-eeb60c6df9a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F39A4EEA0>]}
[0m11:17:03.401707 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m11:17:03.427639 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m11:17:03.524197 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:17:03.525208 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:17:03.542319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5995e8f6-e4a1-4b0c-964d-eeb60c6df9a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F39A99EE0>]}
[0m11:17:03.555458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5995e8f6-e4a1-4b0c-964d-eeb60c6df9a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F39AF4F50>]}
[0m11:17:03.556534 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m11:17:03.558583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5995e8f6-e4a1-4b0c-964d-eeb60c6df9a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F39AF4890>]}
[0m11:17:03.563685 [info ] [MainThread]: 
[0m11:17:03.566709 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m11:17:03.571808 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m11:17:03.600196 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m11:17:03.602212 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m11:17:03.603228 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:17:03.605240 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:17:03.612781 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m11:17:03.614792 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m11:17:03.626877 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:17:03.628904 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m11:17:03.630932 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m11:17:03.631943 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:17:03.635998 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:17:03.641038 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m11:17:03.745119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5995e8f6-e4a1-4b0c-964d-eeb60c6df9a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F397EB1A0>]}
[0m11:17:03.747130 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:17:03.748137 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:17:03.750165 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m11:17:03.752183 [info ] [MainThread]: 
[0m11:17:03.758496 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m11:17:03.760511 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m11:17:03.762525 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m11:17:03.764537 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m11:17:03.800004 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m11:17:03.803054 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 11:17:03.764537 => 11:17:03.802015
[0m11:17:03.804067 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m11:17:03.920118 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:17:03.921131 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m11:17:03.922141 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m11:17:03.924165 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m11:17:03.925175 [debug] [Thread-6 (]: fabricsparknb adapter: Reusing session: 0
[0m11:17:03.927210 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m11:17:11.267624 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m11:17:11.270651 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m11:17:11.272666 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m11:17:11.273671 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m11:17:11.311078 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 11:17:03.805087 => 11:17:11.310069
[0m11:17:11.313101 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5995e8f6-e4a1-4b0c-964d-eeb60c6df9a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F39D86BD0>]}
[0m11:17:11.316157 [info ] [Thread-6 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 7.55s]
[0m11:17:11.318181 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m11:17:11.321229 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m11:17:11.323244 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m11:17:11.326281 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m11:17:11.327289 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m11:17:11.336378 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m11:17:11.338396 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 11:17:11.328298 => 11:17:11.338396
[0m11:17:11.340412 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m11:17:11.393488 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m11:17:11.396585 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m11:17:11.398625 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m11:17:11.399637 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m11:17:11.405676 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 11:17:11.341416 => 11:17:11.404671
[0m11:17:11.407687 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5995e8f6-e4a1-4b0c-964d-eeb60c6df9a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F39E43320>]}
[0m11:17:11.409706 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m11:17:11.411731 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m11:17:11.415778 [debug] [MainThread]: On master: ROLLBACK
[0m11:17:11.418849 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:17:11.419870 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m11:17:11.421894 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m11:17:11.422900 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:17:11.423909 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:17:11.426969 [info ] [MainThread]: 
[0m11:17:11.429082 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 7.86 seconds (7.86s).
[0m11:17:11.431098 [debug] [MainThread]: Command end result
[0m11:17:11.444238 [info ] [MainThread]: 
[0m11:17:11.445245 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:17:11.448278 [info ] [MainThread]: 
[0m11:17:11.450297 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m11:17:11.454336 [debug] [MainThread]: Command `cli run` succeeded at 11:17:11.454336 after 8.62 seconds
[0m11:17:11.456347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F394AF110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F39E41430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024F39E42C60>]}
[0m11:17:11.457364 [debug] [MainThread]: Flushing usage events
[0m11:17:49.601851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D010E1B770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D010E1A570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D010E1B020>]}


============================== 11:17:49.604265 | 68cda7bf-b740-466a-ae0c-1dc9edb60db6 ==============================
[0m11:17:49.604265 [info ] [MainThread]: Running with dbt=1.7.4
[0m11:17:49.606305 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m11:17:49.920882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '68cda7bf-b740-466a-ae0c-1dc9edb60db6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D010B2B6E0>]}
[0m11:17:50.083809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '68cda7bf-b740-466a-ae0c-1dc9edb60db6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D010E5E720>]}
[0m11:17:50.085982 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m11:17:50.113158 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m11:17:50.210004 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:17:50.212027 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:17:50.228995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '68cda7bf-b740-466a-ae0c-1dc9edb60db6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D0110666F0>]}
[0m11:17:50.242846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '68cda7bf-b740-466a-ae0c-1dc9edb60db6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D01108B3B0>]}
[0m11:17:50.243852 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m11:17:50.245862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '68cda7bf-b740-466a-ae0c-1dc9edb60db6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D01108A9C0>]}
[0m11:17:50.251053 [info ] [MainThread]: 
[0m11:17:50.253508 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m11:17:50.259569 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m11:17:50.288902 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m11:17:50.291995 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m11:17:50.293000 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:17:50.295011 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:17:50.302072 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m11:17:50.305187 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m11:17:50.317275 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:17:50.318280 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m11:17:50.319286 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m11:17:50.321299 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m11:17:50.326417 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:17:50.331461 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m11:17:50.437664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '68cda7bf-b740-466a-ae0c-1dc9edb60db6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D0110E2870>]}
[0m11:17:50.440684 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:17:50.441690 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:17:50.443706 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m11:17:50.445719 [info ] [MainThread]: 
[0m11:17:50.453786 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m11:17:50.455801 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m11:17:50.458818 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m11:17:50.459824 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m11:17:50.495227 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m11:17:50.497252 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 11:17:50.460830 => 11:17:50.497252
[0m11:17:50.499262 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m11:17:50.615723 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:17:50.617734 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m11:17:50.618739 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m11:17:50.620777 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m11:17:50.621784 [debug] [Thread-6 (]: fabricsparknb adapter: Reusing session: 0
[0m11:17:50.623798 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m13:02:37.151578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012C321ED5E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012C321EC4D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012C321ED640>]}


============================== 13:02:37.152586 | 4fab7214-5e84-4f8f-9df3-fbbde115efd6 ==============================
[0m13:02:37.152586 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:02:37.152586 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:02:37.251681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4fab7214-5e84-4f8f-9df3-fbbde115efd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012C31A03C80>]}
[0m13:02:37.300973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4fab7214-5e84-4f8f-9df3-fbbde115efd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012C31B1A840>]}
[0m13:02:37.301983 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:02:37.313052 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:02:37.806324 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:02:37.806324 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:02:37.810356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4fab7214-5e84-4f8f-9df3-fbbde115efd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012C323001A0>]}
[0m13:02:37.824500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4fab7214-5e84-4f8f-9df3-fbbde115efd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012C333F5A90>]}
[0m13:02:37.824500 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:02:37.825510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4fab7214-5e84-4f8f-9df3-fbbde115efd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012C3210EBD0>]}
[0m13:02:37.826519 [info ] [MainThread]: 
[0m13:02:37.827529 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:02:37.828539 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:02:37.833573 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m13:02:37.834580 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m13:02:37.834580 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:02:37.835585 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m13:02:37.836601 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m13:02:37.837607 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m13:02:37.839626 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:02:37.840636 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m13:02:37.840636 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m13:02:37.840636 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m13:02:37.841648 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:02:37.842655 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:02:37.864844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4fab7214-5e84-4f8f-9df3-fbbde115efd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012C334462D0>]}
[0m13:02:37.864844 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:02:37.864844 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:02:37.865857 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:02:37.866901 [info ] [MainThread]: 
[0m13:02:37.869953 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m13:02:37.870962 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:02:37.870962 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:02:37.871972 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:02:37.876001 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:02:37.876001 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:02:37.871972 => 13:02:37.876001
[0m13:02:37.877010 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:02:37.904217 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:02:37.905222 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:02:37.905222 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:02:37.906227 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:02:37.906227 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m13:02:37.907231 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:02:37.942626 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m13:02:37.943702 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:02:37.944706 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m13:02:37.944706 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:02:37.954277 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 13:02:37.877010 => 13:02:37.954277
[0m13:02:37.954277 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4fab7214-5e84-4f8f-9df3-fbbde115efd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012C335AA840>]}
[0m13:02:37.955281 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.08s]
[0m13:02:37.956286 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m13:02:37.956286 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m13:02:37.957292 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m13:02:37.957292 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m13:02:37.958298 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m13:02:37.959303 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m13:02:37.960320 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 13:02:37.958298 => 13:02:37.960320
[0m13:02:37.960320 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m13:02:37.972405 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m13:02:37.974417 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m13:02:37.974417 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m13:02:37.975424 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:02:37.977465 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 13:02:37.960320 => 13:02:37.977465
[0m13:02:37.977465 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4fab7214-5e84-4f8f-9df3-fbbde115efd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012C335A9A00>]}
[0m13:02:37.978471 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m13:02:37.979477 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m13:02:37.980484 [debug] [MainThread]: On master: ROLLBACK
[0m13:02:37.981493 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:02:37.981493 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m13:02:37.982500 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:02:37.982500 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:02:37.982500 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:02:37.983510 [info ] [MainThread]: 
[0m13:02:37.983510 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m13:02:37.984518 [debug] [MainThread]: Command end result
[0m13:02:37.991609 [info ] [MainThread]: 
[0m13:02:37.991609 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:02:37.992615 [info ] [MainThread]: 
[0m13:02:37.992615 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:02:37.993624 [debug] [MainThread]: Command `cli run` succeeded at 13:02:37.993624 after 0.88 seconds
[0m13:02:37.993624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012C322BC260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012C31E8C530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012C321A7B60>]}
[0m13:02:37.994632 [debug] [MainThread]: Flushing usage events
[0m13:08:35.390678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D53BC14C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D53AF7890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D53BC1610>]}


============================== 13:08:35.392709 | ffdc4a31-a1b3-4c22-bc8a-ae380d4395cc ==============================
[0m13:08:35.392709 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:08:35.394776 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:08:35.701901 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ffdc4a31-a1b3-4c22-bc8a-ae380d4395cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D53D37A10>]}
[0m13:08:35.862212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ffdc4a31-a1b3-4c22-bc8a-ae380d4395cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D53E079E0>]}
[0m13:08:35.864309 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:08:35.888731 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:08:35.979280 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:08:35.981290 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:08:35.997778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ffdc4a31-a1b3-4c22-bc8a-ae380d4395cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D53E33F20>]}
[0m13:08:36.013375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ffdc4a31-a1b3-4c22-bc8a-ae380d4395cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D53EF3980>]}
[0m13:08:36.014382 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:08:36.016395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ffdc4a31-a1b3-4c22-bc8a-ae380d4395cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D53B9DF70>]}
[0m13:08:36.021419 [info ] [MainThread]: 
[0m13:08:36.023436 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:08:36.028702 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:08:36.053490 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m13:08:36.054471 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m13:08:36.056302 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:20:44.410879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002064756F950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020647307F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002064756F740>]}


============================== 13:20:44.410879 | 365b67f6-aa2b-498a-b2f2-6b979a469362 ==============================
[0m13:20:44.410879 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:20:44.411898 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:20:44.504427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '365b67f6-aa2b-498a-b2f2-6b979a469362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002064759A780>]}
[0m13:20:44.551642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '365b67f6-aa2b-498a-b2f2-6b979a469362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020646EB3B00>]}
[0m13:20:44.552645 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:20:44.564720 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:20:45.042614 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:20:45.042614 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:20:45.046633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '365b67f6-aa2b-498a-b2f2-6b979a469362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002064866FB30>]}
[0m13:20:45.056698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '365b67f6-aa2b-498a-b2f2-6b979a469362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020648765D30>]}
[0m13:20:45.057709 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:20:45.057709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '365b67f6-aa2b-498a-b2f2-6b979a469362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020648643A70>]}
[0m13:20:45.058717 [info ] [MainThread]: 
[0m13:20:45.059726 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:20:45.060733 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:20:45.066783 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m13:20:45.066783 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m13:20:45.067792 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:20:45.067792 [debug] [ThreadPool]: fabricsparknb adapter: Submitted: {'code': 'show databases', 'kind': 'sql'} https://api.fabric.microsoft.com/v1/workspaces/bab084ca-748d-438e-94ad-405428bd5694/lakehouses/ccb45a7d-60fc-447b-b1d3-713e05f55e9a/livyapi/versions/2023-12-01/sessions/0/statements
[0m13:20:45.067792 [debug] [ThreadPool]: fabricsparknb adapter: Using CLI auth
[0m13:20:47.085699 [debug] [ThreadPool]: fabricsparknb adapter: CLI - Fetched Access Token
[0m13:20:48.565657 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m13:20:48.567693 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'id'
[0m13:20:48.569770 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_schemas
[0m13:20:48.570788 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  'id'
[0m13:20:48.571800 [info ] [MainThread]: 
[0m13:20:48.572805 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 3.51 seconds (3.51s).
[0m13:20:48.572805 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    'id'
[0m13:20:48.574824 [debug] [MainThread]: Command `cli run` failed at 13:20:48.574824 after 4.20 seconds
[0m13:20:48.575833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020644589DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002064759AAE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206486D4AA0>]}
[0m13:20:48.575833 [debug] [MainThread]: Flushing usage events
[0m13:21:20.290919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002572A8E1A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002572A8E3170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002572A8E3410>]}


============================== 13:21:20.293937 | 873e98f3-6e24-4dcc-bd96-d15f43b89536 ==============================
[0m13:21:20.293937 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:21:20.295380 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:21:20.608425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '873e98f3-6e24-4dcc-bd96-d15f43b89536', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002572AA5FEC0>]}
[0m13:21:20.772152 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '873e98f3-6e24-4dcc-bd96-d15f43b89536', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002572A956B10>]}
[0m13:21:20.774160 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:21:20.800593 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:21:20.884459 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:21:20.885464 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:21:20.902761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '873e98f3-6e24-4dcc-bd96-d15f43b89536', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002572A832480>]}
[0m13:21:20.915862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '873e98f3-6e24-4dcc-bd96-d15f43b89536', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002572ABE01A0>]}
[0m13:21:20.917873 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:21:20.918879 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '873e98f3-6e24-4dcc-bd96-d15f43b89536', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002572AAD17C0>]}
[0m13:21:20.923924 [info ] [MainThread]: 
[0m13:21:20.926963 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:21:20.930990 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:21:20.960513 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m13:21:20.962631 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m13:21:20.963635 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:21:25.528204 [debug] [ThreadPool]: fabricsparknb adapter: Submitted: {'code': 'show databases', 'kind': 'sql'} https://api.fabric.microsoft.com/v1/workspaces/bab084ca-748d-438e-94ad-405428bd5694/lakehouses/ccb45a7d-60fc-447b-b1d3-713e05f55e9a/livyapi/versions/2023-12-01/sessions/0/statements
[0m13:21:25.530428 [debug] [ThreadPool]: fabricsparknb adapter: Using CLI auth
[0m13:21:26.109202 [debug] [ThreadPool]: fabricsparknb adapter: CLI - Fetched Access Token
[0m13:21:26.777452 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m13:21:26.779463 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: 'id'
[0m13:21:26.781474 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro list_schemas
[0m13:21:26.782480 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  'id'
[0m13:21:26.786922 [info ] [MainThread]: 
[0m13:21:26.788412 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 5.86 seconds (5.86s).
[0m13:21:26.790972 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    'id'
[0m13:21:26.793990 [debug] [MainThread]: Command `cli run` failed at 13:21:26.792985 after 6.58 seconds
[0m13:21:26.796000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002572A800DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002572AD635C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002572AD639B0>]}
[0m13:21:26.797395 [debug] [MainThread]: Flushing usage events
[0m13:22:15.886932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E99D12BA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E99A025A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E99A03080>]}


============================== 13:22:15.888943 | 14cdb11c-7179-4185-b96d-23aef63e6ba3 ==============================
[0m13:22:15.888943 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:22:15.890951 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m13:22:16.204365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '14cdb11c-7179-4185-b96d-23aef63e6ba3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E99EA9D90>]}
[0m13:22:16.363968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '14cdb11c-7179-4185-b96d-23aef63e6ba3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E9A075EB0>]}
[0m13:22:16.366983 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:22:16.392400 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:22:16.480639 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:22:16.482650 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:22:16.499187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '14cdb11c-7179-4185-b96d-23aef63e6ba3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E99FA5490>]}
[0m13:22:16.512286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '14cdb11c-7179-4185-b96d-23aef63e6ba3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E9A0D9670>]}
[0m13:22:16.514296 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:22:16.515301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '14cdb11c-7179-4185-b96d-23aef63e6ba3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E99BE1160>]}
[0m13:22:16.520326 [info ] [MainThread]: 
[0m13:22:16.523546 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:22:16.529230 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:22:16.555845 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m13:22:16.556853 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m13:22:16.558294 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:22:26.407938 [debug] [ThreadPool]: SQL status: OK in 9.850000381469727 seconds
[0m13:22:26.417818 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m13:22:26.419826 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m13:22:26.432005 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:22:26.433528 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m13:22:26.434532 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m13:22:28.717920 [debug] [ThreadPool]: SQL status: OK in 2.2799999713897705 seconds
[0m13:22:28.721790 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:22:28.727193 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:22:28.837489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '14cdb11c-7179-4185-b96d-23aef63e6ba3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023E9A291430>]}
[0m13:22:28.839530 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:22:28.840538 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:22:28.842611 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:22:28.844985 [info ] [MainThread]: 
[0m13:22:28.854655 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m13:22:28.856672 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:22:28.858916 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:22:28.861440 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:22:28.894750 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:22:28.897112 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:22:28.862179 => 13:22:28.896090
[0m13:22:28.898118 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:22:29.035488 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:22:29.036498 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:22:29.038521 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:22:29.040545 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m13:22:29.042568 [debug] [Thread-7 (]: fabricsparknb adapter: Reusing session: 0
[0m13:22:31.199337 [debug] [Thread-7 (]: SQL status: OK in 2.1600000858306885 seconds
[0m13:23:07.544664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026BA8C1F9B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026BA8C1EFC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026BA8C1EB10>]}


============================== 13:23:07.546673 | a93bc66b-ec65-49c7-b356-0da96b468746 ==============================
[0m13:23:07.546673 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:23:07.548683 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:23:07.858810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a93bc66b-ec65-49c7-b356-0da96b468746', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026BA8950C80>]}
[0m13:23:08.018894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a93bc66b-ec65-49c7-b356-0da96b468746', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026BA8E3EBD0>]}
[0m13:23:08.021909 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:23:08.048721 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:23:08.151126 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:23:08.153158 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:23:08.170351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a93bc66b-ec65-49c7-b356-0da96b468746', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026BA889F1D0>]}
[0m13:23:08.182683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a93bc66b-ec65-49c7-b356-0da96b468746', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026BA8EC1C10>]}
[0m13:23:08.184788 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:23:08.185861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a93bc66b-ec65-49c7-b356-0da96b468746', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026BA8EEEDB0>]}
[0m13:23:08.191129 [info ] [MainThread]: 
[0m13:23:08.194904 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:23:08.201022 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:23:08.230202 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m13:23:08.232239 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m13:23:08.233244 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:25:07.390124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAF133C4A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAF133C530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAF133C4D0>]}


============================== 13:25:07.392134 | 99faf6a1-169e-4c07-b89b-85d89e53770d ==============================
[0m13:25:07.392134 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:25:07.394152 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:25:07.702959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '99faf6a1-169e-4c07-b89b-85d89e53770d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAF137FD70>]}
[0m13:25:07.862664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '99faf6a1-169e-4c07-b89b-85d89e53770d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAF155D700>]}
[0m13:25:07.864678 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:25:07.891617 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:25:07.982729 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:25:07.984739 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:25:08.001859 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '99faf6a1-169e-4c07-b89b-85d89e53770d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAF158A660>]}
[0m13:25:08.020014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '99faf6a1-169e-4c07-b89b-85d89e53770d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAF15F0F50>]}
[0m13:25:08.021526 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:25:08.023044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '99faf6a1-169e-4c07-b89b-85d89e53770d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAF04F1AF0>]}
[0m13:25:08.029127 [info ] [MainThread]: 
[0m13:25:08.032402 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:25:08.039040 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:25:08.067269 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m13:25:08.069313 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases
  
[0m13:25:08.071352 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:26:00.565953 [debug] [ThreadPool]: SQL status: OK in 52.4900016784668 seconds
[0m13:26:00.573215 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m13:26:00.575225 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m13:26:00.587459 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:26:00.589467 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m13:26:00.590684 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m13:26:02.575424 [debug] [ThreadPool]: SQL status: OK in 1.9800000190734863 seconds
[0m13:26:02.580465 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:26:02.586445 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:26:02.688522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '99faf6a1-169e-4c07-b89b-85d89e53770d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DAF17D0B60>]}
[0m13:26:02.690625 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:26:02.691635 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:26:02.694889 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:26:02.695896 [info ] [MainThread]: 
[0m13:26:02.702942 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m13:26:02.704959 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:26:02.707297 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:26:02.709312 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:26:02.744601 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:26:02.747625 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:26:02.710320 => 13:26:02.746617
[0m13:26:02.748633 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:26:02.863280 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:26:02.864285 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:26:02.865290 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:26:02.867300 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m13:26:02.868306 [debug] [Thread-7 (]: fabricsparknb adapter: Reusing session: 0
[0m13:26:04.262668 [debug] [Thread-7 (]: SQL status: OK in 1.399999976158142 seconds
[0m13:26:06.811173 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m13:26:06.814200 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:26:06.816217 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m13:26:53.077729 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C84077D580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C84077C380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C84077C080>]}


============================== 13:26:53.078739 | bdd0cc51-95c0-496b-8cdc-723d53151636 ==============================
[0m13:26:53.078739 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:26:53.078739 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:26:53.178935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bdd0cc51-95c0-496b-8cdc-723d53151636', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C84077C710>]}
[0m13:26:53.230395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bdd0cc51-95c0-496b-8cdc-723d53151636', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C84191FE90>]}
[0m13:26:53.231405 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:26:53.237457 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:26:53.280743 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:26:53.281748 [debug] [MainThread]: Partial parsing: updated file: dbt_fabricspark://macros\adapters\schema.sql
[0m13:26:53.319010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bdd0cc51-95c0-496b-8cdc-723d53151636', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C84191EC30>]}
[0m13:26:53.325056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bdd0cc51-95c0-496b-8cdc-723d53151636', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8419D66F0>]}
[0m13:26:53.325056 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:26:53.326065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bdd0cc51-95c0-496b-8cdc-723d53151636', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C841AC83B0>]}
[0m13:26:53.327073 [info ] [MainThread]: 
[0m13:26:53.328081 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:26:53.329090 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:26:53.336151 [debug] [ThreadPool]: Using fabricsparknb connection "list_schemas"
[0m13:26:53.336151 [debug] [ThreadPool]: On list_schemas: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "list_schemas"} */

    show databases blah
  
[0m13:26:53.336151 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:26:53.337159 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m13:26:53.339207 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m13:26:53.339207 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m13:26:53.342234 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:26:53.342234 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake"
[0m13:26:53.343244 [debug] [ThreadPool]: On create__datalake: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */

    select 1
  
[0m13:26:53.343244 [debug] [ThreadPool]: SQL status: OK in 0.0 seconds
[0m13:26:53.344254 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:26:53.345261 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:26:53.359456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bdd0cc51-95c0-496b-8cdc-723d53151636', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C84191F2F0>]}
[0m13:26:53.360463 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:26:53.360463 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:26:53.360463 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:26:53.361473 [info ] [MainThread]: 
[0m13:26:53.364498 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m13:26:53.364498 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:26:53.365505 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:26:53.365505 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:26:53.369544 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:26:53.370549 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:26:53.365505 => 13:26:53.370549
[0m13:26:53.370549 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:26:53.395705 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:26:53.395705 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:26:53.396715 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:26:53.396715 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:26:53.396715 [debug] [Thread-1 (]: fabricsparknb adapter: Reusing session: 0
[0m13:26:53.397723 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:26:53.415838 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m13:26:53.416857 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:26:53.416857 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m13:26:53.417864 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:26:53.425928 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 13:26:53.370549 => 13:26:53.425928
[0m13:26:53.426938 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bdd0cc51-95c0-496b-8cdc-723d53151636', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C841B89D60>]}
[0m13:26:53.426938 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.06s]
[0m13:26:53.427946 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m13:26:53.427946 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m13:26:53.428953 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m13:26:53.428953 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m13:26:53.428953 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m13:26:53.430967 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m13:26:53.431991 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 13:26:53.429961 => 13:26:53.431991
[0m13:26:53.431991 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m13:26:53.443056 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m13:26:53.444061 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m13:26:53.444061 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m13:26:53.445066 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:26:53.446073 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 13:26:53.431991 => 13:26:53.446073
[0m13:26:53.446073 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bdd0cc51-95c0-496b-8cdc-723d53151636', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C841B89400>]}
[0m13:26:53.447078 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m13:26:53.447078 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m13:26:53.448098 [debug] [MainThread]: On master: ROLLBACK
[0m13:26:53.448098 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:26:53.449106 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m13:26:53.449106 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:26:53.449106 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:26:53.449106 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:26:53.450111 [info ] [MainThread]: 
[0m13:26:53.450111 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.12 seconds (0.12s).
[0m13:26:53.451117 [debug] [MainThread]: Command end result
[0m13:26:53.455144 [info ] [MainThread]: 
[0m13:26:53.456151 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:26:53.456151 [info ] [MainThread]: 
[0m13:26:53.456151 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:26:53.457159 [debug] [MainThread]: Command `cli run` succeeded at 13:26:53.457159 after 0.40 seconds
[0m13:26:53.458166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C83F279100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C84012BEC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C840129D00>]}
[0m13:26:53.458166 [debug] [MainThread]: Flushing usage events
[0m13:30:33.428878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F76F28CA10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F76F28CAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F76F28C6E0>]}


============================== 13:30:33.431908 | 780945b8-9927-496e-8340-8e99c8d6ca5f ==============================
[0m13:30:33.431908 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:30:33.434195 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:30:33.744744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '780945b8-9927-496e-8340-8e99c8d6ca5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F76F416C30>]}
[0m13:30:33.904585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '780945b8-9927-496e-8340-8e99c8d6ca5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F76F4E7EC0>]}
[0m13:30:33.906648 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:30:33.934771 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:30:34.042534 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:30:34.045554 [debug] [MainThread]: Partial parsing: updated file: dbt_fabricspark://macros\adapters\schema.sql
[0m13:30:34.197853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '780945b8-9927-496e-8340-8e99c8d6ca5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F76F5F15B0>]}
[0m13:30:34.211967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '780945b8-9927-496e-8340-8e99c8d6ca5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F76F5AA690>]}
[0m13:30:34.213982 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:30:34.214988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '780945b8-9927-496e-8340-8e99c8d6ca5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F76EA1E4B0>]}
[0m13:30:34.221026 [info ] [MainThread]: 
[0m13:30:34.224443 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:30:34.231015 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:30:45.861293 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:30:45.925628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '780945b8-9927-496e-8340-8e99c8d6ca5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002F76F540FE0>]}
[0m13:30:45.927099 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:30:45.928106 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:30:45.930386 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:30:45.932395 [info ] [MainThread]: 
[0m13:30:45.940310 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m13:30:45.942239 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:30:45.944479 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:30:45.945482 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:30:45.979954 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:30:45.982970 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:30:45.946542 => 13:30:45.981964
[0m13:30:45.983976 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:30:46.125094 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:30:46.127148 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:30:46.129072 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:30:46.130951 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m13:31:38.221659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017826CDC920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017826CDCE00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017826CDC770>]}


============================== 13:31:38.222665 | 28169389-8b1b-46ae-a3eb-d89f6fa9ae15 ==============================
[0m13:31:38.222665 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:31:38.222665 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:31:38.322878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '28169389-8b1b-46ae-a3eb-d89f6fa9ae15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017826D0ECF0>]}
[0m13:31:38.372251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '28169389-8b1b-46ae-a3eb-d89f6fa9ae15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017826A91F40>]}
[0m13:31:38.373260 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:31:38.379330 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:31:38.420584 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:31:38.420584 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:31:38.424619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '28169389-8b1b-46ae-a3eb-d89f6fa9ae15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017827EE40E0>]}
[0m13:31:38.429683 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '28169389-8b1b-46ae-a3eb-d89f6fa9ae15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017827F2E780>]}
[0m13:31:38.429683 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:31:38.430690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '28169389-8b1b-46ae-a3eb-d89f6fa9ae15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017827DC1B80>]}
[0m13:31:38.431696 [info ] [MainThread]: 
[0m13:31:38.432702 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:31:38.433708 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:31:38.437730 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:31:38.449805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '28169389-8b1b-46ae-a3eb-d89f6fa9ae15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017827F2C800>]}
[0m13:31:38.449805 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:31:38.450814 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:31:38.450814 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:31:38.451824 [info ] [MainThread]: 
[0m13:31:38.453840 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m13:31:38.454848 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:31:38.455856 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:31:38.455856 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:31:38.460914 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:31:38.461920 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:31:38.455856 => 13:31:38.461920
[0m13:31:38.461920 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:31:38.494375 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:31:38.494375 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:31:38.495387 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:31:38.495387 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:31:38.496403 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:31:38.516060 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m13:31:38.517070 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:31:38.518077 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m13:31:38.518077 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:31:38.527180 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 13:31:38.461920 => 13:31:38.527180
[0m13:31:38.528193 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '28169389-8b1b-46ae-a3eb-d89f6fa9ae15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017827E596A0>]}
[0m13:31:38.528193 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.07s]
[0m13:31:38.529246 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m13:31:38.529246 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m13:31:38.529246 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m13:31:38.530258 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m13:31:38.530258 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m13:31:38.532280 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m13:31:38.533291 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 13:31:38.531270 => 13:31:38.533291
[0m13:31:38.533291 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m13:31:38.546450 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m13:31:38.547458 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m13:31:38.547458 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m13:31:38.548469 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:31:38.549515 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 13:31:38.534303 => 13:31:38.549515
[0m13:31:38.549515 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '28169389-8b1b-46ae-a3eb-d89f6fa9ae15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000178280A1190>]}
[0m13:31:38.550570 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m13:31:38.550570 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m13:31:38.551588 [debug] [MainThread]: On master: ROLLBACK
[0m13:31:38.551588 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:31:38.552600 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m13:31:38.552600 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:31:38.552600 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:31:38.552600 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:31:38.553605 [info ] [MainThread]: 
[0m13:31:38.553605 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.12 seconds (0.12s).
[0m13:31:38.554609 [debug] [MainThread]: Command end result
[0m13:31:38.558631 [info ] [MainThread]: 
[0m13:31:38.558631 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:31:38.559636 [info ] [MainThread]: 
[0m13:31:38.559636 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:31:38.560642 [debug] [MainThread]: Command `cli run` succeeded at 13:31:38.560642 after 0.36 seconds
[0m13:31:38.560642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017825DE80E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017826A91280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017825DC3FB0>]}
[0m13:31:38.560642 [debug] [MainThread]: Flushing usage events
[0m13:34:54.273791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A5739DA30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A5739D700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A5739D9D0>]}


============================== 13:34:54.273791 | cc71b111-ef0e-484e-b2ed-3eba2e55282a ==============================
[0m13:34:54.273791 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:34:54.274797 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:34:54.369784 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cc71b111-ef0e-484e-b2ed-3eba2e55282a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A57473230>]}
[0m13:34:54.418056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cc71b111-ef0e-484e-b2ed-3eba2e55282a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A56389A00>]}
[0m13:34:54.419061 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:34:54.425094 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:34:54.463364 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:34:54.464379 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:34:54.467408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cc71b111-ef0e-484e-b2ed-3eba2e55282a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A56295F40>]}
[0m13:34:54.474452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cc71b111-ef0e-484e-b2ed-3eba2e55282a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A575CCB00>]}
[0m13:34:54.474452 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:34:54.475457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cc71b111-ef0e-484e-b2ed-3eba2e55282a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A55BB40B0>]}
[0m13:34:54.476469 [info ] [MainThread]: 
[0m13:34:54.477481 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:34:54.478492 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:34:54.483529 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:34:54.497625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cc71b111-ef0e-484e-b2ed-3eba2e55282a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A575CE4E0>]}
[0m13:34:54.498631 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:34:54.498631 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:34:54.498631 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:34:54.499636 [info ] [MainThread]: 
[0m13:34:54.501647 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m13:34:54.502652 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:34:54.502652 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:34:54.503657 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:34:54.507693 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:34:54.508703 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:34:54.503657 => 13:34:54.508198
[0m13:34:54.508703 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:34:54.540095 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:34:54.540095 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:34:54.540095 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:34:54.541100 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:34:54.566265 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:34:54.566265 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: [Errno 2] No such file or directory: './target/notebooks/notebook.ipynb'
[0m13:34:54.567271 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 13:34:54.509208 => 13:34:54.566265
[0m13:34:54.571308 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  [Errno 2] No such file or directory: './target/notebooks/notebook.ipynb'
[0m13:34:54.572314 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cc71b111-ef0e-484e-b2ed-3eba2e55282a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A576A6F90>]}
[0m13:34:54.572314 [error] [Thread-1 (]: 1 of 2 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.07s]
[0m13:34:54.573319 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m13:34:54.574325 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m13:34:54.574325 [info ] [Thread-1 (]: 2 of 2 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m13:34:54.575330 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m13:34:54.576335 [debug] [MainThread]: On master: ROLLBACK
[0m13:34:54.576335 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:34:54.576335 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m13:34:54.577341 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:34:54.577341 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:34:54.577341 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:34:54.578346 [info ] [MainThread]: 
[0m13:34:54.578346 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.10 seconds (0.10s).
[0m13:34:54.579351 [debug] [MainThread]: Command end result
[0m13:34:54.584379 [info ] [MainThread]: 
[0m13:34:54.584379 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:34:54.585385 [info ] [MainThread]: 
[0m13:34:54.586434 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  [Errno 2] No such file or directory: './target/notebooks/notebook.ipynb'
[0m13:34:54.586434 [info ] [MainThread]: 
[0m13:34:54.586434 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m13:34:54.588450 [debug] [MainThread]: Command `cli run` failed at 13:34:54.588450 after 0.34 seconds
[0m13:34:54.588450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A55C5E000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A5606D9A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A56296570>]}
[0m13:34:54.588450 [debug] [MainThread]: Flushing usage events
[0m13:36:15.539923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A704901550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A7049016A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A704901A30>]}


============================== 13:36:15.540471 | 37d3ad01-d1b1-49f6-ad92-2251fca989d9 ==============================
[0m13:36:15.540471 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:36:15.540983 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:36:15.640626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '37d3ad01-d1b1-49f6-ad92-2251fca989d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A704901A60>]}
[0m13:36:15.688919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '37d3ad01-d1b1-49f6-ad92-2251fca989d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A70455A900>]}
[0m13:36:15.689923 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:36:15.695976 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:36:15.735311 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:36:15.735311 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:36:15.739346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '37d3ad01-d1b1-49f6-ad92-2251fca989d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A705B05670>]}
[0m13:36:15.750791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '37d3ad01-d1b1-49f6-ad92-2251fca989d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A705BA4380>]}
[0m13:36:15.751800 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:36:15.752808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '37d3ad01-d1b1-49f6-ad92-2251fca989d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A703DC03B0>]}
[0m13:36:15.753820 [info ] [MainThread]: 
[0m13:36:15.754831 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:36:15.755901 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:36:15.760936 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:36:15.775924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '37d3ad01-d1b1-49f6-ad92-2251fca989d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A704902E10>]}
[0m13:36:15.776931 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:36:15.776931 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:36:15.776931 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:36:15.777943 [info ] [MainThread]: 
[0m13:36:15.780971 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m13:36:15.781978 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:36:15.781978 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:36:15.782984 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:36:15.788047 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:36:15.789056 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:36:15.782984 => 13:36:15.789056
[0m13:36:15.789056 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:36:15.819266 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:36:15.820272 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:36:15.820272 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:36:15.820272 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:36:15.841401 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m13:36:15.859519 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m13:36:15.860526 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:36:15.860526 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m13:36:15.861533 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:36:15.869605 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 13:36:15.789056 => 13:36:15.869605
[0m13:36:15.869605 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '37d3ad01-d1b1-49f6-ad92-2251fca989d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A7059E3E90>]}
[0m13:36:15.870611 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.09s]
[0m13:36:15.870611 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m13:36:15.871617 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m13:36:15.871617 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m13:36:15.872627 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m13:36:15.872627 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m13:36:15.874644 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m13:36:15.874644 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 13:36:15.872627 => 13:36:15.874644
[0m13:36:15.875656 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m13:36:15.886722 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m13:36:15.887728 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m13:36:15.887728 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m13:36:15.888732 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:36:15.889737 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 13:36:15.875656 => 13:36:15.889737
[0m13:36:15.890742 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '37d3ad01-d1b1-49f6-ad92-2251fca989d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A705CACEF0>]}
[0m13:36:15.890742 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m13:36:15.891747 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m13:36:15.892756 [debug] [MainThread]: On master: ROLLBACK
[0m13:36:15.892756 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:36:15.892756 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m13:36:15.892756 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:36:15.893762 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:36:15.893762 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:36:15.893762 [info ] [MainThread]: 
[0m13:36:15.894767 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m13:36:15.894767 [debug] [MainThread]: Command end result
[0m13:36:15.899816 [info ] [MainThread]: 
[0m13:36:15.899816 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:36:15.899816 [info ] [MainThread]: 
[0m13:36:15.900827 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:36:15.901836 [debug] [MainThread]: Command `cli run` succeeded at 13:36:15.900827 after 0.38 seconds
[0m13:36:15.901836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A705A420F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A705AB3F80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A705A8A690>]}
[0m13:36:15.901836 [debug] [MainThread]: Flushing usage events
[0m13:38:56.192281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E592DE18B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E592DE1D60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E592DE17F0>]}


============================== 13:38:56.193286 | 9e3c7027-59c7-4a97-bf35-52d21a66a0a5 ==============================
[0m13:38:56.193286 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:38:56.193286 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:38:56.286896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9e3c7027-59c7-4a97-bf35-52d21a66a0a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E5929F34A0>]}
[0m13:38:56.335149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9e3c7027-59c7-4a97-bf35-52d21a66a0a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E5929F34A0>]}
[0m13:38:56.336154 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:38:56.342185 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:38:56.377400 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:38:56.377400 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:38:56.381427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9e3c7027-59c7-4a97-bf35-52d21a66a0a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E592E21160>]}
[0m13:38:56.395548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9e3c7027-59c7-4a97-bf35-52d21a66a0a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E594080920>]}
[0m13:38:56.395548 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:38:56.395548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e3c7027-59c7-4a97-bf35-52d21a66a0a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E591FFB680>]}
[0m13:38:56.396563 [info ] [MainThread]: 
[0m13:38:56.397568 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:38:56.398575 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:38:56.402598 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:38:56.414700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9e3c7027-59c7-4a97-bf35-52d21a66a0a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E591D7E870>]}
[0m13:38:56.414700 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:38:56.415705 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:38:56.415705 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:38:56.416711 [info ] [MainThread]: 
[0m13:38:56.419729 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m13:38:56.419729 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:38:56.420737 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:38:56.420737 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:38:56.425771 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:38:56.426785 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:38:56.420737 => 13:38:56.426785
[0m13:38:56.426785 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:38:56.460122 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:38:56.461126 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:38:56.461126 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:38:56.461126 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:38:56.494522 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m13:38:56.517725 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m13:38:56.518739 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:38:56.518739 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m13:38:56.519746 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:38:56.528810 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 13:38:56.426785 => 13:38:56.527793
[0m13:38:56.528810 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e3c7027-59c7-4a97-bf35-52d21a66a0a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E592E20BC0>]}
[0m13:38:56.529815 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.11s]
[0m13:38:56.529815 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m13:38:56.530820 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m13:38:56.530820 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m13:38:56.531825 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m13:38:56.531825 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m13:38:56.534251 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m13:38:56.535254 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 13:38:56.532830 => 13:38:56.535254
[0m13:38:56.535254 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m13:38:56.547370 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m13:38:56.548381 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m13:38:56.548886 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m13:38:56.551668 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:38:56.552677 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 13:38:56.536257 => 13:38:56.552677
[0m13:38:56.553181 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9e3c7027-59c7-4a97-bf35-52d21a66a0a5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E59418E030>]}
[0m13:38:56.553687 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m13:38:56.554697 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m13:38:56.555707 [debug] [MainThread]: On master: ROLLBACK
[0m13:38:56.555707 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:38:56.556212 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m13:38:56.556718 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:38:56.556718 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:38:56.557223 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:38:56.557728 [info ] [MainThread]: 
[0m13:38:56.558233 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m13:38:56.558738 [debug] [MainThread]: Command end result
[0m13:38:56.564291 [info ] [MainThread]: 
[0m13:38:56.565302 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:38:56.565302 [info ] [MainThread]: 
[0m13:38:56.566323 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:38:56.567329 [debug] [MainThread]: Command `cli run` succeeded at 13:38:56.567329 after 0.40 seconds
[0m13:38:56.567329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E593F8D790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E594018470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E592D483B0>]}
[0m13:38:56.568334 [debug] [MainThread]: Flushing usage events
[0m13:39:44.805182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001986E5C8980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001986E5C8620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001986E5C87A0>]}


============================== 13:39:44.808208 | 6f3e67c1-0446-4678-94de-c0eaa9de05fb ==============================
[0m13:39:44.808208 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:39:44.810235 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:39:45.126912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6f3e67c1-0446-4678-94de-c0eaa9de05fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001986E62C920>]}
[0m13:39:45.286748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6f3e67c1-0446-4678-94de-c0eaa9de05fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001986E7E4830>]}
[0m13:39:45.288758 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:39:45.315134 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:39:45.401470 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:39:45.403482 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:39:45.420749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6f3e67c1-0446-4678-94de-c0eaa9de05fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001986E70B050>]}
[0m13:39:45.433827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6f3e67c1-0446-4678-94de-c0eaa9de05fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001986E8BEAE0>]}
[0m13:39:45.434832 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:39:45.436868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6f3e67c1-0446-4678-94de-c0eaa9de05fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001986E656CF0>]}
[0m13:39:45.442427 [info ] [MainThread]: 
[0m13:39:45.445094 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:39:45.449955 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:39:51.047490 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:39:51.119751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6f3e67c1-0446-4678-94de-c0eaa9de05fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001986E6C16D0>]}
[0m13:39:51.121738 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:39:51.123308 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:39:51.126330 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:39:51.128077 [info ] [MainThread]: 
[0m13:39:51.135875 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m13:39:51.136883 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:39:51.139913 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:39:51.141932 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:39:51.179474 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:39:51.183524 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:39:51.142939 => 13:39:51.182513
[0m13:39:51.184531 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:39:51.318943 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:39:51.320953 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:39:51.322476 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:39:51.323376 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m13:41:22.110676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C30176E7B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C37FBA6420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C37FE40170>]}


============================== 13:41:22.113698 | 5dfe9035-5b46-4358-a598-f1d8097d45ef ==============================
[0m13:41:22.113698 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:41:22.114712 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m13:41:22.426593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5dfe9035-5b46-4358-a598-f1d8097d45ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C30179DAC0>]}
[0m13:41:22.590944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5dfe9035-5b46-4358-a598-f1d8097d45ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3018AE030>]}
[0m13:41:22.593033 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:41:22.621496 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:41:22.708848 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:41:22.709948 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:41:22.726985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5dfe9035-5b46-4358-a598-f1d8097d45ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C301726C60>]}
[0m13:41:22.741610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5dfe9035-5b46-4358-a598-f1d8097d45ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3019E2120>]}
[0m13:41:22.743134 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:41:22.744655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5dfe9035-5b46-4358-a598-f1d8097d45ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C301957D10>]}
[0m13:41:22.750207 [info ] [MainThread]: 
[0m13:41:22.753117 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:41:22.758007 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:41:22.804760 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:41:22.869512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5dfe9035-5b46-4358-a598-f1d8097d45ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3019E3140>]}
[0m13:41:22.870752 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:41:22.872773 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:41:22.874785 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:41:22.876693 [info ] [MainThread]: 
[0m13:41:22.885453 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m13:41:22.886461 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:41:22.889716 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:41:22.890721 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:41:22.925922 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:41:22.927943 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:41:22.891726 => 13:41:22.927943
[0m13:41:22.932366 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:41:23.059321 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:41:23.061353 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:41:23.062359 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:41:23.064368 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m13:46:38.436302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023881434D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238814366C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023881436810>]}


============================== 13:46:38.438313 | 6329fb2b-9386-49dd-a1e9-3d5ee22709fc ==============================
[0m13:46:38.438313 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:46:38.440325 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:46:38.754714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6329fb2b-9386-49dd-a1e9-3d5ee22709fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238814D6570>]}
[0m13:46:38.913697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6329fb2b-9386-49dd-a1e9-3d5ee22709fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023881692E40>]}
[0m13:46:38.915717 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:46:38.942287 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:46:39.035897 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:46:39.036902 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:46:39.054355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6329fb2b-9386-49dd-a1e9-3d5ee22709fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002388162B7D0>]}
[0m13:46:39.067572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6329fb2b-9386-49dd-a1e9-3d5ee22709fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238816F1EB0>]}
[0m13:46:39.069593 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:46:39.070602 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6329fb2b-9386-49dd-a1e9-3d5ee22709fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238816664E0>]}
[0m13:46:39.075630 [info ] [MainThread]: 
[0m13:46:39.079036 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:46:39.083791 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:46:39.131588 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:46:39.197446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6329fb2b-9386-49dd-a1e9-3d5ee22709fc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000238816F3F20>]}
[0m13:46:39.199512 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:46:39.200534 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:46:39.202545 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:46:39.203550 [info ] [MainThread]: 
[0m13:46:39.210603 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m13:46:39.211611 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:46:39.214812 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:46:39.215818 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:46:39.251365 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:46:39.254418 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:46:39.216824 => 13:46:39.253409
[0m13:46:39.256433 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:46:45.670981 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:46:45.671986 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:46:45.673998 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:46:45.675006 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m13:46:52.877438 [debug] [Thread-6 (]: SQL status: OK in 7.199999809265137 seconds
[0m13:46:55.254606 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m13:54:53.950392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000268A4EF1730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000268A4EF19D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000268A4EF04D0>]}


============================== 13:54:53.952409 | 94ccca5d-93a7-4bd7-8504-8ce0dd2538c1 ==============================
[0m13:54:53.952409 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:54:53.954686 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:54:54.269648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '94ccca5d-93a7-4bd7-8504-8ce0dd2538c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000268A4E7A540>]}
[0m13:54:54.430761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '94ccca5d-93a7-4bd7-8504-8ce0dd2538c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000268A50FFEC0>]}
[0m13:54:54.432771 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:54:54.466503 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:54:55.004161 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:54:55.005209 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:54:55.022705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '94ccca5d-93a7-4bd7-8504-8ce0dd2538c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000268A505D5B0>]}
[0m13:54:55.036952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '94ccca5d-93a7-4bd7-8504-8ce0dd2538c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000268A5151EB0>]}
[0m13:54:55.037961 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:54:55.039979 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '94ccca5d-93a7-4bd7-8504-8ce0dd2538c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000268A51515B0>]}
[0m13:54:55.045199 [info ] [MainThread]: 
[0m13:54:55.047801 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:54:55.051871 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:54:55.105227 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:54:55.181003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '94ccca5d-93a7-4bd7-8504-8ce0dd2538c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000268A50D46E0>]}
[0m13:54:55.183021 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:54:55.184031 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:54:55.186297 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:54:55.187620 [info ] [MainThread]: 
[0m13:54:55.195705 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m13:54:55.197722 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:54:55.199739 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:54:55.200744 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:54:55.237561 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:54:55.240660 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:54:55.202381 => 13:54:55.239571
[0m13:54:55.242678 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:54:55.420920 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 13:54:55.243686 => 13:54:55.419914
[0m13:54:55.424346 [error] [Thread-6 (]: [31mUnhandled error while executing [0m
'NoneType' object has no attribute 'group'
[0m13:54:55.479459 [debug] [Thread-6 (]: Traceback (most recent call last):
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 372, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 323, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 419, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\run.py", line 291, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 126, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricsparknb\impl.py", line 449, in execute
    livysession.LivySession.execute(sql=sql)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricsparknb\livysession.py", line 245, in execute
    json_string = re.search(r'/\* (.*) \*/', sql).group(1)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'group'

[0m13:54:55.481469 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '94ccca5d-93a7-4bd7-8504-8ce0dd2538c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000268A50D7560>]}
[0m13:54:55.483480 [error] [Thread-6 (]: 1 of 2 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.28s]
[0m13:54:55.485491 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m13:54:55.488962 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m13:54:55.490984 [info ] [Thread-6 (]: 2 of 2 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m13:54:55.493005 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m13:54:55.496173 [debug] [MainThread]: On master: ROLLBACK
[0m13:54:55.497967 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:54:55.499480 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:54:55.500485 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:54:55.501490 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:54:55.503901 [info ] [MainThread]: 
[0m13:54:55.506650 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.46 seconds (0.46s).
[0m13:54:55.508666 [debug] [MainThread]: Command end result
[0m13:54:55.520775 [info ] [MainThread]: 
[0m13:54:55.521846 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:54:55.524891 [info ] [MainThread]: 
[0m13:54:55.527951 [error] [MainThread]:   'NoneType' object has no attribute 'group'
[0m13:54:55.529170 [info ] [MainThread]: 
[0m13:54:55.531180 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m13:54:55.535355 [debug] [MainThread]: Command `cli run` failed at 13:54:55.534335 after 1.68 seconds
[0m13:54:55.536361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000268A4FBB6E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000268A5151490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000268A51517F0>]}
[0m13:54:55.538068 [debug] [MainThread]: Flushing usage events
[0m13:55:31.274626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237F48FC3B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237F1BDC0B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237F4283C20>]}


============================== 13:55:31.276633 | b3b39119-bcc9-4133-8c68-9947d63d2853 ==============================
[0m13:55:31.276633 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:55:31.278815 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:55:31.595013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b3b39119-bcc9-4133-8c68-9947d63d2853', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237F4A06D80>]}
[0m13:55:31.756863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b3b39119-bcc9-4133-8c68-9947d63d2853', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237F4B0F860>]}
[0m13:55:31.758877 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:55:31.785820 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:55:31.869980 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:55:31.870984 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:55:31.888088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b3b39119-bcc9-4133-8c68-9947d63d2853', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237F4B0EC60>]}
[0m13:55:31.902763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b3b39119-bcc9-4133-8c68-9947d63d2853', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237F4B7D4C0>]}
[0m13:55:31.904290 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:55:31.905811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b3b39119-bcc9-4133-8c68-9947d63d2853', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237F09200B0>]}
[0m13:55:31.910886 [info ] [MainThread]: 
[0m13:55:31.913407 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:55:31.918458 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:55:31.966184 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:55:32.030614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b3b39119-bcc9-4133-8c68-9947d63d2853', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237F44CB6E0>]}
[0m13:55:32.032806 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:55:32.033816 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:55:32.035838 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:55:32.037850 [info ] [MainThread]: 
[0m13:55:32.043901 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m13:55:32.045682 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:55:32.048602 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:55:32.049612 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:55:32.083770 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:55:32.087844 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:55:32.050619 => 13:55:32.086829
[0m13:55:32.088852 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:56:51.131701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D13CD2D940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D13CD2CFB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D13CD2C680>]}


============================== 13:56:51.133714 | 73da62d2-f8bd-4547-8674-f8bde1267ac8 ==============================
[0m13:56:51.133714 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:56:51.135727 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:56:51.457488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '73da62d2-f8bd-4547-8674-f8bde1267ac8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D13CE771D0>]}
[0m13:56:51.620802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '73da62d2-f8bd-4547-8674-f8bde1267ac8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D13CCF2720>]}
[0m13:56:51.623896 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:56:51.649843 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:56:51.741950 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:56:51.742955 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:56:51.761089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '73da62d2-f8bd-4547-8674-f8bde1267ac8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D13CE37890>]}
[0m13:56:51.776897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '73da62d2-f8bd-4547-8674-f8bde1267ac8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D13CFA4290>]}
[0m13:56:51.777903 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:56:51.780172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '73da62d2-f8bd-4547-8674-f8bde1267ac8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D13CCF16D0>]}
[0m13:56:51.784836 [info ] [MainThread]: 
[0m13:56:51.787856 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:56:51.792893 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:56:51.842221 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:56:51.910045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '73da62d2-f8bd-4547-8674-f8bde1267ac8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D13CE9EFF0>]}
[0m13:56:51.912126 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:56:51.915439 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:56:51.918963 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:56:51.920987 [info ] [MainThread]: 
[0m13:56:51.928061 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m13:56:51.930090 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:56:51.932109 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:56:51.934125 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:56:51.968603 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:56:51.971825 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:56:51.935133 => 13:56:51.970786
[0m13:56:51.973907 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:56:55.575973 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 13:56:51.975075 => 13:56:55.574964
[0m13:56:55.580778 [error] [Thread-6 (]: [31mUnhandled error while executing [0m
'NoneType' object has no attribute 'group'
[0m13:56:55.588361 [debug] [Thread-6 (]: Traceback (most recent call last):
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 372, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 323, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 419, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\run.py", line 291, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 126, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricsparknb\impl.py", line 449, in execute
    livysession.LivySession.execute(sql=sql)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricsparknb\livysession.py", line 245, in execute
    json_string = re.search(r'/\* (.*) \*/', sql).group(1)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'group'

[0m13:56:55.590379 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '73da62d2-f8bd-4547-8674-f8bde1267ac8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D13CCF18B0>]}
[0m13:56:55.594524 [error] [Thread-6 (]: 1 of 2 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 3.66s]
[0m13:56:55.597656 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m13:56:55.602804 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m13:56:55.605838 [info ] [Thread-6 (]: 2 of 2 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m13:56:55.607859 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m13:56:55.611689 [debug] [MainThread]: On master: ROLLBACK
[0m13:56:55.613758 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:56:55.615085 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:56:55.617116 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:56:55.618130 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:56:55.621163 [info ] [MainThread]: 
[0m13:56:55.623174 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 3.83 seconds (3.83s).
[0m13:56:55.626214 [debug] [MainThread]: Command end result
[0m13:56:55.644578 [info ] [MainThread]: 
[0m13:56:55.646619 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:56:55.648640 [info ] [MainThread]: 
[0m13:56:55.649902 [error] [MainThread]:   'NoneType' object has no attribute 'group'
[0m13:56:55.651920 [info ] [MainThread]: 
[0m13:56:55.652939 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m13:56:55.657990 [debug] [MainThread]: Command `cli run` failed at 13:56:55.657990 after 4.61 seconds
[0m13:56:55.663201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D13A1026F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D13CD5CEF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D13D1DB4D0>]}
[0m13:56:55.665217 [debug] [MainThread]: Flushing usage events
[0m13:57:25.506478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C08ED9610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C08ED9220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C08ED91F0>]}


============================== 13:57:25.508489 | 49285f7b-10fb-46e0-859a-3fd4d922c7ec ==============================
[0m13:57:25.508489 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:57:25.510500 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m13:57:25.821310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '49285f7b-10fb-46e0-859a-3fd4d922c7ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C08ED9B80>]}
[0m13:57:25.983315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '49285f7b-10fb-46e0-859a-3fd4d922c7ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C08EA5700>]}
[0m13:57:25.990894 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:57:26.015676 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:57:26.105260 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:57:26.106267 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:57:26.124481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '49285f7b-10fb-46e0-859a-3fd4d922c7ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C090221B0>]}
[0m13:57:26.140587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '49285f7b-10fb-46e0-859a-3fd4d922c7ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C091DF050>]}
[0m13:57:26.141593 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:57:26.143603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '49285f7b-10fb-46e0-859a-3fd4d922c7ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C08EDA570>]}
[0m13:57:26.148634 [info ] [MainThread]: 
[0m13:57:26.151682 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:57:26.157329 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:57:26.204048 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:57:26.266796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '49285f7b-10fb-46e0-859a-3fd4d922c7ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C091DD1F0>]}
[0m13:57:26.269396 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:57:26.270871 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:57:26.273072 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:57:26.274077 [info ] [MainThread]: 
[0m13:57:26.282130 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m13:57:26.284140 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:57:26.286300 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:57:26.288323 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:57:26.322236 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:57:26.325721 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:57:26.289329 => 13:57:26.324713
[0m13:57:26.326728 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:57:26.495953 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 13:57:26.328740 => 13:57:26.494947
[0m13:57:26.497977 [error] [Thread-6 (]: [31mUnhandled error while executing [0m
'NoneType' object has no attribute 'group'
[0m13:57:26.504122 [debug] [Thread-6 (]: Traceback (most recent call last):
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 372, in safe_run
    result = self.compile_and_execute(manifest, ctx)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 323, in compile_and_execute
    result = self.run(ctx.node, manifest)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\base.py", line 419, in run
    return self.execute(compiled_node, manifest)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\task\run.py", line 291, in execute
    result = MacroGenerator(
             ^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 126, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 330, in __call__
    return self.call_macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\clients\jinja.py", line 257, in call_macro
    return macro(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 763, in __call__
    return self._invoke(arguments, autoescape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 777, in _invoke
    rv = self._func(*arguments)
         ^^^^^^^^^^^^^^^^^^^^^^
  File "<template>", line 52, in macro
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\sandbox.py", line 393, in call
    return __context.call(__obj, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\jinja2\runtime.py", line 298, in call
    return __obj(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricsparknb\impl.py", line 449, in execute
    livysession.LivySession.execute(sql=sql)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\dbt\adapters\fabricsparknb\livysession.py", line 245, in execute
    json_string = re.search(r'/\* (.*) \*/', sql).group(1)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'group'

[0m13:57:26.506131 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '49285f7b-10fb-46e0-859a-3fd4d922c7ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C0912C830>]}
[0m13:57:26.508341 [error] [Thread-6 (]: 1 of 2 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.22s]
[0m13:57:26.510621 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m13:57:26.515741 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m13:57:26.516747 [info ] [Thread-6 (]: 2 of 2 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m13:57:26.518922 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m13:57:26.521971 [debug] [MainThread]: On master: ROLLBACK
[0m13:57:26.522987 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:57:26.525495 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:57:26.527548 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:57:26.528556 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:57:26.531603 [info ] [MainThread]: 
[0m13:57:26.533175 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.38 seconds (0.38s).
[0m13:57:26.534686 [debug] [MainThread]: Command end result
[0m13:57:26.549203 [info ] [MainThread]: 
[0m13:57:26.551495 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:57:26.553506 [info ] [MainThread]: 
[0m13:57:26.554514 [error] [MainThread]:   'NoneType' object has no attribute 'group'
[0m13:57:26.556538 [info ] [MainThread]: 
[0m13:57:26.557546 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m13:57:26.561673 [debug] [MainThread]: Command `cli run` failed at 13:57:26.561673 after 1.14 seconds
[0m13:57:26.564713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C08A5ECF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C0912E930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C08ED8AD0>]}
[0m13:57:26.565767 [debug] [MainThread]: Flushing usage events
[0m13:58:21.808192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001347A52E8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001347A52D070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001347A52CAD0>]}


============================== 13:58:21.810202 | 37381367-0e78-484c-a8c7-2e391399461c ==============================
[0m13:58:21.810202 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:58:21.812214 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m13:58:22.132252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '37381367-0e78-484c-a8c7-2e391399461c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001347A4BA8D0>]}
[0m13:58:22.298369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '37381367-0e78-484c-a8c7-2e391399461c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001347A743290>]}
[0m13:58:22.300633 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:58:22.327457 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:58:22.412017 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:58:22.413022 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:58:22.431202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '37381367-0e78-484c-a8c7-2e391399461c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001347A039B20>]}
[0m13:58:22.444277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '37381367-0e78-484c-a8c7-2e391399461c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001347A7F37D0>]}
[0m13:58:22.446293 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:58:22.447298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '37381367-0e78-484c-a8c7-2e391399461c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001347A7F29F0>]}
[0m13:58:22.453372 [info ] [MainThread]: 
[0m13:58:22.456826 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:58:22.462249 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:58:22.508507 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:58:22.572032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '37381367-0e78-484c-a8c7-2e391399461c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001347A7F1970>]}
[0m13:58:22.574053 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:58:22.576072 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:58:22.578106 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:58:22.580123 [info ] [MainThread]: 
[0m13:58:22.588297 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m13:58:22.591329 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:58:22.593397 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:58:22.595410 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:58:22.632099 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:58:22.634604 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:58:22.596416 => 13:58:22.633588
[0m13:58:22.636920 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:59:02.977287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E60B6DCD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E60B6C3B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E6090CB00>]}


============================== 13:59:02.980303 | 6112b233-abbb-4da8-bf53-d8497127b518 ==============================
[0m13:59:02.980303 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:59:02.981309 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:59:03.295926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6112b233-abbb-4da8-bf53-d8497127b518', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E609A0C80>]}
[0m13:59:03.455333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6112b233-abbb-4da8-bf53-d8497127b518', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E60D7B740>]}
[0m13:59:03.457343 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:59:03.484516 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:59:03.568751 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:59:03.569762 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:59:03.587415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6112b233-abbb-4da8-bf53-d8497127b518', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E60D1D670>]}
[0m13:59:03.600955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6112b233-abbb-4da8-bf53-d8497127b518', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E60E2B950>]}
[0m13:59:03.602552 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:59:03.604163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6112b233-abbb-4da8-bf53-d8497127b518', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E60E2AD50>]}
[0m13:59:03.609241 [info ] [MainThread]: 
[0m13:59:03.612395 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:59:03.618495 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:59:03.665599 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:59:03.730010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6112b233-abbb-4da8-bf53-d8497127b518', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019E60E29A30>]}
[0m13:59:03.732197 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:59:03.733335 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:59:03.735350 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:59:03.737368 [info ] [MainThread]: 
[0m13:59:03.747902 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m13:59:03.750917 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:59:03.753213 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:59:03.755241 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:59:03.790120 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:59:03.792130 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:59:03.756247 => 13:59:03.791125
[0m13:59:03.794195 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:06:05.943893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F601E10A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F601A62E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F601E126F0>]}


============================== 14:06:05.945904 | 64432dac-aac9-4ab0-a4bb-b5516567bf46 ==============================
[0m14:06:05.945904 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:06:05.947920 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:06:06.260770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '64432dac-aac9-4ab0-a4bb-b5516567bf46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F601B50E00>]}
[0m14:06:06.421223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '64432dac-aac9-4ab0-a4bb-b5516567bf46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F60206B2F0>]}
[0m14:06:06.424376 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:06:06.449668 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:06:06.534505 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:06:06.535787 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:06:06.552962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '64432dac-aac9-4ab0-a4bb-b5516567bf46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6020C0590>]}
[0m14:06:06.573167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '64432dac-aac9-4ab0-a4bb-b5516567bf46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F602123110>]}
[0m14:06:06.574178 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:06:06.576193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '64432dac-aac9-4ab0-a4bb-b5516567bf46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F601B50E00>]}
[0m14:06:06.581228 [info ] [MainThread]: 
[0m14:06:06.584568 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:06:06.589878 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:06:14.629113 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:06:14.692740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '64432dac-aac9-4ab0-a4bb-b5516567bf46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F601EB4A10>]}
[0m14:06:14.694823 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:06:14.695832 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:06:14.699906 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:06:14.701927 [info ] [MainThread]: 
[0m14:06:14.711669 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m14:06:14.714927 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:06:14.716951 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:06:14.718963 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:06:14.755428 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:06:14.758473 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:06:14.720045 => 14:06:14.757458
[0m14:06:14.759487 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:06:14.891422 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:06:14.892430 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:06:14.894443 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:06:14.895449 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m14:06:51.919737 [debug] [Thread-7 (]: SQL status: OK in 37.029998779296875 seconds
[0m14:06:53.251666 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:06:53.253681 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:06:53.255053 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:07:02.974884 [debug] [Thread-7 (]: SQL status: OK in 9.720000267028809 seconds
[0m14:07:03.014260 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:06:14.760501 => 14:07:03.013257
[0m14:07:03.017291 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '64432dac-aac9-4ab0-a4bb-b5516567bf46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F602502BD0>]}
[0m14:07:03.020326 [info ] [Thread-7 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 48.30s]
[0m14:07:03.022351 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:07:03.028485 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m14:07:03.030737 [info ] [Thread-7 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:07:03.034829 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m14:07:03.037911 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:07:03.053350 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:07:03.057446 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:07:03.039958 => 14:07:03.056422
[0m14:07:03.059016 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:07:03.133206 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:07:03.138277 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:07:03.140311 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:07:03.149436 [debug] [Thread-7 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:07:03.158578 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:07:03.059915 => 14:07:03.158073
[0m14:07:03.161103 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '64432dac-aac9-4ab0-a4bb-b5516567bf46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F602367F20>]}
[0m14:07:03.163655 [info ] [Thread-7 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.13s]
[0m14:07:03.166655 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:07:03.175794 [debug] [MainThread]: On master: ROLLBACK
[0m14:07:03.178822 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:07:03.180959 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:07:03.183118 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:07:03.185137 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:07:03.186146 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:07:03.189192 [info ] [MainThread]: 
[0m14:07:03.192219 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 56.61 seconds (56.61s).
[0m14:07:03.194233 [debug] [MainThread]: Command end result
[0m14:07:03.214706 [info ] [MainThread]: 
[0m14:07:03.218793 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:07:03.220826 [info ] [MainThread]: 
[0m14:07:03.222843 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:07:03.227904 [debug] [MainThread]: Command `cli run` succeeded at 14:07:03.227904 after 57.36 seconds
[0m14:07:03.230869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F601C3F9E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6035E8DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6035E9A60>]}
[0m14:07:03.233411 [debug] [MainThread]: Flushing usage events
[0m14:10:27.033901 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C119646420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1197818E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C119782600>]}


============================== 14:10:27.033901 | 15d48fe0-1fe7-4f99-badc-594a54eccf36 ==============================
[0m14:10:27.033901 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:10:27.034906 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:10:27.128079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '15d48fe0-1fe7-4f99-badc-594a54eccf36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1198538F0>]}
[0m14:10:27.175325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '15d48fe0-1fe7-4f99-badc-594a54eccf36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1194DD8B0>]}
[0m14:10:27.177335 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:10:27.184391 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:10:27.220627 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:10:27.220627 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:10:27.223643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '15d48fe0-1fe7-4f99-badc-594a54eccf36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1197823C0>]}
[0m14:10:27.229717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '15d48fe0-1fe7-4f99-badc-594a54eccf36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C11A988860>]}
[0m14:10:27.229717 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:10:27.229717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15d48fe0-1fe7-4f99-badc-594a54eccf36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C119687020>]}
[0m14:10:27.230733 [info ] [MainThread]: 
[0m14:10:27.231747 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:10:27.232758 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:10:27.236804 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:10:27.248932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '15d48fe0-1fe7-4f99-badc-594a54eccf36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C11A84FE90>]}
[0m14:10:27.248932 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:10:27.249943 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:10:27.249943 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:10:27.249943 [info ] [MainThread]: 
[0m14:10:27.251973 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m14:10:27.251973 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:10:27.252987 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:10:27.252987 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:10:27.257036 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:10:27.258051 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:10:27.252987 => 14:10:27.258051
[0m14:10:27.258051 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:10:27.287302 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:10:27.288308 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:10:27.288308 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:10:27.288308 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:10:27.314602 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m14:10:27.337930 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:10:27.338936 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:10:27.338936 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:10:27.340952 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:10:27.350016 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:10:27.258051 => 14:10:27.349010
[0m14:10:27.351023 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15d48fe0-1fe7-4f99-badc-594a54eccf36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C11A93D9A0>]}
[0m14:10:27.352059 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.10s]
[0m14:10:27.352059 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:10:27.353071 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m14:10:27.353071 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:10:27.354086 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m14:10:27.354086 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:10:27.356107 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:10:27.357114 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:10:27.355099 => 14:10:27.357114
[0m14:10:27.357114 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:10:27.370313 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:10:27.372325 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:10:27.372325 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:10:27.374353 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:10:27.375362 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:10:27.357114 => 14:10:27.375362
[0m14:10:27.376369 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '15d48fe0-1fe7-4f99-badc-594a54eccf36', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C11AAF2ED0>]}
[0m14:10:27.376369 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m14:10:27.377381 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:10:27.378394 [debug] [MainThread]: On master: ROLLBACK
[0m14:10:27.378394 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:10:27.378394 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:10:27.378394 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:10:27.379403 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:10:27.379403 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:10:27.379403 [info ] [MainThread]: 
[0m14:10:27.380413 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m14:10:27.380413 [debug] [MainThread]: Command end result
[0m14:10:27.385482 [info ] [MainThread]: 
[0m14:10:27.385482 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:10:27.386491 [info ] [MainThread]: 
[0m14:10:27.386491 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:10:27.387500 [debug] [MainThread]: Command `cli run` succeeded at 14:10:27.387500 after 0.38 seconds
[0m14:10:27.387500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C119780EC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C119780800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1197818E0>]}
[0m14:10:27.387500 [debug] [MainThread]: Flushing usage events
[0m14:11:16.010204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271EA67E030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271EADBD820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271EADBD9A0>]}


============================== 14:11:16.011210 | e622eaa0-e709-40fd-b2d6-1e339e2435a8 ==============================
[0m14:11:16.011210 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:11:16.012242 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:11:16.111897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e622eaa0-e709-40fd-b2d6-1e339e2435a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271EAC8D820>]}
[0m14:11:16.159211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e622eaa0-e709-40fd-b2d6-1e339e2435a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271EBF9DCA0>]}
[0m14:11:16.160216 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:11:16.167262 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:11:16.203515 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:11:16.203515 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:11:16.206537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e622eaa0-e709-40fd-b2d6-1e339e2435a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271EBEFC4D0>]}
[0m14:11:16.212589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e622eaa0-e709-40fd-b2d6-1e339e2435a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271EC01BE60>]}
[0m14:11:16.212589 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:11:16.213596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e622eaa0-e709-40fd-b2d6-1e339e2435a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271E7B150D0>]}
[0m14:11:16.214603 [info ] [MainThread]: 
[0m14:11:16.215611 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:11:16.216618 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:11:16.220711 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:11:16.231775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e622eaa0-e709-40fd-b2d6-1e339e2435a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271EC0198E0>]}
[0m14:11:16.232802 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:11:16.232802 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:11:16.232802 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:11:16.233811 [info ] [MainThread]: 
[0m14:11:16.234816 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m14:11:16.235825 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:11:16.235825 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:11:16.235825 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:11:16.239858 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:11:16.240863 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:11:16.236839 => 14:11:16.240863
[0m14:11:16.240863 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:11:16.271089 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:11:16.271089 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:11:16.271089 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:11:16.272094 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:11:16.301530 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m14:11:16.326898 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:11:16.327906 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:11:16.327906 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:11:16.329935 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:11:16.337510 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:11:16.241869 => 14:11:16.337510
[0m14:11:16.338523 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e622eaa0-e709-40fd-b2d6-1e339e2435a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271EC150AD0>]}
[0m14:11:16.339535 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.10s]
[0m14:11:16.340043 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:11:16.341224 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m14:11:16.341736 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:11:16.342756 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m14:11:16.343269 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:11:16.345291 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:11:16.346298 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:11:16.343269 => 14:11:16.346298
[0m14:11:16.347309 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:11:16.358589 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:11:16.359591 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:11:16.359591 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:11:16.360599 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:11:16.362630 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:11:16.347309 => 14:11:16.361614
[0m14:11:16.362630 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e622eaa0-e709-40fd-b2d6-1e339e2435a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271EC16D820>]}
[0m14:11:16.363644 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m14:11:16.363644 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:11:16.364656 [debug] [MainThread]: On master: ROLLBACK
[0m14:11:16.364656 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:11:16.364656 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:11:16.365667 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:11:16.365667 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:11:16.365667 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:11:16.366674 [info ] [MainThread]: 
[0m14:11:16.366674 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m14:11:16.367686 [debug] [MainThread]: Command end result
[0m14:11:16.371749 [info ] [MainThread]: 
[0m14:11:16.371749 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:11:16.372756 [info ] [MainThread]: 
[0m14:11:16.372756 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:11:16.373764 [debug] [MainThread]: Command `cli run` succeeded at 14:11:16.373764 after 0.38 seconds
[0m14:11:16.373764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271EADBD820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271EADBD9A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271EADBDA30>]}
[0m14:11:16.373764 [debug] [MainThread]: Flushing usage events
[0m14:11:51.064385 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B399FD5670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B399FD58B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B399FD5490>]}


============================== 14:11:51.064385 | 2ae92c94-b9a6-44df-9348-746da591e431 ==============================
[0m14:11:51.064385 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:11:51.065391 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:11:51.160711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2ae92c94-b9a6-44df-9348-746da591e431', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B399ED6AE0>]}
[0m14:11:51.209584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2ae92c94-b9a6-44df-9348-746da591e431', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B399D37800>]}
[0m14:11:51.210595 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:11:51.216667 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:11:51.251926 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:11:51.251926 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:11:51.255947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2ae92c94-b9a6-44df-9348-746da591e431', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B39B181CA0>]}
[0m14:11:51.260993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2ae92c94-b9a6-44df-9348-746da591e431', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B39B2186B0>]}
[0m14:11:51.260993 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:11:51.262004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2ae92c94-b9a6-44df-9348-746da591e431', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B39B0052E0>]}
[0m14:11:51.263013 [info ] [MainThread]: 
[0m14:11:51.263013 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:11:51.265046 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:11:51.268080 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:11:51.280182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2ae92c94-b9a6-44df-9348-746da591e431', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B39B21A210>]}
[0m14:11:51.281191 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:11:51.281191 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:11:51.281191 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:11:51.282200 [info ] [MainThread]: 
[0m14:11:51.283209 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m14:11:51.284221 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:11:51.284221 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:11:51.285232 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:11:51.288252 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:11:51.289257 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:11:51.285232 => 14:11:51.289257
[0m14:11:51.289257 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:11:51.318780 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:11:51.319785 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:11:51.319785 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:11:51.320793 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:11:51.346727 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m14:11:51.375718 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:11:51.378758 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:11:51.379772 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:11:51.397076 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m14:11:51.405674 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:11:51.290263 => 14:11:51.405674
[0m14:11:51.406179 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2ae92c94-b9a6-44df-9348-746da591e431', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B39B3758B0>]}
[0m14:11:51.406684 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.12s]
[0m14:11:51.407189 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:11:51.407694 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m14:11:51.408201 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:11:51.408711 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m14:11:51.409223 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:11:51.411267 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:11:51.412274 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:11:51.409727 => 14:11:51.412274
[0m14:11:51.412274 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:11:51.424388 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:11:51.425396 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:11:51.425396 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:11:51.427560 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:11:51.428940 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:11:51.412274 => 14:11:51.428940
[0m14:11:51.429544 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2ae92c94-b9a6-44df-9348-746da591e431', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B39B374E00>]}
[0m14:11:51.429544 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m14:11:51.430707 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:11:51.431715 [debug] [MainThread]: On master: ROLLBACK
[0m14:11:51.431715 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:11:51.431715 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:11:51.432721 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:11:51.432721 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:11:51.432721 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:11:51.433727 [info ] [MainThread]: 
[0m14:11:51.433727 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m14:11:51.434740 [debug] [MainThread]: Command end result
[0m14:11:51.439775 [info ] [MainThread]: 
[0m14:11:51.440782 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:11:51.440782 [info ] [MainThread]: 
[0m14:11:51.441792 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:11:51.442821 [debug] [MainThread]: Command `cli run` succeeded at 14:11:51.442821 after 0.40 seconds
[0m14:11:51.442821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B39B07D790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B399F666F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B399F64EC0>]}
[0m14:11:51.443829 [debug] [MainThread]: Flushing usage events
[0m14:17:40.965690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2DDEDCA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2DDED3D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2DDECE60>]}


============================== 14:17:40.967705 | 522e452f-19cc-4ffb-8bcb-df5c8997fd27 ==============================
[0m14:17:40.967705 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:17:40.969717 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:17:41.278407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '522e452f-19cc-4ffb-8bcb-df5c8997fd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2DEB7C50>]}
[0m14:17:41.441367 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '522e452f-19cc-4ffb-8bcb-df5c8997fd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2DFD63C0>]}
[0m14:17:41.444385 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:17:41.484066 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:17:42.026124 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:17:42.027654 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:17:42.045502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '522e452f-19cc-4ffb-8bcb-df5c8997fd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2DF5F2C0>]}
[0m14:17:42.064889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '522e452f-19cc-4ffb-8bcb-df5c8997fd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2E0AEF90>]}
[0m14:17:42.065895 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:17:42.067905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '522e452f-19cc-4ffb-8bcb-df5c8997fd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2E0AE630>]}
[0m14:17:42.073414 [info ] [MainThread]: 
[0m14:17:42.075893 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:17:42.080384 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:17:49.471625 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:17:49.546814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '522e452f-19cc-4ffb-8bcb-df5c8997fd27', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D2DFF8560>]}
[0m14:17:49.548826 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:17:49.550863 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:17:49.553885 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:17:49.555896 [info ] [MainThread]: 
[0m14:17:49.564502 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m14:17:49.566010 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:17:49.569030 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:17:49.570037 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:17:49.612155 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:17:49.616192 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:17:49.571061 => 14:17:49.615185
[0m14:17:49.618207 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:17:49.762262 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:17:49.764274 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:17:49.765283 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:17:49.766289 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m14:20:25.340352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000178068C7380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001780732DEB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017806D12F60>]}


============================== 14:20:25.341868 | df216994-d109-47b8-af8e-2456ae7a5811 ==============================
[0m14:20:25.341868 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:20:25.343877 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:20:25.710462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'df216994-d109-47b8-af8e-2456ae7a5811', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017806F8E570>]}
[0m14:20:25.870779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'df216994-d109-47b8-af8e-2456ae7a5811', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000178076262D0>]}
[0m14:20:25.872788 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:20:25.901383 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:20:25.987358 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:20:25.988363 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:20:26.005476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'df216994-d109-47b8-af8e-2456ae7a5811', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017807545610>]}
[0m14:20:26.019563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'df216994-d109-47b8-af8e-2456ae7a5811', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000178076FF080>]}
[0m14:20:26.021574 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:20:26.022579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df216994-d109-47b8-af8e-2456ae7a5811', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017806F59AC0>]}
[0m14:20:26.027606 [info ] [MainThread]: 
[0m14:20:26.031223 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:20:26.036603 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:20:26.085285 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:20:26.150191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df216994-d109-47b8-af8e-2456ae7a5811', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000178076FE420>]}
[0m14:20:26.151198 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:20:26.153216 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:20:26.155227 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:20:26.156884 [info ] [MainThread]: 
[0m14:20:26.164925 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m14:20:26.166938 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:20:26.169360 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:20:26.171372 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:20:26.208010 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:20:26.210503 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:20:26.172377 => 14:20:26.209497
[0m14:20:26.213541 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:20:26.338770 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:20:26.339774 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:20:26.341792 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:20:26.342801 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:27:17.068545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021569EED880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021569EED7C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021569EEE570>]}


============================== 14:27:17.070555 | 2738df75-dfed-47d5-9866-34fdce88ecef ==============================
[0m14:27:17.070555 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:27:17.072567 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m14:27:17.383226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2738df75-dfed-47d5-9866-34fdce88ecef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021569F46BD0>]}
[0m14:27:17.542112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2738df75-dfed-47d5-9866-34fdce88ecef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002156A0D4500>]}
[0m14:27:17.544123 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:27:17.571752 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:27:17.655374 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:27:17.657385 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:27:17.674537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2738df75-dfed-47d5-9866-34fdce88ecef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002156A060320>]}
[0m14:27:17.687621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2738df75-dfed-47d5-9866-34fdce88ecef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002156A162720>]}
[0m14:27:17.689632 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:27:17.690638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2738df75-dfed-47d5-9866-34fdce88ecef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002156A09C8C0>]}
[0m14:27:17.695663 [info ] [MainThread]: 
[0m14:27:17.698979 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:27:17.704031 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:27:17.755379 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:27:17.823480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2738df75-dfed-47d5-9866-34fdce88ecef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002156A163740>]}
[0m14:27:17.825517 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:27:17.826601 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:27:17.829436 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:27:17.831112 [info ] [MainThread]: 
[0m14:27:17.837477 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m14:27:17.839489 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:27:17.842577 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:27:17.843582 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:27:17.879860 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:27:17.882116 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:27:17.844588 => 14:27:17.881110
[0m14:27:17.884128 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:27:18.010261 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:27:18.012271 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:27:18.014055 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:27:18.015774 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:35:05.320086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BECB26690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BECB26780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BECB253D0>]}


============================== 14:35:05.323435 | 599994df-57db-4896-9b30-fdf7076155db ==============================
[0m14:35:05.323435 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:35:05.325475 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:35:05.659272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '599994df-57db-4896-9b30-fdf7076155db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BECB247A0>]}
[0m14:35:05.823157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '599994df-57db-4896-9b30-fdf7076155db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BECD47B60>]}
[0m14:35:05.826173 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:35:05.858788 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:35:06.495680 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:35:06.496990 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:35:06.515058 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '599994df-57db-4896-9b30-fdf7076155db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BECA787D0>]}
[0m14:35:06.529266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '599994df-57db-4896-9b30-fdf7076155db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BECDD03B0>]}
[0m14:35:06.530817 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:35:06.531824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '599994df-57db-4896-9b30-fdf7076155db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BECB841A0>]}
[0m14:35:06.537752 [info ] [MainThread]: 
[0m14:35:06.540770 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:35:06.545202 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:35:06.598417 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:35:06.672770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '599994df-57db-4896-9b30-fdf7076155db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021BECDD27E0>]}
[0m14:35:06.675066 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:35:06.676197 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:35:06.679217 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:35:06.681232 [info ] [MainThread]: 
[0m14:35:06.688810 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m14:35:06.689816 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:35:06.693222 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:35:06.694227 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:35:06.730278 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:35:06.733530 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:35:06.695234 => 14:35:06.732293
[0m14:35:06.734535 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:35:06.865099 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:35:06.867050 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:35:06.868608 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:35:06.869616 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m14:41:16.261494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C8B0BCE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C8B0BC7A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C8B0BCB30>]}


============================== 14:41:16.263504 | 6dda3fa5-97fc-4ae8-b2ca-64cfa81c49a8 ==============================
[0m14:41:16.263504 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:41:16.265519 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:41:16.579961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6dda3fa5-97fc-4ae8-b2ca-64cfa81c49a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C8AB48C80>]}
[0m14:41:16.741499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6dda3fa5-97fc-4ae8-b2ca-64cfa81c49a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C8B2A7890>]}
[0m14:41:16.743516 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:41:16.772782 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:41:16.861657 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:41:16.862662 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:41:16.879773 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6dda3fa5-97fc-4ae8-b2ca-64cfa81c49a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C8B0BF5C0>]}
[0m14:41:16.895148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6dda3fa5-97fc-4ae8-b2ca-64cfa81c49a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C8B37F1D0>]}
[0m14:41:16.897160 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:41:16.898170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6dda3fa5-97fc-4ae8-b2ca-64cfa81c49a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C8A7C00B0>]}
[0m14:41:16.904221 [info ] [MainThread]: 
[0m14:41:16.907250 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:41:16.912311 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:41:16.961803 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:41:17.025306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6dda3fa5-97fc-4ae8-b2ca-64cfa81c49a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C8B117F80>]}
[0m14:41:17.027320 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:41:17.028327 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:41:17.030342 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:41:17.032385 [info ] [MainThread]: 
[0m14:41:17.041879 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m14:41:17.044894 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:41:17.048214 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:41:17.050233 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:41:17.085683 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:41:17.089733 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:41:17.051237 => 14:41:17.088709
[0m14:41:17.092808 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:41:17.221273 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:41:17.223291 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:41:17.224299 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:41:17.226317 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:41:17.227326 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:41:17.229346 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: type object 'Project' has no attribute 'project_root'
[0m14:41:17.230354 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:41:17.093838 => 14:41:17.230354
[0m14:41:17.284498 [debug] [Thread-6 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  type object 'Project' has no attribute 'project_root'
[0m14:41:17.286517 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6dda3fa5-97fc-4ae8-b2ca-64cfa81c49a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C8B512630>]}
[0m14:41:17.288534 [error] [Thread-6 (]: 1 of 2 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.24s]
[0m14:41:17.290552 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:41:17.293572 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m14:41:17.295596 [info ] [Thread-6 (]: 2 of 2 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m14:41:17.297615 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:41:17.303688 [debug] [MainThread]: On master: ROLLBACK
[0m14:41:17.304693 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:41:17.306717 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:41:17.307725 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:41:17.309239 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:41:17.310764 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:41:17.312775 [info ] [MainThread]: 
[0m14:41:17.313779 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.41 seconds (0.41s).
[0m14:41:17.316818 [debug] [MainThread]: Command end result
[0m14:41:17.333057 [info ] [MainThread]: 
[0m14:41:17.336096 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:41:17.338111 [info ] [MainThread]: 
[0m14:41:17.339117 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  type object 'Project' has no attribute 'project_root'
[0m14:41:17.342160 [info ] [MainThread]: 
[0m14:41:17.345214 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m14:41:17.348255 [debug] [MainThread]: Command `cli run` failed at 14:41:17.347245 after 1.17 seconds
[0m14:41:17.350275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C8B04B6E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C8B048560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020C8B0496D0>]}
[0m14:41:17.351283 [debug] [MainThread]: Flushing usage events
[0m14:46:36.616573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CB5B90B30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CB8E4E420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CB8FC5EB0>]}


============================== 14:46:36.619662 | 4359e759-572b-4bd3-9cf1-132340227437 ==============================
[0m14:46:36.619662 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:46:36.621678 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:46:36.941560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4359e759-572b-4bd3-9cf1-132340227437', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CB929A570>]}
[0m14:46:37.107708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4359e759-572b-4bd3-9cf1-132340227437', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CB9522C60>]}
[0m14:46:37.109719 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:46:37.138051 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:46:37.226709 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:46:37.227714 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:46:37.246199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4359e759-572b-4bd3-9cf1-132340227437', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CB957C440>]}
[0m14:46:37.260155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4359e759-572b-4bd3-9cf1-132340227437', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CB95CF3B0>]}
[0m14:46:37.262172 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:46:37.263179 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4359e759-572b-4bd3-9cf1-132340227437', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CB8B4C5C0>]}
[0m14:46:37.269185 [info ] [MainThread]: 
[0m14:46:37.271618 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:46:37.275642 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:46:37.323664 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:46:37.387606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4359e759-572b-4bd3-9cf1-132340227437', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CB95CD430>]}
[0m14:46:37.389615 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:46:37.391345 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:46:37.394145 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:46:37.396158 [info ] [MainThread]: 
[0m14:46:37.405727 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m14:46:37.407746 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:46:37.410768 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:46:37.411777 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:46:37.449379 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:46:37.452687 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:46:37.412784 => 14:46:37.451416
[0m14:46:37.454698 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:46:37.584141 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:46:37.586207 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:46:37.587211 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:46:37.589220 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:47:13.212135 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:47:13.213646 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: type object 'Project' has no attribute 'project_root'
[0m14:47:13.215693 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:46:37.455857 => 14:47:13.215161
[0m14:47:13.226048 [debug] [Thread-6 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  type object 'Project' has no attribute 'project_root'
[0m14:47:13.228062 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4359e759-572b-4bd3-9cf1-132340227437', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CB97B6FC0>]}
[0m14:47:13.230075 [error] [Thread-6 (]: 1 of 2 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 35.82s]
[0m14:47:13.232497 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:47:13.236542 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m14:47:13.237865 [info ] [Thread-6 (]: 2 of 2 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m14:47:13.240338 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:47:13.243601 [debug] [MainThread]: On master: ROLLBACK
[0m14:47:13.245615 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:47:13.246624 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:47:13.248383 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:47:13.248886 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:47:13.250902 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:47:13.252915 [info ] [MainThread]: 
[0m14:47:13.254932 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 35.98 seconds (35.98s).
[0m14:47:13.256947 [debug] [MainThread]: Command end result
[0m14:47:13.269688 [info ] [MainThread]: 
[0m14:47:13.271710 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:47:13.273725 [info ] [MainThread]: 
[0m14:47:13.274733 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  type object 'Project' has no attribute 'project_root'
[0m14:47:13.277467 [info ] [MainThread]: 
[0m14:47:13.278989 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m14:47:13.283041 [debug] [MainThread]: Command `cli run` failed at 14:47:13.282036 after 36.75 seconds
[0m14:47:13.285053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CB90ACE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CB96F7740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020CB96F71A0>]}
[0m14:47:13.286060 [debug] [MainThread]: Flushing usage events
[0m14:49:05.654066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002447967C9E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024479548CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244793ED8B0>]}


============================== 14:49:05.657081 | 09e5c499-6a1b-44e3-a73e-91c22af20db1 ==============================
[0m14:49:05.657081 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:49:05.659092 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:49:05.975564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '09e5c499-6a1b-44e3-a73e-91c22af20db1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002447963D190>]}
[0m14:49:06.137435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '09e5c499-6a1b-44e3-a73e-91c22af20db1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244797C32F0>]}
[0m14:49:06.139452 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:49:06.173489 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:49:06.695497 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:49:06.697513 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:49:06.714620 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '09e5c499-6a1b-44e3-a73e-91c22af20db1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244797EC800>]}
[0m14:49:06.727687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '09e5c499-6a1b-44e3-a73e-91c22af20db1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024479942D80>]}
[0m14:49:06.729698 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:49:06.730704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '09e5c499-6a1b-44e3-a73e-91c22af20db1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000244796ADA00>]}
[0m14:49:06.735728 [info ] [MainThread]: 
[0m14:49:06.739656 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:49:06.743884 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:49:06.795892 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:49:06.864433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '09e5c499-6a1b-44e3-a73e-91c22af20db1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024479940FE0>]}
[0m14:49:06.866444 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:49:06.867474 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:49:06.870495 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:49:06.871498 [info ] [MainThread]: 
[0m14:49:06.877530 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m14:49:06.879541 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:49:06.882557 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:49:06.883570 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:49:06.919504 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:49:06.922539 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:49:06.884576 => 14:49:06.921522
[0m14:49:06.924560 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:49:07.058786 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:49:07.060796 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:49:07.061802 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:49:07.062807 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:54:23.242925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212C1ADE900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212C1ADD130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212C1ADC980>]}


============================== 14:54:23.245019 | c99e8b6f-c710-4c2a-be3d-b1afc45bfefe ==============================
[0m14:54:23.245019 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:54:23.247029 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m14:54:23.560658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c99e8b6f-c710-4c2a-be3d-b1afc45bfefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212C1A99D60>]}
[0m14:54:23.723459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c99e8b6f-c710-4c2a-be3d-b1afc45bfefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212C16CD9A0>]}
[0m14:54:23.725469 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:54:23.755125 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:54:23.843783 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:54:23.845793 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:54:23.862939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c99e8b6f-c710-4c2a-be3d-b1afc45bfefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212C1C880E0>]}
[0m14:54:23.876050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c99e8b6f-c710-4c2a-be3d-b1afc45bfefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212C1D9EE70>]}
[0m14:54:23.878060 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:54:23.879064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c99e8b6f-c710-4c2a-be3d-b1afc45bfefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212C10C80B0>]}
[0m14:54:23.884089 [info ] [MainThread]: 
[0m14:54:23.887875 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:54:23.893443 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:54:23.942287 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:54:24.007182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c99e8b6f-c710-4c2a-be3d-b1afc45bfefe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000212C1D9CF50>]}
[0m14:54:24.008187 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:54:24.009193 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:54:24.011203 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:54:24.013213 [info ] [MainThread]: 
[0m14:54:24.019268 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m14:54:24.021760 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:54:24.023893 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:54:24.025610 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:54:24.064893 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:54:24.067970 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:54:24.026377 => 14:54:24.066960
[0m14:54:24.068978 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:54:24.198217 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:54:24.199222 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:54:24.200227 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:54:24.202238 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m15:14:26.316997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F1E7D9D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F1E7E030>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F1E7D850>]}


============================== 15:14:26.316997 | 16986df4-ada5-4490-8926-60ec98820c25 ==============================
[0m15:14:26.316997 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:14:26.318004 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:14:26.413236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '16986df4-ada5-4490-8926-60ec98820c25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F2F68C80>]}
[0m15:14:26.460557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '16986df4-ada5-4490-8926-60ec98820c25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F1DD71A0>]}
[0m15:14:26.460557 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:14:26.461563 [error] [MainThread]: Encountered an error:
SparkAdapter.__init__() takes 1 positional argument but 2 were given
[0m15:14:26.473655 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 90, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 75, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 168, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 197, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 244, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\cli\requires.py", line 267, in wrapper
    register_adapter(runtime_config)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\factory.py", line 176, in register_adapter
    FACTORY.register_adapter(config)
  File "C:\Users\jramp\source\gitjr\dbt-fabricspark\.env\Lib\site-packages\dbt\adapters\factory.py", line 105, in register_adapter
    adapter: Adapter = adapter_type(config)  # type: ignore
                       ^^^^^^^^^^^^^^^^^^^^
TypeError: SparkAdapter.__init__() takes 1 positional argument but 2 were given

[0m15:14:26.474664 [debug] [MainThread]: Command `cli run` failed at 15:14:26.473655 after 0.19 seconds
[0m15:14:26.474664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F1DD71A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F1CFFA10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C2F306EF90>]}
[0m15:14:26.474664 [debug] [MainThread]: Flushing usage events
[0m15:16:15.174935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020659C5D730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020659C5D970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020659C5D6D0>]}


============================== 15:16:15.174935 | 70a675da-4b2c-464d-9389-8357e2d65e01 ==============================
[0m15:16:15.174935 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:16:15.175957 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:16:15.269546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '70a675da-4b2c-464d-9389-8357e2d65e01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020659BFA810>]}
[0m15:16:15.316856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '70a675da-4b2c-464d-9389-8357e2d65e01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065AE4EAB0>]}
[0m15:16:15.317867 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:16:15.329337 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:16:15.804996 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:16:15.806001 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:16:15.809017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '70a675da-4b2c-464d-9389-8357e2d65e01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020659C5DFA0>]}
[0m15:16:15.814043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '70a675da-4b2c-464d-9389-8357e2d65e01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065AEB8AD0>]}
[0m15:16:15.815049 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:16:15.815049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '70a675da-4b2c-464d-9389-8357e2d65e01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020659BB6AB0>]}
[0m15:16:15.816054 [info ] [MainThread]: 
[0m15:16:15.817059 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:16:15.817059 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:16:15.828136 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:16:15.845230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '70a675da-4b2c-464d-9389-8357e2d65e01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065AEBA5A0>]}
[0m15:16:15.845230 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:16:15.846235 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:16:15.846235 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:16:15.846235 [info ] [MainThread]: 
[0m15:16:15.848246 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:16:15.849251 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:16:15.849251 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:16:15.850257 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:16:15.854331 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:16:15.855336 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:16:15.850257 => 15:16:15.855336
[0m15:16:15.855336 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:16:15.884576 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:16:15.885596 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:16:15.886617 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:16:15.886617 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:16:15.887637 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:16:15.887637 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: name 'dbt_config' is not defined
[0m15:16:15.887637 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:16:15.855336 => 15:16:15.887637
[0m15:16:15.955780 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  name 'dbt_config' is not defined
[0m15:16:15.956790 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '70a675da-4b2c-464d-9389-8357e2d65e01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065AEBA5A0>]}
[0m15:16:15.957295 [error] [Thread-1 (]: 1 of 2 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.11s]
[0m15:16:15.958305 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:16:15.959852 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:16:15.960356 [info ] [Thread-1 (]: 2 of 2 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m15:16:15.960861 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:16:15.962381 [debug] [MainThread]: On master: ROLLBACK
[0m15:16:15.963404 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:16:15.964428 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:16:15.964935 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:16:15.964935 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:16:15.965441 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:16:15.966969 [info ] [MainThread]: 
[0m15:16:15.967989 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m15:16:15.969006 [debug] [MainThread]: Command end result
[0m15:16:15.978246 [info ] [MainThread]: 
[0m15:16:15.978758 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m15:16:15.979265 [info ] [MainThread]: 
[0m15:16:15.979771 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  name 'dbt_config' is not defined
[0m15:16:15.980279 [info ] [MainThread]: 
[0m15:16:15.980783 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m15:16:15.982298 [debug] [MainThread]: Command `cli run` failed at 15:16:15.982298 after 0.83 seconds
[0m15:16:15.982802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065985AE70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020659979C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002065AEBC9E0>]}
[0m15:16:15.983307 [debug] [MainThread]: Flushing usage events
[0m15:16:44.373985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA4D356A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA4D35130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA4D349B0>]}


============================== 15:16:44.377010 | 1b15e7df-cc32-4f0f-b57c-849a23592af2 ==============================
[0m15:16:44.377010 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:16:44.379024 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m15:16:44.695465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1b15e7df-cc32-4f0f-b57c-849a23592af2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA49F9D00>]}
[0m15:16:44.859534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1b15e7df-cc32-4f0f-b57c-849a23592af2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021AA4F3C7A0>]}
[0m15:16:44.861545 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:20:02.575227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002141F8E49B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002141F8E40B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002141F8E64B0>]}


============================== 15:20:02.578579 | f5e86769-3eb9-40e6-83b5-7321a0a5c7e1 ==============================
[0m15:20:02.578579 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:20:02.580596 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:20:02.910321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f5e86769-3eb9-40e6-83b5-7321a0a5c7e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002141F5A0C80>]}
[0m15:20:03.074706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f5e86769-3eb9-40e6-83b5-7321a0a5c7e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002141CB800E0>]}
[0m15:20:03.076719 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:20:05.567059 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:20:06.143128 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:20:06.144134 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:20:06.161491 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f5e86769-3eb9-40e6-83b5-7321a0a5c7e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002141FB296A0>]}
[0m15:20:06.174575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f5e86769-3eb9-40e6-83b5-7321a0a5c7e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002141FB79AC0>]}
[0m15:20:06.176668 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:20:06.177671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f5e86769-3eb9-40e6-83b5-7321a0a5c7e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002141FB78FE0>]}
[0m15:20:06.183706 [info ] [MainThread]: 
[0m15:20:06.186491 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:20:06.191257 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:20:06.244810 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:20:06.314503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f5e86769-3eb9-40e6-83b5-7321a0a5c7e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002141F7D2F00>]}
[0m15:20:06.315830 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:20:06.317596 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:20:06.319611 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:20:06.321627 [info ] [MainThread]: 
[0m15:20:06.327903 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m15:20:06.329416 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:20:06.332178 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:20:06.333364 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:20:06.368711 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:20:06.371763 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:20:06.334373 => 15:20:06.371763
[0m15:20:06.373782 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:20:06.500457 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:20:06.501468 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:20:06.503483 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:20:06.504492 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m15:20:22.372374 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m15:23:54.229687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187A63A7CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187A63A57C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187A63A50A0>]}


============================== 15:23:54.231699 | dfa3adbc-71c5-4d8e-bacc-27065b8aef27 ==============================
[0m15:23:54.231699 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:23:54.233710 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:28:29.652938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A24EC42690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A24EC42BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A24EC42210>]}


============================== 15:28:29.653948 | 2bae061a-1f15-45cd-8b9a-f71f7050d241 ==============================
[0m15:28:29.653948 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:28:29.653948 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:28:29.752763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2bae061a-1f15-45cd-8b9a-f71f7050d241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A24EB886E0>]}
[0m15:28:29.811281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2bae061a-1f15-45cd-8b9a-f71f7050d241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A24FD11DC0>]}
[0m15:28:29.812287 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:28:29.819348 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:28:29.860664 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:28:29.860664 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:28:29.864695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2bae061a-1f15-45cd-8b9a-f71f7050d241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A24ED0B320>]}
[0m15:28:29.870769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2bae061a-1f15-45cd-8b9a-f71f7050d241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A24FE4C0B0>]}
[0m15:28:29.870769 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:28:29.871778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2bae061a-1f15-45cd-8b9a-f71f7050d241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A24EA6CE30>]}
[0m15:28:29.872786 [info ] [MainThread]: 
[0m15:28:29.873794 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:28:29.874801 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:28:29.878839 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:28:29.894982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2bae061a-1f15-45cd-8b9a-f71f7050d241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A24FE4E510>]}
[0m15:28:29.894982 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:28:29.895988 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:28:29.895988 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:28:29.897006 [info ] [MainThread]: 
[0m15:28:29.917162 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:28:29.918171 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:28:29.919180 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:28:29.919180 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:28:29.926237 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:28:29.927242 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:28:29.920192 => 15:28:29.927242
[0m15:28:29.927242 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:28:29.955436 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:28:29.927242 => 15:28:29.955436
[0m15:28:30.035202 [debug] [Thread-1 (]: Compilation Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  SparkAdapter.execute() got an unexpected keyword argument 'fetch'
  
  > in macro materialization_incremental_fabricspark (macros\materializations\models\incremental\incremental.sql)
  > called by model my_first_dbt_model (models\example\my_first_dbt_model.sql)
[0m15:28:30.036207 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2bae061a-1f15-45cd-8b9a-f71f7050d241', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A24FE3F290>]}
[0m15:28:30.037229 [error] [Thread-1 (]: 1 of 2 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.12s]
[0m15:28:30.037229 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:28:30.038235 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:28:30.039242 [info ] [Thread-1 (]: 2 of 2 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m15:28:30.039242 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:28:30.040252 [debug] [MainThread]: On master: ROLLBACK
[0m15:28:30.041259 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:28:30.041259 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:28:30.041259 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:28:30.042266 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:28:30.042266 [info ] [MainThread]: 
[0m15:28:30.043273 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m15:28:30.044281 [debug] [MainThread]: Command end result
[0m15:28:30.053378 [info ] [MainThread]: 
[0m15:28:30.054384 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m15:28:30.055397 [info ] [MainThread]: 
[0m15:28:30.056412 [error] [MainThread]:   Compilation Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  SparkAdapter.execute() got an unexpected keyword argument 'fetch'
  
  > in macro materialization_incremental_fabricspark (macros\materializations\models\incremental\incremental.sql)
  > called by model my_first_dbt_model (models\example\my_first_dbt_model.sql)
[0m15:28:30.057430 [info ] [MainThread]: 
[0m15:28:30.058448 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m15:28:30.060482 [debug] [MainThread]: Command `cli run` failed at 15:28:30.059466 after 0.43 seconds
[0m15:28:30.060482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A24E98FE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A24E1F0BF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A24EC40B00>]}
[0m15:28:30.060482 [debug] [MainThread]: Flushing usage events
[0m15:31:19.898149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B3193CE60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B35D55DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B35D56F30>]}


============================== 15:31:19.898149 | 3fff6860-e0c0-4922-a595-e8dd3a504e72 ==============================
[0m15:31:19.898149 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:31:19.899155 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:31:19.993083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3fff6860-e0c0-4922-a595-e8dd3a504e72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B35AF8E30>]}
[0m15:31:20.041414 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3fff6860-e0c0-4922-a595-e8dd3a504e72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B35B75CA0>]}
[0m15:31:20.042418 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:31:20.049464 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:31:20.086689 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:31:20.086689 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:31:20.089703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3fff6860-e0c0-4922-a595-e8dd3a504e72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B35D571D0>]}
[0m15:31:20.094727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3fff6860-e0c0-4922-a595-e8dd3a504e72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B37094980>]}
[0m15:31:20.095732 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:31:20.095732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3fff6860-e0c0-4922-a595-e8dd3a504e72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B35203530>]}
[0m15:31:20.096746 [info ] [MainThread]: 
[0m15:31:20.097752 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:31:20.098757 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:31:20.102802 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:31:20.114903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3fff6860-e0c0-4922-a595-e8dd3a504e72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B370968D0>]}
[0m15:31:20.115909 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:31:20.115909 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:31:20.116913 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:31:20.117919 [info ] [MainThread]: 
[0m15:31:20.120936 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:31:20.120936 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:31:20.121943 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:31:20.121943 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:31:20.126980 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:31:20.129012 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:31:20.121943 => 15:31:20.128004
[0m15:31:20.129012 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:31:20.164541 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:31:20.165546 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:31:20.166552 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:31:20.166552 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:31:20.167557 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:31:20.167557 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: name 'dbt_config' is not defined
[0m15:31:20.168562 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:31:20.129012 => 15:31:20.167557
[0m15:31:20.174603 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  name 'dbt_config' is not defined
[0m15:31:20.174603 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3fff6860-e0c0-4922-a595-e8dd3a504e72', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B35A50290>]}
[0m15:31:20.175608 [error] [Thread-1 (]: 1 of 2 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.05s]
[0m15:31:20.176616 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:31:20.177622 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:31:20.178628 [info ] [Thread-1 (]: 2 of 2 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m15:31:20.178628 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:31:20.179633 [debug] [MainThread]: On master: ROLLBACK
[0m15:31:20.180639 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:31:20.180639 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:31:20.180639 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:31:20.180639 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:31:20.181645 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:31:20.181645 [info ] [MainThread]: 
[0m15:31:20.181645 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.08 seconds (0.08s).
[0m15:31:20.182650 [debug] [MainThread]: Command end result
[0m15:31:20.191736 [info ] [MainThread]: 
[0m15:31:20.191736 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m15:31:20.191736 [info ] [MainThread]: 
[0m15:31:20.192742 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  name 'dbt_config' is not defined
[0m15:31:20.192742 [info ] [MainThread]: 
[0m15:31:20.193747 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m15:31:20.194753 [debug] [MainThread]: Command `cli run` failed at 15:31:20.194753 after 0.32 seconds
[0m15:31:20.195758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B3193CE60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B35A50290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017B35203530>]}
[0m15:31:20.195758 [debug] [MainThread]: Flushing usage events
[0m15:32:22.558529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0F781FD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0F7828A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0F782960>]}


============================== 15:32:22.559535 | 4b139cab-520f-43b1-8e78-a66cb1eb8bda ==============================
[0m15:32:22.559535 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:32:22.559535 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m15:32:22.655154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4b139cab-520f-43b1-8e78-a66cb1eb8bda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0E373200>]}
[0m15:32:22.703408 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4b139cab-520f-43b1-8e78-a66cb1eb8bda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0E71E180>]}
[0m15:32:22.704417 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:32:22.710451 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:32:22.754099 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:32:22.754099 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:32:22.757114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4b139cab-520f-43b1-8e78-a66cb1eb8bda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0F81D160>]}
[0m15:32:22.763155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4b139cab-520f-43b1-8e78-a66cb1eb8bda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0FA109E0>]}
[0m15:32:22.763155 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:32:22.763155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4b139cab-520f-43b1-8e78-a66cb1eb8bda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0E6F9D30>]}
[0m15:32:22.764161 [info ] [MainThread]: 
[0m15:32:22.765166 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:32:22.766172 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:32:22.770205 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:32:22.783288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4b139cab-520f-43b1-8e78-a66cb1eb8bda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0FA12570>]}
[0m15:32:22.783288 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:32:22.784294 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:32:22.784294 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:32:22.785300 [info ] [MainThread]: 
[0m15:32:22.786306 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:32:22.787313 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:32:22.787313 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:32:22.788321 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:32:22.792343 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:32:22.794355 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:32:22.788321 => 15:32:22.793348
[0m15:32:22.794355 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:32:22.865308 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:32:22.875465 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:32:22.794355 => 15:32:22.875465
[0m15:32:22.876470 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b139cab-520f-43b1-8e78-a66cb1eb8bda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0F921FA0>]}
[0m15:32:22.877478 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32m[0m in 0.09s]
[0m15:32:22.878486 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:32:22.879493 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:32:22.880500 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m15:32:22.880500 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:32:22.881506 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:32:22.883517 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:32:22.884523 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:32:22.881506 => 15:32:22.884523
[0m15:32:22.885529 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:32:22.897634 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:32:22.898639 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:32:22.885529 => 15:32:22.898639
[0m15:32:22.899644 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4b139cab-520f-43b1-8e78-a66cb1eb8bda', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0FB1C080>]}
[0m15:32:22.899644 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32m[0m in 0.02s]
[0m15:32:22.900649 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:32:22.900649 [debug] [MainThread]: On master: ROLLBACK
[0m15:32:22.901655 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:32:22.901655 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:32:22.901655 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:32:22.902660 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:32:22.902660 [info ] [MainThread]: 
[0m15:32:22.902660 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m15:32:22.903665 [debug] [MainThread]: Command end result
[0m15:32:22.908710 [info ] [MainThread]: 
[0m15:32:22.908710 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:32:22.908710 [info ] [MainThread]: 
[0m15:32:22.909716 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:32:22.909716 [debug] [MainThread]: Command `cli run` succeeded at 15:32:22.909716 after 0.37 seconds
[0m15:32:22.910722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0DB96600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0F96D790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FF0DEC1580>]}
[0m15:32:22.910722 [debug] [MainThread]: Flushing usage events
[0m15:33:48.523329 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE72616420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE726C1C40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE7235B620>]}


============================== 15:33:48.526756 | 8bff0fcc-ff04-4bed-a928-12c581da1889 ==============================
[0m15:33:48.526756 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:33:48.528824 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:33:50.531892 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8bff0fcc-ff04-4bed-a928-12c581da1889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE71F6A0C0>]}
[0m15:33:50.699399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8bff0fcc-ff04-4bed-a928-12c581da1889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE72D866C0>]}
[0m15:33:50.702416 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:33:51.897346 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:33:51.986118 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:33:51.987122 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:33:52.004626 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8bff0fcc-ff04-4bed-a928-12c581da1889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE72D26420>]}
[0m15:33:52.019463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8bff0fcc-ff04-4bed-a928-12c581da1889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE72E094F0>]}
[0m15:33:52.020978 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:33:52.022292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8bff0fcc-ff04-4bed-a928-12c581da1889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE72C14920>]}
[0m15:33:52.027351 [info ] [MainThread]: 
[0m15:33:52.030375 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:33:52.034401 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:33:52.083544 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:33:52.147940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8bff0fcc-ff04-4bed-a928-12c581da1889', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE726C1C40>]}
[0m15:33:52.148947 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:33:52.150961 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:33:52.152972 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:33:52.154991 [info ] [MainThread]: 
[0m15:33:52.161323 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m15:33:52.163336 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:33:52.165415 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:33:52.167426 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:33:52.204831 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:33:52.207309 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:33:52.168432 => 15:33:52.206843
[0m15:33:52.209317 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:37:00.624564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57FB69D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57EB5C230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57FB68710>]}


============================== 15:37:00.626596 | 79ea6c3a-c92a-40ad-a855-b270092e95de ==============================
[0m15:37:00.626596 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:37:00.629638 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:37:03.026865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '79ea6c3a-c92a-40ad-a855-b270092e95de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57FC7B0B0>]}
[0m15:37:03.188798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '79ea6c3a-c92a-40ad-a855-b270092e95de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57FB27EC0>]}
[0m15:37:03.190809 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:37:08.844807 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:37:08.938832 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:37:08.939840 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:37:08.958381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '79ea6c3a-c92a-40ad-a855-b270092e95de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57FDE3260>]}
[0m15:37:08.974066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '79ea6c3a-c92a-40ad-a855-b270092e95de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57FE4E7B0>]}
[0m15:37:08.975073 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:37:08.977086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '79ea6c3a-c92a-40ad-a855-b270092e95de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57FE4DB50>]}
[0m15:37:08.982596 [info ] [MainThread]: 
[0m15:37:08.985449 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:37:08.991095 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:37:09.038487 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:37:09.106083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '79ea6c3a-c92a-40ad-a855-b270092e95de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57FE4DC70>]}
[0m15:37:09.108620 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:37:09.109630 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:37:09.112675 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:37:09.113681 [info ] [MainThread]: 
[0m15:37:09.121666 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m15:37:09.123685 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:37:09.126981 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:37:09.128491 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:37:09.165544 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:37:09.168710 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:37:09.129496 => 15:37:09.167705
[0m15:37:09.169914 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:37:09.298906 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:37:09.170923 => 15:37:09.298906
[0m15:37:09.314524 [debug] [Thread-7 (]: Compilation Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Object of type RuntimeConfig is not JSON serializable
  
  > in macro materialization_incremental_fabricspark (macros\materializations\models\incremental\incremental.sql)
  > called by model my_first_dbt_model (models\example\my_first_dbt_model.sql)
[0m15:37:09.316534 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '79ea6c3a-c92a-40ad-a855-b270092e95de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57FFD3830>]}
[0m15:37:09.317796 [error] [Thread-7 (]: 1 of 2 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.19s]
[0m15:37:09.319849 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:37:09.322874 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m15:37:09.324886 [info ] [Thread-7 (]: 2 of 2 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m15:37:09.326901 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:37:09.330774 [debug] [MainThread]: On master: ROLLBACK
[0m15:37:09.331785 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:37:11.460903 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:37:11.462919 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:37:11.464937 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:37:11.467007 [info ] [MainThread]: 
[0m15:37:11.469017 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 2.48 seconds (2.48s).
[0m15:37:11.471289 [debug] [MainThread]: Command end result
[0m15:37:11.482813 [info ] [MainThread]: 
[0m15:37:11.484879 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m15:37:11.486115 [info ] [MainThread]: 
[0m15:37:11.488124 [error] [MainThread]:   Compilation Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Object of type RuntimeConfig is not JSON serializable
  
  > in macro materialization_incremental_fabricspark (macros\materializations\models\incremental\incremental.sql)
  > called by model my_first_dbt_model (models\example\my_first_dbt_model.sql)
[0m15:37:11.489508 [info ] [MainThread]: 
[0m15:37:11.490798 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=1 TOTAL=2
[0m15:37:11.493813 [debug] [MainThread]: Command `cli run` failed at 15:37:11.493813 after 10.99 seconds
[0m15:37:11.495836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57F7631A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57FDE3C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F57FDE2EA0>]}
[0m15:37:11.496843 [debug] [MainThread]: Flushing usage events
[0m15:41:22.757760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223F67C6960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223F67C4E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223F67C4D70>]}


============================== 15:41:22.761830 | 700d0581-adf2-453f-be50-5fa63a31a1d8 ==============================
[0m15:41:22.761830 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:41:22.763843 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:41:26.858334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '700d0581-adf2-453f-be50-5fa63a31a1d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223F663C9E0>]}
[0m15:41:27.030022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '700d0581-adf2-453f-be50-5fa63a31a1d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223F6A33170>]}
[0m15:41:27.033041 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:41:27.060280 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:41:27.153113 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:41:27.155123 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:41:27.172707 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '700d0581-adf2-453f-be50-5fa63a31a1d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223F6A5EDE0>]}
[0m15:41:27.187359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '700d0581-adf2-453f-be50-5fa63a31a1d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223F6ACA3F0>]}
[0m15:41:27.188880 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:41:27.190486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '700d0581-adf2-453f-be50-5fa63a31a1d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223F6AC9730>]}
[0m15:41:27.196024 [info ] [MainThread]: 
[0m15:41:27.199040 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:41:27.203078 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:41:27.249861 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:41:27.314533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '700d0581-adf2-453f-be50-5fa63a31a1d8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000223F6540320>]}
[0m15:41:27.315541 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:41:27.317562 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:41:27.319574 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:41:27.320580 [info ] [MainThread]: 
[0m15:41:27.328798 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m15:41:27.330810 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:41:27.332821 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:41:27.334832 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:41:27.370051 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:41:27.372070 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:41:27.335837 => 15:41:27.371065
[0m15:41:27.373884 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:41:59.837821 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:41:59.839836 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:41:59.841858 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/* 'project_root': testproj */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:41:59.844169 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m15:43:16.887629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022ACCF582F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022ACCCDFFE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022ACCCDDC40>]}


============================== 15:43:16.889637 | 741e1738-9f53-430c-ba3e-a6467c7e1891 ==============================
[0m15:43:16.889637 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:43:16.891648 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:43:17.201793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '741e1738-9f53-430c-ba3e-a6467c7e1891', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022ACCF1E5A0>]}
[0m15:43:17.361780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '741e1738-9f53-430c-ba3e-a6467c7e1891', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022ACD2BF320>]}
[0m15:43:17.363792 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:43:17.390124 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:43:17.478919 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:43:17.479925 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:43:17.497533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '741e1738-9f53-430c-ba3e-a6467c7e1891', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022ACD1B9BE0>]}
[0m15:43:17.510595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '741e1738-9f53-430c-ba3e-a6467c7e1891', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022ACD36E2A0>]}
[0m15:43:17.511601 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:43:17.513632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '741e1738-9f53-430c-ba3e-a6467c7e1891', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022ACD294500>]}
[0m15:43:17.518694 [info ] [MainThread]: 
[0m15:43:17.521960 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:43:17.528043 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:43:17.576003 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:43:17.643178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '741e1738-9f53-430c-ba3e-a6467c7e1891', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022ACD177EC0>]}
[0m15:43:17.644187 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:43:17.645194 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:43:17.648175 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:43:17.649957 [info ] [MainThread]: 
[0m15:43:17.656059 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m15:43:17.658078 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:43:17.660527 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:43:17.661533 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:43:17.698102 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:43:17.702298 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:43:17.662537 => 15:43:17.701119
[0m15:43:17.704312 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:44:10.367837 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:44:10.369857 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:44:10.371874 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/* "project_root": "testproj" */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:44:10.373891 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m15:48:06.197725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D3DD105F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D3DD10230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D3B0000B0>]}


============================== 15:48:06.201080 | 46eb0155-71c2-4705-9ea6-17d5c3bc958f ==============================
[0m15:48:06.201080 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:48:06.203095 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:48:06.533359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '46eb0155-71c2-4705-9ea6-17d5c3bc958f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D3D1C3530>]}
[0m15:48:06.698952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '46eb0155-71c2-4705-9ea6-17d5c3bc958f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D3DF4C9B0>]}
[0m15:48:06.701686 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:48:06.727684 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:48:06.812070 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:48:06.814092 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:48:06.832201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '46eb0155-71c2-4705-9ea6-17d5c3bc958f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D3DE71460>]}
[0m15:48:06.845279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '46eb0155-71c2-4705-9ea6-17d5c3bc958f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D3E040E00>]}
[0m15:48:06.846285 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:48:06.848297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '46eb0155-71c2-4705-9ea6-17d5c3bc958f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D3E0403E0>]}
[0m15:48:06.853853 [info ] [MainThread]: 
[0m15:48:06.856500 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:48:06.862431 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:48:06.909981 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:48:06.974871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '46eb0155-71c2-4705-9ea6-17d5c3bc958f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D3E043380>]}
[0m15:48:06.977145 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:48:06.979092 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:48:06.981110 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:48:06.983121 [info ] [MainThread]: 
[0m15:48:06.993179 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m15:48:06.995316 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:48:06.998341 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:48:06.999350 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:48:07.035678 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:48:07.038704 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:48:07.000358 => 15:48:07.037694
[0m15:48:07.039713 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:48:12.900106 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:48:12.902120 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:48:12.904137 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/* "project_root": "testproj" */

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:48:12.905209 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m15:50:36.214624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BD657230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BD656CF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BD6555B0>]}


============================== 15:50:36.217139 | e8f000b9-6591-4122-9046-6f0e1de0f298 ==============================
[0m15:50:36.217139 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:50:36.218143 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:50:36.544619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e8f000b9-6591-4122-9046-6f0e1de0f298', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BD7BB0B0>]}
[0m15:50:36.708073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e8f000b9-6591-4122-9046-6f0e1de0f298', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BD564950>]}
[0m15:50:36.711094 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:50:36.745672 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:50:37.303150 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:50:37.305158 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:50:37.323467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e8f000b9-6591-4122-9046-6f0e1de0f298', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BD8D9B50>]}
[0m15:50:37.336525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e8f000b9-6591-4122-9046-6f0e1de0f298', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BD980D40>]}
[0m15:50:37.338541 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:50:37.339547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8f000b9-6591-4122-9046-6f0e1de0f298', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BD0F15B0>]}
[0m15:50:37.345721 [info ] [MainThread]: 
[0m15:50:37.347952 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:50:37.352193 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:50:37.404483 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:50:37.475431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8f000b9-6591-4122-9046-6f0e1de0f298', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214BD6E9F40>]}
[0m15:50:37.476440 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:50:37.478456 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:50:37.480536 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:50:37.482546 [info ] [MainThread]: 
[0m15:50:37.489592 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m15:50:37.490613 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:50:37.493639 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:50:37.495653 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:50:37.531029 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:50:37.534173 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:50:37.496665 => 15:50:37.533163
[0m15:50:37.535181 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:50:43.591214 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:50:43.593229 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:50:43.595935 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:50:43.597447 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m15:53:51.835858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002FB0AE01850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002FB0AE01D00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002FB0AE02A80>]}


============================== 15:53:51.835858 | 6099df16-c12c-407a-b4ad-c40ed2d93114 ==============================
[0m15:53:51.835858 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:53:51.836864 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:53:51.929428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6099df16-c12c-407a-b4ad-c40ed2d93114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002FB0AE9E900>]}
[0m15:53:51.978315 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6099df16-c12c-407a-b4ad-c40ed2d93114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002FB09C8C9B0>]}
[0m15:53:51.979318 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:53:51.985361 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:53:52.027814 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:53:52.028818 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:53:52.031837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6099df16-c12c-407a-b4ad-c40ed2d93114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002FB0AE251F0>]}
[0m15:53:52.036890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6099df16-c12c-407a-b4ad-c40ed2d93114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002FB0B038680>]}
[0m15:53:52.037896 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:53:52.037896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6099df16-c12c-407a-b4ad-c40ed2d93114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002FB0AEFC0B0>]}
[0m15:53:52.038904 [info ] [MainThread]: 
[0m15:53:52.039911 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:53:52.040918 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:53:52.044948 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:53:52.058068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6099df16-c12c-407a-b4ad-c40ed2d93114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002FB09C8E570>]}
[0m15:53:52.059077 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:53:52.059077 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:53:52.060100 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:53:52.060100 [info ] [MainThread]: 
[0m15:53:52.064140 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:53:52.064140 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:53:52.065147 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:53:52.065147 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:53:52.070200 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:53:52.070200 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:53:52.065147 => 15:53:52.070200
[0m15:53:52.071210 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:53:52.099458 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:53:52.099458 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:53:52.099458 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:53:52.099458 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:53:52.127751 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m15:53:52.156678 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:53:52.157695 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:53:52.158706 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m15:53:52.171870 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:53:52.181990 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:53:52.071210 => 15:53:52.181990
[0m15:53:52.183003 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6099df16-c12c-407a-b4ad-c40ed2d93114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002FB0AE00E90>]}
[0m15:53:52.184012 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.12s]
[0m15:53:52.185021 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:53:52.185021 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:53:52.186030 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m15:53:52.186030 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:53:52.186030 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:53:52.189055 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:53:52.190108 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:53:52.187038 => 15:53:52.190108
[0m15:53:52.191117 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:53:52.204191 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:53:52.204191 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:53:52.205211 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m15:53:52.207221 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:53:52.209233 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:53:52.191117 => 15:53:52.208228
[0m15:53:52.209233 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6099df16-c12c-407a-b4ad-c40ed2d93114', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002FB0B2C4BF0>]}
[0m15:53:52.210238 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m15:53:52.210238 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:53:52.212255 [debug] [MainThread]: On master: ROLLBACK
[0m15:53:52.212255 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:53:52.212255 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:53:52.213261 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:53:52.213261 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:53:52.213261 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:53:52.214265 [info ] [MainThread]: 
[0m15:53:52.214265 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m15:53:52.215274 [debug] [MainThread]: Command end result
[0m15:53:52.220296 [info ] [MainThread]: 
[0m15:53:52.221314 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:53:52.221314 [info ] [MainThread]: 
[0m15:53:52.221314 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:53:52.222324 [debug] [MainThread]: Command `cli run` succeeded at 15:53:52.222324 after 0.41 seconds
[0m15:53:52.223330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002FB09A3AE70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002FB09A9A480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002FB09A99340>]}
[0m15:53:52.223330 [debug] [MainThread]: Flushing usage events
[0m15:56:32.214773 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018354381DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000183543823F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018354381D90>]}


============================== 15:56:32.214773 | 5f2382ee-062f-46e4-8e4b-e98175b4df2e ==============================
[0m15:56:32.214773 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:56:32.215778 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m15:56:32.318421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5f2382ee-062f-46e4-8e4b-e98175b4df2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001835411D820>]}
[0m15:56:32.366666 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5f2382ee-062f-46e4-8e4b-e98175b4df2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018352883CE0>]}
[0m15:56:32.367671 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:56:32.374775 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:56:32.411972 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:56:32.411972 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:56:32.414984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5f2382ee-062f-46e4-8e4b-e98175b4df2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018355463230>]}
[0m15:56:32.420006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5f2382ee-062f-46e4-8e4b-e98175b4df2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000183555C80B0>]}
[0m15:56:32.421014 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:56:32.421014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5f2382ee-062f-46e4-8e4b-e98175b4df2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018354357B90>]}
[0m15:56:32.422024 [info ] [MainThread]: 
[0m15:56:32.423031 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:56:32.424040 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:56:32.427071 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:56:32.439135 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5f2382ee-062f-46e4-8e4b-e98175b4df2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001835327EC30>]}
[0m15:56:32.440141 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:56:32.440141 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:56:32.441157 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:56:32.441157 [info ] [MainThread]: 
[0m15:56:32.443171 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:56:32.443171 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:56:32.444177 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:56:32.444177 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:56:32.448210 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:56:32.449215 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:56:32.445183 => 15:56:32.449215
[0m15:56:32.449215 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:56:32.479377 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:56:32.480383 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:56:32.480383 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:56:32.481394 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:56:32.513847 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m15:56:32.537287 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:56:32.538294 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:56:32.538294 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m15:56:32.546374 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:56:32.557662 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:56:32.449215 => 15:56:32.557662
[0m15:56:32.558677 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f2382ee-062f-46e4-8e4b-e98175b4df2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018355730410>]}
[0m15:56:32.559688 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.11s]
[0m15:56:32.560697 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:56:32.560697 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:56:32.561704 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m15:56:32.561704 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:56:32.562709 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:56:32.564721 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:56:32.565726 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:56:32.562709 => 15:56:32.565726
[0m15:56:32.565726 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:56:32.582983 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:56:32.583990 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:56:32.584996 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m15:56:32.587020 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:56:32.589039 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:56:32.565726 => 15:56:32.589039
[0m15:56:32.590071 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5f2382ee-062f-46e4-8e4b-e98175b4df2e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018355733500>]}
[0m15:56:32.590071 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m15:56:32.591076 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:56:32.592081 [debug] [MainThread]: On master: ROLLBACK
[0m15:56:32.592081 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:56:32.593091 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:56:32.593091 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:56:32.593091 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:56:32.593091 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:56:32.594097 [info ] [MainThread]: 
[0m15:56:32.594097 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m15:56:32.595102 [debug] [MainThread]: Command end result
[0m15:56:32.601178 [info ] [MainThread]: 
[0m15:56:32.601178 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:56:32.602186 [info ] [MainThread]: 
[0m15:56:32.602186 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:56:32.603196 [debug] [MainThread]: Command `cli run` succeeded at 15:56:32.603196 after 0.41 seconds
[0m15:56:32.604203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018353CB6360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000183555591F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018355391E20>]}
[0m15:56:32.604203 [debug] [MainThread]: Flushing usage events
[0m15:57:52.573067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A39401520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A39402BA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A39401340>]}


============================== 15:57:52.573573 | 2569bbb1-098c-4fe8-86b8-c4ad34925c47 ==============================
[0m15:57:52.573573 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:57:52.574078 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:57:52.668104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2569bbb1-098c-4fe8-86b8-c4ad34925c47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A3A459CD0>]}
[0m15:57:52.715354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2569bbb1-098c-4fe8-86b8-c4ad34925c47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A3A5DF0B0>]}
[0m15:57:52.716359 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:57:52.723406 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:57:52.760615 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:57:52.760615 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:57:52.763629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2569bbb1-098c-4fe8-86b8-c4ad34925c47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A3A5AD2E0>]}
[0m15:57:52.768654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2569bbb1-098c-4fe8-86b8-c4ad34925c47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A3A6580B0>]}
[0m15:57:52.769660 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:57:52.769660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2569bbb1-098c-4fe8-86b8-c4ad34925c47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A388CDB80>]}
[0m15:57:52.771709 [info ] [MainThread]: 
[0m15:57:52.771709 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:57:52.772722 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:57:52.777762 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:57:52.790866 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2569bbb1-098c-4fe8-86b8-c4ad34925c47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A3A45B650>]}
[0m15:57:52.791872 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:57:52.792878 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:57:52.792878 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:57:52.793884 [info ] [MainThread]: 
[0m15:57:52.796900 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:57:52.796900 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:57:52.797906 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:57:52.797906 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:57:52.801961 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:57:52.802971 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:57:52.797906 => 15:57:52.802971
[0m15:57:52.802971 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:57:52.836616 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:57:52.837678 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:57:52.837678 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:57:52.838693 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:57:52.869031 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m15:57:52.894289 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:57:52.896305 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:57:52.896305 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m15:57:52.910427 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:57:52.920496 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:57:52.803982 => 15:57:52.920496
[0m15:57:52.921503 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2569bbb1-098c-4fe8-86b8-c4ad34925c47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A388CDB80>]}
[0m15:57:52.921503 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.12s]
[0m15:57:52.922510 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:57:52.923515 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:57:52.923515 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m15:57:52.924520 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:57:52.924520 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:57:52.928607 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:57:52.930645 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:57:52.925539 => 15:57:52.929633
[0m15:57:52.930645 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:57:52.946868 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:57:52.948882 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:57:52.948882 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m15:57:52.951902 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:57:52.953917 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:57:52.931656 => 15:57:52.952909
[0m15:57:52.953917 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2569bbb1-098c-4fe8-86b8-c4ad34925c47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A3A7AE900>]}
[0m15:57:52.954922 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m15:57:52.954922 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:57:52.956997 [debug] [MainThread]: On master: ROLLBACK
[0m15:57:52.958007 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:57:52.958007 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:57:52.958007 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:57:52.959016 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:57:52.959016 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:57:52.960026 [info ] [MainThread]: 
[0m15:57:52.960026 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.19 seconds (0.19s).
[0m15:57:52.961035 [debug] [MainThread]: Command end result
[0m15:57:52.968075 [info ] [MainThread]: 
[0m15:57:52.969081 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:57:52.969081 [info ] [MainThread]: 
[0m15:57:52.970087 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:57:52.972152 [debug] [MainThread]: Command `cli run` succeeded at 15:57:52.972152 after 0.42 seconds
[0m15:57:52.973167 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A39401520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A39401340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A39401D60>]}
[0m15:57:52.974184 [debug] [MainThread]: Flushing usage events
[0m16:03:27.290032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203EE711B20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203EE711EE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203EE7118B0>]}


============================== 16:03:27.290032 | f7084abf-fa3e-4f7b-9741-8793771528e5 ==============================
[0m16:03:27.290032 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:03:27.291038 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:03:27.390608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f7084abf-fa3e-4f7b-9741-8793771528e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203EE409850>]}
[0m16:03:27.437850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f7084abf-fa3e-4f7b-9741-8793771528e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203EE7113D0>]}
[0m16:03:27.438854 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:03:27.450915 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:03:27.925459 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:03:27.926464 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:03:27.929481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f7084abf-fa3e-4f7b-9741-8793771528e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203EE712570>]}
[0m16:03:27.939536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f7084abf-fa3e-4f7b-9741-8793771528e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203EF970680>]}
[0m16:03:27.939536 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:03:27.940554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7084abf-fa3e-4f7b-9741-8793771528e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203ECBC3EF0>]}
[0m16:03:27.941559 [info ] [MainThread]: 
[0m16:03:27.942565 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:03:27.943571 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:03:27.951615 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m16:03:27.968721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7084abf-fa3e-4f7b-9741-8793771528e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203EE6CEAB0>]}
[0m16:03:27.969726 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:03:27.969726 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:03:27.970736 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:03:27.970736 [info ] [MainThread]: 
[0m16:03:27.974799 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:03:27.974799 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m16:03:27.975807 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:03:27.975807 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:03:27.980845 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:03:27.981852 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:03:27.975807 => 16:03:27.981852
[0m16:03:27.981852 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:03:28.017247 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:03:28.018265 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:03:28.018265 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:03:28.019269 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:03:28.052525 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m16:03:28.082611 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m16:03:28.083116 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:03:28.083620 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m16:03:28.089759 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:03:28.097873 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:03:27.981852 => 16:03:28.097873
[0m16:03:28.098380 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7084abf-fa3e-4f7b-9741-8793771528e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203EF885280>]}
[0m16:03:28.098884 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.12s]
[0m16:03:28.099390 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:03:28.100401 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:03:28.100401 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m16:03:28.101411 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m16:03:28.101411 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m16:03:28.102946 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m16:03:28.103955 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 16:03:28.101411 => 16:03:28.103451
[0m16:03:28.103955 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m16:03:28.115062 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m16:03:28.116067 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:03:28.116067 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m16:03:28.121105 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:03:28.122111 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 16:03:28.104460 => 16:03:28.122111
[0m16:03:28.123116 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7084abf-fa3e-4f7b-9741-8793771528e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203EFA7AAE0>]}
[0m16:03:28.123116 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m16:03:28.124121 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:03:28.124121 [debug] [MainThread]: On master: ROLLBACK
[0m16:03:28.125127 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:03:28.125127 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:03:28.125127 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:03:28.125127 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:03:28.126133 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:03:28.126133 [info ] [MainThread]: 
[0m16:03:28.126133 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m16:03:28.127139 [debug] [MainThread]: Command end result
[0m16:03:28.131160 [info ] [MainThread]: 
[0m16:03:28.132166 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:03:28.132166 [info ] [MainThread]: 
[0m16:03:28.132166 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m16:03:28.133172 [debug] [MainThread]: Command `cli run` succeeded at 16:03:28.133172 after 0.88 seconds
[0m16:03:28.133172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203EE6CEAB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203EF885280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203EE710EC0>]}
[0m16:03:28.134217 [debug] [MainThread]: Flushing usage events
[0m16:10:51.829300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0C7FF5670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0C7FF4740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0C7FF50A0>]}


============================== 16:10:51.832315 | de21438f-9190-4717-927b-40e69bf749cb ==============================
[0m16:10:51.832315 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:10:51.834325 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:10:52.170215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'de21438f-9190-4717-927b-40e69bf749cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0C805D070>]}
[0m16:10:52.367805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'de21438f-9190-4717-927b-40e69bf749cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0C7914500>]}
[0m16:10:52.370965 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:10:52.408593 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:10:52.507235 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:10:52.509245 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:10:52.526342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'de21438f-9190-4717-927b-40e69bf749cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0C81814F0>]}
[0m16:10:52.540841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'de21438f-9190-4717-927b-40e69bf749cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0C8320C50>]}
[0m16:10:52.542853 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:10:52.544663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de21438f-9190-4717-927b-40e69bf749cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0C7F22600>]}
[0m16:10:52.550000 [info ] [MainThread]: 
[0m16:10:52.552075 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:10:52.556505 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:10:52.606183 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m16:10:52.668858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'de21438f-9190-4717-927b-40e69bf749cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0C83230E0>]}
[0m16:10:52.670883 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:10:52.673349 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:10:52.675413 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:10:52.676920 [info ] [MainThread]: 
[0m16:10:52.686327 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m16:10:52.688338 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m16:10:52.690349 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:10:52.691354 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:10:52.727091 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:10:52.737507 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:10:52.692375 => 16:10:52.737507
[0m16:10:52.739523 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:13:56.353517 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:13:56.354521 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:13:56.356536 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:13:56.357543 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m16:17:58.514875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A184710740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A184711940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A184710830>]}


============================== 16:17:58.516892 | 99ce1e4b-ddae-468e-85df-629593d9d88d ==============================
[0m16:17:58.516892 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:17:58.518906 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:17:58.841160 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '99ce1e4b-ddae-468e-85df-629593d9d88d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A1846D86B0>]}
[0m16:17:59.006284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '99ce1e4b-ddae-468e-85df-629593d9d88d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A1849601A0>]}
[0m16:17:59.008295 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:17:59.035428 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:17:59.123641 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:17:59.125658 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:17:59.144305 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '99ce1e4b-ddae-468e-85df-629593d9d88d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A18481DA60>]}
[0m16:17:59.159565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '99ce1e4b-ddae-468e-85df-629593d9d88d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A184A34CB0>]}
[0m16:17:59.161581 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:17:59.163099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '99ce1e4b-ddae-468e-85df-629593d9d88d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A18489BBF0>]}
[0m16:17:59.168683 [info ] [MainThread]: 
[0m16:17:59.171754 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:17:59.177346 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:17:59.229915 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m16:17:59.295318 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '99ce1e4b-ddae-468e-85df-629593d9d88d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A18498BEC0>]}
[0m16:17:59.297329 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:17:59.298335 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:17:59.300824 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:17:59.302366 [info ] [MainThread]: 
[0m16:17:59.308923 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m16:17:59.310956 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m16:17:59.312966 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:17:59.314992 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:17:59.349257 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:17:59.351293 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:17:59.315997 => 16:17:59.351293
[0m16:17:59.353304 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:18:05.414035 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:18:05.417060 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:18:05.418070 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:18:05.420082 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m16:18:21.298794 [debug] [Thread-7 (]: SQL status: OK in 15.880000114440918 seconds
[0m16:18:23.795895 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m16:18:23.800953 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:18:23.801962 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m16:18:27.472530 [debug] [Thread-7 (]: SQL status: OK in 3.6700000762939453 seconds
[0m16:18:27.509949 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:17:59.354309 => 16:18:27.508938
[0m16:18:27.513244 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99ce1e4b-ddae-468e-85df-629593d9d88d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A184DE26F0>]}
[0m16:18:27.515308 [info ] [Thread-7 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 28.20s]
[0m16:18:27.517322 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:18:27.519344 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m16:18:27.521370 [info ] [Thread-7 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m16:18:27.524409 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m16:18:27.525422 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m16:18:27.535094 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m16:18:27.536601 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 16:18:27.526434 => 16:18:27.536601
[0m16:18:27.538777 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m16:18:27.594824 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m16:18:27.597881 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:18:27.598895 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m16:18:27.614370 [debug] [Thread-7 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:18:27.620418 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 16:18:27.538777 => 16:18:27.620418
[0m16:18:27.623446 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99ce1e4b-ddae-468e-85df-629593d9d88d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A184DE16D0>]}
[0m16:18:27.624473 [info ] [Thread-7 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.10s]
[0m16:18:27.626488 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:18:27.630521 [debug] [MainThread]: On master: ROLLBACK
[0m16:18:27.632537 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:18:27.633545 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:18:27.634551 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:18:27.635714 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:18:27.637727 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:18:27.639756 [info ] [MainThread]: 
[0m16:18:27.640762 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 28.47 seconds (28.47s).
[0m16:18:27.643782 [debug] [MainThread]: Command end result
[0m16:18:27.662510 [info ] [MainThread]: 
[0m16:18:27.663524 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:18:27.665740 [info ] [MainThread]: 
[0m16:18:27.666751 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m16:18:27.669778 [debug] [MainThread]: Command `cli run` succeeded at 16:18:27.669778 after 29.24 seconds
[0m16:18:27.671793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A18431CC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A184D0D640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A184D0C980>]}
[0m16:18:27.672827 [debug] [MainThread]: Flushing usage events
[0m16:19:56.118791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151F2F6DF40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151F2F6E1E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151F2F6E570>]}


============================== 16:19:56.118791 | 28940648-73bc-4637-b1ab-86d5d8041d4d ==============================
[0m16:19:56.118791 [info ] [MainThread]: Running with dbt=1.7.4
[0m16:19:56.119797 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:19:56.222042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '28940648-73bc-4637-b1ab-86d5d8041d4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151F2DF1070>]}
[0m16:19:56.270307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '28940648-73bc-4637-b1ab-86d5d8041d4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151F412D370>]}
[0m16:19:56.271313 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:19:56.283388 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m16:19:56.743677 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:19:56.743677 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:19:56.747734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '28940648-73bc-4637-b1ab-86d5d8041d4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151F40BE1E0>]}
[0m16:19:56.752816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '28940648-73bc-4637-b1ab-86d5d8041d4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151F41DBF80>]}
[0m16:19:56.752816 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:19:56.752816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '28940648-73bc-4637-b1ab-86d5d8041d4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151F2D5E930>]}
[0m16:19:56.754849 [info ] [MainThread]: 
[0m16:19:56.754849 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:19:56.755865 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:19:56.763971 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m16:19:56.781108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '28940648-73bc-4637-b1ab-86d5d8041d4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151F2C8F890>]}
[0m16:19:56.782117 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:19:56.782117 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:19:56.782117 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:19:56.783123 [info ] [MainThread]: 
[0m16:19:56.785140 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:19:56.785140 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m16:19:56.786147 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:19:56.786147 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:19:56.792180 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:19:56.793185 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:19:56.786147 => 16:19:56.792180
[0m16:19:56.793185 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:19:56.828661 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:19:56.829668 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:19:56.829668 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:19:56.830677 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:19:56.861982 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m16:19:56.881101 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m16:19:56.882108 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:19:56.882108 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m16:19:56.888158 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:19:56.896267 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:19:56.793691 => 16:19:56.895252
[0m16:19:56.896267 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '28940648-73bc-4637-b1ab-86d5d8041d4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151F432EA50>]}
[0m16:19:56.897283 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.11s]
[0m16:19:56.897283 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:19:56.898299 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:19:56.898299 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m16:19:56.899310 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m16:19:56.899310 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m16:19:56.901323 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m16:19:56.901840 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 16:19:56.899310 => 16:19:56.901840
[0m16:19:56.902346 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m16:19:56.913405 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m16:19:56.913405 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:19:56.913405 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m16:19:56.915418 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:19:56.916424 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 16:19:56.902346 => 16:19:56.916424
[0m16:19:56.916424 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '28940648-73bc-4637-b1ab-86d5d8041d4d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151F432D4C0>]}
[0m16:19:56.917430 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m16:19:56.917430 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:19:56.918442 [debug] [MainThread]: On master: ROLLBACK
[0m16:19:56.918442 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:19:56.919449 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:19:56.919449 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:19:56.919449 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:19:56.919449 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:19:56.920454 [info ] [MainThread]: 
[0m16:19:56.920454 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m16:19:56.921460 [debug] [MainThread]: Command end result
[0m16:19:56.925484 [info ] [MainThread]: 
[0m16:19:56.925484 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:19:56.925484 [info ] [MainThread]: 
[0m16:19:56.926490 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m16:19:56.926490 [debug] [MainThread]: Command `cli run` succeeded at 16:19:56.926490 after 0.84 seconds
[0m16:19:56.927496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151F2F6E1E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151F2F6DF40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000151F2C8C3E0>]}
[0m16:19:56.927496 [debug] [MainThread]: Flushing usage events
[0m18:29:44.898153 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002571F3C82C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002571F3C9A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002571F232930>]}


============================== 18:29:44.900190 | b659ccbc-3ddd-4e9f-a86a-86f22b0f1453 ==============================
[0m18:29:44.900190 [info ] [MainThread]: Running with dbt=1.7.4
[0m18:29:44.902201 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m18:29:45.221456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b659ccbc-3ddd-4e9f-a86a-86f22b0f1453', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002571F7DABD0>]}
[0m18:29:45.382954 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b659ccbc-3ddd-4e9f-a86a-86f22b0f1453', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002571F58C740>]}
[0m18:29:45.384974 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:29:45.419433 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m18:29:45.953588 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:29:45.955604 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:29:45.972722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b659ccbc-3ddd-4e9f-a86a-86f22b0f1453', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002571F75EC30>]}
[0m18:29:45.985797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b659ccbc-3ddd-4e9f-a86a-86f22b0f1453', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002571FA24D40>]}
[0m18:29:45.987808 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m18:29:45.988815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b659ccbc-3ddd-4e9f-a86a-86f22b0f1453', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002571F71B380>]}
[0m18:29:45.994862 [info ] [MainThread]: 
[0m18:29:45.997365 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:29:46.003368 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:29:46.057438 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:29:46.128687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b659ccbc-3ddd-4e9f-a86a-86f22b0f1453', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002571CA40050>]}
[0m18:29:46.130564 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:29:46.132729 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:29:46.134702 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:29:46.136731 [info ] [MainThread]: 
[0m18:29:46.143849 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m18:29:46.145863 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:29:46.149045 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:29:46.150168 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:29:46.184803 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:29:46.187797 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:29:46.151178 => 18:29:46.186812
[0m18:29:46.189382 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:29:46.313420 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:29:46.315431 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:29:46.316968 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:29:46.318978 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m18:29:46.365935 [debug] [Thread-7 (]: SQL status: OK in 0.05000000074505806 seconds
[0m18:29:51.571244 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:29:51.575355 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:29:51.577033 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:29:51.582206 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m18:29:51.618892 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:29:46.190391 => 18:29:51.617887
[0m18:29:51.620913 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b659ccbc-3ddd-4e9f-a86a-86f22b0f1453', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002571FBC8B00>]}
[0m18:29:51.622923 [info ] [Thread-7 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 5.47s]
[0m18:29:51.625982 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:29:51.629047 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m18:29:51.630053 [info ] [Thread-7 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m18:29:51.633068 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m18:29:51.634073 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:29:51.643292 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:29:51.645868 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:29:51.635078 => 18:29:51.644861
[0m18:29:51.646875 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:29:51.701034 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:29:51.703044 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:29:51.704051 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m18:29:51.710462 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m18:29:51.716495 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:29:51.647880 => 18:29:51.715490
[0m18:29:51.718506 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b659ccbc-3ddd-4e9f-a86a-86f22b0f1453', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002571FBC9310>]}
[0m18:29:51.720516 [info ] [Thread-7 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.09s]
[0m18:29:51.722883 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:29:51.726182 [debug] [MainThread]: On master: ROLLBACK
[0m18:29:51.728488 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:29:51.729508 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:29:51.730513 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:29:51.732523 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:29:51.733528 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:29:51.735538 [info ] [MainThread]: 
[0m18:29:51.737549 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 5.74 seconds (5.74s).
[0m18:29:51.739987 [debug] [MainThread]: Command end result
[0m18:29:51.753116 [info ] [MainThread]: 
[0m18:29:51.754131 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:29:51.756148 [info ] [MainThread]: 
[0m18:29:51.759181 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m18:29:51.763214 [debug] [MainThread]: Command `cli run` succeeded at 18:29:51.763214 after 6.97 seconds
[0m18:29:51.766241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002571F2334D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002571FD48E90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002571FD49160>]}
[0m18:29:51.767248 [debug] [MainThread]: Flushing usage events
[0m18:32:12.709577 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022667D28380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022667677F80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002266736F680>]}


============================== 18:32:12.712609 | 8b894cb8-bde7-4210-9239-c79d6bb02df9 ==============================
[0m18:32:12.712609 [info ] [MainThread]: Running with dbt=1.7.4
[0m18:32:12.714620 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m18:32:13.027516 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8b894cb8-bde7-4210-9239-c79d6bb02df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022664FAFDD0>]}
[0m18:32:13.193102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8b894cb8-bde7-4210-9239-c79d6bb02df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022667F83B90>]}
[0m18:32:13.196116 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:32:13.224361 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m18:32:13.315969 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:32:13.316974 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:32:13.334156 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8b894cb8-bde7-4210-9239-c79d6bb02df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022667EB4350>]}
[0m18:32:13.348251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8b894cb8-bde7-4210-9239-c79d6bb02df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022667FF4F80>]}
[0m18:32:13.349764 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m18:32:13.351307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8b894cb8-bde7-4210-9239-c79d6bb02df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022667FF4500>]}
[0m18:32:13.356355 [info ] [MainThread]: 
[0m18:32:13.359428 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:32:13.363706 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:32:13.411550 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:32:13.475705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8b894cb8-bde7-4210-9239-c79d6bb02df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022667FF70B0>]}
[0m18:32:13.477768 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:32:13.479235 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:32:13.481253 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:32:13.482266 [info ] [MainThread]: 
[0m18:32:13.489448 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m18:32:13.491460 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:32:13.493860 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:32:13.494865 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:32:13.529057 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:32:13.532076 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:32:13.495869 => 18:32:13.531069
[0m18:32:13.533083 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:32:13.659703 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:32:13.660707 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:32:13.662716 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:32:13.663721 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m18:32:13.711113 [debug] [Thread-7 (]: SQL status: OK in 0.05000000074505806 seconds
[0m18:32:13.793594 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:32:13.795610 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:32:13.796620 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:32:13.809842 [debug] [Thread-7 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:32:13.845743 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:32:13.534089 => 18:32:13.845743
[0m18:32:13.848766 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8b894cb8-bde7-4210-9239-c79d6bb02df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002266819CFE0>]}
[0m18:32:13.849771 [info ] [Thread-7 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.36s]
[0m18:32:13.852787 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:32:13.854828 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m18:32:13.856339 [info ] [Thread-7 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m18:32:13.858403 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m18:32:13.860416 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:32:13.868493 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:32:13.870705 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:32:13.861453 => 18:32:13.870705
[0m18:32:13.872163 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:32:13.922332 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:32:13.924341 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:32:13.925345 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m18:32:13.930372 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m18:32:13.936435 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:32:13.874174 => 18:32:13.935404
[0m18:32:13.938452 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8b894cb8-bde7-4210-9239-c79d6bb02df9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002266819C3E0>]}
[0m18:32:13.940944 [info ] [Thread-7 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m18:32:13.943162 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:32:13.946189 [debug] [MainThread]: On master: ROLLBACK
[0m18:32:13.948201 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:32:13.949735 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:32:13.951220 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:32:13.952230 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:32:13.953824 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:32:13.956346 [info ] [MainThread]: 
[0m18:32:13.958361 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.60 seconds (0.60s).
[0m18:32:13.960453 [debug] [MainThread]: Command end result
[0m18:32:13.972048 [info ] [MainThread]: 
[0m18:32:13.974390 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:32:13.975398 [info ] [MainThread]: 
[0m18:32:13.977418 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m18:32:13.980497 [debug] [MainThread]: Command `cli run` succeeded at 18:32:13.980497 after 1.35 seconds
[0m18:32:13.982537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022667CEFCB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022667E7EF60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022667FF5B20>]}
[0m18:32:13.983542 [debug] [MainThread]: Flushing usage events
[0m18:34:18.426605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD49CAA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD49D490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD49C680>]}


============================== 18:34:18.429120 | fda6064b-7bf6-47ee-94a7-df784d003b77 ==============================
[0m18:34:18.429120 [info ] [MainThread]: Running with dbt=1.7.4
[0m18:34:18.431877 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m18:34:18.748451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fda6064b-7bf6-47ee-94a7-df784d003b77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232ACD95D00>]}
[0m18:34:18.911420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fda6064b-7bf6-47ee-94a7-df784d003b77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD6F64B0>]}
[0m18:34:18.914448 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:34:18.949863 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m18:34:19.546343 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:34:19.548371 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:34:19.565638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fda6064b-7bf6-47ee-94a7-df784d003b77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD156FC0>]}
[0m18:34:19.584789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fda6064b-7bf6-47ee-94a7-df784d003b77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD7551C0>]}
[0m18:34:19.585794 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m18:34:19.587805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fda6064b-7bf6-47ee-94a7-df784d003b77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD754920>]}
[0m18:34:19.592832 [info ] [MainThread]: 
[0m18:34:19.595176 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:34:19.599813 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:34:19.649356 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:34:19.718564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fda6064b-7bf6-47ee-94a7-df784d003b77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD49D7F0>]}
[0m18:34:19.721590 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:34:19.722595 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:34:19.724606 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:34:19.726616 [info ] [MainThread]: 
[0m18:34:19.734797 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m18:34:19.736809 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:34:19.738820 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:34:19.740830 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:34:19.773748 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:34:19.776784 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:34:19.741835 => 18:34:19.775774
[0m18:34:19.777789 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:34:19.908508 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:34:19.909513 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:34:19.911525 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:34:19.912530 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m18:34:19.966437 [debug] [Thread-7 (]: SQL status: OK in 0.05000000074505806 seconds
[0m18:34:20.054833 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:34:20.057859 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:34:20.058868 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:34:20.069948 [debug] [Thread-7 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:34:20.104081 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:34:19.778794 => 18:34:20.104081
[0m18:34:20.107110 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fda6064b-7bf6-47ee-94a7-df784d003b77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD962BA0>]}
[0m18:34:20.109119 [info ] [Thread-7 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.37s]
[0m18:34:20.111129 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:34:20.112135 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m18:34:20.114794 [info ] [Thread-7 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m18:34:20.117159 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m18:34:20.118520 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:34:20.126625 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:34:20.128636 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:34:20.119527 => 18:34:20.128636
[0m18:34:20.130647 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:34:20.181107 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:34:20.183133 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:34:20.185146 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m18:34:20.190173 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m18:34:20.196202 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:34:20.131483 => 18:34:20.195197
[0m18:34:20.198227 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fda6064b-7bf6-47ee-94a7-df784d003b77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD96BD40>]}
[0m18:34:20.200807 [info ] [Thread-7 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m18:34:20.202701 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:34:20.205908 [debug] [MainThread]: On master: ROLLBACK
[0m18:34:20.206918 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:34:20.208929 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:34:20.209934 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:34:20.211944 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:34:20.212949 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:34:20.215333 [info ] [MainThread]: 
[0m18:34:20.216339 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.62 seconds (0.62s).
[0m18:34:20.219354 [debug] [MainThread]: Command end result
[0m18:34:20.229428 [info ] [MainThread]: 
[0m18:34:20.231790 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:34:20.232811 [info ] [MainThread]: 
[0m18:34:20.234822 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m18:34:20.237838 [debug] [MainThread]: Command `cli run` succeeded at 18:34:20.236832 after 1.91 seconds
[0m18:34:20.238843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD6F7620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD6C6630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000232AD6C6690>]}
[0m18:34:20.239849 [debug] [MainThread]: Flushing usage events
[0m18:36:03.643863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147B3B95310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147B3B95040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147B3B94590>]}


============================== 18:36:03.646877 | e113d13b-1ac3-492e-8988-4a6a11abb99f ==============================
[0m18:36:03.646877 [info ] [MainThread]: Running with dbt=1.7.4
[0m18:36:03.648887 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m18:36:03.973052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e113d13b-1ac3-492e-8988-4a6a11abb99f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147B3AC8C80>]}
[0m18:36:04.135618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e113d13b-1ac3-492e-8988-4a6a11abb99f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147B3DC5070>]}
[0m18:36:04.138633 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:36:04.165433 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m18:36:04.253496 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:36:04.254560 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:36:04.271679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e113d13b-1ac3-492e-8988-4a6a11abb99f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147B3CF6FC0>]}
[0m18:36:04.285750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e113d13b-1ac3-492e-8988-4a6a11abb99f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147B3E6C380>]}
[0m18:36:04.287771 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m18:36:04.288776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e113d13b-1ac3-492e-8988-4a6a11abb99f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147B3D248C0>]}
[0m18:36:04.294806 [info ] [MainThread]: 
[0m18:36:04.297618 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:36:04.301821 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:36:04.348891 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:36:04.413533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e113d13b-1ac3-492e-8988-4a6a11abb99f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147B3C231A0>]}
[0m18:36:04.415844 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:36:04.416846 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:36:04.419332 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:36:04.420665 [info ] [MainThread]: 
[0m18:36:04.428838 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m18:36:04.429843 [info ] [Thread-7 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:36:04.433053 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:36:04.434059 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:36:04.469469 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:36:04.471667 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:36:04.435065 => 18:36:04.471667
[0m18:36:04.474063 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:36:04.597894 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:36:04.598900 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:36:04.600910 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:36:04.601915 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m18:36:04.649155 [debug] [Thread-7 (]: SQL status: OK in 0.05000000074505806 seconds
[0m18:36:04.733317 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:36:04.736339 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:36:04.737358 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:36:04.749445 [debug] [Thread-7 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:36:04.786664 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:36:04.475073 => 18:36:04.785659
[0m18:36:04.789164 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e113d13b-1ac3-492e-8988-4a6a11abb99f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147B40060F0>]}
[0m18:36:04.792197 [info ] [Thread-7 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.36s]
[0m18:36:04.794214 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:36:04.796226 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m18:36:04.798241 [info ] [Thread-7 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m18:36:04.800507 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m18:36:04.801513 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:36:04.810912 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:36:04.812922 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:36:04.802519 => 18:36:04.811917
[0m18:36:04.813927 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:36:04.865596 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:36:04.867607 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:36:04.868614 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m18:36:04.873854 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m18:36:04.879887 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:36:04.814935 => 18:36:04.878881
[0m18:36:04.881898 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e113d13b-1ac3-492e-8988-4a6a11abb99f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147B4006F90>]}
[0m18:36:04.883910 [info ] [Thread-7 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m18:36:04.885942 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:36:04.888968 [debug] [MainThread]: On master: ROLLBACK
[0m18:36:04.890990 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:36:04.893013 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:36:04.895030 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:36:04.896038 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:36:04.897045 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:36:04.900073 [info ] [MainThread]: 
[0m18:36:04.901079 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.60 seconds (0.60s).
[0m18:36:04.904095 [debug] [MainThread]: Command end result
[0m18:36:04.917415 [info ] [MainThread]: 
[0m18:36:04.919434 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:36:04.920730 [info ] [MainThread]: 
[0m18:36:04.921739 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m18:36:04.925258 [debug] [MainThread]: Command `cli run` succeeded at 18:36:04.925258 after 1.36 seconds
[0m18:36:04.927280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147B4063200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147B4062E40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000147B40632C0>]}
[0m18:36:04.929292 [debug] [MainThread]: Flushing usage events
[0m18:37:01.307877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DDE37DD60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DDE37D3D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DDE37CC50>]}


============================== 18:37:01.309887 | 80e5a22c-d402-4cc2-99ba-424c4073cad6 ==============================
[0m18:37:01.309887 [info ] [MainThread]: Running with dbt=1.7.4
[0m18:37:01.311899 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:37:01.619220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '80e5a22c-d402-4cc2-99ba-424c4073cad6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DDE447F50>]}
[0m18:37:01.779286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '80e5a22c-d402-4cc2-99ba-424c4073cad6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DDE37CB30>]}
[0m18:37:01.782347 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:37:01.810057 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m18:37:01.894289 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:37:01.896299 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:37:01.913395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '80e5a22c-d402-4cc2-99ba-424c4073cad6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DDE52C170>]}
[0m18:37:01.928503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '80e5a22c-d402-4cc2-99ba-424c4073cad6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DDE63AD20>]}
[0m18:37:01.930018 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m18:37:01.931532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '80e5a22c-d402-4cc2-99ba-424c4073cad6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DDE487410>]}
[0m18:37:01.937582 [info ] [MainThread]: 
[0m18:37:01.940985 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:37:01.946432 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:37:01.993331 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:37:02.056658 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '80e5a22c-d402-4cc2-99ba-424c4073cad6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DDDF4B6E0>]}
[0m18:37:02.058469 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:37:02.059403 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:37:02.062560 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:37:02.063991 [info ] [MainThread]: 
[0m18:37:02.071296 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m18:37:02.073306 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:37:02.075345 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:37:02.077357 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:37:02.112153 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:37:02.114296 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:37:02.078362 => 18:37:02.113288
[0m18:37:02.115301 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:37:02.238492 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:37:02.239497 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:37:02.240504 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:37:02.242591 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:37:02.288840 [debug] [Thread-6 (]: SQL status: OK in 0.05000000074505806 seconds
[0m18:37:02.370159 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:37:02.372178 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:37:02.373186 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:37:02.385767 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:37:02.421367 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:37:02.116307 => 18:37:02.420361
[0m18:37:02.423383 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '80e5a22c-d402-4cc2-99ba-424c4073cad6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DDE4C6BA0>]}
[0m18:37:02.425400 [info ] [Thread-6 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.35s]
[0m18:37:02.427411 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:37:02.429421 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m18:37:02.431432 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m18:37:02.433858 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m18:37:02.435990 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:37:02.445102 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:37:02.447111 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:37:02.437000 => 18:37:02.446107
[0m18:37:02.448517 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:37:02.498461 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:37:02.500517 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:37:02.501522 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m18:37:02.506550 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m18:37:02.512583 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:37:02.449523 => 18:37:02.511578
[0m18:37:02.514594 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '80e5a22c-d402-4cc2-99ba-424c4073cad6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DDE766900>]}
[0m18:37:02.516619 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m18:37:02.517726 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:37:02.522221 [debug] [MainThread]: On master: ROLLBACK
[0m18:37:02.523272 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:37:02.525483 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:37:02.526383 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:37:02.528397 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:37:02.529417 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:37:02.531427 [info ] [MainThread]: 
[0m18:37:02.533400 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.59 seconds (0.59s).
[0m18:37:02.535755 [debug] [MainThread]: Command end result
[0m18:37:02.547134 [info ] [MainThread]: 
[0m18:37:02.549485 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:37:02.551421 [info ] [MainThread]: 
[0m18:37:02.553713 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m18:37:02.556817 [debug] [MainThread]: Command `cli run` succeeded at 18:37:02.555814 after 1.33 seconds
[0m18:37:02.557826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DDDF70B60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DDE5C7980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DDE5C49B0>]}
[0m18:37:02.559842 [debug] [MainThread]: Flushing usage events
[0m18:38:38.866222 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002213213C110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221319CC470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022131DE71A0>]}


============================== 18:38:38.868238 | 320b6201-6146-4389-8efc-d8a4ecc81b0a ==============================
[0m18:38:38.868238 [info ] [MainThread]: Running with dbt=1.7.4
[0m18:38:38.870279 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m18:38:39.173490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '320b6201-6146-4389-8efc-d8a4ecc81b0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022132194770>]}
[0m18:38:39.335169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '320b6201-6146-4389-8efc-d8a4ecc81b0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022132324440>]}
[0m18:38:39.337222 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:38:39.363614 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m18:38:39.453624 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:38:39.454630 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:38:39.471736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '320b6201-6146-4389-8efc-d8a4ecc81b0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002212F6C35F0>]}
[0m18:38:39.485810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '320b6201-6146-4389-8efc-d8a4ecc81b0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221323B6450>]}
[0m18:38:39.486818 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m18:38:39.488847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '320b6201-6146-4389-8efc-d8a4ecc81b0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022131F3E1B0>]}
[0m18:38:39.493894 [info ] [MainThread]: 
[0m18:38:39.496489 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:38:39.500733 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:38:39.547598 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:38:39.611840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '320b6201-6146-4389-8efc-d8a4ecc81b0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221323B7290>]}
[0m18:38:39.613309 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:38:39.615682 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:38:39.617693 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:38:39.618712 [info ] [MainThread]: 
[0m18:38:39.625940 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m18:38:39.627955 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:38:39.629966 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:38:39.631978 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:38:39.666407 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:38:39.668419 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:38:39.632983 => 18:38:39.668419
[0m18:38:39.670454 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:38:39.794190 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:38:39.796197 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:38:39.797205 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:38:39.798209 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:38:39.845831 [debug] [Thread-6 (]: SQL status: OK in 0.05000000074505806 seconds
[0m18:38:39.926884 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:38:39.928893 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:38:39.929901 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:38:39.941855 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:38:39.976652 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:38:39.671461 => 18:38:39.976652
[0m18:38:39.979669 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '320b6201-6146-4389-8efc-d8a4ecc81b0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221325E7B00>]}
[0m18:38:39.981677 [info ] [Thread-6 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.35s]
[0m18:38:39.983686 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:38:39.985698 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m18:38:39.987725 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m18:38:39.990667 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m18:38:39.991676 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:38:40.000959 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:38:40.002966 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:38:39.993414 => 18:38:40.002966
[0m18:38:40.005088 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:38:40.054793 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:38:40.057797 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:38:40.059806 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m18:38:40.063823 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m18:38:40.069848 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:38:40.006093 => 18:38:40.068844
[0m18:38:40.071975 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '320b6201-6146-4389-8efc-d8a4ecc81b0a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221325E7800>]}
[0m18:38:40.073986 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m18:38:40.076160 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:38:40.079603 [debug] [MainThread]: On master: ROLLBACK
[0m18:38:40.080612 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:38:40.082929 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:38:40.084361 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:38:40.085363 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:38:40.087388 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:38:40.089717 [info ] [MainThread]: 
[0m18:38:40.090727 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.59 seconds (0.59s).
[0m18:38:40.094075 [debug] [MainThread]: Command end result
[0m18:38:40.106345 [info ] [MainThread]: 
[0m18:38:40.108527 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:38:40.110579 [info ] [MainThread]: 
[0m18:38:40.111627 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m18:38:40.115675 [debug] [MainThread]: Command `cli run` succeeded at 18:38:40.114652 after 1.33 seconds
[0m18:38:40.116681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002213177F5C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221315AF9E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022132286A20>]}
[0m18:38:40.118691 [debug] [MainThread]: Flushing usage events
[0m21:23:56.017171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F145400E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F1181C140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F13C50170>]}


============================== 21:23:56.020196 | baf79309-b4ce-4157-b7a0-bf64d5f9d177 ==============================
[0m21:23:56.020196 [info ] [MainThread]: Running with dbt=1.7.4
[0m21:23:56.022215 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m21:23:56.345455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'baf79309-b4ce-4157-b7a0-bf64d5f9d177', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F144FB170>]}
[0m21:23:56.510284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'baf79309-b4ce-4157-b7a0-bf64d5f9d177', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F147247A0>]}
[0m21:23:56.512348 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m21:23:56.545807 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m21:23:57.182572 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:23:57.183580 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:23:57.201771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'baf79309-b4ce-4157-b7a0-bf64d5f9d177', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F147B4110>]}
[0m21:23:57.217923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'baf79309-b4ce-4157-b7a0-bf64d5f9d177', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F14802F60>]}
[0m21:23:57.218930 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m21:23:57.220944 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'baf79309-b4ce-4157-b7a0-bf64d5f9d177', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F145FDB80>]}
[0m21:23:57.227029 [info ] [MainThread]: 
[0m21:23:57.229045 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m21:23:57.234088 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m21:23:57.286181 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m21:23:57.357343 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'baf79309-b4ce-4157-b7a0-bf64d5f9d177', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F14800F80>]}
[0m21:23:57.358350 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:23:57.360366 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m21:23:57.362381 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m21:23:57.363389 [info ] [MainThread]: 
[0m21:23:57.372344 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m21:23:57.373350 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m21:23:57.376363 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m21:23:57.377368 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m21:23:57.410751 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m21:23:57.413766 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 21:23:57.378373 => 21:23:57.412761
[0m21:23:57.414771 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m21:23:57.541979 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:23:57.543995 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m21:23:57.545003 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m21:23:57.546011 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m21:23:57.603835 [debug] [Thread-6 (]: SQL status: OK in 0.05999999865889549 seconds
[0m21:23:57.696577 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m21:23:57.698596 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m21:23:57.699817 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m21:23:57.723306 [debug] [Thread-6 (]: SQL status: OK in 0.019999999552965164 seconds
[0m21:23:57.763231 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 21:23:57.415775 => 21:23:57.762171
[0m21:23:57.766692 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'baf79309-b4ce-4157-b7a0-bf64d5f9d177', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F149831A0>]}
[0m21:23:57.770713 [info ] [Thread-6 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.39s]
[0m21:23:57.774251 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m21:23:57.777273 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m21:23:57.779286 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m21:23:57.781489 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m21:23:57.783603 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m21:23:57.793834 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m21:23:57.795845 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 21:23:57.784894 => 21:23:57.794839
[0m21:23:57.797856 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m21:23:57.854378 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m21:23:57.857399 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m21:23:57.858404 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m21:23:57.863430 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m21:23:57.869637 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 21:23:57.798875 => 21:23:57.868632
[0m21:23:57.871648 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'baf79309-b4ce-4157-b7a0-bf64d5f9d177', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F14982210>]}
[0m21:23:57.873672 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.09s]
[0m21:23:57.875684 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m21:23:57.878701 [debug] [MainThread]: On master: ROLLBACK
[0m21:23:57.879708 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:23:57.880716 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m21:23:57.882070 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m21:23:57.884251 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:23:57.885258 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m21:23:57.887722 [info ] [MainThread]: 
[0m21:23:57.889093 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.66 seconds (0.66s).
[0m21:23:57.891114 [debug] [MainThread]: Command end result
[0m21:23:57.905771 [info ] [MainThread]: 
[0m21:23:57.909436 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:23:57.911459 [info ] [MainThread]: 
[0m21:23:57.912466 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m21:23:57.916135 [debug] [MainThread]: Command `cli run` succeeded at 21:23:57.915558 after 2.00 seconds
[0m21:23:57.917872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F1181C140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F139BFB60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F149EAED0>]}
[0m21:23:57.919886 [debug] [MainThread]: Flushing usage events
[0m11:14:44.178881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020983A6DAF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020983A6DEB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020983A6EC00>]}


============================== 11:14:44.178881 | 9358813e-f516-4ee4-8fa9-8912090805a8 ==============================
[0m11:14:44.178881 [info ] [MainThread]: Running with dbt=1.7.4
[0m11:14:44.179889 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m11:14:44.288050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9358813e-f516-4ee4-8fa9-8912090805a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020983B42420>]}
[0m11:14:44.339660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9358813e-f516-4ee4-8fa9-8912090805a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020983A94BC0>]}
[0m11:14:44.340688 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m11:14:44.354826 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m11:14:45.079416 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:14:45.080426 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:14:45.083453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9358813e-f516-4ee4-8fa9-8912090805a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020983B99D00>]}
[0m11:14:45.089524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9358813e-f516-4ee4-8fa9-8912090805a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020983C9D250>]}
[0m11:14:45.089524 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m11:14:45.089524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9358813e-f516-4ee4-8fa9-8912090805a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209829D0170>]}
[0m11:14:45.091546 [info ] [MainThread]: 
[0m11:14:45.091546 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m11:14:45.092561 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m11:14:45.104735 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m11:14:45.122936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9358813e-f516-4ee4-8fa9-8912090805a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020983CF12E0>]}
[0m11:14:45.122936 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:14:45.123945 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:14:45.123945 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m11:14:45.124956 [info ] [MainThread]: 
[0m11:14:45.126978 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m11:14:45.126978 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m11:14:45.127990 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m11:14:45.127990 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m11:14:45.134087 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m11:14:45.135097 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 11:14:45.129004 => 11:14:45.135097
[0m11:14:45.135097 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m11:14:45.166450 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:14:45.167458 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m11:14:45.167458 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m11:14:45.168468 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m11:14:45.198793 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m11:14:45.224079 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m11:14:45.225089 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m11:14:45.226104 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m11:14:45.234222 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m11:14:45.243349 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 11:14:45.136113 => 11:14:45.242340
[0m11:14:45.243349 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9358813e-f516-4ee4-8fa9-8912090805a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020983C9F440>]}
[0m11:14:45.244358 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.12s]
[0m11:14:45.244358 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m11:14:45.245368 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m11:14:45.245368 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m11:14:45.246384 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m11:14:45.246384 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m11:14:45.248420 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m11:14:45.249434 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 11:14:45.247398 => 11:14:45.249434
[0m11:14:45.250449 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m11:14:45.262574 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m11:14:45.263588 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m11:14:45.263588 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m11:14:45.266709 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m11:14:45.268745 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 11:14:45.250449 => 11:14:45.268745
[0m11:14:45.269760 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9358813e-f516-4ee4-8fa9-8912090805a8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020983E0A9F0>]}
[0m11:14:45.270773 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m11:14:45.270773 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m11:14:45.271782 [debug] [MainThread]: On master: ROLLBACK
[0m11:14:45.272800 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:14:45.272800 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m11:14:45.272800 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m11:14:45.273812 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m11:14:45.273812 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m11:14:45.274821 [info ] [MainThread]: 
[0m11:14:45.274821 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m11:14:45.275834 [debug] [MainThread]: Command end result
[0m11:14:45.280873 [info ] [MainThread]: 
[0m11:14:45.281907 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:14:45.281907 [info ] [MainThread]: 
[0m11:14:45.282921 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m11:14:45.283933 [debug] [MainThread]: Command `cli run` succeeded at 11:14:45.283933 after 1.15 seconds
[0m11:14:45.283933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209829952E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020983A97620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000209829D0170>]}
[0m11:14:45.284942 [debug] [MainThread]: Flushing usage events
[0m14:55:07.039379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A62CE41190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A62BDC6FC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A62CE41610>]}


============================== 14:55:07.040388 | da627c82-1967-4c97-92ae-108c80c297d2 ==============================
[0m14:55:07.040388 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:55:07.040388 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:55:07.143663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'da627c82-1967-4c97-92ae-108c80c297d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A62CE64710>]}
[0m14:55:07.195287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'da627c82-1967-4c97-92ae-108c80c297d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A62B2C49B0>]}
[0m14:55:07.196293 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:55:07.213486 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:55:08.038624 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:55:08.038624 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:55:08.042660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'da627c82-1967-4c97-92ae-108c80c297d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A62BDC7560>]}
[0m14:55:08.054816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'da627c82-1967-4c97-92ae-108c80c297d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A62D078B30>]}
[0m14:55:08.055825 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:55:08.055825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da627c82-1967-4c97-92ae-108c80c297d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A62CEDB2C0>]}
[0m14:55:08.056833 [info ] [MainThread]: 
[0m14:55:08.057841 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:55:08.058847 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:55:08.070997 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:55:08.096316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da627c82-1967-4c97-92ae-108c80c297d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A62D07A570>]}
[0m14:55:08.096316 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:55:08.096316 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:55:08.097328 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:55:08.097328 [info ] [MainThread]: 
[0m14:55:08.099349 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m14:55:08.100365 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:55:08.100365 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:55:08.101377 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:55:08.105425 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:55:08.106441 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:55:08.101377 => 14:55:08.106441
[0m14:55:08.106441 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:55:08.138830 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:55:08.138830 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:55:08.139842 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:55:08.139842 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:55:08.168223 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m14:55:08.194525 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:55:08.194525 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:55:08.195533 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:55:08.201631 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:55:08.209707 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:55:08.106441 => 14:55:08.209707
[0m14:55:08.210722 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da627c82-1967-4c97-92ae-108c80c297d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A62D00D0D0>]}
[0m14:55:08.210722 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.11s]
[0m14:55:08.211750 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:55:08.212761 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m14:55:08.212761 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:55:08.212761 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m14:55:08.213769 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:55:08.215793 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:55:08.216810 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:55:08.213769 => 14:55:08.216810
[0m14:55:08.216810 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:55:08.231560 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:55:08.232571 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:55:08.232571 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:55:08.233584 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:55:08.235596 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:55:08.216810 => 14:55:08.235596
[0m14:55:08.236098 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da627c82-1967-4c97-92ae-108c80c297d2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A62D1D19D0>]}
[0m14:55:08.236607 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m14:55:08.237116 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:55:08.238131 [debug] [MainThread]: On master: ROLLBACK
[0m14:55:08.238644 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:55:08.238644 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:55:08.239153 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:55:08.239153 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:55:08.239669 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:55:08.240179 [info ] [MainThread]: 
[0m14:55:08.240179 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m14:55:08.241203 [debug] [MainThread]: Command end result
[0m14:55:08.245809 [info ] [MainThread]: 
[0m14:55:08.246318 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:55:08.246828 [info ] [MainThread]: 
[0m14:55:08.247337 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:55:08.248367 [debug] [MainThread]: Command `cli run` succeeded at 14:55:08.248367 after 1.25 seconds
[0m14:55:08.248877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A62BBFFE60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A62BCCFEC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A62BCCC590>]}
[0m14:55:08.249385 [debug] [MainThread]: Flushing usage events
[0m15:05:09.640913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418F31520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418F301A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418F31B80>]}


============================== 15:05:09.641923 | 4896799f-1c04-43ac-98c3-4204f23ff43d ==============================
[0m15:05:09.641923 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:05:09.641923 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:05:09.744687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4896799f-1c04-43ac-98c3-4204f23ff43d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022419FE7950>]}
[0m15:05:09.798289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4896799f-1c04-43ac-98c3-4204f23ff43d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002241A0905F0>]}
[0m15:05:09.799296 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:05:09.811444 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:05:10.483164 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:05:10.483217 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:05:10.487257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4896799f-1c04-43ac-98c3-4204f23ff43d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002241A093BF0>]}
[0m15:05:10.502417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4896799f-1c04-43ac-98c3-4204f23ff43d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002241A14D070>]}
[0m15:05:10.503427 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:05:10.503427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4896799f-1c04-43ac-98c3-4204f23ff43d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418D7B4A0>]}
[0m15:05:10.504954 [info ] [MainThread]: 
[0m15:05:10.505461 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:05:10.506470 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:05:10.519695 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:05:10.548282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4896799f-1c04-43ac-98c3-4204f23ff43d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418EB7800>]}
[0m15:05:10.549297 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:05:10.550335 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:05:10.551351 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:05:10.552371 [info ] [MainThread]: 
[0m15:05:10.557476 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:05:10.557476 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:05:10.558519 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:05:10.559537 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:05:10.565636 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:05:10.566650 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:05:10.559537 => 15:05:10.566650
[0m15:05:10.567661 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:05:10.601120 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:05:10.601120 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:05:10.602129 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:05:10.602129 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:05:10.639224 [debug] [Thread-1 (]: SQL status: OK in 0.03999999910593033 seconds
[0m15:05:10.670647 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:05:10.670647 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:05:10.671657 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m15:05:10.682816 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:05:10.692965 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:05:10.567661 => 15:05:10.692965
[0m15:05:10.693973 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4896799f-1c04-43ac-98c3-4204f23ff43d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002241A0BEAE0>]}
[0m15:05:10.694983 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.14s]
[0m15:05:10.695998 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:05:10.695998 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:05:10.697013 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m15:05:10.698045 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:05:10.698045 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:05:10.700073 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:05:10.701089 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:05:10.698045 => 15:05:10.701089
[0m15:05:10.702101 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:05:10.715854 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:05:10.717888 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:05:10.717888 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m15:05:10.719925 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:05:10.721947 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:05:10.702101 => 15:05:10.721947
[0m15:05:10.722959 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4896799f-1c04-43ac-98c3-4204f23ff43d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002241A221F40>]}
[0m15:05:10.722959 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m15:05:10.723971 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:05:10.724981 [debug] [MainThread]: On master: ROLLBACK
[0m15:05:10.725993 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:05:10.725993 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:05:10.725993 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:05:10.725993 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:05:10.727007 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:05:10.727007 [info ] [MainThread]: 
[0m15:05:10.728019 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m15:05:10.729051 [debug] [MainThread]: Command end result
[0m15:05:10.735141 [info ] [MainThread]: 
[0m15:05:10.735654 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:05:10.736163 [info ] [MainThread]: 
[0m15:05:10.736163 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:05:10.737185 [debug] [MainThread]: Command `cli run` succeeded at 15:05:10.737185 after 1.15 seconds
[0m15:05:10.737698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418E442F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418E80860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418E80830>]}
[0m15:05:10.738229 [debug] [MainThread]: Flushing usage events
[0m15:07:15.701425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015246EE2BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015246EE30B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015246EE2C00>]}


============================== 15:07:15.701927 | 6ac5092d-d8f2-4259-8ef8-4c90e25f6066 ==============================
[0m15:07:15.701927 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:07:15.702935 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:07:15.801949 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6ac5092d-d8f2-4259-8ef8-4c90e25f6066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015246E30BF0>]}
[0m15:07:15.856669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6ac5092d-d8f2-4259-8ef8-4c90e25f6066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000152470C4230>]}
[0m15:07:15.857679 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:07:15.865772 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:07:15.907254 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:07:15.907254 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:07:15.911321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6ac5092d-d8f2-4259-8ef8-4c90e25f6066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015246F96ED0>]}
[0m15:07:15.919411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6ac5092d-d8f2-4259-8ef8-4c90e25f6066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001524712CA70>]}
[0m15:07:15.919411 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:07:15.920437 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6ac5092d-d8f2-4259-8ef8-4c90e25f6066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001524701B140>]}
[0m15:07:15.921448 [info ] [MainThread]: 
[0m15:07:15.922458 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:07:15.923466 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:07:15.928538 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:07:15.941734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6ac5092d-d8f2-4259-8ef8-4c90e25f6066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015246EB0F20>]}
[0m15:07:15.941734 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:07:15.942746 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:07:15.942746 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:07:15.942746 [info ] [MainThread]: 
[0m15:07:15.945782 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:07:15.945782 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:07:15.946795 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:07:15.946795 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:07:15.951865 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:07:15.953910 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:07:15.946795 => 15:07:15.953910
[0m15:07:15.953910 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:07:15.986895 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:07:15.986895 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:07:15.987919 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:07:15.987919 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:07:16.014234 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m15:07:16.037537 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:07:16.039559 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:07:16.039559 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m15:07:16.046628 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:07:16.056773 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:07:15.954928 => 15:07:16.055763
[0m15:07:16.056773 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6ac5092d-d8f2-4259-8ef8-4c90e25f6066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015246FF5F70>]}
[0m15:07:16.057789 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.11s]
[0m15:07:16.058798 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:07:16.058798 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:07:16.059810 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m15:07:16.060823 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:07:16.060823 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:07:16.062844 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:07:16.063872 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:07:16.060823 => 15:07:16.063872
[0m15:07:16.063872 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:07:16.075972 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:07:16.076981 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:07:16.076981 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m15:07:16.079024 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:07:16.080037 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:07:16.063872 => 15:07:16.080037
[0m15:07:16.081052 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6ac5092d-d8f2-4259-8ef8-4c90e25f6066', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000152472956A0>]}
[0m15:07:16.082069 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m15:07:16.082069 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:07:16.084099 [debug] [MainThread]: On master: ROLLBACK
[0m15:07:16.084099 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:07:16.085110 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:07:16.085110 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:07:16.085110 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:07:16.086124 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:07:16.086124 [info ] [MainThread]: 
[0m15:07:16.087134 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m15:07:16.087134 [debug] [MainThread]: Command end result
[0m15:07:16.093727 [info ] [MainThread]: 
[0m15:07:16.094238 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:07:16.094750 [info ] [MainThread]: 
[0m15:07:16.094750 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:07:16.096282 [debug] [MainThread]: Command `cli run` succeeded at 15:07:16.095771 after 0.42 seconds
[0m15:07:16.096282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015246EE30B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015245C18D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015247064320>]}
[0m15:07:16.096793 [debug] [MainThread]: Flushing usage events
[0m15:12:42.434678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE02871EE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE02873D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE028732F0>]}


============================== 15:12:42.434678 | 1ba9a0fd-8156-495f-a38d-5541c05871c9 ==============================
[0m15:12:42.434678 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:12:42.435688 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:12:42.542484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1ba9a0fd-8156-495f-a38d-5541c05871c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE028B2660>]}
[0m15:12:42.598136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1ba9a0fd-8156-495f-a38d-5541c05871c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE02A4E1E0>]}
[0m15:12:42.599150 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:12:42.606242 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:12:42.643723 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:12:42.643723 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:12:42.647766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1ba9a0fd-8156-495f-a38d-5541c05871c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE02983440>]}
[0m15:12:42.653844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1ba9a0fd-8156-495f-a38d-5541c05871c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE02AB0E90>]}
[0m15:12:42.654854 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:12:42.654854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ba9a0fd-8156-495f-a38d-5541c05871c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE0283F380>]}
[0m15:12:42.655867 [info ] [MainThread]: 
[0m15:12:42.656878 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:12:42.657905 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:12:42.661933 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:12:42.674068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ba9a0fd-8156-495f-a38d-5541c05871c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE02B058B0>]}
[0m15:12:42.675077 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:12:42.675077 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:12:42.676086 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:12:42.676086 [info ] [MainThread]: 
[0m15:12:42.678103 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:12:42.678103 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:12:42.679110 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:12:42.679110 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:12:42.684169 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:12:42.686189 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:12:42.680121 => 15:12:42.686189
[0m15:12:42.686189 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:12:42.716496 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:12:42.717507 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:12:42.717507 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:12:42.717507 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:12:42.740366 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m15:12:42.758612 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:12:42.759619 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:12:42.759619 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m15:12:42.769214 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:12:42.777281 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:12:42.686189 => 15:12:42.777281
[0m15:12:42.777281 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ba9a0fd-8156-495f-a38d-5541c05871c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE02A783E0>]}
[0m15:12:42.778287 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.10s]
[0m15:12:42.779302 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:12:42.779302 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:12:42.780338 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m15:12:42.780338 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:12:42.781351 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:12:42.783370 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:12:42.783370 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:12:42.781351 => 15:12:42.783370
[0m15:12:42.784381 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:12:42.796567 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:12:42.797576 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:12:42.798585 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m15:12:42.799600 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:12:42.800627 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:12:42.784381 => 15:12:42.800627
[0m15:12:42.801134 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ba9a0fd-8156-495f-a38d-5541c05871c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE02C23F80>]}
[0m15:12:42.801641 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m15:12:42.802148 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:12:42.803167 [debug] [MainThread]: On master: ROLLBACK
[0m15:12:42.803167 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:12:42.803677 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:12:42.803677 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:12:42.804189 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:12:42.804189 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:12:42.804701 [info ] [MainThread]: 
[0m15:12:42.805213 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m15:12:42.805725 [debug] [MainThread]: Command end result
[0m15:12:42.809811 [info ] [MainThread]: 
[0m15:12:42.810833 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:12:42.810833 [info ] [MainThread]: 
[0m15:12:42.810833 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:12:42.811843 [debug] [MainThread]: Command `cli run` succeeded at 15:12:42.811843 after 0.40 seconds
[0m15:12:42.812855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE0171FC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE028732F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FE02873D40>]}
[0m15:12:42.812855 [debug] [MainThread]: Flushing usage events
[0m15:14:17.170957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D7660260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D7660D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D76605C0>]}


============================== 15:14:17.172974 | 0074a90e-3545-4d4a-907e-2c9d04916904 ==============================
[0m15:14:17.172974 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:14:17.174990 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m15:14:17.500976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0074a90e-3545-4d4a-907e-2c9d04916904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D76606B0>]}
[0m15:14:17.678146 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0074a90e-3545-4d4a-907e-2c9d04916904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D7847560>]}
[0m15:14:17.684231 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:14:17.711516 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:14:17.800859 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:14:17.801867 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:14:17.821227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0074a90e-3545-4d4a-907e-2c9d04916904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D77D3E00>]}
[0m15:14:17.835887 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0074a90e-3545-4d4a-907e-2c9d04916904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D791AFF0>]}
[0m15:14:17.837918 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:14:17.839445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0074a90e-3545-4d4a-907e-2c9d04916904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D62C1A00>]}
[0m15:14:17.844522 [info ] [MainThread]: 
[0m15:14:17.847558 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:14:17.853728 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:14:17.903286 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:14:17.969539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0074a90e-3545-4d4a-907e-2c9d04916904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D7847A40>]}
[0m15:14:17.971558 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:14:17.972576 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:14:17.975601 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:14:17.976609 [info ] [MainThread]: 
[0m15:14:17.985702 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m15:14:17.987730 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:14:17.990767 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:14:17.992784 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:14:18.031409 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:14:18.035479 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:14:17.993792 => 15:14:18.034465
[0m15:14:18.037501 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:14:18.176320 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:14:18.178338 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:14:18.179348 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:14:18.181380 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m15:14:18.237174 [debug] [Thread-6 (]: SQL status: OK in 0.05000000074505806 seconds
[0m15:14:18.324788 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:14:18.326808 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:14:18.327822 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m15:14:18.338939 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:14:18.374348 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:14:18.038515 => 15:14:18.374348
[0m15:14:18.377375 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0074a90e-3545-4d4a-907e-2c9d04916904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D76BA750>]}
[0m15:14:18.379397 [info ] [Thread-6 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.39s]
[0m15:14:18.381421 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:14:18.384486 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m15:14:18.386521 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m15:14:18.389554 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:14:18.391584 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:14:18.399664 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:14:18.402696 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:14:18.392594 => 15:14:18.401687
[0m15:14:18.403706 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:14:18.456410 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:14:18.459448 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:14:18.461473 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m15:14:18.466527 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m15:14:18.471570 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:14:18.404718 => 15:14:18.471570
[0m15:14:18.474600 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0074a90e-3545-4d4a-907e-2c9d04916904', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D7A467B0>]}
[0m15:14:18.476616 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.09s]
[0m15:14:18.477625 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:14:18.481669 [debug] [MainThread]: On master: ROLLBACK
[0m15:14:18.483695 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:14:18.485722 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:14:18.486734 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:14:18.488754 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:14:18.489783 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:14:18.492812 [info ] [MainThread]: 
[0m15:14:18.493819 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.65 seconds (0.65s).
[0m15:14:18.495840 [debug] [MainThread]: Command end result
[0m15:14:18.512085 [info ] [MainThread]: 
[0m15:14:18.513099 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:14:18.515123 [info ] [MainThread]: 
[0m15:14:18.517149 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:14:18.521237 [debug] [MainThread]: Command `cli run` succeeded at 15:14:18.520221 after 1.44 seconds
[0m15:14:18.523276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D76BABD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D7A464E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B8D7A46660>]}
[0m15:14:18.525320 [debug] [MainThread]: Flushing usage events
[0m15:15:37.756064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA27610260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA27611010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA26F2AC30>]}


============================== 15:15:37.758079 | 407d4453-1545-4790-b2de-dc010837601d ==============================
[0m15:15:37.758079 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:15:37.760097 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:15:38.098292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '407d4453-1545-4790-b2de-dc010837601d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA276D3F80>]}
[0m15:15:38.273207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '407d4453-1545-4790-b2de-dc010837601d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA277173B0>]}
[0m15:15:38.275226 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:15:38.303542 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:15:38.399716 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:15:38.400724 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:15:38.419994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '407d4453-1545-4790-b2de-dc010837601d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA270E51C0>]}
[0m15:15:38.435159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '407d4453-1545-4790-b2de-dc010837601d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA278CB530>]}
[0m15:15:38.436172 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:15:38.438201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '407d4453-1545-4790-b2de-dc010837601d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA278CAB10>]}
[0m15:15:38.444260 [info ] [MainThread]: 
[0m15:15:38.446282 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:15:38.452394 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:15:38.499987 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:15:38.574234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '407d4453-1545-4790-b2de-dc010837601d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA278C96D0>]}
[0m15:15:38.575245 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:15:38.577272 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:15:38.579301 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:15:38.581328 [info ] [MainThread]: 
[0m15:15:38.590477 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m15:15:38.592503 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:15:38.594530 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:15:38.596558 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:15:38.635401 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:15:38.637425 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:15:38.597569 => 15:15:38.636414
[0m15:15:38.639478 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:15:38.775747 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:15:38.777765 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:15:38.778776 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:15:38.780796 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m15:15:38.837790 [debug] [Thread-6 (]: SQL status: OK in 0.05999999865889549 seconds
[0m15:15:38.927086 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:15:38.929601 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:15:38.930611 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m15:15:38.945799 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:15:38.985458 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:15:38.641528 => 15:15:38.985458
[0m15:15:38.988506 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '407d4453-1545-4790-b2de-dc010837601d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA27783590>]}
[0m15:15:38.991540 [info ] [Thread-6 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.39s]
[0m15:15:38.995707 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:15:38.998819 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m15:15:39.000841 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m15:15:39.003881 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:15:39.005911 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:15:39.015105 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:15:39.020231 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:15:39.006926 => 15:15:39.019208
[0m15:15:39.022267 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:15:39.077334 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:15:39.079350 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:15:39.081376 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m15:15:39.086461 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m15:15:39.093550 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:15:39.023293 => 15:15:39.092539
[0m15:15:39.095574 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '407d4453-1545-4790-b2de-dc010837601d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA27B16AB0>]}
[0m15:15:39.097589 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.09s]
[0m15:15:39.099605 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:15:39.102657 [debug] [MainThread]: On master: ROLLBACK
[0m15:15:39.104699 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:15:39.105711 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:15:39.107734 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:15:39.108745 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:15:39.109758 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:15:39.111782 [info ] [MainThread]: 
[0m15:15:39.113809 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.67 seconds (0.67s).
[0m15:15:39.116860 [debug] [MainThread]: Command end result
[0m15:15:39.133714 [info ] [MainThread]: 
[0m15:15:39.135766 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:15:39.138822 [info ] [MainThread]: 
[0m15:15:39.140852 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:15:39.146077 [debug] [MainThread]: Command `cli run` succeeded at 15:15:39.145034 after 1.47 seconds
[0m15:15:39.148112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA27072420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA275632F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA27AB9C10>]}
[0m15:15:39.150243 [debug] [MainThread]: Flushing usage events
[0m15:24:40.691971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D734C80350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D731F476E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D734B6E1E0>]}


============================== 15:24:40.693997 | c8c51475-e59b-47bb-8980-27d5c29cb68d ==============================
[0m15:24:40.693997 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:24:40.696040 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:24:41.033185 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c8c51475-e59b-47bb-8980-27d5c29cb68d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D734C3F020>]}
[0m15:24:41.207078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c8c51475-e59b-47bb-8980-27d5c29cb68d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D734E668A0>]}
[0m15:24:41.210140 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:24:41.242794 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:24:41.340985 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:24:41.343021 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:24:41.364995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c8c51475-e59b-47bb-8980-27d5c29cb68d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D7350573E0>]}
[0m15:24:41.380183 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c8c51475-e59b-47bb-8980-27d5c29cb68d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D734F42C60>]}
[0m15:24:41.382231 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:24:41.384299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8c51475-e59b-47bb-8980-27d5c29cb68d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D734CB0DD0>]}
[0m15:24:41.390399 [info ] [MainThread]: 
[0m15:24:41.393454 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:24:41.400620 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:24:41.451682 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:24:41.520719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8c51475-e59b-47bb-8980-27d5c29cb68d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D7341703B0>]}
[0m15:24:41.522746 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:24:41.523762 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:24:41.526818 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:24:41.527831 [info ] [MainThread]: 
[0m15:24:41.534941 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m15:24:41.536973 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:24:41.542085 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:24:41.544121 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:24:41.583653 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:24:41.587743 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:24:41.545139 => 15:24:41.586721
[0m15:24:41.589795 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:24:41.722728 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:24:41.723735 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:24:41.725751 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:24:41.726761 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m15:24:41.774409 [debug] [Thread-6 (]: SQL status: OK in 0.05000000074505806 seconds
[0m15:24:41.862925 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:24:41.864944 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:24:41.865953 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m15:24:41.878098 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:24:41.917955 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:24:41.590818 => 15:24:41.916924
[0m15:24:41.921028 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8c51475-e59b-47bb-8980-27d5c29cb68d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D7350C4110>]}
[0m15:24:41.923058 [info ] [Thread-6 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.38s]
[0m15:24:41.926093 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:24:41.928122 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m15:24:41.929153 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m15:24:41.932191 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:24:41.933198 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:24:41.943351 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:24:41.945395 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:24:41.934208 => 15:24:41.944383
[0m15:24:41.947421 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:24:42.004142 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:24:42.007202 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:24:42.009229 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m15:24:42.013279 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m15:24:42.020387 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:24:41.948440 => 15:24:42.019376
[0m15:24:42.022425 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8c51475-e59b-47bb-8980-27d5c29cb68d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D7350C52B0>]}
[0m15:24:42.025482 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.09s]
[0m15:24:42.027539 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:24:42.030571 [debug] [MainThread]: On master: ROLLBACK
[0m15:24:42.031586 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:24:42.033613 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:24:42.034625 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:24:42.035637 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:24:42.037690 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:24:42.039731 [info ] [MainThread]: 
[0m15:24:42.041771 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.65 seconds (0.65s).
[0m15:24:42.045954 [debug] [MainThread]: Command end result
[0m15:24:42.062361 [info ] [MainThread]: 
[0m15:24:42.065453 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:24:42.067520 [info ] [MainThread]: 
[0m15:24:42.069550 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:24:42.072608 [debug] [MainThread]: Command `cli run` succeeded at 15:24:42.071587 after 1.47 seconds
[0m15:24:42.075662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D734936600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D734CB1AC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D7350C4B30>]}
[0m15:24:42.077735 [debug] [MainThread]: Flushing usage events
[0m15:25:46.903448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A153D0B00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A153D1E80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A153D29F0>]}


============================== 15:25:46.905470 | 2c62e5d6-6587-4316-8702-ab0dc3af6153 ==============================
[0m15:25:46.905470 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:25:46.908526 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:25:47.255849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2c62e5d6-6587-4316-8702-ab0dc3af6153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A154D8B30>]}
[0m15:25:47.426739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2c62e5d6-6587-4316-8702-ab0dc3af6153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A12AFD370>]}
[0m15:25:47.429763 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:25:47.458095 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:25:47.548842 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:25:47.550863 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:25:47.571116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2c62e5d6-6587-4316-8702-ab0dc3af6153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A15515610>]}
[0m15:25:47.587306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2c62e5d6-6587-4316-8702-ab0dc3af6153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A1568F380>]}
[0m15:25:47.588316 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:25:47.590339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c62e5d6-6587-4316-8702-ab0dc3af6153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A151012E0>]}
[0m15:25:47.595404 [info ] [MainThread]: 
[0m15:25:47.598431 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:25:47.602477 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:25:47.653480 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:25:47.721810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c62e5d6-6587-4316-8702-ab0dc3af6153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A1531FE60>]}
[0m15:25:47.723829 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:25:47.725856 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:25:47.728896 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:25:47.729938 [info ] [MainThread]: 
[0m15:25:47.739077 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m15:25:47.740084 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:25:47.743115 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:25:47.745161 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:25:47.782044 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:25:47.785213 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:25:47.746173 => 15:25:47.784200
[0m15:25:47.787241 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:25:47.925901 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:25:47.927929 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:25:47.930004 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:25:47.931015 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m15:25:47.985923 [debug] [Thread-6 (]: SQL status: OK in 0.05000000074505806 seconds
[0m15:25:48.079688 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:25:48.082811 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:25:48.084840 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m15:25:48.099037 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:25:48.143403 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:25:47.789275 => 15:25:48.143403
[0m15:25:48.146426 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c62e5d6-6587-4316-8702-ab0dc3af6153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A15870D70>]}
[0m15:25:48.148446 [info ] [Thread-6 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.40s]
[0m15:25:48.152603 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:25:48.156680 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m15:25:48.157689 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m15:25:48.160741 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:25:48.161753 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:25:48.171874 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:25:48.173892 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:25:48.162763 => 15:25:48.172883
[0m15:25:48.174915 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:25:48.231977 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:25:48.234001 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:25:48.236061 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m15:25:48.242148 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m15:25:48.248210 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:25:48.175925 => 15:25:48.247201
[0m15:25:48.252296 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c62e5d6-6587-4316-8702-ab0dc3af6153', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A1587D280>]}
[0m15:25:48.255346 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.09s]
[0m15:25:48.257378 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:25:48.260410 [debug] [MainThread]: On master: ROLLBACK
[0m15:25:48.262433 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:25:48.263446 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:25:48.264457 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:25:48.266499 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:25:48.267532 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:25:48.270580 [info ] [MainThread]: 
[0m15:25:48.272612 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.67 seconds (0.67s).
[0m15:25:48.275649 [debug] [MainThread]: Command end result
[0m15:25:48.291981 [info ] [MainThread]: 
[0m15:25:48.295037 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:25:48.297079 [info ] [MainThread]: 
[0m15:25:48.298092 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:25:48.303199 [debug] [MainThread]: Command `cli run` succeeded at 15:25:48.302157 after 1.49 seconds
[0m15:25:48.305227 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A151AEBD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A15385D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020A15493260>]}
[0m15:25:48.308263 [debug] [MainThread]: Flushing usage events
[0m15:30:19.451943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001498CD03440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001498C8F2FF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001498CD01670>]}


============================== 15:30:19.455002 | 2304d174-96ac-454f-a50c-1137b7f8d558 ==============================
[0m15:30:19.455002 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:30:19.456031 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:30:19.821163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2304d174-96ac-454f-a50c-1137b7f8d558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001498CDF98E0>]}
[0m15:30:19.994120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2304d174-96ac-454f-a50c-1137b7f8d558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001498CF0CB60>]}
[0m15:30:19.996635 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:30:20.031717 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:30:20.771615 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:30:20.773645 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:30:20.793006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2304d174-96ac-454f-a50c-1137b7f8d558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001498CE6EAE0>]}
[0m15:30:20.807700 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2304d174-96ac-454f-a50c-1137b7f8d558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001498CFC6810>]}
[0m15:30:20.809725 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:30:20.810740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2304d174-96ac-454f-a50c-1137b7f8d558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001498CFC6000>]}
[0m15:30:20.817910 [info ] [MainThread]: 
[0m15:30:20.820427 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:30:20.827604 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:30:20.881624 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:30:20.957742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2304d174-96ac-454f-a50c-1137b7f8d558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001498CADE870>]}
[0m15:30:20.959766 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:30:20.961800 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:30:20.963827 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:30:20.965351 [info ] [MainThread]: 
[0m15:30:20.971959 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m15:30:20.972970 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:30:20.976189 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:30:20.977205 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:30:21.016564 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:30:21.020678 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:30:20.978219 => 15:30:21.019112
[0m15:30:21.022748 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:30:21.162522 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:30:21.163532 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:30:21.165592 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:30:21.166603 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m15:30:21.223084 [debug] [Thread-6 (]: SQL status: OK in 0.05999999865889549 seconds
[0m15:30:21.318532 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:30:21.321066 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:30:21.322079 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m15:30:21.332712 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:30:21.373004 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:30:21.023776 => 15:30:21.373004
[0m15:30:21.375517 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2304d174-96ac-454f-a50c-1137b7f8d558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001498CC818B0>]}
[0m15:30:21.377541 [info ] [Thread-6 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.40s]
[0m15:30:21.380610 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:30:21.383679 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m15:30:21.385779 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m15:30:21.388844 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:30:21.390862 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:30:21.400036 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:30:21.403106 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:30:21.391872 => 15:30:21.402094
[0m15:30:21.404116 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:30:21.461770 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:30:21.465335 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:30:21.466871 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m15:30:21.471956 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m15:30:21.477495 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:30:21.405118 => 15:30:21.477495
[0m15:30:21.480569 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2304d174-96ac-454f-a50c-1137b7f8d558', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001498D1A8B30>]}
[0m15:30:21.482599 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.09s]
[0m15:30:21.485669 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:30:21.489736 [debug] [MainThread]: On master: ROLLBACK
[0m15:30:21.490747 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:30:21.492772 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:30:21.493785 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:30:21.495844 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:30:21.496859 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:30:21.499942 [info ] [MainThread]: 
[0m15:30:21.503022 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.68 seconds (0.68s).
[0m15:30:21.506565 [debug] [MainThread]: Command end result
[0m15:30:21.523962 [info ] [MainThread]: 
[0m15:30:21.527507 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:30:21.528525 [info ] [MainThread]: 
[0m15:30:21.530565 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:30:21.534631 [debug] [MainThread]: Command `cli run` succeeded at 15:30:21.533613 after 2.20 seconds
[0m15:30:21.537745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001498CADC980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001498CBECDD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001498CC826C0>]}
[0m15:30:21.541984 [debug] [MainThread]: Flushing usage events
[0m18:01:41.062260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002704501F290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002704501F440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002704501F140>]}


============================== 18:01:41.062260 | 1d4b249f-79c3-499e-9a76-5f6680f6e309 ==============================
[0m18:01:41.062260 [info ] [MainThread]: Running with dbt=1.7.4
[0m18:01:41.063268 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m18:01:41.173481 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1d4b249f-79c3-499e-9a76-5f6680f6e309', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027043618230>]}
[0m18:01:41.224956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1d4b249f-79c3-499e-9a76-5f6680f6e309', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027046204E30>]}
[0m18:01:41.225963 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:01:41.240124 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m18:01:41.927840 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:01:41.928848 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:01:41.931871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1d4b249f-79c3-499e-9a76-5f6680f6e309', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270461D9460>]}
[0m18:01:41.947042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1d4b249f-79c3-499e-9a76-5f6680f6e309', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002704625CDD0>]}
[0m18:01:41.947042 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m18:01:41.947042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1d4b249f-79c3-499e-9a76-5f6680f6e309', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027046133CE0>]}
[0m18:01:41.949064 [info ] [MainThread]: 
[0m18:01:41.949064 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:01:41.950087 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:01:41.957153 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:01:41.976345 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1d4b249f-79c3-499e-9a76-5f6680f6e309', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002704501F740>]}
[0m18:01:41.976345 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:01:41.976345 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:01:41.977353 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:01:41.977353 [info ] [MainThread]: 
[0m18:01:41.980390 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:01:41.980390 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:01:41.981399 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:01:41.981399 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:01:41.985428 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:01:41.986435 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:01:41.981399 => 18:01:41.985428
[0m18:01:41.986435 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:01:42.015761 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:01:42.015761 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:01:42.016771 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:01:42.016771 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:01:42.046184 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m18:01:42.071419 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:01:42.073445 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:01:42.074465 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:01:42.081530 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:01:42.090622 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:01:41.986435 => 18:01:42.090622
[0m18:01:42.090622 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d4b249f-79c3-499e-9a76-5f6680f6e309', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270461D8D40>]}
[0m18:01:42.091630 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.11s]
[0m18:01:42.092637 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:01:42.092637 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:01:42.093646 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m18:01:42.093646 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m18:01:42.094654 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:01:42.095664 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:01:42.096682 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:01:42.094654 => 18:01:42.096682
[0m18:01:42.097691 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:01:42.109821 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:01:42.110828 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:01:42.111837 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m18:01:42.112846 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:01:42.114866 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:01:42.097691 => 18:01:42.114866
[0m18:01:42.114866 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d4b249f-79c3-499e-9a76-5f6680f6e309', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027046343710>]}
[0m18:01:42.115875 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m18:01:42.116898 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:01:42.116898 [debug] [MainThread]: On master: ROLLBACK
[0m18:01:42.117912 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:01:42.117912 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:01:42.118923 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:01:42.118923 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:01:42.118923 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:01:42.119938 [info ] [MainThread]: 
[0m18:01:42.119938 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m18:01:42.120951 [debug] [MainThread]: Command end result
[0m18:01:42.126015 [info ] [MainThread]: 
[0m18:01:42.126015 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:01:42.127023 [info ] [MainThread]: 
[0m18:01:42.127023 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m18:01:42.129050 [debug] [MainThread]: Command `cli run` succeeded at 18:01:42.128033 after 1.10 seconds
[0m18:01:42.129050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027044C3CAA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027044F5E840>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270445B18B0>]}
[0m18:01:42.129050 [debug] [MainThread]: Flushing usage events
[0m13:34:35.739369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AFF4D6180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AFF4D5790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AFF4D5610>]}


============================== 13:34:35.742997 | b0ba5bcb-2420-40a0-a6a5-144384ba00d4 ==============================
[0m13:34:35.742997 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:34:35.745079 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:34:36.203093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b0ba5bcb-2420-40a0-a6a5-144384ba00d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AFF60F0E0>]}
[0m13:34:36.433266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b0ba5bcb-2420-40a0-a6a5-144384ba00d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AFF6E2C60>]}
[0m13:34:36.437388 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:34:36.485144 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:34:38.424269 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:34:38.425814 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:34:38.451125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b0ba5bcb-2420-40a0-a6a5-144384ba00d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AFF6E3530>]}
[0m13:34:38.474349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b0ba5bcb-2420-40a0-a6a5-144384ba00d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AFF7927E0>]}
[0m13:34:38.476416 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:34:38.478478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b0ba5bcb-2420-40a0-a6a5-144384ba00d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AFF5BEF60>]}
[0m13:34:38.488421 [info ] [MainThread]: 
[0m13:34:38.492634 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:34:38.499303 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:34:38.574296 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:34:38.667617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b0ba5bcb-2420-40a0-a6a5-144384ba00d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AFF7F1610>]}
[0m13:34:38.670711 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:34:38.672762 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:34:38.675836 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:34:38.679018 [info ] [MainThread]: 
[0m13:34:38.691372 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m13:34:38.697339 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:34:38.701511 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:34:38.704583 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:34:38.745192 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:34:38.749779 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:34:38.705605 => 13:34:38.749265
[0m13:34:38.752368 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:34:38.911870 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:34:38.914943 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:34:38.918075 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:34:38.921182 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m13:34:39.043764 [debug] [Thread-6 (]: SQL status: OK in 0.11999999731779099 seconds
[0m13:34:39.173988 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m13:34:39.178607 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:34:39.180141 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m13:34:39.209712 [debug] [Thread-6 (]: SQL status: OK in 0.029999999329447746 seconds
[0m13:34:39.263030 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 13:34:38.755027 => 13:34:39.262008
[0m13:34:39.267644 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0ba5bcb-2420-40a0-a6a5-144384ba00d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AFF8BAAE0>]}
[0m13:34:39.270209 [info ] [Thread-6 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.57s]
[0m13:34:39.277232 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m13:34:39.281372 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m13:34:39.284462 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m13:34:39.289150 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m13:34:39.293371 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m13:34:39.308961 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m13:34:39.315224 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 13:34:39.295457 => 13:34:39.313128
[0m13:34:39.317803 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m13:34:39.396047 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m13:34:39.398076 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m13:34:39.400103 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m13:34:39.407404 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m13:34:39.415532 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 13:34:39.319335 => 13:34:39.414502
[0m13:34:39.421910 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b0ba5bcb-2420-40a0-a6a5-144384ba00d4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AFFB53B00>]}
[0m13:34:39.425022 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.13s]
[0m13:34:39.428116 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m13:34:39.433276 [debug] [MainThread]: On master: ROLLBACK
[0m13:34:39.435322 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:34:39.437388 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m13:34:39.438435 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:34:39.440493 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:34:39.442530 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:34:39.445057 [info ] [MainThread]: 
[0m13:34:39.448130 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.95 seconds (0.95s).
[0m13:34:39.451279 [debug] [MainThread]: Command end result
[0m13:34:39.470464 [info ] [MainThread]: 
[0m13:34:39.473590 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:34:39.476166 [info ] [MainThread]: 
[0m13:34:39.477705 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:34:39.482380 [debug] [MainThread]: Command `cli run` succeeded at 13:34:39.481360 after 3.89 seconds
[0m13:34:39.485041 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AFF200380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AFF756090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016AFFB53B00>]}
[0m13:34:39.488815 [debug] [MainThread]: Flushing usage events
[0m13:48:09.409307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002613CC94A40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002613CC94440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002613CC941A0>]}


============================== 13:48:09.415036 | ad251777-2590-454e-aef5-45e8e0700414 ==============================
[0m13:48:09.415036 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:48:09.419782 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m13:48:09.871198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ad251777-2590-454e-aef5-45e8e0700414', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002613CDCFB90>]}
[0m13:48:10.092557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ad251777-2590-454e-aef5-45e8e0700414', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026139F37C50>]}
[0m13:48:10.097698 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:48:10.154165 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:48:12.125668 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:48:12.127242 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:48:12.155295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ad251777-2590-454e-aef5-45e8e0700414', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002613BCB5070>]}
[0m13:48:12.178243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ad251777-2590-454e-aef5-45e8e0700414', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002613CEF1F70>]}
[0m13:48:12.180298 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:48:12.182850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ad251777-2590-454e-aef5-45e8e0700414', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002613C581F70>]}
[0m13:48:12.192754 [info ] [MainThread]: 
[0m13:48:12.196369 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:48:12.206789 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:48:12.273692 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:48:12.374753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ad251777-2590-454e-aef5-45e8e0700414', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002613CF5BDA0>]}
[0m13:48:12.377846 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:48:12.379392 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:48:12.382982 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:48:12.385088 [info ] [MainThread]: 
[0m13:48:12.398641 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m13:48:12.401213 [info ] [Thread-6 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:48:12.408087 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:48:12.411699 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:48:12.459801 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:48:12.466024 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:48:12.412727 => 13:48:12.464465
[0m13:48:12.469153 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:48:12.668702 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:48:12.671326 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:48:12.673967 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:48:12.676224 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m13:48:12.789759 [debug] [Thread-6 (]: SQL status: OK in 0.10999999940395355 seconds
[0m13:48:12.928698 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m13:48:12.935084 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:48:12.938356 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m13:48:12.965967 [debug] [Thread-6 (]: SQL status: OK in 0.019999999552965164 seconds
[0m13:48:13.019052 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 13:48:12.470706 => 13:48:13.018533
[0m13:48:13.023824 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad251777-2590-454e-aef5-45e8e0700414', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002613D0D8410>]}
[0m13:48:13.029687 [info ] [Thread-6 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.62s]
[0m13:48:13.033867 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m13:48:13.037535 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m13:48:13.040646 [info ] [Thread-6 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m13:48:13.044349 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m13:48:13.048633 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m13:48:13.061889 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m13:48:13.066046 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 13:48:13.051355 => 13:48:13.064475
[0m13:48:13.068657 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m13:48:13.137659 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m13:48:13.141275 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m13:48:13.143324 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m13:48:13.154160 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m13:48:13.161928 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 13:48:13.070229 => 13:48:13.160897
[0m13:48:13.164560 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad251777-2590-454e-aef5-45e8e0700414', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002613D198DD0>]}
[0m13:48:13.167116 [info ] [Thread-6 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.12s]
[0m13:48:13.170227 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m13:48:13.175945 [debug] [MainThread]: On master: ROLLBACK
[0m13:48:13.178537 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:48:13.180148 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m13:48:13.182200 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:48:13.183220 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:48:13.184746 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:48:13.188917 [info ] [MainThread]: 
[0m13:48:13.191523 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.99 seconds (0.99s).
[0m13:48:13.194102 [debug] [MainThread]: Command end result
[0m13:48:13.211879 [info ] [MainThread]: 
[0m13:48:13.214984 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:48:13.217577 [info ] [MainThread]: 
[0m13:48:13.219642 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:48:13.223759 [debug] [MainThread]: Command `cli run` succeeded at 13:48:13.222730 after 4.00 seconds
[0m13:48:13.225302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002613CA2F6E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002613CA2F4A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002613CECC440>]}
[0m13:48:13.226903 [debug] [MainThread]: Flushing usage events
[0m13:53:33.715555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD5AA1E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD5AA3B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD5AA2480>]}


============================== 13:53:33.717659 | 3d5247a0-20cc-4de4-b5e3-01be4caa1317 ==============================
[0m13:53:33.717659 [info ] [MainThread]: Running with dbt=1.7.4
[0m13:53:33.718735 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:53:33.881116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3d5247a0-20cc-4de4-b5e3-01be4caa1317', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD5AA1C10>]}
[0m13:53:33.948017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3d5247a0-20cc-4de4-b5e3-01be4caa1317', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD6C56930>]}
[0m13:53:33.951102 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:53:33.964620 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m13:53:34.021778 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:53:34.022288 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:53:34.027400 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3d5247a0-20cc-4de4-b5e3-01be4caa1317', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD6C57DA0>]}
[0m13:53:34.037294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3d5247a0-20cc-4de4-b5e3-01be4caa1317', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD6CBCE90>]}
[0m13:53:34.038850 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:53:34.039360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d5247a0-20cc-4de4-b5e3-01be4caa1317', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD6AD3200>]}
[0m13:53:34.041956 [info ] [MainThread]: 
[0m13:53:34.043512 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:53:34.045051 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:53:34.050762 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:53:34.065574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d5247a0-20cc-4de4-b5e3-01be4caa1317', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD6CBEB70>]}
[0m13:53:34.067614 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:53:34.068632 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:53:34.069679 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:53:34.069679 [info ] [MainThread]: 
[0m13:53:34.072738 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m13:53:34.072738 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:53:34.073241 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:53:34.074252 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:53:34.079397 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:53:34.081429 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:53:34.074252 => 13:53:34.080416
[0m13:53:34.081429 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:53:34.122104 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:53:34.123635 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:53:34.123635 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:53:34.124179 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:53:34.163702 [debug] [Thread-1 (]: SQL status: OK in 0.03999999910593033 seconds
[0m13:53:34.187926 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m13:53:34.190511 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:53:34.191027 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m13:53:34.206067 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m13:53:34.217469 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 13:53:34.081429 => 13:53:34.217469
[0m13:53:34.219544 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d5247a0-20cc-4de4-b5e3-01be4caa1317', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD6BAA570>]}
[0m13:53:34.220583 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.15s]
[0m13:53:34.221620 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m13:53:34.223154 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m13:53:34.223663 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m13:53:34.224172 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m13:53:34.224682 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m13:53:34.226718 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m13:53:34.227742 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 13:53:34.225188 => 13:53:34.227742
[0m13:53:34.228254 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m13:53:34.247206 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m13:53:34.250312 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m13:53:34.250825 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m13:53:34.253945 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:53:34.256517 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 13:53:34.228765 => 13:53:34.256009
[0m13:53:34.257537 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d5247a0-20cc-4de4-b5e3-01be4caa1317', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD6E296A0>]}
[0m13:53:34.258048 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m13:53:34.259066 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m13:53:34.260599 [debug] [MainThread]: On master: ROLLBACK
[0m13:53:34.261119 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:53:34.261636 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m13:53:34.261636 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:53:34.262162 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:53:34.262679 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:53:34.263795 [info ] [MainThread]: 
[0m13:53:34.264842 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m13:53:34.265372 [debug] [MainThread]: Command end result
[0m13:53:34.276164 [info ] [MainThread]: 
[0m13:53:34.277186 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:53:34.277699 [info ] [MainThread]: 
[0m13:53:34.278805 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:53:34.280384 [debug] [MainThread]: Command `cli run` succeeded at 13:53:34.280384 after 0.60 seconds
[0m13:53:34.280897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD59A76B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD59ECD40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EDD59ED160>]}
[0m13:53:34.281927 [debug] [MainThread]: Flushing usage events
[0m14:00:54.466288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028916130200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028916131FA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028916131700>]}


============================== 14:00:54.467292 | a67bcf36-75a6-4c21-bc73-aa7d100c8365 ==============================
[0m14:00:54.467292 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:00:54.467807 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:00:54.607990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a67bcf36-75a6-4c21-bc73-aa7d100c8365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028915E6A780>]}
[0m14:00:54.671900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a67bcf36-75a6-4c21-bc73-aa7d100c8365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028916131070>]}
[0m14:00:54.675604 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:00:54.717064 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:00:56.581799 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:00:56.582832 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:00:56.588440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a67bcf36-75a6-4c21-bc73-aa7d100c8365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028917303140>]}
[0m14:00:56.610647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a67bcf36-75a6-4c21-bc73-aa7d100c8365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002891733A030>]}
[0m14:00:56.611668 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:00:56.612693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a67bcf36-75a6-4c21-bc73-aa7d100c8365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028915E6BD10>]}
[0m14:00:56.614808 [info ] [MainThread]: 
[0m14:00:56.615831 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:00:56.619032 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:00:56.634016 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:00:56.661865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a67bcf36-75a6-4c21-bc73-aa7d100c8365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289172127E0>]}
[0m14:00:56.662395 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:00:56.662924 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:00:56.663445 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:00:56.664495 [info ] [MainThread]: 
[0m14:00:56.669616 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m14:00:56.670641 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:00:56.672259 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:00:56.672768 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:00:56.680441 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:00:56.682492 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:00:56.672768 => 14:00:56.681974
[0m14:00:56.682492 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:00:56.723694 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:00:56.724711 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:00:56.725730 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:00:56.725730 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:00:56.790383 [debug] [Thread-1 (]: SQL status: OK in 0.05999999865889549 seconds
[0m14:00:56.838332 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:00:56.841418 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:00:56.842468 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:00:56.860064 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m14:00:56.874440 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:00:56.683003 => 14:00:56.874440
[0m14:00:56.876488 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a67bcf36-75a6-4c21-bc73-aa7d100c8365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002891727DD30>]}
[0m14:00:56.877508 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.21s]
[0m14:00:56.878526 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:00:56.879584 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m14:00:56.880607 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:00:56.881625 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m14:00:56.881625 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:00:56.884699 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:00:56.885727 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:00:56.881625 => 14:00:56.885727
[0m14:00:56.886749 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:00:56.906889 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:00:56.908433 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:00:56.910000 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:00:56.912589 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:00:56.915701 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:00:56.886749 => 14:00:56.914643
[0m14:00:56.920327 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a67bcf36-75a6-4c21-bc73-aa7d100c8365', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289174A6FF0>]}
[0m14:00:56.922431 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.04s]
[0m14:00:56.923974 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:00:56.925540 [debug] [MainThread]: On master: ROLLBACK
[0m14:00:56.927108 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:00:56.927624 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:00:56.928138 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:00:56.928668 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:00:56.929190 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:00:56.930238 [info ] [MainThread]: 
[0m14:00:56.931280 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.31 seconds (0.31s).
[0m14:00:56.933366 [debug] [MainThread]: Command end result
[0m14:00:56.946315 [info ] [MainThread]: 
[0m14:00:56.947866 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:00:56.948378 [info ] [MainThread]: 
[0m14:00:56.949498 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:00:56.952077 [debug] [MainThread]: Command `cli run` succeeded at 14:00:56.951553 after 2.56 seconds
[0m14:00:56.953106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002891588D310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028916133F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000289154805C0>]}
[0m14:00:56.953635 [debug] [MainThread]: Flushing usage events
[0m14:02:29.892819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D068C22A20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D068C23FE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D068C21940>]}


============================== 14:02:29.893831 | f4afe53a-d5a4-4fe2-906b-8dcea93908e8 ==============================
[0m14:02:29.893831 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:02:29.894854 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m14:02:30.036988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f4afe53a-d5a4-4fe2-906b-8dcea93908e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D068BEF830>]}
[0m14:02:30.101460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f4afe53a-d5a4-4fe2-906b-8dcea93908e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D068BEF440>]}
[0m14:02:30.105684 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:02:30.118313 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:02:30.176395 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:02:30.177412 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:02:30.183499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f4afe53a-d5a4-4fe2-906b-8dcea93908e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D068CD07A0>]}
[0m14:02:30.197777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f4afe53a-d5a4-4fe2-906b-8dcea93908e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D069E2D670>]}
[0m14:02:30.198796 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:02:30.199806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4afe53a-d5a4-4fe2-906b-8dcea93908e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D068920DD0>]}
[0m14:02:30.201827 [info ] [MainThread]: 
[0m14:02:30.203867 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:02:30.207039 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:02:30.214518 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:02:30.231608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4afe53a-d5a4-4fe2-906b-8dcea93908e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D068BEF740>]}
[0m14:02:30.232632 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:02:30.232632 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:02:30.233647 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:02:30.234664 [info ] [MainThread]: 
[0m14:02:30.238799 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m14:02:30.239832 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:02:30.240852 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:02:30.241876 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:02:30.248993 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:02:30.251119 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:02:30.241876 => 14:02:30.251119
[0m14:02:30.252297 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:02:30.294330 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:02:30.296370 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:02:30.296370 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:02:30.296370 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:02:30.337755 [debug] [Thread-1 (]: SQL status: OK in 0.03999999910593033 seconds
[0m14:02:30.366556 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:02:30.370642 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:02:30.370642 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:02:30.394305 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m14:02:30.410364 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:02:30.252297 => 14:02:30.410364
[0m14:02:30.412892 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4afe53a-d5a4-4fe2-906b-8dcea93908e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D069FAAA80>]}
[0m14:02:30.413912 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.17s]
[0m14:02:30.414925 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:02:30.415946 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m14:02:30.415946 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:02:30.416964 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m14:02:30.417983 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:02:30.423125 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:02:30.424150 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:02:30.418997 => 14:02:30.424150
[0m14:02:30.425167 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:02:30.441482 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:02:30.443525 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:02:30.444543 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:02:30.446676 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:02:30.450821 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:02:30.425167 => 14:02:30.449737
[0m14:02:30.451824 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4afe53a-d5a4-4fe2-906b-8dcea93908e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D069FA8F50>]}
[0m14:02:30.453863 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m14:02:30.454889 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:02:30.455899 [debug] [MainThread]: On master: ROLLBACK
[0m14:02:30.456911 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:02:30.456911 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:02:30.456911 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:02:30.456911 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:02:30.457925 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:02:30.457925 [info ] [MainThread]: 
[0m14:02:30.458939 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.26 seconds (0.26s).
[0m14:02:30.459951 [debug] [MainThread]: Command end result
[0m14:02:30.468172 [info ] [MainThread]: 
[0m14:02:30.469184 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:02:30.470215 [info ] [MainThread]: 
[0m14:02:30.470215 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:02:30.471264 [debug] [MainThread]: Command `cli run` succeeded at 14:02:30.471264 after 0.61 seconds
[0m14:02:30.472276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D068BA3FB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D068BA2480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D068BA3920>]}
[0m14:02:30.472276 [debug] [MainThread]: Flushing usage events
[0m14:05:36.176150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CD1B1E1820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CD1B1E1A90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CD1B1E1F70>]}


============================== 14:05:36.177172 | a49f8230-0f47-4c4a-a2b1-b2045899343b ==============================
[0m14:05:36.177172 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:05:36.177685 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:05:36.315110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a49f8230-0f47-4c4a-a2b1-b2045899343b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CD1B18A720>]}
[0m14:05:36.390104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a49f8230-0f47-4c4a-a2b1-b2045899343b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CD1C38E930>]}
[0m14:05:36.393204 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:05:36.405548 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:05:36.469096 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:05:36.470098 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:05:36.477519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a49f8230-0f47-4c4a-a2b1-b2045899343b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CD1B1E1430>]}
[0m14:05:36.490542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a49f8230-0f47-4c4a-a2b1-b2045899343b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CD1C3F15E0>]}
[0m14:05:36.492076 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:05:36.492591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a49f8230-0f47-4c4a-a2b1-b2045899343b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CD1B1AE9F0>]}
[0m14:05:36.495687 [info ] [MainThread]: 
[0m14:05:36.497223 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:05:36.499331 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:05:36.505462 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:05:36.535880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a49f8230-0f47-4c4a-a2b1-b2045899343b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CD1B1AF740>]}
[0m14:05:36.538036 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:05:36.538567 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:05:36.540159 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:05:36.541343 [info ] [MainThread]: 
[0m14:05:36.549341 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m14:05:36.550904 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:05:36.552457 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:05:36.552973 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:05:36.560673 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:05:36.562212 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:05:36.553497 => 14:05:36.561701
[0m14:05:36.562722 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:05:36.610110 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:05:36.612181 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:05:36.613227 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:05:36.614301 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:05:36.661454 [debug] [Thread-1 (]: SQL status: OK in 0.05000000074505806 seconds
[0m14:05:36.690885 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:05:36.693960 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:05:36.696066 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:05:36.717919 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m14:05:36.734935 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:05:36.562722 => 14:05:36.734935
[0m14:05:36.738032 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a49f8230-0f47-4c4a-a2b1-b2045899343b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CD1C56B350>]}
[0m14:05:36.739046 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.19s]
[0m14:05:36.740061 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:05:36.741067 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m14:05:36.742086 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:05:36.745190 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m14:05:36.746205 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:05:36.748245 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:05:36.749264 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:05:36.746205 => 14:05:36.749264
[0m14:05:36.750278 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:05:36.776316 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:05:36.779398 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:05:36.780432 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:05:36.783478 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:05:36.786526 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:05:36.750794 => 14:05:36.786526
[0m14:05:36.788575 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a49f8230-0f47-4c4a-a2b1-b2045899343b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CD1C569280>]}
[0m14:05:36.790808 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.04s]
[0m14:05:36.791838 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:05:36.795932 [debug] [MainThread]: On master: ROLLBACK
[0m14:05:36.796948 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:05:36.797967 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:05:36.797967 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:05:36.797967 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:05:36.798988 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:05:36.798988 [info ] [MainThread]: 
[0m14:05:36.800006 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.30 seconds (0.30s).
[0m14:05:36.801020 [debug] [MainThread]: Command end result
[0m14:05:36.811250 [info ] [MainThread]: 
[0m14:05:36.813315 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:05:36.814348 [info ] [MainThread]: 
[0m14:05:36.814862 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:05:36.816922 [debug] [MainThread]: Command `cli run` succeeded at 14:05:36.816922 after 0.67 seconds
[0m14:05:36.817433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CD1B12CAA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CD1B05FF50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CD1B05FCE0>]}
[0m14:05:36.818460 [debug] [MainThread]: Flushing usage events
[0m14:43:53.731828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029B3DAF3BF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029B3DAF2990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029B3DAF0BF0>]}


============================== 14:43:53.731828 | e8fd141c-a7b3-4bbc-8022-4f61efa5f13a ==============================
[0m14:43:53.731828 [info ] [MainThread]: Running with dbt=1.7.4
[0m14:43:53.732845 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:43:53.887974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e8fd141c-a7b3-4bbc-8022-4f61efa5f13a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029B3EB26420>]}
[0m14:43:53.950133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e8fd141c-a7b3-4bbc-8022-4f61efa5f13a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029B3EC76270>]}
[0m14:43:53.951703 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:43:53.968363 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m14:43:55.285783 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:43:55.285783 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:43:55.291916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e8fd141c-a7b3-4bbc-8022-4f61efa5f13a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029B3EBFDDC0>]}
[0m14:43:55.330373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e8fd141c-a7b3-4bbc-8022-4f61efa5f13a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029B3ECFD6A0>]}
[0m14:43:55.331938 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:43:55.332990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8fd141c-a7b3-4bbc-8022-4f61efa5f13a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029B3D7F2570>]}
[0m14:43:55.336669 [info ] [MainThread]: 
[0m14:43:55.338255 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:43:55.342713 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:43:55.352023 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:43:55.372302 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8fd141c-a7b3-4bbc-8022-4f61efa5f13a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029B3ED52A50>]}
[0m14:43:55.374385 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:43:55.375904 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:43:55.376940 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:43:55.377980 [info ] [MainThread]: 
[0m14:43:55.381066 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m14:43:55.381066 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:43:55.383109 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:43:55.383109 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:43:55.390910 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:43:55.392972 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:43:55.383109 => 14:43:55.392972
[0m14:43:55.392972 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:43:55.461266 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:43:55.463418 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:43:55.463418 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:43:55.464462 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:43:55.545552 [debug] [Thread-1 (]: SQL status: OK in 0.07999999821186066 seconds
[0m14:43:55.575936 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:43:55.580082 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:43:55.581107 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:43:55.600201 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m14:43:55.615152 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:43:55.393990 => 14:43:55.614142
[0m14:43:55.615665 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8fd141c-a7b3-4bbc-8022-4f61efa5f13a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029B3EC779E0>]}
[0m14:43:55.616671 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.23s]
[0m14:43:55.618918 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:43:55.619951 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m14:43:55.620975 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:43:55.621998 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m14:43:55.623008 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:43:55.626546 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:43:55.629667 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:43:55.623008 => 14:43:55.628640
[0m14:43:55.629667 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:43:55.645546 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:43:55.648605 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:43:55.648605 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:43:55.650686 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:43:55.652740 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:43:55.630715 => 14:43:55.652740
[0m14:43:55.653762 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e8fd141c-a7b3-4bbc-8022-4f61efa5f13a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029B3EE65FD0>]}
[0m14:43:55.654877 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m14:43:55.655896 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:43:55.660087 [debug] [MainThread]: On master: ROLLBACK
[0m14:43:55.662241 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:43:55.663251 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:43:55.663251 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:43:55.663251 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:43:55.664374 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:43:55.665804 [info ] [MainThread]: 
[0m14:43:55.666833 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.33 seconds (0.33s).
[0m14:43:55.667863 [debug] [MainThread]: Command end result
[0m14:43:55.682912 [info ] [MainThread]: 
[0m14:43:55.683930 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:43:55.683930 [info ] [MainThread]: 
[0m14:43:55.684957 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:43:55.688057 [debug] [MainThread]: Command `cli run` succeeded at 14:43:55.687027 after 2.01 seconds
[0m14:43:55.689154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029B3D3ACAA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029B3EF9BE30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029B3EB72CC0>]}
[0m14:43:55.690165 [debug] [MainThread]: Flushing usage events
[0m15:06:53.091037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BE5D041D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BE5D05E20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BE5D06570>]}


============================== 15:06:53.093573 | 10725539-6dd4-4a4f-bc4f-37082307eb78 ==============================
[0m15:06:53.093573 [info ] [MainThread]: Running with dbt=1.7.4
[0m15:06:53.094676 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:06:53.242677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '10725539-6dd4-4a4f-bc4f-37082307eb78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BE4BB15B0>]}
[0m15:06:53.302493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '10725539-6dd4-4a4f-bc4f-37082307eb78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BE5C4C3E0>]}
[0m15:06:53.304021 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:06:53.322506 [debug] [MainThread]: checksum: 248e8aba381ba2d577dce7ab50d2010c2aac45d2b27e6e6214cdd96d2d41fabc, vars: {}, profile: , target: , version: 1.7.4
[0m15:06:54.420974 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:06:54.420974 [debug] [MainThread]: Partial parsing: updated file: dbt_fabricspark://macros\adapters\schema.sql
[0m15:06:54.462221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '10725539-6dd4-4a4f-bc4f-37082307eb78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BE7066DE0>]}
[0m15:06:54.468850 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '10725539-6dd4-4a4f-bc4f-37082307eb78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BE6F0D970>]}
[0m15:06:54.469868 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:06:54.469868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '10725539-6dd4-4a4f-bc4f-37082307eb78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BE7043080>]}
[0m15:06:54.472957 [info ] [MainThread]: 
[0m15:06:54.472957 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:06:54.474981 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:06:54.494048 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:06:54.534775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '10725539-6dd4-4a4f-bc4f-37082307eb78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BE6F6A0C0>]}
[0m15:06:54.535795 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:06:54.536822 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:06:54.537862 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:06:54.538982 [info ] [MainThread]: 
[0m15:06:54.549827 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:06:54.550856 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:06:54.552395 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:06:54.552910 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:06:54.563687 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:06:54.564701 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:06:54.554011 => 15:06:54.564701
[0m15:06:54.565720 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:06:54.641548 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:06:54.642600 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:06:54.643625 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:06:54.644660 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:06:54.731141 [debug] [Thread-1 (]: SQL status: OK in 0.09000000357627869 seconds
[0m15:06:54.811619 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:06:54.816481 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:06:54.817582 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m15:06:54.843908 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m15:06:54.861924 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:06:54.565720 => 15:06:54.861924
[0m15:06:54.864207 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '10725539-6dd4-4a4f-bc4f-37082307eb78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BE6EAADB0>]}
[0m15:06:54.866260 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.31s]
[0m15:06:54.868330 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:06:54.869355 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:06:54.870372 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m15:06:54.872391 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:06:54.872894 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:06:54.875966 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:06:54.876995 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:06:54.872894 => 15:06:54.876995
[0m15:06:54.876995 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:06:54.896108 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:06:54.898169 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:06:54.899190 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.4", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m15:06:54.902732 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:06:54.908998 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:06:54.878017 => 15:06:54.907950
[0m15:06:54.911123 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '10725539-6dd4-4a4f-bc4f-37082307eb78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BE7173950>]}
[0m15:06:54.911123 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.04s]
[0m15:06:54.912629 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:06:54.913638 [debug] [MainThread]: On master: ROLLBACK
[0m15:06:54.914665 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:06:54.914665 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:06:54.914665 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:06:54.915682 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:06:54.915682 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:06:54.915682 [info ] [MainThread]: 
[0m15:06:54.916705 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.44 seconds (0.44s).
[0m15:06:54.918213 [debug] [MainThread]: Command end result
[0m15:06:54.930110 [info ] [MainThread]: 
[0m15:06:54.931128 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:06:54.932133 [info ] [MainThread]: 
[0m15:06:54.932637 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:06:54.933655 [debug] [MainThread]: Command `cli run` succeeded at 15:06:54.932637 after 1.91 seconds
[0m15:06:54.933655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BE55526F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BE5C7F7D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BE5D61CD0>]}
[0m15:06:54.934683 [debug] [MainThread]: Flushing usage events
[0m15:25:59.491641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277DFFC0B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277DFFC01A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277DFFC03B0>]}


============================== 15:25:59.492657 | 8fe0a7da-6825-4181-8b80-12efe48df194 ==============================
[0m15:25:59.492657 [info ] [MainThread]: Running with dbt=1.7.14
[0m15:25:59.493682 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:25:59.593513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8fe0a7da-6825-4181-8b80-12efe48df194', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277DCE13EC0>]}
[0m15:25:59.627401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8fe0a7da-6825-4181-8b80-12efe48df194', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277DF9789B0>]}
[0m15:25:59.627401 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:25:59.644741 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m15:25:59.651489 [info ] [MainThread]: Unable to do partial parsing because of a version mismatch
[0m15:25:59.652557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8fe0a7da-6825-4181-8b80-12efe48df194', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E123B3E0>]}
[0m15:26:01.163304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8fe0a7da-6825-4181-8b80-12efe48df194', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E128F5F0>]}
[0m15:26:01.169419 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8fe0a7da-6825-4181-8b80-12efe48df194', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277DFFF7800>]}
[0m15:26:01.170436 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m15:26:01.171489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8fe0a7da-6825-4181-8b80-12efe48df194', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E112AE10>]}
[0m15:26:01.173530 [info ] [MainThread]: 
[0m15:26:01.174546 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:26:01.175563 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:26:01.181674 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m15:26:01.195918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8fe0a7da-6825-4181-8b80-12efe48df194', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E125E870>]}
[0m15:26:01.195918 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:26:01.195918 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:26:01.196934 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:26:01.196934 [info ] [MainThread]: 
[0m15:26:01.199997 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:26:01.199997 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m15:26:01.201010 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:26:01.201010 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:26:01.206128 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:26:01.208154 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:26:01.201010 => 15:26:01.207143
[0m15:26:01.208154 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:26:01.240666 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:26:01.241675 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:26:01.241675 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:26:01.241675 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:26:01.278310 [debug] [Thread-1 (]: SQL status: OK in 0.03999999910593033 seconds
[0m15:26:01.299867 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:26:01.300875 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:26:01.301887 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m15:26:01.315134 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:26:01.326374 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:26:01.208154 => 15:26:01.325363
[0m15:26:01.327387 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8fe0a7da-6825-4181-8b80-12efe48df194', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277DFF78A70>]}
[0m15:26:01.327387 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.13s]
[0m15:26:01.328397 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:26:01.329407 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:26:01.329407 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m15:26:01.330418 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m15:26:01.330418 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:26:01.332440 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:26:01.333454 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:26:01.330418 => 15:26:01.333454
[0m15:26:01.333454 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:26:01.347727 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:26:01.349770 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:26:01.350783 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m15:26:01.351793 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:26:01.353815 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:26:01.334464 => 15:26:01.352805
[0m15:26:01.353874 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8fe0a7da-6825-4181-8b80-12efe48df194', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E14B3080>]}
[0m15:26:01.354877 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m15:26:01.355891 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:26:01.356905 [debug] [MainThread]: On master: ROLLBACK
[0m15:26:01.357920 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:26:01.357920 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:26:01.357920 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:26:01.358936 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:26:01.358936 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:26:01.359959 [info ] [MainThread]: 
[0m15:26:01.359959 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.19 seconds (0.19s).
[0m15:26:01.360978 [debug] [MainThread]: Command end result
[0m15:26:01.365028 [info ] [MainThread]: 
[0m15:26:01.366039 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:26:01.366039 [info ] [MainThread]: 
[0m15:26:01.367053 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:26:01.367053 [debug] [MainThread]: Command `cli run` succeeded at 15:26:01.367053 after 1.91 seconds
[0m15:26:01.368072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E1205C40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E1384380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000277E12694F0>]}
[0m15:26:01.368072 [debug] [MainThread]: Flushing usage events
[0m21:02:38.495963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CACC64DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CACC64290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CACC65100>]}


============================== 21:02:38.496970 | cc512349-5856-40e2-b04a-ea114ca89da8 ==============================
[0m21:02:38.496970 [info ] [MainThread]: Running with dbt=1.7.14
[0m21:02:38.496970 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:02:38.589019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cc512349-5856-40e2-b04a-ea114ca89da8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CAC62C710>]}
[0m21:02:38.622390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cc512349-5856-40e2-b04a-ea114ca89da8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CACA283B0>]}
[0m21:02:38.623399 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m21:02:38.634589 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m21:02:39.240632 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:02:39.241645 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:02:39.246704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cc512349-5856-40e2-b04a-ea114ca89da8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CACCA7D10>]}
[0m21:02:39.259826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cc512349-5856-40e2-b04a-ea114ca89da8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CADDFCEF0>]}
[0m21:02:39.259826 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m21:02:39.259826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cc512349-5856-40e2-b04a-ea114ca89da8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CACCDBBC0>]}
[0m21:02:39.260834 [info ] [MainThread]: 
[0m21:02:39.261842 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m21:02:39.262851 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m21:02:39.271946 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m21:02:39.293205 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cc512349-5856-40e2-b04a-ea114ca89da8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CADD39A90>]}
[0m21:02:39.294214 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:02:39.294214 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m21:02:39.295223 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m21:02:39.295223 [info ] [MainThread]: 
[0m21:02:39.297239 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m21:02:39.297239 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m21:02:39.298248 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m21:02:39.298248 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m21:02:39.302295 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m21:02:39.303302 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 21:02:39.298248 => 21:02:39.303302
[0m21:02:39.303302 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m21:02:39.332603 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:02:39.333610 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m21:02:39.333610 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m21:02:39.333610 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:02:39.366984 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m21:02:39.402564 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m21:02:39.404601 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m21:02:39.405148 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m21:02:39.422090 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m21:02:39.433277 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 21:02:39.303302 => 21:02:39.433277
[0m21:02:39.434293 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cc512349-5856-40e2-b04a-ea114ca89da8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CACCDBA70>]}
[0m21:02:39.434802 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.14s]
[0m21:02:39.435851 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m21:02:39.436892 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m21:02:39.437410 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m21:02:39.437924 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m21:02:39.438456 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m21:02:39.441002 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m21:02:39.442019 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 21:02:39.438456 => 21:02:39.442019
[0m21:02:39.442530 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m21:02:39.457457 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m21:02:39.457960 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m21:02:39.458963 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m21:02:39.459971 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m21:02:39.460989 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 21:02:39.442530 => 21:02:39.460989
[0m21:02:39.462010 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cc512349-5856-40e2-b04a-ea114ca89da8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CAE0AC290>]}
[0m21:02:39.462010 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m21:02:39.463030 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m21:02:39.464579 [debug] [MainThread]: On master: ROLLBACK
[0m21:02:39.464579 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:02:39.464579 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m21:02:39.465594 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m21:02:39.465594 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:02:39.465594 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m21:02:39.466604 [info ] [MainThread]: 
[0m21:02:39.466604 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m21:02:39.467629 [debug] [MainThread]: Command end result
[0m21:02:39.472743 [info ] [MainThread]: 
[0m21:02:39.473767 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:02:39.473767 [info ] [MainThread]: 
[0m21:02:39.474789 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m21:02:39.474789 [debug] [MainThread]: Command `cli run` succeeded at 21:02:39.474789 after 1.02 seconds
[0m21:02:39.475806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CADE86180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CACCDBA70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CACCDBBC0>]}
[0m21:02:39.475806 [debug] [MainThread]: Flushing usage events
[0m21:07:16.217247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286A43F4530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286A43F46E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286A43F4440>]}


============================== 21:07:16.217247 | 2f0bcb3a-1c9a-4d5a-8dad-3f4d9413cdf5 ==============================
[0m21:07:16.217247 [info ] [MainThread]: Running with dbt=1.7.14
[0m21:07:16.218277 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m21:07:16.299104 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2f0bcb3a-1c9a-4d5a-8dad-3f4d9413cdf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286A3F69E50>]}
[0m21:07:16.334553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2f0bcb3a-1c9a-4d5a-8dad-3f4d9413cdf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286A436B7A0>]}
[0m21:07:16.335569 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m21:07:16.342170 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m21:07:16.395023 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:07:16.396041 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:07:16.400114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2f0bcb3a-1c9a-4d5a-8dad-3f4d9413cdf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286A445A480>]}
[0m21:07:16.408289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2f0bcb3a-1c9a-4d5a-8dad-3f4d9413cdf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286A5598B00>]}
[0m21:07:16.408289 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m21:07:16.409306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f0bcb3a-1c9a-4d5a-8dad-3f4d9413cdf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286A43BD220>]}
[0m21:07:16.411370 [info ] [MainThread]: 
[0m21:07:16.412393 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m21:07:16.413412 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m21:07:16.418530 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m21:07:16.431694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f0bcb3a-1c9a-4d5a-8dad-3f4d9413cdf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286A4387140>]}
[0m21:07:16.432735 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:07:16.432735 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m21:07:16.433751 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m21:07:16.433751 [info ] [MainThread]: 
[0m21:07:16.435790 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m21:07:16.436806 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m21:07:16.436806 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m21:07:16.436806 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m21:07:16.441862 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m21:07:16.443903 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 21:07:16.437816 => 21:07:16.442882
[0m21:07:16.443903 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m21:07:16.473271 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:07:16.473271 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m21:07:16.474282 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m21:07:16.474282 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:07:16.497058 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m21:07:16.518401 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m21:07:16.520447 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m21:07:16.521458 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m21:07:16.539976 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m21:07:16.550164 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 21:07:16.443903 => 21:07:16.550164
[0m21:07:16.551183 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f0bcb3a-1c9a-4d5a-8dad-3f4d9413cdf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286A572B470>]}
[0m21:07:16.552215 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.11s]
[0m21:07:16.552736 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m21:07:16.553760 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m21:07:16.554828 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m21:07:16.555870 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m21:07:16.556916 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m21:07:16.561024 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m21:07:16.562041 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 21:07:16.556916 => 21:07:16.562041
[0m21:07:16.562549 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m21:07:16.575893 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m21:07:16.576912 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m21:07:16.577420 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m21:07:16.579461 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m21:07:16.580998 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 21:07:16.562549 => 21:07:16.580998
[0m21:07:16.581513 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f0bcb3a-1c9a-4d5a-8dad-3f4d9413cdf5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286A5802450>]}
[0m21:07:16.582025 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m21:07:16.582543 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m21:07:16.583576 [debug] [MainThread]: On master: ROLLBACK
[0m21:07:16.584090 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:07:16.584603 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m21:07:16.584603 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m21:07:16.585223 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:07:16.585223 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m21:07:16.585729 [info ] [MainThread]: 
[0m21:07:16.586245 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m21:07:16.586761 [debug] [MainThread]: Command end result
[0m21:07:16.592361 [info ] [MainThread]: 
[0m21:07:16.592361 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:07:16.592874 [info ] [MainThread]: 
[0m21:07:16.593386 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m21:07:16.593898 [debug] [MainThread]: Command `cli run` succeeded at 21:07:16.593898 after 0.40 seconds
[0m21:07:16.594410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286A34A2AE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286A4385880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000286A44D1EE0>]}
[0m21:07:16.594410 [debug] [MainThread]: Flushing usage events
[0m21:08:53.294745 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A85A9B89E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A85A9B8BF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A85A9B8830>]}


============================== 21:08:53.295761 | 5934e869-8a8d-4eeb-9e2a-2e34ff843b19 ==============================
[0m21:08:53.295761 [info ] [MainThread]: Running with dbt=1.7.14
[0m21:08:53.296283 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m21:08:53.393781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5934e869-8a8d-4eeb-9e2a-2e34ff843b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A85A1E2F90>]}
[0m21:08:53.428265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5934e869-8a8d-4eeb-9e2a-2e34ff843b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A85A8EBDA0>]}
[0m21:08:53.429273 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m21:08:53.435341 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m21:08:53.475817 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:08:53.476841 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:08:53.483003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5934e869-8a8d-4eeb-9e2a-2e34ff843b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A85BA5EA20>]}
[0m21:08:53.488092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5934e869-8a8d-4eeb-9e2a-2e34ff843b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A85BB6C4D0>]}
[0m21:08:53.488092 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m21:08:53.489100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5934e869-8a8d-4eeb-9e2a-2e34ff843b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A859EBD490>]}
[0m21:08:53.490109 [info ] [MainThread]: 
[0m21:08:53.491118 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m21:08:53.491118 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m21:08:53.495168 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m21:08:53.508296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5934e869-8a8d-4eeb-9e2a-2e34ff843b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A85BB6DC70>]}
[0m21:08:53.510355 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:08:53.510355 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m21:08:53.511366 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m21:08:53.511366 [info ] [MainThread]: 
[0m21:08:53.515404 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m21:08:53.515404 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m21:08:53.516415 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m21:08:53.516415 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m21:08:53.521458 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m21:08:53.521458 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 21:08:53.517426 => 21:08:53.521458
[0m21:08:53.522466 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m21:08:53.551825 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:08:53.551825 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m21:08:53.552834 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m21:08:53.552834 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:08:53.575470 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m21:08:53.598893 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m21:08:53.599901 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m21:08:53.600911 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m21:08:53.613633 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m21:08:53.624898 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 21:08:53.522466 => 21:08:53.624898
[0m21:08:53.626944 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5934e869-8a8d-4eeb-9e2a-2e34ff843b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A85BCEC320>]}
[0m21:08:53.627961 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.11s]
[0m21:08:53.628985 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m21:08:53.629504 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m21:08:53.630019 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m21:08:53.630540 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m21:08:53.632098 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m21:08:53.634652 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m21:08:53.635671 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 21:08:53.632608 => 21:08:53.635671
[0m21:08:53.635671 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m21:08:53.649554 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m21:08:53.651097 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m21:08:53.651615 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m21:08:53.653677 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m21:08:53.655201 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 21:08:53.636179 => 21:08:53.655201
[0m21:08:53.655711 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5934e869-8a8d-4eeb-9e2a-2e34ff843b19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A85BDC2690>]}
[0m21:08:53.656225 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m21:08:53.657246 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m21:08:53.657757 [debug] [MainThread]: On master: ROLLBACK
[0m21:08:53.658264 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:08:53.658774 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m21:08:53.658774 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m21:08:53.658774 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:08:53.659281 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m21:08:53.659792 [info ] [MainThread]: 
[0m21:08:53.660298 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m21:08:53.660809 [debug] [MainThread]: Command end result
[0m21:08:53.665426 [info ] [MainThread]: 
[0m21:08:53.665936 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:08:53.665936 [info ] [MainThread]: 
[0m21:08:53.666446 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m21:08:53.667465 [debug] [MainThread]: Command `cli run` succeeded at 21:08:53.666446 after 0.40 seconds
[0m21:08:53.667465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A85BAC0590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A85BAC3A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A85BAC33B0>]}
[0m21:08:53.667465 [debug] [MainThread]: Flushing usage events
[0m06:58:22.871793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E00504110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E00504200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E00504470>]}


============================== 06:58:22.872800 | 1274f45e-03d3-406b-82bc-e0efff15eac3 ==============================
[0m06:58:22.872800 [info ] [MainThread]: Running with dbt=1.7.14
[0m06:58:22.873808 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m06:58:22.954468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1274f45e-03d3-406b-82bc-e0efff15eac3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E7FD29A00>]}
[0m06:58:22.986647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1274f45e-03d3-406b-82bc-e0efff15eac3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E001E5670>]}
[0m06:58:22.987664 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m06:58:23.001767 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m06:58:23.794280 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:58:23.794280 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:58:23.798307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1274f45e-03d3-406b-82bc-e0efff15eac3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E00445C10>]}
[0m06:58:23.803357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1274f45e-03d3-406b-82bc-e0efff15eac3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E01695EB0>]}
[0m06:58:23.803357 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m06:58:23.804365 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1274f45e-03d3-406b-82bc-e0efff15eac3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E015AEAB0>]}
[0m06:58:23.805373 [info ] [MainThread]: 
[0m06:58:23.806379 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m06:58:23.806379 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m06:58:23.814445 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m06:58:23.832557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1274f45e-03d3-406b-82bc-e0efff15eac3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E015D3AA0>]}
[0m06:58:23.832557 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:58:23.832557 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:58:23.833564 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m06:58:23.833564 [info ] [MainThread]: 
[0m06:58:23.835584 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m06:58:23.836592 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m06:58:23.836592 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m06:58:23.837600 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m06:58:23.841645 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m06:58:23.842653 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 06:58:23.837600 => 06:58:23.842653
[0m06:58:23.842653 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m06:58:23.869834 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:58:23.870839 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m06:58:23.870839 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m06:58:23.871843 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m06:58:23.899016 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m06:58:23.923258 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m06:58:23.924272 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m06:58:23.925292 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m06:58:23.935391 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m06:58:23.944485 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 06:58:23.842653 => 06:58:23.944485
[0m06:58:23.945492 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1274f45e-03d3-406b-82bc-e0efff15eac3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E7FF87620>]}
[0m06:58:23.945492 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.11s]
[0m06:58:23.946498 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m06:58:23.946498 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m06:58:23.947504 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m06:58:23.947504 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m06:58:23.948511 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m06:58:23.949516 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m06:58:23.950541 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 06:58:23.948511 => 06:58:23.950541
[0m06:58:23.951549 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m06:58:23.963668 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m06:58:23.964676 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m06:58:23.964676 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m06:58:23.966717 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m06:58:23.967724 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 06:58:23.951549 => 06:58:23.967724
[0m06:58:23.968733 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1274f45e-03d3-406b-82bc-e0efff15eac3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E0191CCB0>]}
[0m06:58:23.968733 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m06:58:23.969740 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m06:58:23.970748 [debug] [MainThread]: On master: ROLLBACK
[0m06:58:23.970748 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:58:23.970748 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m06:58:23.971753 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m06:58:23.971753 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:58:23.971753 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:58:23.972761 [info ] [MainThread]: 
[0m06:58:23.972761 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m06:58:23.973768 [debug] [MainThread]: Command end result
[0m06:58:23.978818 [info ] [MainThread]: 
[0m06:58:23.979836 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:58:23.979836 [info ] [MainThread]: 
[0m06:58:23.979836 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m06:58:23.980871 [debug] [MainThread]: Command `cli run` succeeded at 06:58:23.980871 after 1.15 seconds
[0m06:58:23.981881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E00504200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E00504470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026E00505460>]}
[0m06:58:23.981881 [debug] [MainThread]: Flushing usage events
[0m06:59:39.708847 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014116E74500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014116E74950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014116E744A0>]}


============================== 06:59:39.708847 | be8a5803-95de-43d2-bb18-c2aaa13820ff ==============================
[0m06:59:39.708847 [info ] [MainThread]: Running with dbt=1.7.14
[0m06:59:39.709852 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m06:59:39.785323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'be8a5803-95de-43d2-bb18-c2aaa13820ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014116E2C1D0>]}
[0m06:59:39.817509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'be8a5803-95de-43d2-bb18-c2aaa13820ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014116E74170>]}
[0m06:59:39.818514 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m06:59:39.825552 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m06:59:39.868842 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:59:39.869847 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:59:39.872862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'be8a5803-95de-43d2-bb18-c2aaa13820ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014117FED6D0>]}
[0m06:59:39.877890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'be8a5803-95de-43d2-bb18-c2aaa13820ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014118014DD0>]}
[0m06:59:39.878896 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m06:59:39.878896 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be8a5803-95de-43d2-bb18-c2aaa13820ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014116DD9E20>]}
[0m06:59:39.879902 [info ] [MainThread]: 
[0m06:59:39.880907 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m06:59:39.881913 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m06:59:39.885945 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m06:59:39.898007 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be8a5803-95de-43d2-bb18-c2aaa13820ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014116EB3500>]}
[0m06:59:39.898007 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:59:39.898007 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:59:39.899012 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m06:59:39.899012 [info ] [MainThread]: 
[0m06:59:39.901035 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m06:59:39.901035 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m06:59:39.902040 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m06:59:39.902040 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m06:59:39.906061 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m06:59:39.907065 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 06:59:39.902040 => 06:59:39.907065
[0m06:59:39.907065 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m06:59:39.935228 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:59:39.935228 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m06:59:39.936233 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m06:59:39.936233 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m06:59:39.957368 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m06:59:39.975481 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m06:59:39.976486 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m06:59:39.976486 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m06:59:39.992599 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m06:59:39.999658 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 06:59:39.908071 => 06:59:39.999658
[0m06:59:40.000663 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be8a5803-95de-43d2-bb18-c2aaa13820ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141181A8770>]}
[0m06:59:40.000663 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.10s]
[0m06:59:40.001669 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m06:59:40.002674 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m06:59:40.002674 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m06:59:40.003680 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m06:59:40.003680 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m06:59:40.005690 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m06:59:40.005690 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 06:59:40.003680 => 06:59:40.005690
[0m06:59:40.006697 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m06:59:40.017779 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m06:59:40.017779 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m06:59:40.018784 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m06:59:40.019790 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m06:59:40.020795 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 06:59:40.006697 => 06:59:40.020795
[0m06:59:40.021804 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be8a5803-95de-43d2-bb18-c2aaa13820ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141181A8770>]}
[0m06:59:40.021804 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m06:59:40.022820 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m06:59:40.023829 [debug] [MainThread]: On master: ROLLBACK
[0m06:59:40.023829 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:59:40.023829 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m06:59:40.023829 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m06:59:40.024838 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:59:40.024838 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:59:40.024838 [info ] [MainThread]: 
[0m06:59:40.025848 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m06:59:40.025848 [debug] [MainThread]: Command end result
[0m06:59:40.029899 [info ] [MainThread]: 
[0m06:59:40.030908 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:59:40.030908 [info ] [MainThread]: 
[0m06:59:40.030908 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m06:59:40.031917 [debug] [MainThread]: Command `cli run` succeeded at 06:59:40.031917 after 0.35 seconds
[0m06:59:40.031917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014116C58C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001411817E5A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001411644F8C0>]}
[0m06:59:40.032926 [debug] [MainThread]: Flushing usage events
[0m14:24:11.096308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D98B904110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D98B9043E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D98B9040E0>]}


============================== 14:24:11.096308 | fc2ad060-97c7-4894-bf1d-3e68e41652ff ==============================
[0m14:24:11.096308 [info ] [MainThread]: Running with dbt=1.7.14
[0m14:24:11.097319 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:24:11.182324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fc2ad060-97c7-4894-bf1d-3e68e41652ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D98B6B8B60>]}
[0m14:24:11.217743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fc2ad060-97c7-4894-bf1d-3e68e41652ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D98B854770>]}
[0m14:24:11.218751 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:24:11.232898 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m14:24:11.972150 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m14:24:11.972150 [debug] [MainThread]: Partial parsing: added file: testproj://seeds\sample.csv
[0m14:24:12.042045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fc2ad060-97c7-4894-bf1d-3e68e41652ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D98CBF3C50>]}
[0m14:24:12.049129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fc2ad060-97c7-4894-bf1d-3e68e41652ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D98CAC07A0>]}
[0m14:24:12.050140 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:24:12.050140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc2ad060-97c7-4894-bf1d-3e68e41652ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D9881302F0>]}
[0m14:24:12.051152 [info ] [MainThread]: 
[0m14:24:12.052167 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:24:12.053177 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:24:12.063289 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:24:12.084524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc2ad060-97c7-4894-bf1d-3e68e41652ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D98CAC25A0>]}
[0m14:24:12.085532 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:24:12.085532 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:24:12.086540 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:24:12.086540 [info ] [MainThread]: 
[0m14:24:12.089607 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m14:24:12.089607 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:24:12.090619 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:24:12.090619 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:24:12.095668 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:24:12.095668 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:24:12.091629 => 14:24:12.095668
[0m14:24:12.096676 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:24:12.135132 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:24:12.136139 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:24:12.136139 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:24:12.137148 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:24:12.168528 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m14:24:12.199971 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:24:12.199971 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:24:12.200981 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:24:12.213155 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:24:12.222246 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:24:12.096676 => 14:24:12.222246
[0m14:24:12.222246 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc2ad060-97c7-4894-bf1d-3e68e41652ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D98CAC2E10>]}
[0m14:24:12.223255 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.13s]
[0m14:24:12.224267 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:24:12.224267 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m14:24:12.225300 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:24:12.225300 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m14:24:12.226311 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:24:12.227319 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:24:12.228327 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:24:12.226311 => 14:24:12.228327
[0m14:24:12.229336 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:24:12.241466 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:24:12.242476 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:24:12.242476 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:24:12.244509 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:24:12.246551 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:24:12.229336 => 14:24:12.246551
[0m14:24:12.246551 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc2ad060-97c7-4894-bf1d-3e68e41652ff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D98CCAF320>]}
[0m14:24:12.247559 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m14:24:12.247559 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:24:12.248569 [debug] [MainThread]: On master: ROLLBACK
[0m14:24:12.248569 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:24:12.249578 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:24:12.249578 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:24:12.249578 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:24:12.249578 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:24:12.250585 [info ] [MainThread]: 
[0m14:24:12.250585 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m14:24:12.251594 [debug] [MainThread]: Command end result
[0m14:24:12.256675 [info ] [MainThread]: 
[0m14:24:12.256675 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:24:12.257683 [info ] [MainThread]: 
[0m14:24:12.257683 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:24:12.258698 [debug] [MainThread]: Command `cli run` succeeded at 14:24:12.258698 after 1.20 seconds
[0m14:24:12.258698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D98B9043E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D98B9040E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D98B905130>]}
[0m14:24:12.258698 [debug] [MainThread]: Flushing usage events
[0m14:26:21.269708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A66C4CE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A66C4DA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A66C4530>]}


============================== 14:26:21.269708 | 28ff1658-eeb6-4e36-b173-67eb675928fa ==============================
[0m14:26:21.269708 [info ] [MainThread]: Running with dbt=1.7.14
[0m14:26:21.270714 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m14:26:21.353221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '28ff1658-eeb6-4e36-b173-67eb675928fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A65F5D30>]}
[0m14:26:21.390085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '28ff1658-eeb6-4e36-b173-67eb675928fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A672B9B0>]}
[0m14:26:21.391156 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:26:21.397418 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m14:26:21.448088 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:26:21.448088 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:26:21.452143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '28ff1658-eeb6-4e36-b173-67eb675928fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A77D2600>]}
[0m14:26:21.458188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '28ff1658-eeb6-4e36-b173-67eb675928fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A7859040>]}
[0m14:26:21.458188 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m14:26:21.458188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '28ff1658-eeb6-4e36-b173-67eb675928fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A65F7080>]}
[0m14:26:21.460208 [info ] [MainThread]: 
[0m14:26:21.460208 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:26:21.461217 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:26:21.465251 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:26:21.478432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '28ff1658-eeb6-4e36-b173-67eb675928fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A672BCE0>]}
[0m14:26:21.478432 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:26:21.479439 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:26:21.479439 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:26:21.480449 [info ] [MainThread]: 
[0m14:26:21.482488 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m14:26:21.482488 [info ] [Thread-1 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m14:26:21.483500 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m14:26:21.483500 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m14:26:21.484510 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 14:26:21.483500 => 14:26:21.484510
[0m14:26:21.484510 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m14:26:21.515390 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:26:21.515390 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:26:21.515390 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`id` bigint,`name` string,`email` string)
    
    
    
    
    
  
[0m14:26:21.516402 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:26:21.542682 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m14:26:21.553830 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m14:26:21.556854 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:26:21.557860 [debug] [Thread-1 (]: On seed.testproj.sample: 
          insert into datalake.sample values
          (cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string))
      ...
[0m14:26:21.557860 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:

          insert into datalake.sample values
          (cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string))
      
[0m14:26:21.557860 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'node_id'
[0m14:26:21.558882 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 14:26:21.484510 => 14:26:21.558882
[0m14:26:21.660948 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  'node_id'
[0m14:26:21.660948 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '28ff1658-eeb6-4e36-b173-67eb675928fa', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A78B6510>]}
[0m14:26:21.661959 [error] [Thread-1 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.18s]
[0m14:26:21.662972 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m14:26:21.663988 [debug] [MainThread]: On master: ROLLBACK
[0m14:26:21.663988 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:26:21.664999 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:26:21.664999 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:26:21.666027 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:26:21.666027 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:26:21.667041 [info ] [MainThread]: 
[0m14:26:21.667041 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.21 seconds (0.21s).
[0m14:26:21.668054 [debug] [MainThread]: Command end result
[0m14:26:21.677171 [info ] [MainThread]: 
[0m14:26:21.679203 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:26:21.679203 [info ] [MainThread]: 
[0m14:26:21.680315 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'node_id'
[0m14:26:21.681317 [info ] [MainThread]: 
[0m14:26:21.681317 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m14:26:21.683349 [debug] [MainThread]: Command `cli seed` failed at 14:26:21.682339 after 0.44 seconds
[0m14:26:21.683349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A656EFF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A5524CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6A662FB60>]}
[0m14:26:21.684365 [debug] [MainThread]: Flushing usage events
[0m07:10:45.025548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205BEAB4470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205BEAB45C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205BEAB4890>]}


============================== 07:10:45.026562 | 9b251f8a-0eda-4869-b373-548039d055ec ==============================
[0m07:10:45.026562 [info ] [MainThread]: Running with dbt=1.7.14
[0m07:10:45.026562 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m07:10:45.134334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9b251f8a-0eda-4869-b373-548039d055ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205BEA7ECC0>]}
[0m07:10:45.174142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9b251f8a-0eda-4869-b373-548039d055ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205BE066E40>]}
[0m07:10:45.175150 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m07:10:45.190405 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m07:10:46.477146 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:10:46.477657 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:10:46.481251 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9b251f8a-0eda-4869-b373-548039d055ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205BFC2E420>]}
[0m07:10:46.512248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9b251f8a-0eda-4869-b373-548039d055ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205BFC60B30>]}
[0m07:10:46.513253 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m07:10:46.514277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b251f8a-0eda-4869-b373-548039d055ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205BFB87AA0>]}
[0m07:10:46.515809 [info ] [MainThread]: 
[0m07:10:46.517478 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m07:10:46.518998 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m07:10:46.530867 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m07:10:46.560883 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b251f8a-0eda-4869-b373-548039d055ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205BD54CCB0>]}
[0m07:10:46.562420 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m07:10:46.562929 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m07:10:46.563955 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m07:10:46.564477 [info ] [MainThread]: 
[0m07:10:46.568071 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m07:10:46.568579 [info ] [Thread-1 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m07:10:46.569598 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m07:10:46.570105 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m07:10:46.570105 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 07:10:46.570105 => 07:10:46.570105
[0m07:10:46.570615 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m07:10:46.612727 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m07:10:46.613741 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m07:10:46.614750 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`id` bigint,`name` string,`email` string)
    
    
    
    
    
  
[0m07:10:46.614750 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m07:10:46.658994 [debug] [Thread-1 (]: SQL status: OK in 0.03999999910593033 seconds
[0m07:10:46.671701 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m07:10:46.675770 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m07:10:46.676806 [debug] [Thread-1 (]: On seed.testproj.sample: 
          insert into datalake.sample values
          (cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string))
      ...
[0m07:10:46.677415 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:

          insert into datalake.sample values
          (cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string))
      
[0m07:10:46.677918 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'node_id'
[0m07:10:46.678934 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 07:10:46.571121 => 07:10:46.678426
[0m07:10:46.775738 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  'node_id'
[0m07:10:46.776758 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b251f8a-0eda-4869-b373-548039d055ec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205BFDC35C0>]}
[0m07:10:46.777274 [error] [Thread-1 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.21s]
[0m07:10:46.778309 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m07:10:46.781381 [debug] [MainThread]: On master: ROLLBACK
[0m07:10:46.781381 [debug] [MainThread]: Opening a new connection, currently in state init
[0m07:10:46.782417 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m07:10:46.782925 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m07:10:46.783440 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m07:10:46.783946 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m07:10:46.784455 [info ] [MainThread]: 
[0m07:10:46.784962 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m07:10:46.785470 [debug] [MainThread]: Command end result
[0m07:10:46.795225 [info ] [MainThread]: 
[0m07:10:46.795737 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m07:10:46.796250 [info ] [MainThread]: 
[0m07:10:46.796759 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'node_id'
[0m07:10:46.797266 [info ] [MainThread]: 
[0m07:10:46.797266 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m07:10:46.798794 [debug] [MainThread]: Command `cli seed` failed at 07:10:46.798281 after 1.84 seconds
[0m07:10:46.798794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205BDFF3980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205BFCAD280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000205BEAF7C80>]}
[0m07:10:46.799303 [debug] [MainThread]: Flushing usage events
[0m07:13:28.727155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002798FF3C3E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002798FC47C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002798FC47B30>]}


============================== 07:13:28.731233 | ef39fd74-ff3c-47a3-a1d7-22844c63baef ==============================
[0m07:13:28.731233 [info ] [MainThread]: Running with dbt=1.7.14
[0m07:13:28.734444 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m07:13:29.059093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ef39fd74-ff3c-47a3-a1d7-22844c63baef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002799005AC90>]}
[0m07:13:29.201428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ef39fd74-ff3c-47a3-a1d7-22844c63baef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002798FF3DD90>]}
[0m07:13:29.207058 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m07:13:29.240361 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m07:13:29.345768 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:13:29.347353 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:13:29.370520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ef39fd74-ff3c-47a3-a1d7-22844c63baef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002798FF0FB60>]}
[0m07:13:29.392705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ef39fd74-ff3c-47a3-a1d7-22844c63baef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027990171400>]}
[0m07:13:29.394742 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m07:13:29.395758 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ef39fd74-ff3c-47a3-a1d7-22844c63baef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002799002A9F0>]}
[0m07:13:29.402378 [info ] [MainThread]: 
[0m07:13:29.406497 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m07:13:29.411619 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m07:13:29.464513 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m07:13:29.569890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ef39fd74-ff3c-47a3-a1d7-22844c63baef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027990173560>]}
[0m07:13:29.572448 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m07:13:29.575597 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m07:13:29.579694 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m07:13:29.581749 [info ] [MainThread]: 
[0m07:13:29.597322 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m07:13:29.599894 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m07:13:29.604018 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m07:13:29.607171 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m07:13:29.609211 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 07:13:29.608188 => 07:13:29.608188
[0m07:13:29.610740 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m07:13:29.775891 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m07:13:29.777912 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m07:13:29.779421 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`id` bigint,`name` string,`email` string)
    
    
    
    
    
  
[0m07:13:29.781445 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m07:13:29.839165 [debug] [Thread-6 (]: SQL status: OK in 0.05999999865889549 seconds
[0m07:13:29.883741 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m07:13:29.905301 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m07:13:29.906839 [debug] [Thread-6 (]: On seed.testproj.sample: 
          insert into datalake.sample values
          (cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string))
      ...
[0m07:13:29.908363 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:

          insert into datalake.sample values
          (cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string)),(cast(%s as bigint),cast(%s as string),cast(%s as string))
      
[0m07:13:29.909884 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: 'node_id'
[0m07:13:29.911937 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 07:13:29.611755 => 07:13:29.911421
[0m07:13:29.922625 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  'node_id'
[0m07:13:29.924682 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ef39fd74-ff3c-47a3-a1d7-22844c63baef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027990335E20>]}
[0m07:13:29.927284 [error] [Thread-6 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.32s]
[0m07:13:29.929325 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m07:13:29.934409 [debug] [MainThread]: On master: ROLLBACK
[0m07:13:29.936452 [debug] [MainThread]: Opening a new connection, currently in state init
[0m07:13:29.937973 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m07:13:29.939503 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m07:13:29.941029 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m07:13:29.942639 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m07:13:29.945183 [info ] [MainThread]: 
[0m07:13:29.946715 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.54 seconds (0.54s).
[0m07:13:29.948755 [debug] [MainThread]: Command end result
[0m07:13:29.972472 [info ] [MainThread]: 
[0m07:13:29.974519 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m07:13:29.976579 [info ] [MainThread]: 
[0m07:13:29.978117 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'node_id'
[0m07:13:29.980177 [info ] [MainThread]: 
[0m07:13:29.982257 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m07:13:29.987471 [debug] [MainThread]: Command `cli seed` failed at 07:13:29.986427 after 1.36 seconds
[0m07:13:29.990587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002798FD290A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002798FFB62D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002798FFB4DA0>]}
[0m07:13:29.993171 [debug] [MainThread]: Flushing usage events
[0m10:00:28.401394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019349F04B00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019349F04D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019349F04A40>]}


============================== 10:00:28.402402 | 280a8d07-c27b-44ff-bc42-52db1747aa15 ==============================
[0m10:00:28.402402 [info ] [MainThread]: Running with dbt=1.7.14
[0m10:00:28.403415 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m10:00:28.491539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '280a8d07-c27b-44ff-bc42-52db1747aa15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000193498CD850>]}
[0m10:00:28.525956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '280a8d07-c27b-44ff-bc42-52db1747aa15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001934AFAD160>]}
[0m10:00:28.527972 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m10:00:28.548278 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m10:00:28.992360 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 0 files changed.
[0m10:00:28.992360 [debug] [MainThread]: Partial parsing: deleted file: testproj://seeds\sample.csv
[0m10:00:28.998417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '280a8d07-c27b-44ff-bc42-52db1747aa15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001934B07A270>]}
[0m10:00:29.004507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '280a8d07-c27b-44ff-bc42-52db1747aa15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001934B0B0A40>]}
[0m10:00:29.004507 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m10:00:29.004507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '280a8d07-c27b-44ff-bc42-52db1747aa15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019349CC0B60>]}
[0m10:00:29.005517 [info ] [MainThread]: 
[0m10:00:29.006525 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m10:00:29.006525 [debug] [MainThread]: Command end result
[0m10:00:29.011586 [info ] [MainThread]: 
[0m10:00:29.012601 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:00:29.012601 [info ] [MainThread]: 
[0m10:00:29.013620 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=0 SKIP=0 TOTAL=0
[0m10:00:29.014628 [debug] [MainThread]: Command `cli seed` succeeded at 10:00:29.013620 after 0.64 seconds
[0m10:00:29.014628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019349BD97C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019349DC7170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019349DF9250>]}
[0m10:00:29.014628 [debug] [MainThread]: Flushing usage events
[0m10:00:40.319494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB21AF45C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB21AF4830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB21AF4560>]}


============================== 10:00:40.319494 | 602af488-2799-49c9-bec4-c2a7c8fcdc8c ==============================
[0m10:00:40.319494 [info ] [MainThread]: Running with dbt=1.7.14
[0m10:00:40.320503 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m10:00:40.405899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '602af488-2799-49c9-bec4-c2a7c8fcdc8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB21B5A0C0>]}
[0m10:00:40.440284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '602af488-2799-49c9-bec4-c2a7c8fcdc8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB2138B170>]}
[0m10:00:40.441291 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m10:00:40.448351 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m10:00:40.500058 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:00:40.501065 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:00:40.504088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '602af488-2799-49c9-bec4-c2a7c8fcdc8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB21ABD010>]}
[0m10:00:40.510142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '602af488-2799-49c9-bec4-c2a7c8fcdc8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB22C94E30>]}
[0m10:00:40.510142 [info ] [MainThread]: Found 2 models, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m10:00:40.511172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '602af488-2799-49c9-bec4-c2a7c8fcdc8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB21B5B2F0>]}
[0m10:00:40.513193 [info ] [MainThread]: 
[0m10:00:40.513193 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m10:00:40.514203 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m10:00:40.528368 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m10:00:40.548607 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '602af488-2799-49c9-bec4-c2a7c8fcdc8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB22C96870>]}
[0m10:00:40.549620 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m10:00:40.549620 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m10:00:40.550635 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m10:00:40.550635 [info ] [MainThread]: 
[0m10:00:40.552653 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m10:00:40.553664 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m10:00:40.553664 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m10:00:40.554678 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m10:00:40.559817 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m10:00:40.560827 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 10:00:40.554678 => 10:00:40.560827
[0m10:00:40.561843 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m10:00:40.592257 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m10:00:40.592257 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m10:00:40.592257 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m10:00:40.593266 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m10:00:40.624627 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m10:00:40.655044 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m10:00:40.657062 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m10:00:40.657062 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m10:00:40.668231 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m10:00:40.679436 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 10:00:40.561843 => 10:00:40.679436
[0m10:00:40.680450 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '602af488-2799-49c9-bec4-c2a7c8fcdc8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB22E2A060>]}
[0m10:00:40.681464 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.13s]
[0m10:00:40.682478 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m10:00:40.682478 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m10:00:40.683489 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m10:00:40.684498 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m10:00:40.684498 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m10:00:40.687544 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m10:00:40.688556 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 10:00:40.684498 => 10:00:40.688556
[0m10:00:40.688556 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m10:00:40.704035 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m10:00:40.707080 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m10:00:40.708092 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m10:00:40.710230 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m10:00:40.712252 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 10:00:40.688556 => 10:00:40.712252
[0m10:00:40.713262 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '602af488-2799-49c9-bec4-c2a7c8fcdc8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB22E2B440>]}
[0m10:00:40.714308 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m10:00:40.715320 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m10:00:40.716332 [debug] [MainThread]: On master: ROLLBACK
[0m10:00:40.716332 [debug] [MainThread]: Opening a new connection, currently in state init
[0m10:00:40.717343 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m10:00:40.717343 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m10:00:40.718446 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m10:00:40.718446 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m10:00:40.719451 [info ] [MainThread]: 
[0m10:00:40.720468 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.21 seconds (0.21s).
[0m10:00:40.721483 [debug] [MainThread]: Command end result
[0m10:00:40.730744 [info ] [MainThread]: 
[0m10:00:40.731779 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:00:40.731779 [info ] [MainThread]: 
[0m10:00:40.732788 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m10:00:40.733800 [debug] [MainThread]: Command `cli run` succeeded at 10:00:40.733800 after 0.44 seconds
[0m10:00:40.734869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB21389940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB22C35A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EB22C37380>]}
[0m10:00:40.734869 [debug] [MainThread]: Flushing usage events
[0m13:17:35.874062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C415629340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C418379040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C41837A7B0>]}


============================== 13:17:35.876098 | 074531af-fccf-44f1-815f-f27f2dfbb7a0 ==============================
[0m13:17:35.876098 [info ] [MainThread]: Running with dbt=1.7.14
[0m13:17:35.876098 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:17:35.976174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '074531af-fccf-44f1-815f-f27f2dfbb7a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4185A88F0>]}
[0m13:17:36.012178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '074531af-fccf-44f1-815f-f27f2dfbb7a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C417CBC6E0>]}
[0m13:17:36.012178 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:17:36.030377 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m13:17:36.618053 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m13:17:36.619063 [debug] [MainThread]: Partial parsing: added file: testproj://seeds\sample.csv
[0m13:17:36.695976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '074531af-fccf-44f1-815f-f27f2dfbb7a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C41998DA00>]}
[0m13:17:36.702044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '074531af-fccf-44f1-815f-f27f2dfbb7a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C41989A0F0>]}
[0m13:17:36.703055 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:17:36.704070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '074531af-fccf-44f1-815f-f27f2dfbb7a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C419899640>]}
[0m13:17:36.705077 [info ] [MainThread]: 
[0m13:17:36.706085 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:17:36.707091 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:17:36.715155 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:17:36.740991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '074531af-fccf-44f1-815f-f27f2dfbb7a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4186A5910>]}
[0m13:17:36.740991 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:17:36.740991 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:17:36.741999 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:17:36.741999 [info ] [MainThread]: 
[0m13:17:36.744015 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m13:17:36.745022 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m13:17:36.745022 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m13:17:36.746032 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m13:17:36.750609 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m13:17:36.751115 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 13:17:36.746032 => 13:17:36.751115
[0m13:17:36.752123 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m13:17:36.785079 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:17:36.786093 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:17:36.786093 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m13:17:36.786093 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:17:36.823201 [debug] [Thread-1 (]: SQL status: OK in 0.03999999910593033 seconds
[0m13:17:36.854091 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m13:17:36.855119 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m13:17:36.855119 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m13:17:36.862181 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m13:17:36.871764 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 13:17:36.752123 => 13:17:36.871764
[0m13:17:36.872774 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '074531af-fccf-44f1-815f-f27f2dfbb7a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4199A1880>]}
[0m13:17:36.873782 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.13s]
[0m13:17:36.873782 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m13:17:36.874790 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m13:17:36.875802 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m13:17:36.876830 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m13:17:36.877858 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m13:17:36.880893 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m13:17:36.882910 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 13:17:36.877858 => 13:17:36.881899
[0m13:17:36.883922 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m13:17:36.897111 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m13:17:36.898119 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m13:17:36.899126 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m13:17:36.900663 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m13:17:36.902178 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 13:17:36.883922 => 13:17:36.902178
[0m13:17:36.903191 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '074531af-fccf-44f1-815f-f27f2dfbb7a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C419A07C50>]}
[0m13:17:36.904199 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m13:17:36.905212 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m13:17:36.906225 [debug] [MainThread]: On master: ROLLBACK
[0m13:17:36.906225 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:17:36.906225 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m13:17:36.907235 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:17:36.907235 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:17:36.907235 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:17:36.908244 [info ] [MainThread]: 
[0m13:17:36.909252 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m13:17:36.910258 [debug] [MainThread]: Command end result
[0m13:17:36.917849 [info ] [MainThread]: 
[0m13:17:36.919882 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:17:36.920929 [info ] [MainThread]: 
[0m13:17:36.921941 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m13:17:36.923974 [debug] [MainThread]: Command `cli run` succeeded at 13:17:36.923974 after 1.09 seconds
[0m13:17:36.924989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C418341D00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C415629340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C4178BA5A0>]}
[0m13:17:36.924989 [debug] [MainThread]: Flushing usage events
[0m13:18:15.939141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC02B596A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC02E18470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC02E18650>]}


============================== 13:18:15.939141 | e7e33663-2aeb-4b40-b2d1-e57ef6de8bb4 ==============================
[0m13:18:15.939141 [info ] [MainThread]: Running with dbt=1.7.14
[0m13:18:15.940147 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:18:16.029124 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e7e33663-2aeb-4b40-b2d1-e57ef6de8bb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC02A1C860>]}
[0m13:18:16.062932 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e7e33663-2aeb-4b40-b2d1-e57ef6de8bb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC02EF0950>]}
[0m13:18:16.063434 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:18:16.073054 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m13:18:16.124127 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:18:16.125135 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:18:16.129166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e7e33663-2aeb-4b40-b2d1-e57ef6de8bb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC02E7F320>]}
[0m13:18:16.136275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e7e33663-2aeb-4b40-b2d1-e57ef6de8bb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC02FD9400>]}
[0m13:18:16.137295 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:18:16.138309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e7e33663-2aeb-4b40-b2d1-e57ef6de8bb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC02E1BE90>]}
[0m13:18:16.140329 [info ] [MainThread]: 
[0m13:18:16.140831 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:18:16.141837 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:18:16.147964 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:18:16.160586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e7e33663-2aeb-4b40-b2d1-e57ef6de8bb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC01BDA390>]}
[0m13:18:16.161092 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:18:16.161092 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:18:16.162156 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:18:16.162156 [info ] [MainThread]: 
[0m13:18:16.164613 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m13:18:16.165623 [info ] [Thread-1 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m13:18:16.165623 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m13:18:16.166635 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m13:18:16.166635 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 13:18:16.166635 => 13:18:16.166635
[0m13:18:16.166635 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m13:18:16.202172 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:18:16.203194 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m13:18:16.203194 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m13:18:16.204216 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:18:16.236259 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m13:18:16.245872 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m13:18:16.250972 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m13:18:16.250972 [debug] [Thread-1 (]: On seed.testproj.sample: 
          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m13:18:16.251975 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      
[0m13:18:16.252477 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'node_id'
[0m13:18:16.252477 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 13:18:16.166635 => 13:18:16.252477
[0m13:18:16.314271 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  'node_id'
[0m13:18:16.315280 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e7e33663-2aeb-4b40-b2d1-e57ef6de8bb4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC02F885C0>]}
[0m13:18:16.316311 [error] [Thread-1 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.15s]
[0m13:18:16.317328 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m13:18:16.320886 [debug] [MainThread]: On master: ROLLBACK
[0m13:18:16.320886 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:18:16.321893 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m13:18:16.321893 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:18:16.322903 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:18:16.322903 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:18:16.322903 [info ] [MainThread]: 
[0m13:18:16.323911 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m13:18:16.324920 [debug] [MainThread]: Command end result
[0m13:18:16.335095 [info ] [MainThread]: 
[0m13:18:16.336103 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:18:16.336103 [info ] [MainThread]: 
[0m13:18:16.336103 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'node_id'
[0m13:18:16.337112 [info ] [MainThread]: 
[0m13:18:16.337112 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m13:18:16.338122 [debug] [MainThread]: Command `cli seed` failed at 13:18:16.338122 after 0.43 seconds
[0m13:18:16.339129 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC0278CFE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC04086300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DC02E537D0>]}
[0m13:18:16.339129 [debug] [MainThread]: Flushing usage events
[0m13:19:12.753055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC908B8860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC908B8950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC908B87A0>]}


============================== 13:19:12.754064 | c8b32f5e-6348-4937-8dad-fd76d127031b ==============================
[0m13:19:12.754064 [info ] [MainThread]: Running with dbt=1.7.14
[0m13:19:12.754064 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:19:12.843189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c8b32f5e-6348-4937-8dad-fd76d127031b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC905B5610>]}
[0m13:19:12.881265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c8b32f5e-6348-4937-8dad-fd76d127031b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC908BA540>]}
[0m13:19:12.882273 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:19:12.889334 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m13:19:12.945218 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:19:12.945218 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:19:12.949247 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c8b32f5e-6348-4937-8dad-fd76d127031b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC919CD010>]}
[0m13:19:12.955410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c8b32f5e-6348-4937-8dad-fd76d127031b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC91A4CD40>]}
[0m13:19:12.956413 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:19:12.956413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8b32f5e-6348-4937-8dad-fd76d127031b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC908F72F0>]}
[0m13:19:12.958440 [info ] [MainThread]: 
[0m13:19:12.959452 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:19:12.960455 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:19:12.966726 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:19:12.980338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c8b32f5e-6348-4937-8dad-fd76d127031b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC908BA270>]}
[0m13:19:12.980338 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:19:12.981602 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:19:12.981602 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:19:12.981602 [info ] [MainThread]: 
[0m13:19:12.983618 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m13:19:12.984631 [info ] [Thread-1 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m13:19:12.984631 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m13:19:12.985643 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m13:19:12.985643 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 13:19:12.985643 => 13:19:12.985643
[0m13:19:12.985643 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m13:19:13.021375 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:19:13.022389 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m13:19:13.022389 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m13:19:13.023406 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:19:13.050352 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m13:19:13.060495 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m13:19:13.064150 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m13:19:13.065166 [debug] [Thread-1 (]: On seed.testproj.sample: 
          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m13:19:13.066178 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      
[0m13:19:13.066178 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'node_id'
[0m13:19:13.066178 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 13:19:12.986656 => 13:19:13.066178
[0m13:19:13.070719 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  'node_id'
[0m13:19:13.070719 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c8b32f5e-6348-4937-8dad-fd76d127031b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC919F9C40>]}
[0m13:19:13.071226 [error] [Thread-1 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.09s]
[0m13:19:13.072238 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m13:19:13.074260 [debug] [MainThread]: On master: ROLLBACK
[0m13:19:13.075268 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:19:13.075268 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m13:19:13.075268 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m13:19:13.075268 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:19:13.076277 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:19:13.076277 [info ] [MainThread]: 
[0m13:19:13.076277 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.12 seconds (0.12s).
[0m13:19:13.077285 [debug] [MainThread]: Command end result
[0m13:19:13.082541 [info ] [MainThread]: 
[0m13:19:13.083554 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:19:13.083554 [info ] [MainThread]: 
[0m13:19:13.084574 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'node_id'
[0m13:19:13.084574 [info ] [MainThread]: 
[0m13:19:13.084574 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m13:19:13.085584 [debug] [MainThread]: Command `cli seed` failed at 13:19:13.085584 after 0.36 seconds
[0m13:19:13.086595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC908B8860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC908B8950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AC908B87A0>]}
[0m13:19:13.086595 [debug] [MainThread]: Flushing usage events
[0m13:19:35.839108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5B35B8860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5B35BBFE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5B35037D0>]}


============================== 13:19:35.841148 | 51f460a8-51cd-4630-b6bf-2d7261373d8b ==============================
[0m13:19:35.841148 [info ] [MainThread]: Running with dbt=1.7.14
[0m13:19:35.843165 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m13:19:36.195092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '51f460a8-51cd-4630-b6bf-2d7261373d8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5B36D7EC0>]}
[0m13:19:36.317590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '51f460a8-51cd-4630-b6bf-2d7261373d8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5B36D5430>]}
[0m13:19:36.321115 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:19:36.350462 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m13:19:36.468246 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:19:36.469254 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:19:36.493095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '51f460a8-51cd-4630-b6bf-2d7261373d8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5B36A8080>]}
[0m13:19:36.511279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '51f460a8-51cd-4630-b6bf-2d7261373d8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5B38420F0>]}
[0m13:19:36.513297 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:19:36.515306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '51f460a8-51cd-4630-b6bf-2d7261373d8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5B36D6A80>]}
[0m13:19:36.521935 [info ] [MainThread]: 
[0m13:19:36.524956 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:19:36.532041 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:19:36.583224 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:19:36.656328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '51f460a8-51cd-4630-b6bf-2d7261373d8b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D5B3840350>]}
[0m13:19:36.658360 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:19:36.660373 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:19:36.662888 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:19:36.664907 [info ] [MainThread]: 
[0m13:19:36.674092 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m13:19:36.676116 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m13:19:36.679160 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m13:19:36.680675 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m13:19:36.682189 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 13:19:36.681181 => 13:19:36.681181
[0m13:19:36.683195 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m13:20:22.928053 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:20:22.931109 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m13:20:22.933127 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m13:20:22.934140 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m13:20:50.927546 [debug] [Thread-6 (]: SQL status: OK in 22.399999618530273 seconds
[0m13:21:02.144181 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m13:21:18.433542 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m13:21:18.435564 [debug] [Thread-6 (]: On seed.testproj.sample: 
          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m13:29:30.872391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B6E7E193A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B6E7E1A180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B6E7E18F80>]}


============================== 13:29:30.875449 | 0085c20f-0707-4bd4-a166-54ad99ec9198 ==============================
[0m13:29:30.875449 [info ] [MainThread]: Running with dbt=1.7.14
[0m13:29:30.877471 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m13:29:31.140393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0085c20f-0707-4bd4-a166-54ad99ec9198', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B6E7C42F30>]}
[0m13:29:31.256368 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0085c20f-0707-4bd4-a166-54ad99ec9198', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B6E7E98380>]}
[0m13:29:31.259403 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m13:29:31.289693 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m13:29:31.392794 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:29:31.394811 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:29:31.414533 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0085c20f-0707-4bd4-a166-54ad99ec9198', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B6E7DEFC20>]}
[0m13:29:31.427661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0085c20f-0707-4bd4-a166-54ad99ec9198', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B6E806C9B0>]}
[0m13:29:31.429672 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m13:29:31.431691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0085c20f-0707-4bd4-a166-54ad99ec9198', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B6E7E1B950>]}
[0m13:29:31.436732 [info ] [MainThread]: 
[0m13:29:31.440253 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m13:29:31.447362 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m13:29:31.496941 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m13:29:31.564138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0085c20f-0707-4bd4-a166-54ad99ec9198', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B6E806EDB0>]}
[0m13:29:31.566153 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:29:31.568170 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m13:29:31.570190 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m13:29:31.571197 [info ] [MainThread]: 
[0m13:29:31.582321 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m13:29:31.584334 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m13:29:31.586348 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m13:29:31.588358 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m13:29:31.589876 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 13:29:31.589368 => 13:29:31.589368
[0m13:29:31.590884 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m13:29:40.746429 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m13:29:40.748634 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m13:29:40.750719 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m13:29:40.752736 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m13:32:47.538469 [debug] [Thread-6 (]: SQL status: OK in 184.72000122070312 seconds
[0m13:32:49.072306 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m13:32:49.085618 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m13:32:49.087660 [debug] [Thread-6 (]: On seed.testproj.sample: 
          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m06:30:28.303782 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022364E1CE30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022364B58E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022364E1C0B0>]}


============================== 06:30:28.306803 | 859310e3-81ee-4744-bda1-42e6d198c873 ==============================
[0m06:30:28.306803 [info ] [MainThread]: Running with dbt=1.7.14
[0m06:30:28.308813 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m06:30:28.564058 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '859310e3-81ee-4744-bda1-42e6d198c873', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022364E95EB0>]}
[0m06:30:28.673937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '859310e3-81ee-4744-bda1-42e6d198c873', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022364E1E300>]}
[0m06:30:28.675947 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m06:30:28.716254 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m06:30:29.392828 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:30:29.394837 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:30:29.411946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '859310e3-81ee-4744-bda1-42e6d198c873', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022364E6B140>]}
[0m06:30:29.425039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '859310e3-81ee-4744-bda1-42e6d198c873', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002236506CBF0>]}
[0m06:30:29.427050 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m06:30:29.428056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '859310e3-81ee-4744-bda1-42e6d198c873', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022364992900>]}
[0m06:30:29.433084 [info ] [MainThread]: 
[0m06:30:29.436115 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m06:30:29.440135 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m06:30:29.490926 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m06:30:29.562728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '859310e3-81ee-4744-bda1-42e6d198c873', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002236506EE70>]}
[0m06:30:29.563735 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:30:29.565210 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:30:29.567224 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m06:30:29.569235 [info ] [MainThread]: 
[0m06:30:29.576399 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m06:30:29.577482 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m06:30:29.580498 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m06:30:29.581504 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m06:30:29.583514 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 06:30:29.582509 => 06:30:29.582509
[0m06:30:29.584519 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m06:30:29.715374 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:30:29.717585 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m06:30:29.718776 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m06:30:29.720791 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m06:32:20.412220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002390F0DD100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002390F0DD1C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002390F0DD160>]}


============================== 06:32:20.412220 | 52f691f9-ab3f-4934-a1cd-38d6c2543196 ==============================
[0m06:32:20.412220 [info ] [MainThread]: Running with dbt=1.7.14
[0m06:32:20.413225 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m06:32:20.495688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '52f691f9-ab3f-4934-a1cd-38d6c2543196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002390EE97500>]}
[0m06:32:20.528876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '52f691f9-ab3f-4934-a1cd-38d6c2543196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002390EA8E1B0>]}
[0m06:32:20.529881 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m06:32:20.535913 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m06:32:20.572356 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:32:20.573402 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:32:20.576542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '52f691f9-ab3f-4934-a1cd-38d6c2543196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239101ECC50>]}
[0m06:32:20.581703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '52f691f9-ab3f-4934-a1cd-38d6c2543196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023910265880>]}
[0m06:32:20.582713 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m06:32:20.582713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '52f691f9-ab3f-4934-a1cd-38d6c2543196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002390ED4F9E0>]}
[0m06:32:20.583723 [info ] [MainThread]: 
[0m06:32:20.584730 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m06:32:20.585755 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m06:32:20.588771 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m06:32:20.600845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '52f691f9-ab3f-4934-a1cd-38d6c2543196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239102BF0E0>]}
[0m06:32:20.601873 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:32:20.601873 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:32:20.602880 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m06:32:20.602880 [info ] [MainThread]: 
[0m06:32:20.604893 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m06:32:20.604893 [info ] [Thread-1 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m06:32:20.605899 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m06:32:20.605899 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m06:32:20.605899 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 06:32:20.605899 => 06:32:20.605899
[0m06:32:20.606905 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m06:32:20.635126 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:32:20.636132 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m06:32:20.636132 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m06:32:20.636132 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m06:32:20.662294 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m06:32:20.670363 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m06:32:20.672374 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m06:32:20.673379 [debug] [Thread-1 (]: On seed.testproj.sample: 
          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m06:32:20.673379 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      
[0m06:32:20.674385 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'project_root'
[0m06:32:20.674385 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 06:32:20.606905 => 06:32:20.674385
[0m06:32:20.713653 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  'project_root'
[0m06:32:20.714660 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '52f691f9-ab3f-4934-a1cd-38d6c2543196', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002391024A4E0>]}
[0m06:32:20.714660 [error] [Thread-1 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.11s]
[0m06:32:20.715665 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m06:32:20.716671 [debug] [MainThread]: On master: ROLLBACK
[0m06:32:20.716671 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:32:20.717676 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m06:32:20.717676 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m06:32:20.717676 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:32:20.717676 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:32:20.718682 [info ] [MainThread]: 
[0m06:32:20.718682 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m06:32:20.719688 [debug] [MainThread]: Command end result
[0m06:32:20.724721 [info ] [MainThread]: 
[0m06:32:20.725728 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m06:32:20.725728 [info ] [MainThread]: 
[0m06:32:20.725728 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'project_root'
[0m06:32:20.726733 [info ] [MainThread]: 
[0m06:32:20.726733 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m06:32:20.727738 [debug] [MainThread]: Command `cli seed` failed at 06:32:20.727738 after 0.34 seconds
[0m06:32:20.727738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002390E728200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002390ED1DC70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000239102D1EB0>]}
[0m06:32:20.727738 [debug] [MainThread]: Flushing usage events
[0m06:33:58.801113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002923BD69B80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002923BD687D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002923BD69E80>]}


============================== 06:33:58.803124 | e52adbf3-546a-4723-aac8-ded4c7eb9c6e ==============================
[0m06:33:58.803124 [info ] [MainThread]: Running with dbt=1.7.14
[0m06:33:58.805757 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m06:33:59.067148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e52adbf3-546a-4723-aac8-ded4c7eb9c6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002923BB58290>]}
[0m06:33:59.177849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e52adbf3-546a-4723-aac8-ded4c7eb9c6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002923AF88BF0>]}
[0m06:33:59.180861 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m06:33:59.208374 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m06:33:59.308546 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:33:59.310555 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:33:59.327659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e52adbf3-546a-4723-aac8-ded4c7eb9c6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002923BEFB470>]}
[0m06:33:59.340724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e52adbf3-546a-4723-aac8-ded4c7eb9c6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002923BFF2C90>]}
[0m06:33:59.341731 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m06:33:59.343740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e52adbf3-546a-4723-aac8-ded4c7eb9c6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002923BFF2660>]}
[0m06:33:59.348766 [info ] [MainThread]: 
[0m06:33:59.351802 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m06:33:59.356860 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m06:33:59.403904 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m06:33:59.468811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e52adbf3-546a-4723-aac8-ded4c7eb9c6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002923BFF0EF0>]}
[0m06:33:59.470826 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:33:59.471888 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:33:59.473943 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m06:33:59.475954 [info ] [MainThread]: 
[0m06:33:59.481999 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m06:33:59.484013 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m06:33:59.486025 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m06:33:59.487031 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m06:33:59.489159 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 06:33:59.488049 => 06:33:59.488049
[0m06:33:59.490164 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m06:34:30.607134 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:34:30.609150 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m06:34:30.611628 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m06:34:30.613651 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m06:52:56.377100 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018891D749B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018891D74C80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018891D748C0>]}


============================== 06:52:56.377100 | 3a33c926-a519-4345-add4-e673c924fb02 ==============================
[0m06:52:56.377100 [info ] [MainThread]: Running with dbt=1.7.14
[0m06:52:56.378119 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m06:52:56.459422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3a33c926-a519-4345-add4-e673c924fb02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018891A45EB0>]}
[0m06:52:56.491585 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3a33c926-a519-4345-add4-e673c924fb02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018891D0A330>]}
[0m06:52:56.492590 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m06:52:56.503645 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m06:52:57.003106 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m06:52:57.004125 [debug] [MainThread]: Partial parsing: added file: dbt_fabricsparknb://macros\materializations\seeds\seed.sql
[0m06:52:57.018231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3a33c926-a519-4345-add4-e673c924fb02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018891F1E570>]}
[0m06:52:57.028304 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3a33c926-a519-4345-add4-e673c924fb02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018892F14920>]}
[0m06:52:57.029309 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 448 macros, 0 groups, 0 semantic models
[0m06:52:57.029309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a33c926-a519-4345-add4-e673c924fb02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018891E740E0>]}
[0m06:52:57.030316 [info ] [MainThread]: 
[0m06:52:57.031321 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m06:52:57.032326 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m06:52:57.041391 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m06:52:57.058495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a33c926-a519-4345-add4-e673c924fb02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000188908C0350>]}
[0m06:52:57.058495 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:52:57.059504 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:52:57.059504 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m06:52:57.060511 [info ] [MainThread]: 
[0m06:52:57.062521 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m06:52:57.062521 [info ] [Thread-1 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m06:52:57.063527 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m06:52:57.063527 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m06:52:57.064532 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 06:52:57.064532 => 06:52:57.064532
[0m06:52:57.064532 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m06:52:57.094733 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:52:57.094733 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m06:52:57.095739 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m06:52:57.095739 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m06:52:57.122998 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m06:52:57.132073 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m06:52:57.134093 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m06:52:57.135104 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      
[0m06:52:57.141140 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m06:52:57.145167 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m06:52:57.155268 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:52:57.156278 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 06:52:57.064532 => 06:52:57.156278
[0m06:52:57.157287 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a33c926-a519-4345-add4-e673c924fb02', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018892FEE780>]}
[0m06:52:57.157287 [info ] [Thread-1 (]: 1 of 1 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.09s]
[0m06:52:57.158293 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m06:52:57.159299 [debug] [MainThread]: On master: ROLLBACK
[0m06:52:57.160306 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:52:57.160306 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m06:52:57.160306 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m06:52:57.161328 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:52:57.161328 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:52:57.161328 [info ] [MainThread]: 
[0m06:52:57.162333 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m06:52:57.162333 [debug] [MainThread]: Command end result
[0m06:52:57.168365 [info ] [MainThread]: 
[0m06:52:57.168365 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:52:57.169370 [info ] [MainThread]: 
[0m06:52:57.169370 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m06:52:57.170374 [debug] [MainThread]: Command `cli seed` succeeded at 06:52:57.170374 after 0.83 seconds
[0m06:52:57.171379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018891CB1370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018893061070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018891A462D0>]}
[0m06:52:57.171379 [debug] [MainThread]: Flushing usage events
[0m06:57:56.945752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF6AF787D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF6AF789B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF6AF78AD0>]}


============================== 06:57:56.946760 | 24a569bb-bd9e-4c73-bfea-28e6f34af11d ==============================
[0m06:57:56.946760 [info ] [MainThread]: Running with dbt=1.7.14
[0m06:57:56.947768 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m06:57:57.030536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '24a569bb-bd9e-4c73-bfea-28e6f34af11d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF6ABB80E0>]}
[0m06:57:57.062801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '24a569bb-bd9e-4c73-bfea-28e6f34af11d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF6ACC2330>]}
[0m06:57:57.063808 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m06:57:57.069871 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m06:57:57.114328 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m06:57:57.115336 [debug] [MainThread]: Partial parsing: updated file: dbt_fabricsparknb://macros\materializations\seeds\seed.sql
[0m06:57:57.129461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '24a569bb-bd9e-4c73-bfea-28e6f34af11d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF6C109370>]}
[0m06:57:57.134503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '24a569bb-bd9e-4c73-bfea-28e6f34af11d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF6C165F10>]}
[0m06:57:57.134503 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 448 macros, 0 groups, 0 semantic models
[0m06:57:57.135511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '24a569bb-bd9e-4c73-bfea-28e6f34af11d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF6C022600>]}
[0m06:57:57.136520 [info ] [MainThread]: 
[0m06:57:57.136520 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m06:57:57.137529 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m06:57:57.142600 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m06:57:57.153689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '24a569bb-bd9e-4c73-bfea-28e6f34af11d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF697C6DE0>]}
[0m06:57:57.154697 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:57:57.154697 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:57:57.154697 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m06:57:57.155705 [info ] [MainThread]: 
[0m06:57:57.157762 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m06:57:57.158773 [info ] [Thread-1 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m06:57:57.158773 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m06:57:57.158773 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m06:57:57.159781 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 06:57:57.159781 => 06:57:57.159781
[0m06:57:57.159781 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m06:57:57.188079 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:57:57.189088 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m06:57:57.189088 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m06:57:57.189088 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m06:57:57.210294 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m06:57:57.218378 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m06:57:57.220394 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m06:57:57.220394 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m06:57:57.221402 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      
[0m06:57:57.221402 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'project_root'
[0m06:57:57.222410 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 06:57:57.159781 => 06:57:57.222410
[0m06:57:57.258754 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  'project_root'
[0m06:57:57.259762 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24a569bb-bd9e-4c73-bfea-28e6f34af11d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF6AE7DDC0>]}
[0m06:57:57.259762 [error] [Thread-1 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.10s]
[0m06:57:57.260771 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m06:57:57.261779 [debug] [MainThread]: On master: ROLLBACK
[0m06:57:57.261779 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:57:57.262805 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m06:57:57.262805 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m06:57:57.262805 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:57:57.262805 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:57:57.263813 [info ] [MainThread]: 
[0m06:57:57.263813 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m06:57:57.264821 [debug] [MainThread]: Command end result
[0m06:57:57.268852 [info ] [MainThread]: 
[0m06:57:57.269860 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m06:57:57.269860 [info ] [MainThread]: 
[0m06:57:57.269860 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'project_root'
[0m06:57:57.270868 [info ] [MainThread]: 
[0m06:57:57.270868 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m06:57:57.271877 [debug] [MainThread]: Command `cli seed` failed at 06:57:57.271877 after 0.35 seconds
[0m06:57:57.271877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF6AD5B0B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF6C25F590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF6ABEF980>]}
[0m06:57:57.272885 [debug] [MainThread]: Flushing usage events
[0m06:58:40.949854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A4E1F5790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A4DDBDFD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A4E0495B0>]}


============================== 06:58:40.951915 | 9ed9db5b-3a0a-492f-a9eb-dea529881522 ==============================
[0m06:58:40.951915 [info ] [MainThread]: Running with dbt=1.7.14
[0m06:58:40.953935 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m06:58:41.215724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9ed9db5b-3a0a-492f-a9eb-dea529881522', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A4E2AB830>]}
[0m06:58:41.324650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9ed9db5b-3a0a-492f-a9eb-dea529881522', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A4E0F21E0>]}
[0m06:58:41.326665 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m06:58:41.354869 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m06:58:41.451375 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:58:41.453391 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:58:41.470542 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9ed9db5b-3a0a-492f-a9eb-dea529881522', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A4E376FF0>]}
[0m06:58:41.483684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9ed9db5b-3a0a-492f-a9eb-dea529881522', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A4E4A1280>]}
[0m06:58:41.485222 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 448 macros, 0 groups, 0 semantic models
[0m06:58:41.486748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9ed9db5b-3a0a-492f-a9eb-dea529881522', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A4E4A0860>]}
[0m06:58:41.491839 [info ] [MainThread]: 
[0m06:58:41.495245 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m06:58:41.501349 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m06:58:41.551281 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m06:58:41.615622 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9ed9db5b-3a0a-492f-a9eb-dea529881522', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A4E2EB500>]}
[0m06:58:41.617641 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:58:41.618650 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:58:41.620987 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m06:58:41.621997 [info ] [MainThread]: 
[0m06:58:41.629057 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m06:58:41.630432 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m06:58:41.633467 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m06:58:41.634873 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m06:58:41.635878 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 06:58:41.635878 => 06:58:41.635878
[0m06:58:41.639000 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m06:59:17.342099 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:59:17.343102 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m06:59:17.345122 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m06:59:17.346135 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m06:59:26.356333 [debug] [Thread-6 (]: SQL status: OK in 7.269999980926514 seconds
[0m06:59:27.333229 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m07:00:50.457581 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m07:00:50.458994 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m07:22:20.217379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA26D4D8B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA26A8D010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA26D4C770>]}


============================== 07:22:20.219402 | f8545d1e-305d-4965-928d-40a5c828a44a ==============================
[0m07:22:20.219402 [info ] [MainThread]: Running with dbt=1.7.14
[0m07:22:20.221423 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m07:22:20.477989 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f8545d1e-305d-4965-928d-40a5c828a44a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA26C7F770>]}
[0m07:22:20.589863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f8545d1e-305d-4965-928d-40a5c828a44a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA26E6B0B0>]}
[0m07:22:20.591897 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m07:22:20.626384 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m07:22:21.259515 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:22:21.261538 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:22:21.278775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f8545d1e-305d-4965-928d-40a5c828a44a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA26F15AF0>]}
[0m07:22:21.292933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f8545d1e-305d-4965-928d-40a5c828a44a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA26F7E870>]}
[0m07:22:21.293953 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 448 macros, 0 groups, 0 semantic models
[0m07:22:21.295975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8545d1e-305d-4965-928d-40a5c828a44a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA26F7DE50>]}
[0m07:22:21.301028 [info ] [MainThread]: 
[0m07:22:21.304061 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m07:22:21.309150 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m07:22:21.366669 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m07:22:21.437610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f8545d1e-305d-4965-928d-40a5c828a44a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA26FD6C90>]}
[0m07:22:21.439631 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m07:22:21.440641 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m07:22:21.442663 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m07:22:21.444683 [info ] [MainThread]: 
[0m07:22:21.451769 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m07:22:21.452780 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m07:22:21.455812 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m07:22:21.456823 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m07:22:21.458845 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 07:22:21.457834 => 07:22:21.457834
[0m07:22:21.459857 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m07:22:34.613104 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m07:22:34.614117 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m07:22:34.616147 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m07:22:34.617163 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m07:22:46.684607 [debug] [Thread-6 (]: SQL status: OK in 10.850000381469727 seconds
[0m07:22:47.358560 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m07:26:18.028986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220756FC8F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220756FC320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220755DACF0>]}


============================== 07:26:18.031007 | d872bc72-d75e-43e1-a9ee-7612e64bc686 ==============================
[0m07:26:18.031007 [info ] [MainThread]: Running with dbt=1.7.14
[0m07:26:18.033030 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m07:26:18.300858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd872bc72-d75e-43e1-a9ee-7612e64bc686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002207566F2C0>]}
[0m07:26:18.418976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd872bc72-d75e-43e1-a9ee-7612e64bc686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022075844560>]}
[0m07:26:18.422015 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m07:26:18.454563 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m07:26:18.550554 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:26:18.552575 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:26:18.570802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd872bc72-d75e-43e1-a9ee-7612e64bc686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022075747830>]}
[0m07:26:18.584483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd872bc72-d75e-43e1-a9ee-7612e64bc686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022075929850>]}
[0m07:26:18.586013 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 448 macros, 0 groups, 0 semantic models
[0m07:26:18.587544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd872bc72-d75e-43e1-a9ee-7612e64bc686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220756D23F0>]}
[0m07:26:18.592643 [info ] [MainThread]: 
[0m07:26:18.595176 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m07:26:18.599248 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m07:26:18.648121 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m07:26:18.715808 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd872bc72-d75e-43e1-a9ee-7612e64bc686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002207592BB90>]}
[0m07:26:18.717835 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m07:26:18.718849 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m07:26:18.720875 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m07:26:18.722927 [info ] [MainThread]: 
[0m07:26:18.729014 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m07:26:18.731050 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m07:26:18.734092 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m07:26:18.735108 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m07:26:18.737154 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 07:26:18.736123 => 07:26:18.736123
[0m07:26:18.738168 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m07:26:26.106894 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m07:26:26.107905 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m07:26:26.109930 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m07:26:26.110940 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m07:26:33.718299 [debug] [Thread-6 (]: SQL status: OK in 7.610000133514404 seconds
[0m07:26:36.210214 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m07:26:38.790628 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m07:26:38.791642 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m07:26:38.806986 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m07:26:42.327543 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m07:26:42.370170 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m07:26:42.375285 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 07:26:18.739182 => 07:26:42.374269
[0m07:26:42.378324 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd872bc72-d75e-43e1-a9ee-7612e64bc686', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022075BFCF80>]}
[0m07:26:42.379335 [info ] [Thread-6 (]: 1 of 1 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 23.64s]
[0m07:26:42.381357 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m07:26:42.386420 [debug] [MainThread]: On master: ROLLBACK
[0m07:26:42.388443 [debug] [MainThread]: Opening a new connection, currently in state init
[0m07:26:42.390505 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m07:26:42.391529 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m07:26:42.393576 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m07:26:42.394591 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m07:26:42.396616 [info ] [MainThread]: 
[0m07:26:42.398638 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 23.80 seconds (23.80s).
[0m07:26:42.400670 [debug] [MainThread]: Command end result
[0m07:26:42.416460 [info ] [MainThread]: 
[0m07:26:42.419027 [info ] [MainThread]: [32mCompleted successfully[0m
[0m07:26:42.420574 [info ] [MainThread]: 
[0m07:26:42.422114 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m07:26:42.427218 [debug] [MainThread]: Command `cli seed` succeeded at 07:26:42.426175 after 24.51 seconds
[0m07:26:42.430313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220755DACF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022075AF9820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022075AFB770>]}
[0m07:26:42.432348 [debug] [MainThread]: Flushing usage events
[0m07:32:38.693090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E342928DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E342928D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E342928FB0>]}


============================== 07:32:38.694101 | fdc9d891-770e-4631-bb9b-92f20b0219fb ==============================
[0m07:32:38.694101 [info ] [MainThread]: Running with dbt=1.7.14
[0m07:32:38.694101 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m07:32:38.772077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fdc9d891-770e-4631-bb9b-92f20b0219fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E342049D00>]}
[0m07:32:38.805511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fdc9d891-770e-4631-bb9b-92f20b0219fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E342928E60>]}
[0m07:32:38.805511 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m07:32:38.812617 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m07:32:38.853645 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m07:32:38.853645 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m07:32:38.857225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fdc9d891-770e-4631-bb9b-92f20b0219fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3439F4170>]}
[0m07:32:38.862277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fdc9d891-770e-4631-bb9b-92f20b0219fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E343AC0950>]}
[0m07:32:38.863290 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 448 macros, 0 groups, 0 semantic models
[0m07:32:38.863290 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fdc9d891-770e-4631-bb9b-92f20b0219fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E343AC0BC0>]}
[0m07:32:38.864818 [info ] [MainThread]: 
[0m07:32:38.865328 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m07:32:38.865840 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m07:32:38.869922 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m07:32:38.882595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fdc9d891-770e-4631-bb9b-92f20b0219fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E343AC2EA0>]}
[0m07:32:38.882595 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m07:32:38.882595 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m07:32:38.883607 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m07:32:38.883607 [info ] [MainThread]: 
[0m07:32:38.885628 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m07:32:38.886639 [info ] [Thread-1 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m07:32:38.886639 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m07:32:38.886639 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m07:32:38.887664 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 07:32:38.887664 => 07:32:38.887664
[0m07:32:38.887664 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m07:32:38.918041 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m07:32:38.918041 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m07:32:38.918041 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m07:32:38.919054 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m07:32:38.940339 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m07:32:38.949467 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m07:32:38.951491 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m07:32:38.951491 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m07:32:38.957042 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m07:32:38.960076 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m07:32:38.971222 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m07:32:38.972233 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 07:32:38.887664 => 07:32:38.971222
[0m07:32:38.972233 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fdc9d891-770e-4631-bb9b-92f20b0219fb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E343B1DF10>]}
[0m07:32:38.973246 [info ] [Thread-1 (]: 1 of 1 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.09s]
[0m07:32:38.973246 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m07:32:38.974257 [debug] [MainThread]: On master: ROLLBACK
[0m07:32:38.974257 [debug] [MainThread]: Opening a new connection, currently in state init
[0m07:32:38.975270 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m07:32:38.975270 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m07:32:38.975270 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m07:32:38.976280 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m07:32:38.976280 [info ] [MainThread]: 
[0m07:32:38.977291 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.11 seconds (0.11s).
[0m07:32:38.977291 [debug] [MainThread]: Command end result
[0m07:32:38.983384 [info ] [MainThread]: 
[0m07:32:38.983384 [info ] [MainThread]: [32mCompleted successfully[0m
[0m07:32:38.983384 [info ] [MainThread]: 
[0m07:32:38.984397 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m07:32:38.985409 [debug] [MainThread]: Command `cli seed` succeeded at 07:32:38.984397 after 0.31 seconds
[0m07:32:38.985409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E34250BD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E3428C1DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E342617950>]}
[0m07:32:38.985409 [debug] [MainThread]: Flushing usage events
[0m16:54:28.825824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025ED75788F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025ED7578AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025ED75787A0>]}


============================== 16:54:28.826832 | addc9bdd-2242-4886-ad8a-a53ab422bea7 ==============================
[0m16:54:28.826832 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:54:28.826832 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:54:28.914980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'addc9bdd-2242-4886-ad8a-a53ab422bea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025ED7216D50>]}
[0m16:54:28.951511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'addc9bdd-2242-4886-ad8a-a53ab422bea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025ED6D4CEC0>]}
[0m16:54:28.953599 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:54:28.965717 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:54:29.828359 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 0 files added, 0 files changed.
[0m16:54:29.828359 [debug] [MainThread]: Partial parsing: deleted file: dbt_fabricsparknb://macros\materializations\seeds\seed.sql
[0m16:54:29.835433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'addc9bdd-2242-4886-ad8a-a53ab422bea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025ED86B85C0>]}
[0m16:54:29.846559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'addc9bdd-2242-4886-ad8a-a53ab422bea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025ED8724560>]}
[0m16:54:29.847574 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:54:29.847574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'addc9bdd-2242-4886-ad8a-a53ab422bea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025ED87242F0>]}
[0m16:54:29.849602 [info ] [MainThread]: 
[0m16:54:29.850614 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:54:29.851621 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:54:29.863882 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m16:54:29.883084 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'addc9bdd-2242-4886-ad8a-a53ab422bea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025ED5D2A030>]}
[0m16:54:29.884123 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:54:29.884123 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:54:29.885138 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:54:29.885138 [info ] [MainThread]: 
[0m16:54:29.888179 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:54:29.889191 [info ] [Thread-1 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m16:54:29.889191 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m16:54:29.890202 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:54:29.890202 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:54:29.890202 => 16:54:29.890202
[0m16:54:29.890202 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:54:29.932615 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:54:29.934690 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:54:29.934690 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m16:54:29.935714 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:54:29.983046 [debug] [Thread-1 (]: SQL status: OK in 0.05000000074505806 seconds
[0m16:54:29.992783 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m16:54:29.995321 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:54:29.995830 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m16:54:30.001412 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:54:30.004977 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m16:54:30.016171 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:54:30.018230 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:54:29.891210 => 16:54:30.018230
[0m16:54:30.018741 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'addc9bdd-2242-4886-ad8a-a53ab422bea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025ED86EA180>]}
[0m16:54:30.019248 [info ] [Thread-1 (]: 1 of 1 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.13s]
[0m16:54:30.020289 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:54:30.021305 [debug] [MainThread]: On master: ROLLBACK
[0m16:54:30.021812 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:54:30.021812 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:54:30.022325 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:54:30.022325 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:54:30.022835 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:54:30.023346 [info ] [MainThread]: 
[0m16:54:30.023853 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m16:54:30.024360 [debug] [MainThread]: Command end result
[0m16:54:30.028927 [info ] [MainThread]: 
[0m16:54:30.029443 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:54:30.029950 [info ] [MainThread]: 
[0m16:54:30.030458 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m16:54:30.030965 [debug] [MainThread]: Command `cli seed` succeeded at 16:54:30.030965 after 1.25 seconds
[0m16:54:30.031472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025ED7509DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025ED753EC30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025ED747E2D0>]}
[0m16:54:30.031472 [debug] [MainThread]: Flushing usage events
[0m16:55:13.826017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6B00D5280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6B00D4BC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6B00D51C0>]}


============================== 16:55:13.826017 | 69f5566d-66ad-4480-8c55-bfc03095e127 ==============================
[0m16:55:13.826017 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:55:13.827026 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:55:13.919862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '69f5566d-66ad-4480-8c55-bfc03095e127', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6B00D4BC0>]}
[0m16:55:13.957380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '69f5566d-66ad-4480-8c55-bfc03095e127', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6AFC743B0>]}
[0m16:55:13.958413 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:55:13.966493 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:55:14.025927 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:55:14.025927 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:55:14.028951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '69f5566d-66ad-4480-8c55-bfc03095e127', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6B0117F20>]}
[0m16:55:14.036050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '69f5566d-66ad-4480-8c55-bfc03095e127', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6B12841D0>]}
[0m16:55:14.036050 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:55:14.037061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '69f5566d-66ad-4480-8c55-bfc03095e127', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6B12843E0>]}
[0m16:55:14.038073 [info ] [MainThread]: 
[0m16:55:14.039084 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:55:14.040093 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:55:14.044131 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m16:55:14.058394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '69f5566d-66ad-4480-8c55-bfc03095e127', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6B00272C0>]}
[0m16:55:14.058394 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:55:14.059402 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:55:14.059402 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:55:14.059402 [info ] [MainThread]: 
[0m16:55:14.061419 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:55:14.062428 [info ] [Thread-1 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m16:55:14.062428 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m16:55:14.063436 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:55:14.063436 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:55:14.063436 => 16:55:14.063436
[0m16:55:14.063436 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:55:14.096847 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:55:14.096847 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:55:14.097855 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m16:55:14.097855 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:55:14.123195 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m16:55:14.131274 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m16:55:14.133300 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:55:14.134309 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m16:55:14.143428 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:55:14.147489 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m16:55:14.159625 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:55:14.160633 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:55:14.064458 => 16:55:14.160633
[0m16:55:14.161639 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '69f5566d-66ad-4480-8c55-bfc03095e127', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6B121E300>]}
[0m16:55:14.161639 [info ] [Thread-1 (]: 1 of 1 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.10s]
[0m16:55:14.162647 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:55:14.163653 [debug] [MainThread]: On master: ROLLBACK
[0m16:55:14.163653 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:55:14.163653 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:55:14.163653 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:55:14.164662 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:55:14.164662 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:55:14.164662 [info ] [MainThread]: 
[0m16:55:14.165670 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m16:55:14.165670 [debug] [MainThread]: Command end result
[0m16:55:14.170744 [info ] [MainThread]: 
[0m16:55:14.170744 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:55:14.171753 [info ] [MainThread]: 
[0m16:55:14.171753 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m16:55:14.172763 [debug] [MainThread]: Command `cli seed` succeeded at 16:55:14.172763 after 0.38 seconds
[0m16:55:14.173775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6B009FF80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6B12D9010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D6B12D95E0>]}
[0m16:55:14.173775 [debug] [MainThread]: Flushing usage events
[0m16:55:27.320331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0BA228410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0BA228380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0BA228770>]}


============================== 16:55:27.320331 | d0e3e063-e843-468e-b841-a5b6e8149f8d ==============================
[0m16:55:27.320331 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:55:27.321338 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:55:27.407378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd0e3e063-e843-468e-b841-a5b6e8149f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0BA1EC1D0>]}
[0m16:55:27.445438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd0e3e063-e843-468e-b841-a5b6e8149f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0BA0F5670>]}
[0m16:55:27.447468 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:55:27.456688 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:55:27.504241 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:55:27.504241 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:55:27.507266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd0e3e063-e843-468e-b841-a5b6e8149f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0BA1EFFE0>]}
[0m16:55:27.514416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd0e3e063-e843-468e-b841-a5b6e8149f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0BB3C4CB0>]}
[0m16:55:27.514416 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:55:27.515434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0e3e063-e843-468e-b841-a5b6e8149f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0B9FD61B0>]}
[0m16:55:27.516447 [info ] [MainThread]: 
[0m16:55:27.517457 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:55:27.519475 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:55:27.523514 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m16:55:27.535663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd0e3e063-e843-468e-b841-a5b6e8149f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0BB3C6B10>]}
[0m16:55:27.536673 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:55:27.536673 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:55:27.537687 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:55:27.537687 [info ] [MainThread]: 
[0m16:55:27.539705 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:55:27.539705 [info ] [Thread-1 (]: 1 of 2 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m16:55:27.540714 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:55:27.540714 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:55:27.544773 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:55:27.545782 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:55:27.540714 => 16:55:27.545782
[0m16:55:27.546789 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:55:27.581329 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:55:27.582345 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:55:27.582345 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:55:27.583377 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:55:27.609776 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m16:55:27.641695 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m16:55:27.642706 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:55:27.642706 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m16:55:27.656023 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:55:27.665113 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:55:27.546789 => 16:55:27.664105
[0m16:55:27.665113 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd0e3e063-e843-468e-b841-a5b6e8149f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0BB41D250>]}
[0m16:55:27.666618 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.13s]
[0m16:55:27.667630 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:55:27.669670 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:55:27.669670 [info ] [Thread-1 (]: 2 of 2 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m16:55:27.670681 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now model.testproj.my_second_dbt_model)
[0m16:55:27.670681 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m16:55:27.672706 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m16:55:27.673714 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 16:55:27.671696 => 16:55:27.673714
[0m16:55:27.673714 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m16:55:27.689966 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m16:55:27.690975 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:55:27.690975 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m16:55:27.691987 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:55:27.694032 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 16:55:27.674726 => 16:55:27.694032
[0m16:55:27.695040 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd0e3e063-e843-468e-b841-a5b6e8149f8d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0BB562390>]}
[0m16:55:27.695040 [info ] [Thread-1 (]: 2 of 2 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m16:55:27.696049 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:55:27.697109 [debug] [MainThread]: On master: ROLLBACK
[0m16:55:27.697109 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:55:27.698112 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:55:27.698112 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:55:27.698112 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:55:27.698112 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:55:27.699121 [info ] [MainThread]: 
[0m16:55:27.699121 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 0 minutes and 0.18 seconds (0.18s).
[0m16:55:27.700131 [debug] [MainThread]: Command end result
[0m16:55:27.705177 [info ] [MainThread]: 
[0m16:55:27.705177 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:55:27.706188 [info ] [MainThread]: 
[0m16:55:27.706188 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m16:55:27.708218 [debug] [MainThread]: Command `cli run` succeeded at 16:55:27.708218 after 0.41 seconds
[0m16:55:27.708218 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0B9BCFA70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0BA1B9B80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D0BB2F2FC0>]}
[0m16:55:27.709233 [debug] [MainThread]: Flushing usage events
[0m16:57:17.938237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000243193A85C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000243193A87D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000243193A8CE0>]}


============================== 16:57:17.939245 | 2af47dd7-1def-454c-a40e-7c2715f8146d ==============================
[0m16:57:17.939245 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:57:17.939245 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:57:18.033168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2af47dd7-1def-454c-a40e-7c2715f8146d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002431898FFB0>]}
[0m16:57:18.070663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2af47dd7-1def-454c-a40e-7c2715f8146d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024318B3B530>]}
[0m16:57:18.071681 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:57:18.080816 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:57:18.134555 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:57:18.135567 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:57:18.139602 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2af47dd7-1def-454c-a40e-7c2715f8146d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002431A519670>]}
[0m16:57:18.146715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2af47dd7-1def-454c-a40e-7c2715f8146d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002431A541A30>]}
[0m16:57:18.147733 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m16:57:18.150776 [info ] [MainThread]: 
[0m16:57:18.151796 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:57:18.153818 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:57:18.158915 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m16:57:18.173142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2af47dd7-1def-454c-a40e-7c2715f8146d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002431898EB70>]}
[0m16:57:18.174157 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:57:18.174157 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:57:18.175165 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:57:18.175165 [info ] [MainThread]: 
[0m16:57:18.177184 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:57:18.178194 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m16:57:18.179207 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:57:18.179207 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:57:18.185329 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:57:18.188409 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:57:18.179207 => 16:57:18.187364
[0m16:57:18.188409 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:57:18.224946 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:57:18.224946 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:57:18.225956 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:57:18.225956 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:57:18.256413 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m16:57:18.275620 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m16:57:18.276627 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:57:18.276627 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m16:57:18.287840 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:57:18.295427 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:57:18.188409 => 16:57:18.295427
[0m16:57:18.296434 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2af47dd7-1def-454c-a40e-7c2715f8146d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000243193A8470>]}
[0m16:57:18.296434 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.12s]
[0m16:57:18.297444 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:57:18.297444 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:57:18.298454 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m16:57:18.298454 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:57:18.299465 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:57:18.299465 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:57:18.299465 => 16:57:18.299465
[0m16:57:18.299465 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:57:18.325870 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:57:18.326886 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m16:57:18.327898 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:57:18.336984 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m16:57:18.338999 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:57:18.340027 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m16:57:18.347126 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:57:18.352203 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m16:57:18.356288 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:57:18.357295 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:57:18.300475 => 16:57:18.356288
[0m16:57:18.357295 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2af47dd7-1def-454c-a40e-7c2715f8146d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002431A4E72C0>]}
[0m16:57:18.358304 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.06s]
[0m16:57:18.358304 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:57:18.359312 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:57:18.359312 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m16:57:18.360320 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m16:57:18.360320 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:57:18.367390 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:57:18.368404 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 16:57:18.360320 => 16:57:18.368404
[0m16:57:18.368404 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:57:18.376517 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:57:18.378535 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:57:18.378535 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m16:57:18.380566 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:57:18.381593 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 16:57:18.369418 => 16:57:18.381593
[0m16:57:18.383642 [debug] [Thread-1 (]: [31mInternal error executing target\run\testproj\models\example\schema.yml\not_null_my_first_dbt_model_id.sql[0m
Internal Error
  dbt internally failed to execute test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: Returned 0 rows, but expected 1 row

This is an error in dbt. Please try again. If the error persists, open an issue at https://github.com/dbt-labs/dbt-core
[0m16:57:18.383642 [error] [Thread-1 (]: 3 of 7 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 0.02s]
[0m16:57:18.392287 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:57:18.393297 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:57:18.393297 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m16:57:18.394309 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m16:57:18.394309 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:57:18.399350 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:57:18.400361 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 16:57:18.395319 => 16:57:18.400361
[0m16:57:18.401401 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:57:18.402415 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:57:18.403430 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:57:18.404444 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m16:57:18.405460 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:57:18.406473 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 16:57:18.401401 => 16:57:18.406473
[0m16:57:18.406473 [debug] [Thread-1 (]: [31mInternal error executing target\run\testproj\models\example\schema.yml\unique_my_first_dbt_model_id.sql[0m
Internal Error
  dbt internally failed to execute test.testproj.unique_my_first_dbt_model_id.16e066b321: Returned 0 rows, but expected 1 row

This is an error in dbt. Please try again. If the error persists, open an issue at https://github.com/dbt-labs/dbt-core
[0m16:57:18.407494 [error] [Thread-1 (]: 4 of 7 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 0.01s]
[0m16:57:18.408508 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:57:18.408508 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:57:18.409520 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m16:57:18.409520 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:57:18.409520 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:57:18.410527 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m16:57:18.410527 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:57:18.411538 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:57:18.411538 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m16:57:18.411538 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:57:18.413560 [debug] [MainThread]: On master: ROLLBACK
[0m16:57:18.413560 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:57:18.414574 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:57:18.414574 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:57:18.414574 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:57:18.415596 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:57:18.416644 [info ] [MainThread]: 
[0m16:57:18.416644 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m16:57:18.418668 [debug] [MainThread]: Command end result
[0m16:57:18.425740 [info ] [MainThread]: 
[0m16:57:18.425740 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:57:18.426752 [info ] [MainThread]: 
[0m16:57:18.426752 [error] [MainThread]:   Internal Error
  dbt internally failed to execute test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: Returned 0 rows, but expected 1 row
[0m16:57:18.426752 [info ] [MainThread]: 
[0m16:57:18.427770 [error] [MainThread]:   Internal Error
  dbt internally failed to execute test.testproj.unique_my_first_dbt_model_id.16e066b321: Returned 0 rows, but expected 1 row
[0m16:57:18.427770 [info ] [MainThread]: 
[0m16:57:18.427770 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=2 SKIP=3 TOTAL=7
[0m16:57:18.428778 [debug] [MainThread]: Command `cli build` failed at 16:57:18.428778 after 0.52 seconds
[0m16:57:18.428778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000243193A8890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000243193A87D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000243193A8CE0>]}
[0m16:57:18.429786 [debug] [MainThread]: Flushing usage events
[0m17:10:04.288174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230A2E98170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230A3BC9790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230A3668560>]}


============================== 17:10:04.291217 | 7694742c-2e90-4a48-be2b-8d993f76a63e ==============================
[0m17:10:04.291217 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:10:04.294260 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:10:04.612192 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7694742c-2e90-4a48-be2b-8d993f76a63e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230A4078740>]}
[0m17:10:04.741508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7694742c-2e90-4a48-be2b-8d993f76a63e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230A3156E40>]}
[0m17:10:04.744534 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:10:04.784638 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:10:05.729272 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:10:05.731293 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:10:05.751603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7694742c-2e90-4a48-be2b-8d993f76a63e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230A3C2E6C0>]}
[0m17:10:05.770427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7694742c-2e90-4a48-be2b-8d993f76a63e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230A4191D90>]}
[0m17:10:05.771441 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:10:05.778519 [info ] [MainThread]: 
[0m17:10:05.783619 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:10:05.791775 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:10:05.854377 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m17:10:05.937912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7694742c-2e90-4a48-be2b-8d993f76a63e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230A41E5790>]}
[0m17:10:05.939935 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:10:05.941955 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:10:05.943975 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:10:05.946001 [info ] [MainThread]: 
[0m17:10:05.955186 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m17:10:05.957209 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m17:10:05.961274 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:10:05.963323 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:10:06.003567 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:10:06.006590 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:10:05.965352 => 17:10:06.005582
[0m17:10:06.009667 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:10:06.149443 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:10:06.151556 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:10:06.153575 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:10:06.155595 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m17:10:10.184944 [debug] [Thread-6 (]: SQL status: OK in 4.03000020980835 seconds
[0m17:10:10.285866 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:10:10.287889 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:10:10.288901 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:10:11.110390 [debug] [Thread-6 (]: SQL status: OK in 0.8199999928474426 seconds
[0m17:10:11.161600 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:10:06.010685 => 17:10:11.160586
[0m17:10:11.165648 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7694742c-2e90-4a48-be2b-8d993f76a63e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230A43D2F30>]}
[0m17:10:11.167691 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 5.21s]
[0m17:10:11.171799 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:10:11.173862 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m17:10:11.175896 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m17:10:11.180989 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:10:11.184078 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m17:10:11.187169 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 17:10:11.186156 => 17:10:11.186156
[0m17:10:11.190238 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m17:10:11.328256 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:10:11.330275 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m17:10:11.869944 [debug] [Thread-6 (]: SQL status: OK in 0.5400000214576721 seconds
[0m17:10:11.912085 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m17:10:12.558685 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:10:12.564949 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:10:13.959504 [debug] [Thread-6 (]: SQL status: OK in 1.3899999856948853 seconds
[0m17:10:13.980087 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:10:14.003671 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:10:14.010839 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 17:10:11.191256 => 17:10:14.009810
[0m17:10:14.013922 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7694742c-2e90-4a48-be2b-8d993f76a63e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000230A43D3D40>]}
[0m17:10:14.016968 [info ] [Thread-6 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 2.83s]
[0m17:10:14.020051 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m17:10:14.023224 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:10:14.025243 [info ] [Thread-6 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:10:14.029318 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:10:14.031344 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:10:14.087317 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:10:14.092464 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:10:14.032354 => 17:10:14.091405
[0m17:10:14.094518 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:10:14.147612 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:10:14.151729 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:10:14.154794 [debug] [Thread-6 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:10:15.467791 [debug] [Thread-6 (]: SQL status: OK in 1.309999942779541 seconds
[0m17:10:15.473941 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:10:14.096564 => 17:10:15.472878
[0m17:10:15.476420 [debug] [Thread-6 (]: [31mInternal error executing target\run\testproj\models\example\schema.yml\not_null_my_first_dbt_model_id.sql[0m
Internal Error
  dbt internally failed to execute test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: Returned 0 rows, but expected 1 row

This is an error in dbt. Please try again. If the error persists, open an issue at https://github.com/dbt-labs/dbt-core
[0m17:10:15.479492 [error] [Thread-6 (]: 3 of 7 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 1.45s]
[0m17:10:15.482644 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:10:15.484670 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:10:15.486722 [info ] [Thread-6 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:10:15.492922 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:10:15.495973 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:10:15.519389 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:10:15.523479 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:10:15.496981 => 17:10:15.522467
[0m17:10:15.526568 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:10:15.541449 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:10:15.547581 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:10:15.549678 [debug] [Thread-6 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:14:59.017445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F2E40620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F2E40710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F2E409E0>]}


============================== 17:14:59.018472 | 14b437ab-d30a-45ec-bff6-e4e729d57206 ==============================
[0m17:14:59.018472 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:14:59.019483 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:14:59.119689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '14b437ab-d30a-45ec-bff6-e4e729d57206', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F2B4BCB0>]}
[0m17:14:59.158835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '14b437ab-d30a-45ec-bff6-e4e729d57206', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F20E51F0>]}
[0m17:14:59.159851 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:14:59.169017 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:14:59.222805 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:14:59.223814 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:14:59.226837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '14b437ab-d30a-45ec-bff6-e4e729d57206', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F3F76510>]}
[0m17:14:59.233911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '14b437ab-d30a-45ec-bff6-e4e729d57206', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F3FC24B0>]}
[0m17:14:59.233911 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:14:59.234922 [info ] [MainThread]: 
[0m17:14:59.235938 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:14:59.236947 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:14:59.241005 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m17:14:59.254702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '14b437ab-d30a-45ec-bff6-e4e729d57206', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F4016900>]}
[0m17:14:59.255714 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:14:59.255714 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:14:59.256722 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:14:59.256722 [info ] [MainThread]: 
[0m17:14:59.258738 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:14:59.258738 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m17:14:59.259748 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:14:59.259748 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:14:59.263791 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:14:59.264799 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:14:59.259748 => 17:14:59.264799
[0m17:14:59.264799 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:14:59.296172 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:14:59.296172 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:14:59.296172 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:14:59.297180 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:14:59.322650 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m17:14:59.340870 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:14:59.341878 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:14:59.341878 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:14:59.351509 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:14:59.359584 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:14:59.265810 => 17:14:59.359584
[0m17:14:59.360624 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '14b437ab-d30a-45ec-bff6-e4e729d57206', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F251BC80>]}
[0m17:14:59.360624 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.10s]
[0m17:14:59.361638 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:14:59.361638 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m17:14:59.362641 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m17:14:59.363143 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:14:59.363143 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m17:14:59.363143 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 17:14:59.363143 => 17:14:59.363143
[0m17:14:59.364157 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m17:14:59.388921 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:14:59.389930 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m17:14:59.390939 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:14:59.398515 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m17:14:59.400531 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:14:59.401538 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:14:59.406587 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:14:59.410633 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:14:59.415757 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:14:59.416768 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 17:14:59.364157 => 17:14:59.416768
[0m17:14:59.417788 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '14b437ab-d30a-45ec-bff6-e4e729d57206', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F4174260>]}
[0m17:14:59.418816 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.05s]
[0m17:14:59.418816 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m17:14:59.419824 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:14:59.419824 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:14:59.420831 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:14:59.420831 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:14:59.427911 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:14:59.428924 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:14:59.420831 => 17:14:59.428924
[0m17:14:59.428924 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:14:59.437013 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:14:59.438034 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:14:59.438034 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:14:59.439050 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:14:59.439050 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:14:59.428924 => 17:14:59.439050
[0m17:14:59.480981 [debug] [Thread-1 (]: Compilation Error in test not_null_my_first_dbt_model_id (models\example\schema.yml)
  string indices must be integers, not 'str'
  
  > in macro materialization_test_default (macros\materializations\tests\test.sql)
  > called by test not_null_my_first_dbt_model_id (models\example\schema.yml)
[0m17:14:59.482019 [error] [Thread-1 (]: 3 of 7 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 0.06s]
[0m17:14:59.483153 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:14:59.484161 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:14:59.485183 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:14:59.486192 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:14:59.486710 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:14:59.491253 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:14:59.491253 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:14:59.486710 => 17:14:59.491253
[0m17:14:59.492261 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:14:59.493269 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:14:59.494277 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:14:59.494277 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:14:59.495284 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:14:59.495284 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:14:59.492261 => 17:14:59.495284
[0m17:14:59.497307 [debug] [Thread-1 (]: Compilation Error in test unique_my_first_dbt_model_id (models\example\schema.yml)
  string indices must be integers, not 'str'
  
  > in macro materialization_test_default (macros\materializations\tests\test.sql)
  > called by test unique_my_first_dbt_model_id (models\example\schema.yml)
[0m17:14:59.497307 [error] [Thread-1 (]: 4 of 7 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 0.01s]
[0m17:14:59.498315 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:14:59.498315 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:14:59.499354 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m17:14:59.499354 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:14:59.500363 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:14:59.500363 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m17:14:59.501372 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:14:59.501372 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:14:59.501372 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m17:14:59.502379 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:14:59.502379 [debug] [MainThread]: On master: ROLLBACK
[0m17:14:59.503386 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:14:59.503386 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:14:59.503386 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:14:59.503386 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:14:59.504396 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:14:59.504396 [info ] [MainThread]: 
[0m17:14:59.504396 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.27 seconds (0.27s).
[0m17:14:59.505403 [debug] [MainThread]: Command end result
[0m17:14:59.511465 [info ] [MainThread]: 
[0m17:14:59.511465 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m17:14:59.511465 [info ] [MainThread]: 
[0m17:14:59.512480 [error] [MainThread]:   Compilation Error in test not_null_my_first_dbt_model_id (models\example\schema.yml)
  string indices must be integers, not 'str'
  
  > in macro materialization_test_default (macros\materializations\tests\test.sql)
  > called by test not_null_my_first_dbt_model_id (models\example\schema.yml)
[0m17:14:59.512480 [info ] [MainThread]: 
[0m17:14:59.513489 [error] [MainThread]:   Compilation Error in test unique_my_first_dbt_model_id (models\example\schema.yml)
  string indices must be integers, not 'str'
  
  > in macro materialization_test_default (macros\materializations\tests\test.sql)
  > called by test unique_my_first_dbt_model_id (models\example\schema.yml)
[0m17:14:59.513489 [info ] [MainThread]: 
[0m17:14:59.514520 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=2 SKIP=3 TOTAL=7
[0m17:14:59.515536 [debug] [MainThread]: Command `cli build` failed at 17:14:59.515536 after 0.53 seconds
[0m17:14:59.516548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F2E5DB80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F2B48B30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0F1E6C200>]}
[0m17:14:59.517061 [debug] [MainThread]: Flushing usage events
[0m17:15:45.056417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001322BFA9F70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001322BFAA330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001322BFA8440>]}


============================== 17:15:45.059449 | ec404add-2749-4057-beb1-3819533b4ed0 ==============================
[0m17:15:45.059449 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:15:45.061478 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m17:15:45.389070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ec404add-2749-4057-beb1-3819533b4ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001322C02B830>]}
[0m17:15:45.514214 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ec404add-2749-4057-beb1-3819533b4ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001322C1658B0>]}
[0m17:15:45.518283 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:15:45.547642 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:15:45.664745 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:15:45.666763 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:15:45.685960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ec404add-2749-4057-beb1-3819533b4ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001322C199DF0>]}
[0m17:15:45.701101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ec404add-2749-4057-beb1-3819533b4ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001322C232CC0>]}
[0m17:15:45.702111 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:15:45.709197 [info ] [MainThread]: 
[0m17:15:45.712222 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:15:45.718456 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:15:45.771351 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m17:15:45.838767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ec404add-2749-4057-beb1-3819533b4ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001322C19A660>]}
[0m17:15:45.839776 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:15:45.841802 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:15:45.843818 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:15:45.845849 [info ] [MainThread]: 
[0m17:15:45.855009 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m17:15:45.857080 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m17:15:45.860115 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:15:45.862139 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:15:45.906541 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:15:45.908561 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:15:45.863147 => 17:15:45.908561
[0m17:15:45.910583 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:15:46.053206 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:15:46.055228 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:15:46.056255 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:15:46.058275 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m17:15:49.426381 [debug] [Thread-6 (]: SQL status: OK in 3.369999885559082 seconds
[0m17:15:49.518364 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:15:49.522432 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:15:49.523940 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:15:51.060923 [debug] [Thread-6 (]: SQL status: OK in 1.5399999618530273 seconds
[0m17:15:51.102967 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:15:45.911598 => 17:15:51.101961
[0m17:15:51.104989 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ec404add-2749-4057-beb1-3819533b4ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001322C423B00>]}
[0m17:15:51.107008 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 5.25s]
[0m17:15:51.110068 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:15:51.112099 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m17:15:51.114126 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m17:15:51.118182 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:15:51.120224 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m17:15:51.124381 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 17:15:51.122358 => 17:15:51.123366
[0m17:15:51.126412 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m17:15:51.268053 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:15:51.271171 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m17:15:57.993381 [debug] [Thread-6 (]: SQL status: OK in 6.71999979019165 seconds
[0m17:15:58.032981 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m17:16:00.011753 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:16:00.014315 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:16:07.046312 [debug] [Thread-6 (]: SQL status: OK in 7.03000020980835 seconds
[0m17:16:07.066785 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:16:07.088162 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:16:07.092739 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 17:15:51.128452 => 17:16:07.092739
[0m17:16:07.096861 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ec404add-2749-4057-beb1-3819533b4ed0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001322C4216D0>]}
[0m17:16:07.097880 [info ] [Thread-6 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 15.98s]
[0m17:16:07.099913 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m17:16:07.101948 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:16:07.102963 [info ] [Thread-6 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:16:07.106001 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:16:07.107017 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:16:07.150245 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:16:07.154330 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:16:07.108035 => 17:16:07.153310
[0m17:16:07.156357 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:16:07.199803 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:16:07.201830 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:16:07.203888 [debug] [Thread-6 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:16:44.013898 [debug] [Thread-6 (]: SQL status: OK in 36.810001373291016 seconds
[0m17:16:44.017087 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:16:07.157370 => 17:16:44.015931
[0m17:16:44.028235 [debug] [Thread-6 (]: Compilation Error in test not_null_my_first_dbt_model_id (models\example\schema.yml)
  string indices must be integers, not 'str'
  
  > in macro materialization_test_default (macros\materializations\tests\test.sql)
  > called by test not_null_my_first_dbt_model_id (models\example\schema.yml)
[0m17:16:44.030259 [error] [Thread-6 (]: 3 of 7 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 36.93s]
[0m17:16:44.033324 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:16:44.035357 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:16:44.038393 [info ] [Thread-6 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:16:44.041435 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:16:44.044534 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:16:44.068970 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:16:44.074064 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:16:44.046568 => 17:16:44.072022
[0m17:16:44.078244 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:16:44.092570 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:16:44.096695 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:16:44.098746 [debug] [Thread-6 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:18:06.758769 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B9F7F8560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B9F7FA0C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B9F7F8170>]}


============================== 17:18:06.762812 | cbcd146e-0f9d-4389-906d-3ebdfe1bd16b ==============================
[0m17:18:06.762812 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:18:06.764832 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m17:18:07.059034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cbcd146e-0f9d-4389-906d-3ebdfe1bd16b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B9FB2ACF0>]}
[0m17:18:07.179457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cbcd146e-0f9d-4389-906d-3ebdfe1bd16b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B9F9E2330>]}
[0m17:18:07.182498 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:18:07.211933 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:18:07.319034 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:18:07.321053 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:18:07.340258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cbcd146e-0f9d-4389-906d-3ebdfe1bd16b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B9FCF3A70>]}
[0m17:18:07.360578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cbcd146e-0f9d-4389-906d-3ebdfe1bd16b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B9FC97680>]}
[0m17:18:07.362177 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:18:07.368763 [info ] [MainThread]: 
[0m17:18:07.372804 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:18:07.382130 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:18:07.441369 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m17:18:07.516665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cbcd146e-0f9d-4389-906d-3ebdfe1bd16b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B9FC95640>]}
[0m17:18:07.519707 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:18:07.521733 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:18:07.523764 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:18:07.525784 [info ] [MainThread]: 
[0m17:18:07.533938 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m17:18:07.535956 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m17:18:07.538987 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:18:07.539998 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:18:07.597034 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:18:07.602217 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:18:07.542024 => 17:18:07.601175
[0m17:18:07.604250 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:18:07.758197 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:18:07.760220 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:18:07.761266 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:18:07.763297 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m17:18:09.741969 [debug] [Thread-6 (]: SQL status: OK in 1.9800000190734863 seconds
[0m17:18:09.845035 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:18:09.848111 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:18:09.849641 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:18:10.443699 [debug] [Thread-6 (]: SQL status: OK in 0.5899999737739563 seconds
[0m17:18:10.490641 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:18:07.605785 => 17:18:10.489614
[0m17:18:10.495725 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cbcd146e-0f9d-4389-906d-3ebdfe1bd16b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B9FE7FDA0>]}
[0m17:18:10.498781 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 2.96s]
[0m17:18:10.501855 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:18:10.503912 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m17:18:10.505947 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m17:18:10.510052 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:18:10.512150 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m17:18:10.514224 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 17:18:10.513165 => 17:18:10.513165
[0m17:18:10.515297 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m17:18:10.681494 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:18:10.684561 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m17:18:11.415175 [debug] [Thread-6 (]: SQL status: OK in 0.7300000190734863 seconds
[0m17:18:11.460557 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m17:18:13.103919 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:18:13.105950 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:18:14.486487 [debug] [Thread-6 (]: SQL status: OK in 1.3799999952316284 seconds
[0m17:18:14.509056 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:18:14.530489 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:18:14.535563 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 17:18:10.517553 => 17:18:14.534550
[0m17:18:14.538621 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cbcd146e-0f9d-4389-906d-3ebdfe1bd16b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027BA0071010>]}
[0m17:18:14.540709 [info ] [Thread-6 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 4.03s]
[0m17:18:14.543806 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m17:18:14.545937 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:18:14.547953 [info ] [Thread-6 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:18:14.551005 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:18:14.554079 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:18:14.606430 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:18:14.611579 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:18:14.555111 => 17:18:14.609526
[0m17:18:14.613628 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:18:14.665955 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:18:14.671138 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:18:14.673169 [debug] [Thread-6 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:18:22.968001 [debug] [Thread-6 (]: SQL status: OK in 8.289999961853027 seconds
[0m17:18:22.971224 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:18:14.614654 => 17:18:22.970013
[0m17:18:22.984454 [debug] [Thread-6 (]: Compilation Error in test not_null_my_first_dbt_model_id (models\example\schema.yml)
  string indices must be integers, not 'str'
  
  > in macro materialization_test_default (macros\materializations\tests\test.sql)
  > called by test not_null_my_first_dbt_model_id (models\example\schema.yml)
[0m17:18:22.988569 [error] [Thread-6 (]: 3 of 7 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 8.44s]
[0m17:18:22.990599 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:18:22.992626 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:18:22.994655 [info ] [Thread-6 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:18:22.998731 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:18:23.000770 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:18:23.021086 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:18:23.023113 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:18:23.001794 => 17:18:23.023113
[0m17:18:23.025133 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:18:23.034299 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:18:23.037332 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:18:23.038342 [debug] [Thread-6 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:18:30.833329 [debug] [Thread-6 (]: SQL status: OK in 7.789999961853027 seconds
[0m17:18:30.838415 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:18:23.026141 => 17:18:30.837406
[0m17:18:30.845552 [debug] [Thread-6 (]: Compilation Error in test unique_my_first_dbt_model_id (models\example\schema.yml)
  string indices must be integers, not 'str'
  
  > in macro materialization_test_default (macros\materializations\tests\test.sql)
  > called by test unique_my_first_dbt_model_id (models\example\schema.yml)
[0m17:18:30.850653 [error] [Thread-6 (]: 4 of 7 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 7.85s]
[0m17:18:30.853690 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:18:30.858937 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m17:18:30.863015 [info ] [Thread-6 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m17:18:30.866055 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:18:30.870152 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:18:30.873290 [info ] [Thread-6 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m17:18:30.874799 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:18:30.877853 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:18:30.880896 [info ] [Thread-6 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m17:18:30.883938 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:18:30.891176 [debug] [MainThread]: On master: ROLLBACK
[0m17:18:30.894239 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:18:30.896261 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:18:30.898286 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:18:30.900320 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:18:30.901350 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:18:30.905544 [info ] [MainThread]: 
[0m17:18:30.909671 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 23.53 seconds (23.53s).
[0m17:18:30.916856 [debug] [MainThread]: Command end result
[0m17:18:30.950706 [info ] [MainThread]: 
[0m17:18:30.953761 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m17:18:30.956825 [info ] [MainThread]: 
[0m17:18:30.958857 [error] [MainThread]:   Compilation Error in test not_null_my_first_dbt_model_id (models\example\schema.yml)
  string indices must be integers, not 'str'
  
  > in macro materialization_test_default (macros\materializations\tests\test.sql)
  > called by test not_null_my_first_dbt_model_id (models\example\schema.yml)
[0m17:18:30.960885 [info ] [MainThread]: 
[0m17:18:30.962912 [error] [MainThread]:   Compilation Error in test unique_my_first_dbt_model_id (models\example\schema.yml)
  string indices must be integers, not 'str'
  
  > in macro materialization_test_default (macros\materializations\tests\test.sql)
  > called by test unique_my_first_dbt_model_id (models\example\schema.yml)
[0m17:18:30.965492 [info ] [MainThread]: 
[0m17:18:30.967537 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=2 SKIP=3 TOTAL=7
[0m17:18:30.974184 [debug] [MainThread]: Command `cli build` failed at 17:18:30.973657 after 24.32 seconds
[0m17:18:30.977757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B9F90D2B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027BA113F710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027BA113F6E0>]}
[0m17:18:30.981952 [debug] [MainThread]: Flushing usage events
[0m17:34:01.556628 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8BBDECBF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8BBDED010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8BBDECB90>]}


============================== 17:34:01.558657 | 2fb6bbe6-4104-44e2-934b-379bff83262c ==============================
[0m17:34:01.558657 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:34:01.561714 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:34:01.658168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2fb6bbe6-4104-44e2-934b-379bff83262c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8BBE236E0>]}
[0m17:34:01.698069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2fb6bbe6-4104-44e2-934b-379bff83262c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8BB296E40>]}
[0m17:34:01.699079 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:34:01.707168 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:34:01.755298 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:34:01.755298 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:34:01.760357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2fb6bbe6-4104-44e2-934b-379bff83262c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8BCF5A0C0>]}
[0m17:34:01.767433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2fb6bbe6-4104-44e2-934b-379bff83262c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8BCF81850>]}
[0m17:34:01.767433 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:34:01.769457 [info ] [MainThread]: 
[0m17:34:01.770466 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:34:01.771474 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:34:01.775520 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m17:34:01.788794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2fb6bbe6-4104-44e2-934b-379bff83262c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8BB5C33B0>]}
[0m17:34:01.789802 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:34:01.789802 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:34:01.790812 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:34:01.790812 [info ] [MainThread]: 
[0m17:34:01.793833 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:34:01.793833 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m17:34:01.794842 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:34:01.794842 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:34:01.798871 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:34:01.799880 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:34:01.794842 => 17:34:01.799880
[0m17:34:01.800897 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:34:01.832303 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:34:01.832303 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:34:01.833326 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:34:01.833326 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:34:01.857626 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m17:34:01.875825 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:34:01.877843 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:34:01.877843 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:34:01.887003 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:34:01.896103 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:34:01.800897 => 17:34:01.896103
[0m17:34:01.897112 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2fb6bbe6-4104-44e2-934b-379bff83262c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8BBE64050>]}
[0m17:34:01.897112 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.10s]
[0m17:34:01.898124 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:34:01.898124 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m17:34:01.899139 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m17:34:01.900153 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:34:01.900153 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m17:34:01.901167 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 17:34:01.901167 => 17:34:01.901167
[0m17:34:01.901167 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m17:34:01.928478 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:34:01.928478 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m17:34:01.929487 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:34:01.938574 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m17:34:01.940607 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:34:01.941614 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:34:01.950783 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:34:01.954833 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:34:01.957870 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:34:01.958876 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 17:34:01.901167 => 17:34:01.958876
[0m17:34:01.959886 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2fb6bbe6-4104-44e2-934b-379bff83262c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8BCF28CE0>]}
[0m17:34:01.959886 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.06s]
[0m17:34:01.960894 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m17:34:01.960894 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:34:01.960894 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:34:01.961904 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:34:01.961904 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:34:01.968966 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:34:01.969987 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:34:01.961904 => 17:34:01.969987
[0m17:34:01.970995 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:34:01.979111 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:34:01.980129 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:34:01.981135 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:34:01.983162 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:34:01.985215 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:34:01.970995 => 17:34:01.985215
[0m17:34:01.986231 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.02s]
[0m17:34:01.986231 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:34:01.987238 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:34:01.987238 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:34:01.987238 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:34:01.988246 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:34:01.992289 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:34:01.993299 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:34:01.988246 => 17:34:01.993299
[0m17:34:01.993299 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:34:01.995324 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:34:01.996334 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:34:01.996334 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:34:01.997341 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:34:01.998349 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:34:01.994316 => 17:34:01.998349
[0m17:34:01.998349 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.01s]
[0m17:34:01.999359 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:34:02.000394 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:34:02.000394 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m17:34:02.001402 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m17:34:02.001402 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:34:02.003421 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:34:02.004429 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:34:02.001402 => 17:34:02.003421
[0m17:34:02.004429 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:34:02.014626 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:34:02.017682 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:34:02.017682 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m17:34:02.020734 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:34:02.022764 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:34:02.004429 => 17:34:02.022764
[0m17:34:02.022764 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2fb6bbe6-4104-44e2-934b-379bff83262c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8BD41D340>]}
[0m17:34:02.023780 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m17:34:02.023780 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:34:02.024789 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:34:02.024789 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m17:34:02.025799 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m17:34:02.025799 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:34:02.028836 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:34:02.029848 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 17:34:02.025799 => 17:34:02.028836
[0m17:34:02.029848 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:34:02.031893 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:34:02.032902 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:34:02.033914 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:34:02.035933 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:34:02.036945 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 17:34:02.029848 => 17:34:02.036945
[0m17:34:02.037956 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m17:34:02.037956 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:34:02.038962 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:34:02.038962 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m17:34:02.039972 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m17:34:02.039972 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:34:02.042994 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:34:02.044007 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 17:34:02.039972 => 17:34:02.044007
[0m17:34:02.044007 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:34:02.046105 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:34:02.048153 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:34:02.048153 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:34:02.050181 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:34:02.052212 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 17:34:02.044007 => 17:34:02.051193
[0m17:34:02.053222 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m17:34:02.054237 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:34:02.055248 [debug] [MainThread]: On master: ROLLBACK
[0m17:34:02.056265 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:34:02.056265 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:34:02.056265 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:34:02.056265 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:34:02.057277 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:34:02.057277 [info ] [MainThread]: 
[0m17:34:02.057277 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m17:34:02.058289 [debug] [MainThread]: Command end result
[0m17:34:02.063360 [info ] [MainThread]: 
[0m17:34:02.064369 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:34:02.064369 [info ] [MainThread]: 
[0m17:34:02.065381 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m17:34:02.065381 [debug] [MainThread]: Command `cli build` succeeded at 17:34:02.065381 after 0.54 seconds
[0m17:34:02.066394 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8BB96FF50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8BB616CC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D8BB562360>]}
[0m17:34:02.066394 [debug] [MainThread]: Flushing usage events
[0m17:35:23.005260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159AB4E6E10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159AE928C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159AE27FFE0>]}


============================== 17:35:23.006267 | 4cb9ff72-ab60-406e-a57c-74779b5e0ace ==============================
[0m17:35:23.006267 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:35:23.007277 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:35:23.104292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4cb9ff72-ab60-406e-a57c-74779b5e0ace', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159AEAA7D70>]}
[0m17:35:23.141807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4cb9ff72-ab60-406e-a57c-74779b5e0ace', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159AEA2E630>]}
[0m17:35:23.142814 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:35:23.153004 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:35:23.206382 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:35:23.207389 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:35:23.211490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4cb9ff72-ab60-406e-a57c-74779b5e0ace', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159AFB64CB0>]}
[0m17:35:23.222681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4cb9ff72-ab60-406e-a57c-74779b5e0ace', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159AFBBE120>]}
[0m17:35:23.222681 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:35:23.224706 [info ] [MainThread]: 
[0m17:35:23.225717 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:35:23.227765 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:35:23.231809 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m17:35:23.246017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4cb9ff72-ab60-406e-a57c-74779b5e0ace', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159AEA2CE90>]}
[0m17:35:23.248054 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:35:23.248054 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:35:23.249069 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:35:23.249069 [info ] [MainThread]: 
[0m17:35:23.254155 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:35:23.255169 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m17:35:23.255169 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:35:23.256314 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:35:23.262373 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:35:23.262876 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:35:23.256314 => 17:35:23.262876
[0m17:35:23.262876 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:35:23.299423 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:35:23.299423 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:35:23.300435 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:35:23.300435 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:35:23.329937 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m17:35:23.351302 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:35:23.353345 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:35:23.353345 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:35:23.364040 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:35:23.374146 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:35:23.263886 => 17:35:23.374146
[0m17:35:23.375157 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb9ff72-ab60-406e-a57c-74779b5e0ace', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159AE700470>]}
[0m17:35:23.376176 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.12s]
[0m17:35:23.376176 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:35:23.377299 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m17:35:23.377299 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m17:35:23.378302 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:35:23.379372 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m17:35:23.379372 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 17:35:23.379372 => 17:35:23.379372
[0m17:35:23.379372 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m17:35:23.416059 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:35:23.417073 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m17:35:23.420129 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:35:23.430325 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m17:35:23.433371 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:35:23.434386 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:35:23.450716 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m17:35:23.456914 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:35:23.462497 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:35:23.463007 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 17:35:23.380377 => 17:35:23.463007
[0m17:35:23.464016 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb9ff72-ab60-406e-a57c-74779b5e0ace', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159AFD42A20>]}
[0m17:35:23.465027 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.09s]
[0m17:35:23.466033 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m17:35:23.466033 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:35:23.466033 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:35:23.467046 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:35:23.468059 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:35:23.477233 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:35:23.480299 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:35:23.468059 => 17:35:23.479280
[0m17:35:23.480299 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:35:23.494584 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:35:23.496605 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:35:23.497618 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:35:23.499643 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:35:23.500663 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:35:23.481323 => 17:35:23.500663
[0m17:35:23.502705 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m17:35:23.503725 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:35:23.504738 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:35:23.504738 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:35:23.505748 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:35:23.506760 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:35:23.512839 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:35:23.513849 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:35:23.506760 => 17:35:23.513849
[0m17:35:23.514861 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:35:23.517968 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:35:23.519998 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:35:23.521011 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:35:23.524068 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:35:23.526117 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:35:23.514861 => 17:35:23.526117
[0m17:35:23.527128 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m17:35:23.528140 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:35:23.528140 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:35:23.529149 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m17:35:23.530160 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m17:35:23.530160 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:35:23.533236 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:35:23.534248 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:35:23.531174 => 17:35:23.534248
[0m17:35:23.535269 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:35:23.549617 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:35:23.551666 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:35:23.552683 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m17:35:23.556776 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:35:23.561886 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:35:23.535269 => 17:35:23.560867
[0m17:35:23.565068 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4cb9ff72-ab60-406e-a57c-74779b5e0ace', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159AFF613A0>]}
[0m17:35:23.567253 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m17:35:23.570521 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:35:23.574283 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:35:23.574876 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m17:35:23.576440 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m17:35:23.576946 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:35:23.586179 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:35:23.589259 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 17:35:23.577457 => 17:35:23.588247
[0m17:35:23.589259 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:35:23.593468 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:35:23.595492 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:35:23.596522 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:35:23.599569 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:35:23.601605 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 17:35:23.590298 => 17:35:23.601605
[0m17:35:23.602620 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.03s]
[0m17:35:23.603635 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:35:23.603635 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:35:23.604644 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m17:35:23.604644 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m17:35:23.605655 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:35:23.610751 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:35:23.612800 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 17:35:23.605655 => 17:35:23.611770
[0m17:35:23.612800 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:35:23.615906 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:35:23.616927 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:35:23.616927 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:35:23.619978 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:35:23.620992 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 17:35:23.612800 => 17:35:23.620992
[0m17:35:23.622008 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.02s]
[0m17:35:23.623019 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:35:23.625087 [debug] [MainThread]: On master: ROLLBACK
[0m17:35:23.626115 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:35:23.626115 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:35:23.626115 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:35:23.627157 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:35:23.627157 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:35:23.628176 [info ] [MainThread]: 
[0m17:35:23.628176 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.40 seconds (0.40s).
[0m17:35:23.629191 [debug] [MainThread]: Command end result
[0m17:35:23.635274 [info ] [MainThread]: 
[0m17:35:23.636288 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:35:23.637307 [info ] [MainThread]: 
[0m17:35:23.637307 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m17:35:23.639422 [debug] [MainThread]: Command `cli build` succeeded at 17:35:23.639336 after 0.67 seconds
[0m17:35:23.639422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159AE7A7A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159AE7A6780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000159AE27FFE0>]}
[0m17:35:23.640425 [debug] [MainThread]: Flushing usage events
[0m17:37:06.235334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A60690D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A6068530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A6068500>]}


============================== 17:37:06.238448 | fbf809db-7955-42c4-b21f-6bd6012fc0e1 ==============================
[0m17:37:06.238448 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:37:06.244147 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:37:06.617706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fbf809db-7955-42c4-b21f-6bd6012fc0e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A60EECF0>]}
[0m17:37:06.759560 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fbf809db-7955-42c4-b21f-6bd6012fc0e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A60B44A0>]}
[0m17:37:06.763605 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:37:06.810725 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:37:06.954861 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:37:06.957934 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:37:06.985633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fbf809db-7955-42c4-b21f-6bd6012fc0e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A62590D0>]}
[0m17:37:07.013684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fbf809db-7955-42c4-b21f-6bd6012fc0e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A629DE20>]}
[0m17:37:07.017904 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:37:07.027651 [info ] [MainThread]: 
[0m17:37:07.033375 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:37:07.041111 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:37:07.123386 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m17:37:07.222452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fbf809db-7955-42c4-b21f-6bd6012fc0e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A625A660>]}
[0m17:37:07.225525 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:37:07.228656 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:37:07.232789 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:37:07.235853 [info ] [MainThread]: 
[0m17:37:07.251352 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m17:37:07.253380 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m17:37:07.258475 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:37:07.261614 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:37:07.312060 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:37:07.317160 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:37:07.262619 => 17:37:07.316144
[0m17:37:07.320746 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:38:11.612567 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:38:11.614623 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:38:11.616667 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:38:11.619738 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m17:38:11.681121 [debug] [Thread-6 (]: SQL status: OK in 0.05999999865889549 seconds
[0m17:38:11.776055 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:38:19.827717 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:38:19.828732 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:38:19.841914 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:38:19.881666 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:37:07.322333 => 17:38:19.881666
[0m17:38:19.885717 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fbf809db-7955-42c4-b21f-6bd6012fc0e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240A64E0F80>]}
[0m17:38:19.886725 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 72.63s]
[0m17:38:19.889823 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:38:19.891853 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m17:38:19.893882 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m17:38:19.895908 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:38:19.897927 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m17:38:19.898937 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 17:38:19.897927 => 17:38:19.897927
[0m17:38:19.900978 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m17:38:22.407080 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:38:22.409122 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m17:38:22.417427 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:38:22.464487 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m17:39:32.790876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221A5B487D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221A5B48170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221A5B484D0>]}


============================== 17:39:32.792891 | 24fed100-e8c8-4672-82c4-0bb1a0ad3e60 ==============================
[0m17:39:32.792891 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:39:32.794905 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m17:39:33.081132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '24fed100-e8c8-4672-82c4-0bb1a0ad3e60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221A5BA0110>]}
[0m17:39:33.211985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '24fed100-e8c8-4672-82c4-0bb1a0ad3e60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221A5BA3410>]}
[0m17:39:33.216064 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:39:33.265964 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:39:34.157123 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:39:34.158132 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:39:34.178378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '24fed100-e8c8-4672-82c4-0bb1a0ad3e60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221A5C6FBC0>]}
[0m17:39:34.198725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '24fed100-e8c8-4672-82c4-0bb1a0ad3e60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221A5D762D0>]}
[0m17:39:34.200247 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:39:34.205828 [info ] [MainThread]: 
[0m17:39:34.208856 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:39:34.215028 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:39:34.271932 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m17:39:34.353168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '24fed100-e8c8-4672-82c4-0bb1a0ad3e60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221A4A7FDD0>]}
[0m17:39:34.355228 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:39:34.356240 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:39:34.359267 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:39:34.361284 [info ] [MainThread]: 
[0m17:39:34.367336 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m17:39:34.369367 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m17:39:34.371386 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:39:34.372397 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:39:34.419301 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:39:34.423861 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:39:34.374421 => 17:39:34.422353
[0m17:39:34.426951 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:39:38.784236 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:39:38.786262 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:39:38.787272 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:39:38.789313 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m17:39:38.848184 [debug] [Thread-6 (]: SQL status: OK in 0.05999999865889549 seconds
[0m17:39:38.957909 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:39:38.961992 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:39:38.964031 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:39:38.990206 [debug] [Thread-6 (]: SQL status: OK in 0.019999999552965164 seconds
[0m17:39:39.031022 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:39:34.429486 => 17:39:39.031022
[0m17:39:39.034065 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24fed100-e8c8-4672-82c4-0bb1a0ad3e60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221A5FC22A0>]}
[0m17:39:39.036082 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 4.66s]
[0m17:39:39.038106 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:39:39.040138 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m17:39:39.042186 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m17:39:39.047279 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:39:39.050378 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m17:39:39.052420 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 17:39:39.051394 => 17:39:39.051394
[0m17:39:39.054447 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m17:39:39.205477 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:39:39.208530 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m17:39:39.216639 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:39:39.261529 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m17:39:41.266268 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:39:41.267277 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:39:41.287626 [debug] [Thread-6 (]: SQL status: OK in 0.019999999552965164 seconds
[0m17:39:41.304946 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:39:41.326445 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:39:41.330510 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 17:39:39.055462 => 17:39:41.329501
[0m17:39:41.332529 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24fed100-e8c8-4672-82c4-0bb1a0ad3e60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221A61BD040>]}
[0m17:39:41.334594 [info ] [Thread-6 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 2.29s]
[0m17:39:41.337637 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m17:39:41.340707 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:39:41.342738 [info ] [Thread-6 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:39:41.346805 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:39:41.348838 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:39:41.396642 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:39:41.400710 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:39:41.349922 => 17:39:41.399701
[0m17:39:41.402753 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:39:41.454947 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:39:41.459032 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:39:41.461074 [debug] [Thread-6 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:39:41.468169 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:39:41.475328 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:39:41.403764 => 17:39:41.474319
[0m17:39:41.479374 [info ] [Thread-6 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.13s]
[0m17:39:41.482422 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:39:41.484463 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:39:41.486495 [info ] [Thread-6 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:39:41.490655 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:39:41.492685 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:39:41.514048 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:39:41.519192 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:39:41.493713 => 17:39:41.517110
[0m17:39:41.521318 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:39:41.533618 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:39:41.537658 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:39:41.543875 [debug] [Thread-6 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:39:41.553126 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:39:41.567697 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:39:41.523349 => 17:39:41.565653
[0m17:39:41.576979 [info ] [Thread-6 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.09s]
[0m17:39:41.583309 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:39:41.589496 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m17:39:41.593563 [info ] [Thread-6 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m17:39:41.601903 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m17:39:41.604986 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:39:41.623626 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:39:41.627805 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:39:41.607042 => 17:39:41.626796
[0m17:39:41.631345 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:39:41.699151 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:39:41.704361 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:39:41.707414 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m17:39:41.717646 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:39:41.728920 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:39:41.633393 => 17:39:41.727911
[0m17:39:41.732470 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24fed100-e8c8-4672-82c4-0bb1a0ad3e60', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221A61AE960>]}
[0m17:39:41.735628 [info ] [Thread-6 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.13s]
[0m17:39:41.738660 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:39:41.741731 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:39:41.743775 [info ] [Thread-6 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m17:39:41.746812 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m17:39:41.748844 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:39:41.768295 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:39:41.771339 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 17:39:41.750899 => 17:39:41.770325
[0m17:39:41.775485 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:39:41.786784 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:39:41.789838 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:39:41.791867 [debug] [Thread-6 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:39:41.800173 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:39:41.807316 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 17:39:41.776516 => 17:39:41.806301
[0m17:39:41.811388 [info ] [Thread-6 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.06s]
[0m17:39:41.815567 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:39:41.817595 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:39:41.819637 [info ] [Thread-6 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m17:39:41.822691 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m17:39:41.825735 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:39:41.844608 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:39:41.848744 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 17:39:41.826765 => 17:39:41.847737
[0m17:39:41.850774 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:39:41.863547 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:39:41.865589 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:39:41.867617 [debug] [Thread-6 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:39:41.875812 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:39:41.883969 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 17:39:41.851804 => 17:39:41.881917
[0m17:39:41.887027 [info ] [Thread-6 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.07s]
[0m17:39:41.892144 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:39:41.898317 [debug] [MainThread]: On master: ROLLBACK
[0m17:39:41.900340 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:39:41.902363 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:39:41.904388 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:39:41.918824 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:39:41.920847 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:39:41.924951 [info ] [MainThread]: 
[0m17:39:41.926985 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 7.72 seconds (7.72s).
[0m17:39:41.933081 [debug] [MainThread]: Command end result
[0m17:39:41.955554 [info ] [MainThread]: 
[0m17:39:41.960677 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:39:41.963728 [info ] [MainThread]: 
[0m17:39:41.966995 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m17:39:41.973589 [debug] [MainThread]: Command `cli build` succeeded at 17:39:41.972573 after 9.30 seconds
[0m17:39:41.976644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221A567EA50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221A6075970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000221A6075A90>]}
[0m17:39:41.978675 [debug] [MainThread]: Flushing usage events
[0m17:44:29.301927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237CC1BC6B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237CC1BCAD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237CC1BC650>]}


============================== 17:44:29.302929 | 33102d3f-6787-42af-9a5b-bf67f791d33b ==============================
[0m17:44:29.302929 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:44:29.303943 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m17:44:29.407090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '33102d3f-6787-42af-9a5b-bf67f791d33b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237CB8A85F0>]}
[0m17:44:29.447839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '33102d3f-6787-42af-9a5b-bf67f791d33b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237CB653920>]}
[0m17:44:29.449879 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:44:29.460117 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:44:29.523261 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:44:29.524278 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:44:29.528359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '33102d3f-6787-42af-9a5b-bf67f791d33b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237CB8A8B90>]}
[0m17:44:29.535524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '33102d3f-6787-42af-9a5b-bf67f791d33b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237CD34D610>]}
[0m17:44:29.536535 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:44:29.538566 [info ] [MainThread]: 
[0m17:44:29.539589 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:44:29.540610 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:44:29.547810 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m17:44:29.570025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '33102d3f-6787-42af-9a5b-bf67f791d33b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237CD3AA8A0>]}
[0m17:44:29.572116 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:44:29.573127 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:44:29.574179 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:44:29.576278 [info ] [MainThread]: 
[0m17:44:29.580917 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:44:29.581927 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m17:44:29.582938 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:44:29.582938 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:44:29.592637 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:44:29.595913 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:44:29.583948 => 17:44:29.595913
[0m17:44:29.596935 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:44:29.639587 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:44:29.640625 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:44:29.640625 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:44:29.641637 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:44:29.674728 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m17:44:29.698552 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:44:29.699611 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:44:29.699611 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:44:29.712854 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:44:29.723029 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:44:29.596935 => 17:44:29.723029
[0m17:44:29.724041 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '33102d3f-6787-42af-9a5b-bf67f791d33b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237CD34CD10>]}
[0m17:44:29.725053 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.14s]
[0m17:44:29.725053 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:44:29.726061 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m17:44:29.726061 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m17:44:29.727070 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:44:29.727070 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m17:44:29.727070 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 17:44:29.727070 => 17:44:29.727070
[0m17:44:29.728079 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m17:44:29.757527 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:44:29.758540 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m17:44:29.760611 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:44:29.771804 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m17:44:29.774839 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:44:29.775346 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:44:29.784547 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:44:29.789617 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:44:29.794712 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:44:29.795719 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 17:44:29.728079 => 17:44:29.795719
[0m17:44:29.796726 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '33102d3f-6787-42af-9a5b-bf67f791d33b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237CD38FB00>]}
[0m17:44:29.796726 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.07s]
[0m17:44:29.797735 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m17:44:29.797735 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:44:29.798743 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:44:29.798743 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:44:29.798743 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:44:29.806882 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:44:29.808902 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:44:29.799752 => 17:44:29.808902
[0m17:44:29.809911 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:44:29.819031 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:44:29.820590 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:44:29.821108 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:44:29.822664 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:44:29.824700 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:44:29.809911 => 17:44:29.824700
[0m17:44:29.825713 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m17:44:29.826221 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:44:29.826728 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:44:29.827236 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:44:29.827744 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:44:29.827744 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:44:29.832319 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:44:29.833336 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:44:29.828253 => 17:44:29.833336
[0m17:44:29.833847 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:44:29.835371 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:44:29.835878 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:44:29.835878 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:44:29.837405 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:44:29.838441 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:44:29.833847 => 17:44:29.838441
[0m17:44:29.839460 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.01s]
[0m17:44:29.839967 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:44:29.840987 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:44:29.840987 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m17:44:29.841999 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m17:44:29.841999 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:44:29.844022 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:44:29.845527 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:44:29.843010 => 17:44:29.845025
[0m17:44:29.845527 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:44:29.857879 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:44:29.859898 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:44:29.859898 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m17:44:29.861925 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:44:29.862929 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:44:29.846037 => 17:44:29.862929
[0m17:44:29.863431 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '33102d3f-6787-42af-9a5b-bf67f791d33b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237CD7B6000>]}
[0m17:44:29.863941 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m17:44:29.864448 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:44:29.864957 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:44:29.865464 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m17:44:29.865975 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m17:44:29.866482 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:44:29.869540 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:44:29.870049 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 17:44:29.866482 => 17:44:29.870049
[0m17:44:29.870556 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:44:29.871568 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:44:29.872586 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:44:29.873601 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:44:29.874616 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:44:29.876645 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 17:44:29.870556 => 17:44:29.875634
[0m17:44:29.876645 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m17:44:29.877663 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:44:29.877663 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:44:29.878169 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m17:44:29.878680 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m17:44:29.879186 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:44:29.882779 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:44:29.883873 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 17:44:29.879186 => 17:44:29.882779
[0m17:44:29.883873 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:44:29.886384 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:44:29.887394 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:44:29.887394 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:44:29.889431 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:44:29.890446 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 17:44:29.884879 => 17:44:29.890446
[0m17:44:29.891457 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m17:44:29.892468 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:44:29.894496 [debug] [MainThread]: On master: ROLLBACK
[0m17:44:29.894496 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:44:29.894496 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:44:29.894496 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:44:29.895499 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:44:29.896001 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:44:29.896509 [info ] [MainThread]: 
[0m17:44:29.896509 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.36 seconds (0.36s).
[0m17:44:29.897522 [debug] [MainThread]: Command end result
[0m17:44:29.902075 [info ] [MainThread]: 
[0m17:44:29.902075 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:44:29.903084 [info ] [MainThread]: 
[0m17:44:29.903084 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m17:44:29.904096 [debug] [MainThread]: Command `cli build` succeeded at 17:44:29.904096 after 0.63 seconds
[0m17:44:29.905106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237CBDB1EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237CD7651F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237CBADA000>]}
[0m17:44:29.905106 [debug] [MainThread]: Flushing usage events
[0m17:45:59.965838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002629326C2C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026292938C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002629326C3E0>]}


============================== 17:45:59.966845 | 8a1f639f-e8f4-4247-ba33-934ffd3d4af3 ==============================
[0m17:45:59.966845 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:45:59.966845 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:46:00.060624 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8a1f639f-e8f4-4247-ba33-934ffd3d4af3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026292F36630>]}
[0m17:46:00.097052 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8a1f639f-e8f4-4247-ba33-934ffd3d4af3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026292E923F0>]}
[0m17:46:00.098073 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:46:00.105143 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:46:00.157452 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:46:00.159530 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:46:00.163571 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8a1f639f-e8f4-4247-ba33-934ffd3d4af3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026294379E20>]}
[0m17:46:00.170636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8a1f639f-e8f4-4247-ba33-934ffd3d4af3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002629440CD10>]}
[0m17:46:00.170636 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:46:00.172654 [info ] [MainThread]: 
[0m17:46:00.173672 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:46:00.174748 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:46:00.180870 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m17:46:00.196230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8a1f639f-e8f4-4247-ba33-934ffd3d4af3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026294316CF0>]}
[0m17:46:00.197239 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:46:00.198248 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:46:00.199258 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:46:00.200273 [info ] [MainThread]: 
[0m17:46:00.202787 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:46:00.202787 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m17:46:00.203796 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:46:00.203796 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:46:00.208898 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:46:00.209916 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:46:00.203796 => 17:46:00.209916
[0m17:46:00.210943 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:46:00.250086 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:46:00.251176 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:46:00.252179 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:46:00.252179 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:46:00.286741 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m17:46:00.307999 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:46:00.309014 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:46:00.310030 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:46:00.323295 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:46:00.332429 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:46:00.210943 => 17:46:00.332429
[0m17:46:00.333437 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a1f639f-e8f4-4247-ba33-934ffd3d4af3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026294422810>]}
[0m17:46:00.334446 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.13s]
[0m17:46:00.334446 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:46:00.335461 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m17:46:00.335461 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m17:46:00.336473 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:46:00.336473 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m17:46:00.337483 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 17:46:00.337483 => 17:46:00.337483
[0m17:46:00.337483 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m17:46:00.377821 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:46:00.378845 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m17:46:00.381905 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:46:00.395198 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m17:46:00.399243 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:46:00.399243 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:46:00.407371 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:46:00.411986 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:46:00.418097 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:46:00.419133 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 17:46:00.337483 => 17:46:00.418097
[0m17:46:00.419133 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a1f639f-e8f4-4247-ba33-934ffd3d4af3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026294423770>]}
[0m17:46:00.420145 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.08s]
[0m17:46:00.421167 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m17:46:00.421167 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:46:00.421167 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:46:00.422180 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:46:00.423191 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:46:00.431288 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:46:00.432297 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:46:00.423191 => 17:46:00.432297
[0m17:46:00.433306 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:46:00.441908 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:46:00.442923 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:46:00.443937 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:46:00.445967 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:46:00.448009 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:46:00.433306 => 17:46:00.448009
[0m17:46:00.449021 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m17:46:00.450031 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:46:00.451047 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:46:00.451047 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:46:00.452072 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:46:00.452072 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:46:00.459199 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:46:00.461246 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:46:00.453086 => 17:46:00.460218
[0m17:46:00.461246 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:46:00.463278 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:46:00.465302 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:46:00.465302 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:46:00.468408 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:46:00.470439 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:46:00.461246 => 17:46:00.469929
[0m17:46:00.470439 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m17:46:00.471447 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:46:00.472457 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:46:00.473476 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m17:46:00.474494 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m17:46:00.474494 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:46:00.477538 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:46:00.479587 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:46:00.474494 => 17:46:00.478555
[0m17:46:00.479587 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:46:00.491851 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:46:00.492864 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:46:00.492864 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m17:46:00.494884 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:46:00.497006 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:46:00.479587 => 17:46:00.495893
[0m17:46:00.498009 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8a1f639f-e8f4-4247-ba33-934ffd3d4af3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002629462A060>]}
[0m17:46:00.498009 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m17:46:00.500042 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:46:00.501054 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:46:00.501054 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m17:46:00.502076 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m17:46:00.502076 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:46:00.510205 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:46:00.512351 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 17:46:00.503090 => 17:46:00.511228
[0m17:46:00.512351 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:46:00.515395 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:46:00.516407 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:46:00.517424 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:46:00.520467 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:46:00.521477 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 17:46:00.513358 => 17:46:00.521477
[0m17:46:00.522488 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.02s]
[0m17:46:00.523500 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:46:00.523500 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:46:00.524516 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m17:46:00.524516 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m17:46:00.525526 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:46:00.528610 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:46:00.529638 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 17:46:00.525526 => 17:46:00.529638
[0m17:46:00.530154 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:46:00.533774 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:46:00.535822 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:46:00.535822 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:46:00.537865 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:46:00.538881 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 17:46:00.530665 => 17:46:00.538374
[0m17:46:00.539390 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m17:46:00.540409 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:46:00.543064 [debug] [MainThread]: On master: ROLLBACK
[0m17:46:00.543578 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:46:00.544089 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:46:00.544089 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:46:00.544605 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:46:00.544605 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:46:00.545112 [info ] [MainThread]: 
[0m17:46:00.545620 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.37 seconds (0.37s).
[0m17:46:00.546633 [debug] [MainThread]: Command end result
[0m17:46:00.558755 [info ] [MainThread]: 
[0m17:46:00.561958 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:46:00.563555 [info ] [MainThread]: 
[0m17:46:00.565170 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m17:46:00.567276 [debug] [MainThread]: Command `cli build` succeeded at 17:46:00.567276 after 0.63 seconds
[0m17:46:00.568327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002629296A120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026293283CE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026293137080>]}
[0m17:46:00.569379 [debug] [MainThread]: Flushing usage events
[0m17:47:53.710680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E24D01C2C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E24D01C3B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E24D01C170>]}


============================== 17:47:53.711690 | fcdfa725-d27e-45b7-a470-9b45a09e8727 ==============================
[0m17:47:53.711690 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:47:53.712705 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:47:53.806099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fcdfa725-d27e-45b7-a470-9b45a09e8727', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E24D096C00>]}
[0m17:47:53.842051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fcdfa725-d27e-45b7-a470-9b45a09e8727', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E24CFA8E00>]}
[0m17:47:53.845146 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:47:53.856342 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:47:53.916043 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:47:53.917056 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:47:53.923195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fcdfa725-d27e-45b7-a470-9b45a09e8727', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E24E0C2480>]}
[0m17:47:53.931296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fcdfa725-d27e-45b7-a470-9b45a09e8727', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E24E1B1AC0>]}
[0m17:47:53.931296 [info ] [MainThread]: Found 2 models, 4 tests, 1 seed, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:47:53.933334 [info ] [MainThread]: 
[0m17:47:53.934346 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:47:53.936365 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:47:53.940439 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m17:47:53.956747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fcdfa725-d27e-45b7-a470-9b45a09e8727', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E24E12AB10>]}
[0m17:47:53.958767 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:47:53.958767 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:47:53.959779 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:47:53.959779 [info ] [MainThread]: 
[0m17:47:53.962805 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:47:53.962805 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m17:47:53.963815 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:47:53.963815 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:47:53.971943 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:47:53.972951 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:47:53.964823 => 17:47:53.971943
[0m17:47:53.972951 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:47:54.007547 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:47:54.008571 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:47:54.008571 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:47:54.009588 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:47:54.038485 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m17:47:54.060938 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:47:54.062952 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:47:54.062952 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:47:54.073046 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:47:54.084313 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:47:53.972951 => 17:47:54.084313
[0m17:47:54.085326 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fcdfa725-d27e-45b7-a470-9b45a09e8727', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E24E321F70>]}
[0m17:47:54.086334 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.12s]
[0m17:47:54.087346 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:47:54.087346 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m17:47:54.088364 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m17:47:54.088364 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:47:54.089374 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m17:47:54.089374 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 17:47:54.089374 => 17:47:54.089374
[0m17:47:54.090414 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m17:47:54.123477 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:47:54.124485 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m17:47:54.127507 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:47:54.136662 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m17:47:54.139711 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:47:54.140723 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:47:54.155007 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:47:54.162075 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:47:54.168195 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:47:54.169209 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 17:47:54.090414 => 17:47:54.168702
[0m17:47:54.170233 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fcdfa725-d27e-45b7-a470-9b45a09e8727', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E24E321C70>]}
[0m17:47:54.171259 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.08s]
[0m17:47:54.172275 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m17:47:54.172785 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:47:54.172785 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:47:54.173809 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:47:54.174317 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:47:54.184537 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:47:54.186562 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:47:54.174317 => 17:47:54.186562
[0m17:47:54.187580 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:47:54.200808 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:47:54.203845 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:47:54.204860 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:47:54.205871 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:47:54.208920 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:47:54.187580 => 17:47:54.208408
[0m17:47:54.210443 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.04s]
[0m17:47:54.211467 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:47:54.212500 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:47:54.213008 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:47:54.213514 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:47:54.214020 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:47:54.220666 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:47:54.222714 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:47:54.214530 => 17:47:54.222199
[0m17:47:54.223221 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:47:54.225762 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:47:54.226781 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:47:54.227296 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:47:54.229873 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:47:54.231408 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:47:54.223732 => 17:47:54.230899
[0m17:47:54.232431 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m17:47:54.233459 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:47:54.234483 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:47:54.235509 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m17:47:54.237042 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m17:47:54.237552 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:47:54.241627 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:47:54.243187 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:47:54.238064 => 17:47:54.243187
[0m17:47:54.243698 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:47:54.259128 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:47:54.260660 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:47:54.260660 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m17:47:54.262684 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:47:54.264702 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:47:54.244210 => 17:47:54.264702
[0m17:47:54.265719 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fcdfa725-d27e-45b7-a470-9b45a09e8727', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E24E63B770>]}
[0m17:47:54.266746 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m17:47:54.267254 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:47:54.267760 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:47:54.268270 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m17:47:54.269292 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m17:47:54.269292 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:47:54.276054 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:47:54.277588 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 17:47:54.269806 => 17:47:54.277077
[0m17:47:54.277588 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:47:54.280140 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:47:54.281714 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:47:54.282225 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:47:54.284288 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:47:54.285823 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 17:47:54.278097 => 17:47:54.285309
[0m17:47:54.286846 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.02s]
[0m17:47:54.287860 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:47:54.288368 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:47:54.289451 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m17:47:54.290470 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m17:47:54.290980 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:47:54.296578 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:47:54.297592 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 17:47:54.290980 => 17:47:54.297592
[0m17:47:54.297592 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:47:54.300662 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:47:54.302241 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:47:54.302757 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:47:54.304859 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:47:54.307941 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 17:47:54.298611 => 17:47:54.306908
[0m17:47:54.309511 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.02s]
[0m17:47:54.311582 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:47:54.313649 [debug] [MainThread]: On master: ROLLBACK
[0m17:47:54.314159 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:47:54.314671 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:47:54.314671 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:47:54.315694 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:47:54.316216 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:47:54.317246 [info ] [MainThread]: 
[0m17:47:54.318269 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.38 seconds (0.38s).
[0m17:47:54.319293 [debug] [MainThread]: Command end result
[0m17:47:54.329525 [info ] [MainThread]: 
[0m17:47:54.330539 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:47:54.331052 [info ] [MainThread]: 
[0m17:47:54.331560 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m17:47:54.332583 [debug] [MainThread]: Command `cli build` succeeded at 17:47:54.332583 after 0.65 seconds
[0m17:47:54.333094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E24CC8FA70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E24CF89EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E24CF8B560>]}
[0m17:47:54.333605 [debug] [MainThread]: Flushing usage events
[0m17:58:04.327045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B566139E20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B569278710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B569278C50>]}


============================== 17:58:04.328060 | fd21ddd5-c31d-4ebe-8a1a-601848be9dc6 ==============================
[0m17:58:04.328060 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:58:04.328060 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:58:04.422126 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fd21ddd5-c31d-4ebe-8a1a-601848be9dc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B56929B800>]}
[0m17:58:04.458556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fd21ddd5-c31d-4ebe-8a1a-601848be9dc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5692E2AE0>]}
[0m17:58:04.459567 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:58:04.478299 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:58:04.495622 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m17:58:04.496634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'fd21ddd5-c31d-4ebe-8a1a-601848be9dc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B5692F69C0>]}
[0m17:58:05.890173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fd21ddd5-c31d-4ebe-8a1a-601848be9dc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B56A676E40>]}
[0m17:58:05.904325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fd21ddd5-c31d-4ebe-8a1a-601848be9dc6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B56A676E40>]}
[0m17:58:05.904325 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m17:58:05.906341 [info ] [MainThread]: 
[0m17:58:05.907349 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:58:05.907349 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:58:05.919605 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake1)
[0m17:58:05.926208 [debug] [ThreadPool]: Creating schema "schema: "datalake1"
"
[0m17:58:05.931250 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:58:05.932258 [debug] [ThreadPool]: Using fabricsparknb connection "create__datalake1"
[0m17:58:05.932258 [debug] [ThreadPool]: On create__datalake1: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake1"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake1"} */
/*{"project_root": "testproj"}*/

    select 1
  
[0m17:58:05.933292 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m17:58:05.933292 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake1"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake1"} */
/*{"project_root": "testproj"}*/

    select 1
  
[0m17:58:05.934302 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: cannot access local variable 'node_id' where it is not associated with a value
[0m17:58:05.934302 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro create_schema
[0m17:58:05.934302 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m17:58:05.936321 [info ] [MainThread]: 
[0m17:58:05.936321 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.03 seconds (0.03s).
[0m17:58:05.937328 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    cannot access local variable 'node_id' where it is not associated with a value
[0m17:58:05.938337 [debug] [MainThread]: Command `cli build` failed at 17:58:05.938337 after 1.66 seconds
[0m17:58:05.939344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B569278710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B568AB6690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B56A4BF140>]}
[0m17:58:05.939344 [debug] [MainThread]: Flushing usage events
[0m18:00:36.838427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D49F1A9460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D49F1A8680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D49F0F7BC0>]}


============================== 18:00:36.843534 | 93991704-4bff-42c9-adcf-d6ba1fb50207 ==============================
[0m18:00:36.843534 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:00:36.847661 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m18:00:37.182771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '93991704-4bff-42c9-adcf-d6ba1fb50207', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D49F23B830>]}
[0m18:00:37.315466 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '93991704-4bff-42c9-adcf-d6ba1fb50207', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D49F360F50>]}
[0m18:00:37.319515 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:00:37.353090 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:00:37.478080 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:00:37.480110 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:00:37.498299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '93991704-4bff-42c9-adcf-d6ba1fb50207', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D49F2CFE90>]}
[0m18:00:37.516544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '93991704-4bff-42c9-adcf-d6ba1fb50207', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D49F3D6360>]}
[0m18:00:37.518564 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 442 macros, 0 groups, 0 semantic models
[0m18:00:37.524630 [info ] [MainThread]: 
[0m18:00:37.527655 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:00:37.534786 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:00:37.588292 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake1)
[0m18:00:37.593467 [debug] [ThreadPool]: Creating schema "schema: "datalake1"
"
[0m18:10:02.241543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019807C48710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019807C489E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019807C486B0>]}


============================== 18:10:02.242559 | 294c8cb0-6cc4-4b23-b837-e3a389396369 ==============================
[0m18:10:02.242559 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:10:02.243574 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m18:10:02.333300 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '294c8cb0-6cc4-4b23-b837-e3a389396369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198073D84A0>]}
[0m18:10:02.368287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '294c8cb0-6cc4-4b23-b837-e3a389396369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019807C485F0>]}
[0m18:10:02.369294 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:10:02.386603 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:10:03.110810 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m18:10:03.111830 [debug] [MainThread]: Partial parsing: added file: dbt_fabricsparknb://macros\adapters\schema.sql
[0m18:10:03.120956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '294c8cb0-6cc4-4b23-b837-e3a389396369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019807DB9700>]}
[0m18:10:03.134099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '294c8cb0-6cc4-4b23-b837-e3a389396369', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019807E0D6A0>]}
[0m18:10:03.134099 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:10:03.136119 [info ] [MainThread]: 
[0m18:10:03.137133 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:10:03.138143 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:10:03.148300 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake1)
[0m18:10:03.149307 [debug] [ThreadPool]: Creating schema "schema: "datalake1"
"
[0m18:10:03.156431 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro create_schema
[0m18:10:03.156431 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: module 'dbt.exceptions' has no attribute 'RuntimeException'
[0m18:10:03.157440 [info ] [MainThread]: 
[0m18:10:03.158452 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m18:10:03.158452 [error] [MainThread]: Encountered an error:
Runtime Error
  module 'dbt.exceptions' has no attribute 'RuntimeException'
[0m18:10:03.160490 [debug] [MainThread]: Command `cli build` failed at 18:10:03.160490 after 0.96 seconds
[0m18:10:03.160490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019807C487D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019807BDC830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019807CCB2C0>]}
[0m18:10:03.161498 [debug] [MainThread]: Flushing usage events
[0m18:10:46.706712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A9018C8AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A9018C8E60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A9018C8E00>]}


============================== 18:10:46.706712 | edccb935-a025-44cd-9e8c-10a5d04af704 ==============================
[0m18:10:46.706712 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:10:46.707723 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m18:10:46.795918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'edccb935-a025-44cd-9e8c-10a5d04af704', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A90067F230>]}
[0m18:10:46.832314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'edccb935-a025-44cd-9e8c-10a5d04af704', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A9002FBEF0>]}
[0m18:10:46.833322 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:10:46.840411 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:10:46.894208 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:10:46.895215 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:10:46.898240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'edccb935-a025-44cd-9e8c-10a5d04af704', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A90193BB60>]}
[0m18:10:46.904310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'edccb935-a025-44cd-9e8c-10a5d04af704', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A901A91430>]}
[0m18:10:46.904310 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:10:46.906329 [info ] [MainThread]: 
[0m18:10:46.906329 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:10:46.908348 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:10:46.912428 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake1)
[0m18:10:46.913447 [debug] [ThreadPool]: Creating schema "schema: "datalake1"
"
[0m18:10:46.921745 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro create_schema
[0m18:10:46.922757 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake1"} */
  /*{"project_root": "testproj"}*/
  
      /*FABRICSPARKNB_ALERT: Schema Does NOT exist and automatic schema creation in Fabric Lakehouse not allowed. Please create the schema datalake1 manually*/ select 1
    
[0m18:10:46.924776 [info ] [MainThread]: 
[0m18:10:46.924776 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m18:10:46.924776 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake1"} */
    /*{"project_root": "testproj"}*/
    
        /*FABRICSPARKNB_ALERT: Schema Does NOT exist and automatic schema creation in Fabric Lakehouse not allowed. Please create the schema datalake1 manually*/ select 1
      
[0m18:10:46.925788 [debug] [MainThread]: Command `cli build` failed at 18:10:46.925788 after 0.24 seconds
[0m18:10:46.926801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A9018C8650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A9797F8860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A9019C7050>]}
[0m18:10:46.926801 [debug] [MainThread]: Flushing usage events
[0m18:13:28.262902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670A2592B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670A258170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670A258440>]}


============================== 18:13:28.263911 | 72dce009-6644-4741-ae0e-02a422b68988 ==============================
[0m18:13:28.263911 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:13:28.263911 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m18:13:28.361766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '72dce009-6644-4741-ae0e-02a422b68988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670A333DA0>]}
[0m18:13:28.397646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '72dce009-6644-4741-ae0e-02a422b68988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670A220320>]}
[0m18:13:28.398653 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:13:28.406263 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:13:28.412898 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m18:13:28.413917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '72dce009-6644-4741-ae0e-02a422b68988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670B4EC0B0>]}
[0m18:13:29.161352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '72dce009-6644-4741-ae0e-02a422b68988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670B6DA000>]}
[0m18:13:29.168423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '72dce009-6644-4741-ae0e-02a422b68988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670B6839B0>]}
[0m18:13:29.169434 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:13:29.171490 [info ] [MainThread]: 
[0m18:13:29.172002 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:13:29.173023 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:13:29.178629 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:13:29.202526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '72dce009-6644-4741-ae0e-02a422b68988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670B79BF20>]}
[0m18:13:29.202526 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:13:29.203537 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:13:29.203537 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:13:29.204544 [info ] [MainThread]: 
[0m18:13:29.208070 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:13:29.208578 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:13:29.209085 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:13:29.209600 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:13:29.215261 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:13:29.217283 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:13:29.210115 => 18:13:29.216274
[0m18:13:29.217283 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:13:29.249757 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:13:29.250762 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:13:29.251265 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:13:29.251265 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:13:29.289444 [debug] [Thread-1 (]: SQL status: OK in 0.03999999910593033 seconds
[0m18:13:29.319285 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:13:29.320810 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:13:29.321317 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:13:29.327374 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:13:29.336538 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:13:29.217283 => 18:13:29.336538
[0m18:13:29.337540 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72dce009-6644-4741-ae0e-02a422b68988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670B682A20>]}
[0m18:13:29.337540 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.13s]
[0m18:13:29.338550 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:13:29.339558 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:13:29.340562 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:13:29.341064 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:13:29.341064 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:13:29.342077 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:13:29.341064 => 18:13:29.341064
[0m18:13:29.342077 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:13:29.369521 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:13:29.371025 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:13:29.373054 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:13:29.384725 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m18:13:29.387763 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:13:29.387763 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m18:13:29.396325 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:13:29.400891 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m18:13:29.404433 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:13:29.405448 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:13:29.342077 => 18:13:29.405448
[0m18:13:29.406461 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72dce009-6644-4741-ae0e-02a422b68988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670B811FD0>]}
[0m18:13:29.406461 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.06s]
[0m18:13:29.407471 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:13:29.407471 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:13:29.408479 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m18:13:29.408479 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m18:13:29.409489 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:13:29.417718 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:13:29.420291 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 18:13:29.409489 => 18:13:29.419277
[0m18:13:29.420805 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:13:29.429421 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:13:29.430951 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:13:29.431458 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:13:29.432466 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:13:29.434486 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 18:13:29.420805 => 18:13:29.433474
[0m18:13:29.434486 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m18:13:29.435494 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:13:29.435494 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:13:29.436510 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m18:13:29.436510 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m18:13:29.437520 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:13:29.442059 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:13:29.444080 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 18:13:29.437520 => 18:13:29.444080
[0m18:13:29.444080 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:13:29.447130 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:13:29.448139 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:13:29.448139 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:13:29.449149 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:13:29.451181 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 18:13:29.445092 => 18:13:29.450670
[0m18:13:29.451181 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.01s]
[0m18:13:29.453205 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:13:29.454217 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:13:29.455227 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m18:13:29.456239 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m18:13:29.456239 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:13:29.460792 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:13:29.461301 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:13:29.456239 => 18:13:29.461301
[0m18:13:29.462327 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:13:29.472469 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:13:29.474490 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:13:29.475502 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m18:13:29.477560 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:13:29.478573 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:13:29.462327 => 18:13:29.478573
[0m18:13:29.480099 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '72dce009-6644-4741-ae0e-02a422b68988', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670BA42AE0>]}
[0m18:13:29.480611 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m18:13:29.481124 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:13:29.481633 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:13:29.482143 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m18:13:29.482651 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m18:13:29.482651 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:13:29.487263 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:13:29.489298 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 18:13:29.483159 => 18:13:29.488790
[0m18:13:29.489809 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:13:29.493448 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:13:29.494466 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:13:29.494977 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:13:29.497009 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:13:29.499053 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 18:13:29.490325 => 18:13:29.498545
[0m18:13:29.499561 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.02s]
[0m18:13:29.500070 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:13:29.500579 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:13:29.501092 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m18:13:29.502114 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m18:13:29.503135 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:13:29.507210 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:13:29.508258 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 18:13:29.503135 => 18:13:29.508258
[0m18:13:29.508771 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:13:29.510295 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:13:29.510804 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:13:29.511319 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:13:29.512875 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:13:29.513900 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 18:13:29.508771 => 18:13:29.513900
[0m18:13:29.514914 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m18:13:29.515938 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:13:29.517469 [debug] [MainThread]: On master: ROLLBACK
[0m18:13:29.517979 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:13:29.518487 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:13:29.518487 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:13:29.518998 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:13:29.518998 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:13:29.520022 [info ] [MainThread]: 
[0m18:13:29.520022 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.35 seconds (0.35s).
[0m18:13:29.521563 [debug] [MainThread]: Command end result
[0m18:13:29.529242 [info ] [MainThread]: 
[0m18:13:29.529754 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:13:29.529754 [info ] [MainThread]: 
[0m18:13:29.530261 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m18:13:29.530768 [debug] [MainThread]: Command `cli build` succeeded at 18:13:29.530768 after 1.30 seconds
[0m18:13:29.531275 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670A3FE540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670B3F2A20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002670B698770>]}
[0m18:13:29.531275 [debug] [MainThread]: Flushing usage events
[0m18:13:57.204604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198A6197F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198A9AD81D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198A9AD8530>]}


============================== 18:13:57.204604 | ac0acf45-0b68-43a7-bbb7-8fc16a9af9e6 ==============================
[0m18:13:57.204604 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:13:57.205612 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m18:13:57.304395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ac0acf45-0b68-43a7-bbb7-8fc16a9af9e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198A9825AC0>]}
[0m18:13:57.341881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ac0acf45-0b68-43a7-bbb7-8fc16a9af9e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198A9A280E0>]}
[0m18:13:57.342900 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:13:57.352146 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:13:57.416676 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:13:57.417695 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:13:57.421780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ac0acf45-0b68-43a7-bbb7-8fc16a9af9e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198A9AD94C0>]}
[0m18:13:57.439086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ac0acf45-0b68-43a7-bbb7-8fc16a9af9e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198AACC6060>]}
[0m18:13:57.439086 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:13:57.441105 [info ] [MainThread]: 
[0m18:13:57.442131 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:13:57.443149 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:13:57.449229 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:13:57.463597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ac0acf45-0b68-43a7-bbb7-8fc16a9af9e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198A9BD6300>]}
[0m18:13:57.464104 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:13:57.464612 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:13:57.464612 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:13:57.464612 [info ] [MainThread]: 
[0m18:13:57.466627 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:13:57.467629 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:13:57.468132 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:13:57.468132 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:13:57.472178 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:13:57.473187 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:13:57.468132 => 18:13:57.473187
[0m18:13:57.473187 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:13:57.506645 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:13:57.507658 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:13:57.507658 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:13:57.507658 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:13:57.537202 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m18:13:57.559581 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:13:57.561603 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:13:57.562112 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:13:57.572214 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:13:57.583454 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:13:57.473187 => 18:13:57.583454
[0m18:13:57.584961 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ac0acf45-0b68-43a7-bbb7-8fc16a9af9e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198AACAD0A0>]}
[0m18:13:57.585984 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.12s]
[0m18:13:57.586493 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:13:57.587002 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:13:57.588033 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:13:57.588541 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:13:57.589047 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:13:57.589047 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:13:57.589047 => 18:13:57.589047
[0m18:13:57.589555 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:13:57.619703 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:13:57.620212 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:13:57.621746 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:13:57.630978 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m18:13:57.635012 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:13:57.635012 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m18:13:57.652794 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m18:13:57.658476 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m18:13:57.664610 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:13:57.665627 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:13:57.589555 => 18:13:57.665627
[0m18:13:57.666650 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ac0acf45-0b68-43a7-bbb7-8fc16a9af9e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198A9C9D7F0>]}
[0m18:13:57.667164 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.08s]
[0m18:13:57.667674 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:13:57.668185 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:13:57.668694 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m18:13:57.669205 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m18:13:57.669713 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:13:57.677351 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:13:57.678374 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 18:13:57.669713 => 18:13:57.678374
[0m18:13:57.678880 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:13:57.688494 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:13:57.689514 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:13:57.690527 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:13:57.692558 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:13:57.695077 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 18:13:57.678880 => 18:13:57.694570
[0m18:13:57.696097 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m18:13:57.696606 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:13:57.697115 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:13:57.697624 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m18:13:57.698132 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m18:13:57.698645 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:13:57.703244 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:13:57.704255 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 18:13:57.698645 => 18:13:57.704255
[0m18:13:57.704255 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:13:57.707321 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:13:57.708358 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:13:57.708358 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:13:57.710392 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:13:57.711409 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 18:13:57.705264 => 18:13:57.711409
[0m18:13:57.712419 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.01s]
[0m18:13:57.713429 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:13:57.713429 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:13:57.714438 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m18:13:57.715448 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m18:13:57.715448 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:13:57.717483 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:13:57.718492 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:13:57.715448 => 18:13:57.718492
[0m18:13:57.718492 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:13:57.732846 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:13:57.733865 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:13:57.733865 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m18:13:57.735904 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:13:57.736914 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:13:57.719500 => 18:13:57.736914
[0m18:13:57.737925 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ac0acf45-0b68-43a7-bbb7-8fc16a9af9e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198AAE3DA30>]}
[0m18:13:57.737925 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m18:13:57.738933 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:13:57.738933 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:13:57.739942 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m18:13:57.739942 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m18:13:57.740951 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:13:57.746030 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:13:57.748130 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 18:13:57.740951 => 18:13:57.748130
[0m18:13:57.748130 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:13:57.750148 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:13:57.751158 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:13:57.751158 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:13:57.752167 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:13:57.754197 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 18:13:57.748130 => 18:13:57.753177
[0m18:13:57.755213 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m18:13:57.755213 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:13:57.756223 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:13:57.756223 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m18:13:57.756223 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m18:13:57.757237 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:13:57.760270 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:13:57.761281 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 18:13:57.757237 => 18:13:57.761281
[0m18:13:57.762368 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:13:57.764382 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:13:57.765396 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:13:57.765396 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:13:57.767415 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:13:57.768442 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 18:13:57.762368 => 18:13:57.768442
[0m18:13:57.769455 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m18:13:57.770471 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:13:57.771490 [debug] [MainThread]: On master: ROLLBACK
[0m18:13:57.771999 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:13:57.771999 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:13:57.772508 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:13:57.772508 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:13:57.773014 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:13:57.773522 [info ] [MainThread]: 
[0m18:13:57.774032 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.33 seconds (0.33s).
[0m18:13:57.775051 [debug] [MainThread]: Command end result
[0m18:13:57.783751 [info ] [MainThread]: 
[0m18:13:57.783751 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:13:57.784762 [info ] [MainThread]: 
[0m18:13:57.784762 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m18:13:57.785774 [debug] [MainThread]: Command `cli build` succeeded at 18:13:57.785774 after 0.61 seconds
[0m18:13:57.786788 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198A9AD8530>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198A9AD82F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000198A9AD8230>]}
[0m18:13:57.786788 [debug] [MainThread]: Flushing usage events
[0m18:15:40.070432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012518A54AA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012518A54CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012518A549E0>]}


============================== 18:15:40.071439 | c171fcfc-f962-4e11-a241-0bff7f3f0111 ==============================
[0m18:15:40.071439 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:15:40.071439 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:15:40.159664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c171fcfc-f962-4e11-a241-0bff7f3f0111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012518A54530>]}
[0m18:15:40.195064 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c171fcfc-f962-4e11-a241-0bff7f3f0111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012518AC7C20>]}
[0m18:15:40.196075 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:15:40.203188 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:15:40.250844 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:15:40.250844 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:15:40.254875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c171fcfc-f962-4e11-a241-0bff7f3f0111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012519B2A120>]}
[0m18:15:40.260456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c171fcfc-f962-4e11-a241-0bff7f3f0111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012519BE9CD0>]}
[0m18:15:40.261469 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:15:40.262482 [info ] [MainThread]: 
[0m18:15:40.263489 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:15:40.264504 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:15:40.268544 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:15:40.282735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c171fcfc-f962-4e11-a241-0bff7f3f0111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012518A97590>]}
[0m18:15:40.284760 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:15:40.284760 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:15:40.285778 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:15:40.285778 [info ] [MainThread]: 
[0m18:15:40.288810 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:15:40.288810 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:15:40.289831 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:15:40.289831 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:15:40.293860 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:15:40.294868 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:15:40.289831 => 18:15:40.294868
[0m18:15:40.294868 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:15:40.329402 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:15:40.329402 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:15:40.329402 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:15:40.330411 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:15:40.362546 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m18:15:40.383989 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:15:40.384505 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:15:40.385514 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:15:40.392589 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:15:40.401686 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:15:40.294868 => 18:15:40.401686
[0m18:15:40.402695 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c171fcfc-f962-4e11-a241-0bff7f3f0111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012519C2D2B0>]}
[0m18:15:40.402695 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.11s]
[0m18:15:40.403702 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:15:40.403702 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:15:40.403702 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:15:40.404712 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:15:40.404712 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:15:40.405721 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:15:40.404712 => 18:15:40.404712
[0m18:15:40.405721 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:15:40.434012 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:15:40.434012 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:15:40.436030 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:15:40.445198 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m18:15:40.448231 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:15:40.449256 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m18:15:40.463427 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:15:40.468013 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m18:15:40.472578 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:15:40.473618 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:15:40.405721 => 18:15:40.473109
[0m18:15:40.474129 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c171fcfc-f962-4e11-a241-0bff7f3f0111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012518AC7DA0>]}
[0m18:15:40.474129 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.07s]
[0m18:15:40.475145 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:15:40.475145 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:15:40.475654 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m18:15:40.476164 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m18:15:40.476673 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:15:40.484817 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:15:40.487933 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 18:15:40.477182 => 18:15:40.487368
[0m18:15:40.488436 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:15:40.497610 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:15:40.498625 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:15:40.498625 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:15:40.501660 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:15:40.503745 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 18:15:40.488949 => 18:15:40.503745
[0m18:15:40.504748 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m18:15:40.505764 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:15:40.505764 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:15:40.506774 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m18:15:40.507781 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m18:15:40.507781 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:15:40.511821 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:15:40.513853 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 18:15:40.507781 => 18:15:40.513853
[0m18:15:40.514868 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:15:40.516911 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:15:40.519012 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:15:40.520016 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:15:40.522035 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:15:40.523044 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 18:15:40.515891 => 18:15:40.523044
[0m18:15:40.524051 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m18:15:40.525059 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:15:40.525059 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:15:40.526067 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m18:15:40.526067 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m18:15:40.527076 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:15:40.529094 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:15:40.529094 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:15:40.527076 => 18:15:40.529094
[0m18:15:40.530103 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:15:40.540747 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:15:40.541767 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:15:40.542276 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m18:15:40.544362 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:15:40.546422 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:15:40.530103 => 18:15:40.546422
[0m18:15:40.547441 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c171fcfc-f962-4e11-a241-0bff7f3f0111', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001251A003CE0>]}
[0m18:15:40.548484 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m18:15:40.550065 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:15:40.551073 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:15:40.551073 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m18:15:40.552087 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m18:15:40.553103 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:15:40.561199 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:15:40.562728 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 18:15:40.553103 => 18:15:40.562213
[0m18:15:40.562728 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:15:40.564256 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:15:40.565290 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:15:40.565805 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:15:40.568348 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:15:40.569363 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 18:15:40.563238 => 18:15:40.569363
[0m18:15:40.570376 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.02s]
[0m18:15:40.570884 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:15:40.571392 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:15:40.571904 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m18:15:40.572410 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m18:15:40.572410 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:15:40.575451 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:15:40.577516 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 18:15:40.572917 => 18:15:40.577516
[0m18:15:40.577516 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:15:40.579537 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:15:40.581601 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:15:40.582614 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:15:40.584637 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:15:40.586657 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 18:15:40.578522 => 18:15:40.585646
[0m18:15:40.586657 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m18:15:40.587666 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:15:40.588674 [debug] [MainThread]: On master: ROLLBACK
[0m18:15:40.588674 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:15:40.588674 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:15:40.589682 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:15:40.589682 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:15:40.589682 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:15:40.590694 [info ] [MainThread]: 
[0m18:15:40.590694 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.33 seconds (0.33s).
[0m18:15:40.591718 [debug] [MainThread]: Command end result
[0m18:15:40.596781 [info ] [MainThread]: 
[0m18:15:40.596781 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:15:40.596781 [info ] [MainThread]: 
[0m18:15:40.597792 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m18:15:40.597792 [debug] [MainThread]: Command `cli build` succeeded at 18:15:40.597792 after 0.55 seconds
[0m18:15:40.598807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012518A54AA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012518A54CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012518A549E0>]}
[0m18:15:40.598807 [debug] [MainThread]: Flushing usage events
[0m19:06:27.860873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027371B496A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027371B49430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027371B49040>]}


============================== 19:06:27.861882 | 9b6185ac-669c-4d6d-82da-2a3ba60d6467 ==============================
[0m19:06:27.861882 [info ] [MainThread]: Running with dbt=1.7.14
[0m19:06:27.861882 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:06:27.943712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9b6185ac-669c-4d6d-82da-2a3ba60d6467', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027371BCB770>]}
[0m19:06:27.975988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9b6185ac-669c-4d6d-82da-2a3ba60d6467', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273719057C0>]}
[0m19:06:27.976999 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:06:27.992137 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m19:06:28.580464 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:06:28.580464 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:06:28.584509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9b6185ac-669c-4d6d-82da-2a3ba60d6467', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027372CD15B0>]}
[0m19:06:28.597633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9b6185ac-669c-4d6d-82da-2a3ba60d6467', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027372D26AE0>]}
[0m19:06:28.598644 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m19:06:28.599654 [info ] [MainThread]: 
[0m19:06:28.600663 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:06:28.601673 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:06:28.612840 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:06:28.632011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b6185ac-669c-4d6d-82da-2a3ba60d6467', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027371A48AA0>]}
[0m19:06:28.632011 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:06:28.632011 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:06:28.633018 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:06:28.633018 [info ] [MainThread]: 
[0m19:06:28.635033 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m19:06:28.635033 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m19:06:28.636053 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:06:28.636053 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:06:28.641090 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:06:28.642097 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:06:28.637061 => 19:06:28.642097
[0m19:06:28.642097 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:06:28.671349 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:06:28.671349 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:06:28.672357 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:06:28.672357 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:06:28.699607 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m19:06:28.726846 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:06:28.727862 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:06:28.727862 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m19:06:28.735928 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m19:06:28.743998 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:06:28.642097 => 19:06:28.743998
[0m19:06:28.743998 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b6185ac-669c-4d6d-82da-2a3ba60d6467', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027372D24DA0>]}
[0m19:06:28.745005 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.11s]
[0m19:06:28.745005 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:06:28.746013 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m19:06:28.746013 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m19:06:28.747025 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m19:06:28.747025 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m19:06:28.747025 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 19:06:28.747025 => 19:06:28.747025
[0m19:06:28.748033 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m19:06:28.773241 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m19:06:28.773241 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m19:06:28.774274 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:06:28.782331 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m19:06:28.784348 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m19:06:28.785356 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m19:06:28.794451 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m19:06:28.797469 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m19:06:28.801499 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:06:28.801499 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 19:06:28.748033 => 19:06:28.801499
[0m19:06:28.802508 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b6185ac-669c-4d6d-82da-2a3ba60d6467', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027372E62540>]}
[0m19:06:28.803516 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.06s]
[0m19:06:28.803516 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m19:06:28.804538 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:06:28.804538 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m19:06:28.805549 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m19:06:28.805549 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:06:28.812618 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:06:28.813629 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 19:06:28.805549 => 19:06:28.813629
[0m19:06:28.813629 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:06:28.821743 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:06:28.823758 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:06:28.823758 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m19:06:28.825775 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:06:28.827790 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 19:06:28.813629 => 19:06:28.826784
[0m19:06:28.827790 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.02s]
[0m19:06:28.828798 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:06:28.828798 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:06:28.829807 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m19:06:28.829807 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m19:06:28.830813 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:06:28.835867 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m19:06:28.835867 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 19:06:28.830813 => 19:06:28.835867
[0m19:06:28.835867 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:06:28.837884 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m19:06:28.837884 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m19:06:28.838891 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m19:06:28.839898 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:06:28.840904 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 19:06:28.836875 => 19:06:28.839898
[0m19:06:28.840904 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.01s]
[0m19:06:28.841911 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:06:28.841911 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m19:06:28.841911 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m19:06:28.842918 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m19:06:28.842918 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:06:28.844932 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:06:28.844932 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:06:28.842918 => 19:06:28.844932
[0m19:06:28.845937 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:06:28.855028 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:06:28.855028 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:06:28.856034 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m19:06:28.857043 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:06:28.858051 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:06:28.845937 => 19:06:28.858051
[0m19:06:28.858051 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b6185ac-669c-4d6d-82da-2a3ba60d6467', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027372D3FEF0>]}
[0m19:06:28.859058 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m19:06:28.859058 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:06:28.860066 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:06:28.860066 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m19:06:28.861074 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m19:06:28.861074 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:06:28.864096 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m19:06:28.865103 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 19:06:28.861074 => 19:06:28.864096
[0m19:06:28.865103 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:06:28.866129 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m19:06:28.867138 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m19:06:28.867138 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m19:06:28.869158 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:06:28.870674 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 19:06:28.865103 => 19:06:28.870674
[0m19:06:28.871184 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m19:06:28.872190 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:06:28.872190 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:06:28.872190 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m19:06:28.873198 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m19:06:28.873198 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:06:28.876233 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:06:28.876233 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 19:06:28.873198 => 19:06:28.876233
[0m19:06:28.877246 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:06:28.878257 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:06:28.878257 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:06:28.879270 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m19:06:28.880282 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:06:28.881303 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 19:06:28.877246 => 19:06:28.881303
[0m19:06:28.881303 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m19:06:28.882311 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:06:28.882311 [debug] [MainThread]: On master: ROLLBACK
[0m19:06:28.883318 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:06:28.883318 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:06:28.883318 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:06:28.884326 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:06:28.884326 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:06:28.884326 [info ] [MainThread]: 
[0m19:06:28.885338 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m19:06:28.886346 [debug] [MainThread]: Command end result
[0m19:06:28.891383 [info ] [MainThread]: 
[0m19:06:28.891383 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:06:28.891383 [info ] [MainThread]: 
[0m19:06:28.892391 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m19:06:28.892391 [debug] [MainThread]: Command `cli build` succeeded at 19:06:28.892391 after 1.07 seconds
[0m19:06:28.893398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000273714B36B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027370D5E660>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027372C5BCE0>]}
[0m19:06:28.893398 [debug] [MainThread]: Flushing usage events
[0m20:37:11.769249 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B8B208410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B8B20A6F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B8B2091C0>]}


============================== 20:37:11.771261 | dadf8c0f-5101-4dd3-b2c3-79f26fb93eba ==============================
[0m20:37:11.771261 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:37:11.773289 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:37:12.033243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dadf8c0f-5101-4dd3-b2c3-79f26fb93eba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B8B1A40B0>]}
[0m20:37:12.146080 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dadf8c0f-5101-4dd3-b2c3-79f26fb93eba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B8B1DF170>]}
[0m20:37:12.149117 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:37:12.183656 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:37:12.807059 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:37:12.808066 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:37:12.825161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dadf8c0f-5101-4dd3-b2c3-79f26fb93eba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B8B258380>]}
[0m20:37:12.840271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dadf8c0f-5101-4dd3-b2c3-79f26fb93eba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B8B474A10>]}
[0m20:37:12.841280 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:37:12.847318 [info ] [MainThread]: 
[0m20:37:12.850857 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:37:12.856975 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:37:12.910948 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:37:12.985106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dadf8c0f-5101-4dd3-b2c3-79f26fb93eba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B8B4AF1A0>]}
[0m20:37:12.986110 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:37:12.988401 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:37:12.990461 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:37:12.992715 [info ] [MainThread]: 
[0m20:37:12.999887 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m20:37:13.000892 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:37:13.004098 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:37:13.005103 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:37:13.047477 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:37:13.050503 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:37:13.006107 => 20:37:13.049498
[0m20:37:13.051513 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:37:19.044821 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:37:19.046334 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:37:19.047342 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:37:19.049360 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:37:19.113125 [debug] [Thread-6 (]: SQL status: OK in 0.05999999865889549 seconds
[0m20:37:19.207676 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m20:37:19.210709 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:37:19.211714 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m20:37:19.225215 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:37:19.260643 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:37:13.052524 => 20:37:19.259640
[0m20:37:19.265690 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dadf8c0f-5101-4dd3-b2c3-79f26fb93eba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B8B685A00>]}
[0m20:37:19.268720 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 6.26s]
[0m20:37:19.272750 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:37:19.273760 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m20:37:19.276385 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m20:37:19.278912 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m20:37:19.280928 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m20:37:19.282298 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 20:37:19.281934 => 20:37:19.281934
[0m20:37:19.283323 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m20:37:19.403506 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:37:19.405516 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:37:19.409760 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m20:37:19.444977 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m20:37:22.378893 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:37:22.380404 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m20:37:22.394142 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:37:22.408015 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m20:37:22.423603 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:37:22.425613 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 20:37:19.284331 => 20:37:22.425613
[0m20:37:22.428004 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dadf8c0f-5101-4dd3-b2c3-79f26fb93eba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B8B687470>]}
[0m20:37:22.430027 [info ] [Thread-6 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 3.15s]
[0m20:37:22.433053 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m20:37:22.434451 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:37:22.436463 [info ] [Thread-6 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m20:37:22.438476 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m20:37:22.439484 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:37:22.472289 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:37:22.475349 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 20:37:22.440492 => 20:37:22.474342
[0m20:37:22.477029 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:37:22.515084 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:37:22.517097 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:37:22.519113 [debug] [Thread-6 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:37:22.523189 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m20:37:22.529938 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 20:37:22.477532 => 20:37:22.529938
[0m20:37:22.532954 [info ] [Thread-6 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.09s]
[0m20:37:22.534964 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:37:22.536974 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:37:22.537979 [info ] [Thread-6 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m20:37:22.541133 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m20:37:22.542196 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:37:22.565020 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:37:22.569050 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 20:37:22.543718 => 20:37:22.569050
[0m20:37:22.572081 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:37:22.583342 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:37:22.586700 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:37:22.588759 [debug] [Thread-6 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:37:22.595691 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:37:22.601781 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 20:37:22.573087 => 20:37:22.601276
[0m20:37:22.604840 [info ] [Thread-6 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.06s]
[0m20:37:22.609558 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:37:22.613865 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m20:37:22.616405 [info ] [Thread-6 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m20:37:22.619916 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m20:37:22.621421 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m20:37:22.631677 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m20:37:22.635722 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 20:37:22.622425 => 20:37:22.634707
[0m20:37:22.637237 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m20:37:22.692277 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m20:37:22.694672 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m20:37:22.696696 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m20:37:22.702765 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m20:37:22.708822 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 20:37:22.638246 => 20:37:22.708822
[0m20:37:22.713183 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dadf8c0f-5101-4dd3-b2c3-79f26fb93eba', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B8B771BE0>]}
[0m20:37:22.716210 [info ] [Thread-6 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.09s]
[0m20:37:22.719241 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:37:22.721256 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:37:22.722262 [info ] [Thread-6 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m20:37:22.725353 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m20:37:22.727629 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:37:22.743369 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:37:22.747404 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 20:37:22.730054 => 20:37:22.745389
[0m20:37:22.749424 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:37:22.760953 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:37:22.765024 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:37:22.767047 [debug] [Thread-6 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:37:22.771073 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m20:37:22.779852 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 20:37:22.750433 => 20:37:22.778839
[0m20:37:22.782981 [info ] [Thread-6 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.06s]
[0m20:37:22.787022 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:37:22.788031 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:37:22.790090 [info ] [Thread-6 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m20:37:22.793607 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m20:37:22.795632 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:37:22.809312 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:37:22.812355 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 20:37:22.796639 => 20:37:22.811350
[0m20:37:22.813359 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:37:22.824268 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:37:22.827157 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:37:22.828673 [debug] [Thread-6 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:37:22.833699 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m20:37:22.840194 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 20:37:22.814365 => 20:37:22.839180
[0m20:37:22.843228 [info ] [Thread-6 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.05s]
[0m20:37:22.846369 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:37:22.850979 [debug] [MainThread]: On master: ROLLBACK
[0m20:37:22.853335 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:37:22.854342 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:37:22.856352 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:37:22.857358 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:37:22.858380 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:37:22.860883 [info ] [MainThread]: 
[0m20:37:22.862894 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 10.01 seconds (10.01s).
[0m20:37:22.867431 [debug] [MainThread]: Command end result
[0m20:37:22.883415 [info ] [MainThread]: 
[0m20:37:22.885428 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:37:22.887440 [info ] [MainThread]: 
[0m20:37:22.888460 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m20:37:22.891935 [debug] [MainThread]: Command `cli build` succeeded at 20:37:22.891935 after 11.24 seconds
[0m20:37:22.893946 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B8B2089E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B8B7CF020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B8B887C80>]}
[0m20:37:22.895962 [debug] [MainThread]: Flushing usage events
[0m20:41:48.912357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBAD78950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBAD79F70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBAD795E0>]}


============================== 20:41:48.914370 | e7ba87dd-cd77-4f1f-9be3-4c11ebac1138 ==============================
[0m20:41:48.914370 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:41:48.916380 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:41:49.183915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e7ba87dd-cd77-4f1f-9be3-4c11ebac1138', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBAE9DEE0>]}
[0m20:41:49.293828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e7ba87dd-cd77-4f1f-9be3-4c11ebac1138', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DB84A4FB0>]}
[0m20:41:49.295838 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:41:49.332361 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:41:49.929484 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:41:49.930488 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:41:49.948616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e7ba87dd-cd77-4f1f-9be3-4c11ebac1138', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBACED490>]}
[0m20:41:49.962692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e7ba87dd-cd77-4f1f-9be3-4c11ebac1138', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBAFC56D0>]}
[0m20:41:49.964723 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:41:49.970769 [info ] [MainThread]: 
[0m20:41:49.973661 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:41:49.979410 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:41:50.033582 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:41:50.107447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e7ba87dd-cd77-4f1f-9be3-4c11ebac1138', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBB05A6F0>]}
[0m20:41:50.109514 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:41:50.111848 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:41:50.113863 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:41:50.115878 [info ] [MainThread]: 
[0m20:41:50.125484 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m20:41:50.127503 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:41:50.131641 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:41:50.132649 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:41:50.172282 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:41:50.175805 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:41:50.133657 => 20:41:50.174793
[0m20:41:50.176813 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:41:53.411336 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:41:53.413352 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:41:53.414359 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:41:53.415365 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:41:53.471991 [debug] [Thread-6 (]: SQL status: OK in 0.05999999865889549 seconds
[0m20:41:53.585414 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m20:41:53.587719 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:41:53.589735 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m20:41:53.608449 [debug] [Thread-6 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:41:53.645579 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:41:50.177818 => 20:41:53.644573
[0m20:41:53.647606 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e7ba87dd-cd77-4f1f-9be3-4c11ebac1138', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBB1F3410>]}
[0m20:41:53.649625 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 3.52s]
[0m20:41:53.651637 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:41:53.653679 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m20:41:53.655173 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m20:41:53.658194 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m20:41:53.659199 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m20:41:53.661210 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 20:41:53.660205 => 20:41:53.660205
[0m20:41:53.662223 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m20:41:53.835083 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:41:53.838160 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:41:53.846311 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:41:53.896419 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m20:41:58.411492 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:41:58.413506 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m20:41:58.425452 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:41:58.438701 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m20:41:58.454160 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:41:58.457174 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 20:41:53.663229 => 20:41:58.456169
[0m20:41:58.459184 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e7ba87dd-cd77-4f1f-9be3-4c11ebac1138', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBB3E5040>]}
[0m20:41:58.460190 [info ] [Thread-6 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 4.80s]
[0m20:41:58.462201 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m20:41:58.464211 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:41:58.465217 [info ] [Thread-6 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m20:41:58.467229 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m20:41:58.469257 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:41:58.500174 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:41:58.503524 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 20:41:58.470271 => 20:41:58.502509
[0m20:41:58.504536 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:41:58.552952 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:41:58.561439 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:41:58.563473 [debug] [Thread-6 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:41:58.572682 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:41:58.580274 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 20:41:58.505738 => 20:41:58.579219
[0m20:41:58.584354 [info ] [Thread-6 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.12s]
[0m20:41:58.586381 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:41:58.588794 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:41:58.590808 [info ] [Thread-6 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m20:41:58.593848 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m20:41:58.595876 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:41:58.631863 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:41:58.634388 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 20:41:58.597473 => 20:41:58.633882
[0m20:41:58.635920 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:41:58.647513 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:41:58.651651 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:41:58.653664 [debug] [Thread-6 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:41:58.659972 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m20:41:58.667227 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 20:41:58.637244 => 20:41:58.666224
[0m20:41:58.670284 [info ] [Thread-6 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.08s]
[0m20:41:58.673568 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:41:58.677604 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m20:41:58.679634 [info ] [Thread-6 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m20:41:58.682893 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m20:41:58.684904 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m20:41:58.694495 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m20:41:58.697024 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 20:41:58.685909 => 20:41:58.695508
[0m20:41:58.697991 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m20:41:58.746670 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m20:41:58.748680 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m20:41:58.749684 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m20:41:58.754279 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m20:41:58.760776 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 20:41:58.698997 => 20:41:58.759773
[0m20:41:58.763822 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e7ba87dd-cd77-4f1f-9be3-4c11ebac1138', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBB2951F0>]}
[0m20:41:58.765848 [info ] [Thread-6 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m20:41:58.767860 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:41:58.769878 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:41:58.770904 [info ] [Thread-6 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m20:41:58.773969 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m20:41:58.774975 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:41:58.789410 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:41:58.791422 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 20:41:58.775979 => 20:41:58.791422
[0m20:41:58.793436 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:41:58.805192 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:41:58.809236 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:41:58.810263 [debug] [Thread-6 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:41:58.815299 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m20:41:58.822140 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 20:41:58.794443 => 20:41:58.821134
[0m20:41:58.826544 [info ] [Thread-6 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.05s]
[0m20:41:58.829566 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:41:58.830573 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:41:58.832632 [info ] [Thread-6 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m20:41:58.835033 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m20:41:58.837050 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:41:58.854368 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:41:58.856884 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 20:41:58.839080 => 20:41:58.855878
[0m20:41:58.858904 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:41:58.869344 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:41:58.872368 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:41:58.874399 [debug] [Thread-6 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:41:58.879448 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m20:41:58.886058 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 20:41:58.859913 => 20:41:58.886058
[0m20:41:58.890363 [info ] [Thread-6 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.06s]
[0m20:41:58.893487 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:41:58.898623 [debug] [MainThread]: On master: ROLLBACK
[0m20:41:58.900027 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:41:58.901030 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:41:58.902055 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:41:58.903061 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:41:58.905284 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:41:58.907304 [info ] [MainThread]: 
[0m20:41:58.909316 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 8.94 seconds (8.94s).
[0m20:41:58.913393 [debug] [MainThread]: Command end result
[0m20:41:58.927124 [info ] [MainThread]: 
[0m20:41:58.928663 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:41:58.932897 [info ] [MainThread]: 
[0m20:41:58.936001 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m20:41:58.941035 [debug] [MainThread]: Command `cli build` succeeded at 20:41:58.940027 after 10.14 seconds
[0m20:41:58.943049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBA331F40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBB3F7380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBB3F7E00>]}
[0m20:41:58.944055 [debug] [MainThread]: Flushing usage events
[0m20:42:13.279418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DB788CF20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBAD7A5D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBAD7AA50>]}


============================== 20:42:13.281428 | 8708edfc-9f16-4e59-b41e-17c2f7584ba7 ==============================
[0m20:42:13.281428 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:42:13.282433 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:42:13.362537 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8708edfc-9f16-4e59-b41e-17c2f7584ba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBAE9F6B0>]}
[0m20:42:13.366564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8708edfc-9f16-4e59-b41e-17c2f7584ba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBB1789B0>]}
[0m20:42:13.368576 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:42:13.392660 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:42:13.481855 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:42:13.483864 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:42:13.499356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8708edfc-9f16-4e59-b41e-17c2f7584ba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBB1F34D0>]}
[0m20:42:13.512781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8708edfc-9f16-4e59-b41e-17c2f7584ba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBC49C950>]}
[0m20:42:13.514792 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:42:13.516801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8708edfc-9f16-4e59-b41e-17c2f7584ba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBB178470>]}
[0m20:42:13.518810 [warn ] [MainThread]: The selection criterion 'tag:my_tag' does not match any nodes
[0m20:42:13.520820 [info ] [MainThread]: 
[0m20:42:13.522215 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m20:42:13.524236 [debug] [MainThread]: Command end result
[0m20:42:13.539601 [debug] [MainThread]: Command `cli run` succeeded at 20:42:13.539096 after 0.26 seconds
[0m20:42:13.541118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBB178620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBB1782F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DBB178560>]}
[0m20:42:13.542135 [debug] [MainThread]: Flushing usage events
[0m20:43:21.024687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC12629340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC126298E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC12629820>]}


============================== 20:43:21.027709 | 194ba5c8-2864-4af7-9c2f-e7158d1639e2 ==============================
[0m20:43:21.027709 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:43:21.029722 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:43:21.310459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '194ba5c8-2864-4af7-9c2f-e7158d1639e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC124190A0>]}
[0m20:43:21.421189 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '194ba5c8-2864-4af7-9c2f-e7158d1639e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC11D94EC0>]}
[0m20:43:21.424205 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:43:21.452802 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:43:21.557572 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:43:21.559580 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:43:21.577039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '194ba5c8-2864-4af7-9c2f-e7158d1639e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC1284CD40>]}
[0m20:43:21.592567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '194ba5c8-2864-4af7-9c2f-e7158d1639e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC128AFE90>]}
[0m20:43:21.594963 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:43:21.601023 [info ] [MainThread]: 
[0m20:43:21.604485 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:43:21.610756 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:43:21.662736 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:43:21.729446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '194ba5c8-2864-4af7-9c2f-e7158d1639e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC129005F0>]}
[0m20:43:21.731460 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:43:21.732516 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:43:21.735031 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:43:21.737042 [info ] [MainThread]: 
[0m20:43:21.747237 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m20:43:21.750260 [info ] [Thread-7 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:43:21.752652 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:43:21.753657 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:43:21.793685 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:43:21.795695 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:43:21.754661 => 20:43:21.794691
[0m20:43:21.798084 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:43:28.731585 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:43:28.732592 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:43:28.733597 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:43:28.735379 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m20:43:28.779633 [debug] [Thread-7 (]: SQL status: OK in 0.05000000074505806 seconds
[0m20:43:28.867920 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m20:43:28.869936 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:43:28.870944 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m20:43:28.884868 [debug] [Thread-7 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:43:28.921629 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:43:21.800120 => 20:43:28.920622
[0m20:43:28.924677 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '194ba5c8-2864-4af7-9c2f-e7158d1639e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC12A8FB60>]}
[0m20:43:28.927699 [info ] [Thread-7 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 7.17s]
[0m20:43:28.931263 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:43:28.933275 [debug] [Thread-7 (]: Began running node seed.testproj.sample
[0m20:43:28.935289 [info ] [Thread-7 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m20:43:28.937667 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m20:43:28.939688 [debug] [Thread-7 (]: Began compiling node seed.testproj.sample
[0m20:43:28.942484 [debug] [Thread-7 (]: Timing info for seed.testproj.sample (compile): 20:43:28.941704 => 20:43:28.941704
[0m20:43:28.943995 [debug] [Thread-7 (]: Began executing node seed.testproj.sample
[0m20:43:29.114246 [debug] [Thread-7 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:43:29.118294 [debug] [Thread-7 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:43:29.129044 [debug] [Thread-7 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:43:29.183316 [debug] [Thread-7 (]: Inserting batches of 500 records
[0m20:43:29.194531 [debug] [Thread-7 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:43:29.196052 [debug] [Thread-7 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m20:43:29.207253 [debug] [Thread-7 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:43:29.221032 [debug] [Thread-7 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m20:43:29.237294 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:43:29.240964 [debug] [Thread-7 (]: Timing info for seed.testproj.sample (execute): 20:43:28.943995 => 20:43:29.240023
[0m20:43:29.244020 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '194ba5c8-2864-4af7-9c2f-e7158d1639e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC12A422D0>]}
[0m20:43:29.245540 [info ] [Thread-7 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.31s]
[0m20:43:29.247565 [debug] [Thread-7 (]: Finished running node seed.testproj.sample
[0m20:43:29.249084 [debug] [Thread-7 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:43:29.250596 [info ] [Thread-7 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m20:43:29.253144 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m20:43:29.254658 [debug] [Thread-7 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:43:29.293187 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:43:29.297226 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 20:43:29.255671 => 20:43:29.296217
[0m20:43:29.298231 [debug] [Thread-7 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:43:29.342437 [debug] [Thread-7 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:43:29.345620 [debug] [Thread-7 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:43:29.346680 [debug] [Thread-7 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:43:29.351728 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m20:43:29.357788 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 20:43:29.300260 => 20:43:29.356783
[0m20:43:29.360905 [info ] [Thread-7 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.11s]
[0m20:43:29.362914 [debug] [Thread-7 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:43:29.364925 [debug] [Thread-7 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:43:29.365928 [info ] [Thread-7 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m20:43:29.368618 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m20:43:29.370399 [debug] [Thread-7 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:43:29.388220 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:43:29.391340 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 20:43:29.371409 => 20:43:29.390229
[0m20:43:29.392348 [debug] [Thread-7 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:43:29.404163 [debug] [Thread-7 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:43:29.407214 [debug] [Thread-7 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:43:29.409238 [debug] [Thread-7 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:43:29.413268 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m20:43:29.420819 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 20:43:29.394360 => 20:43:29.419795
[0m20:43:29.424234 [info ] [Thread-7 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.06s]
[0m20:43:29.427263 [debug] [Thread-7 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:43:29.429275 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m20:43:29.431289 [info ] [Thread-7 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m20:43:29.434409 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m20:43:29.437481 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m20:43:29.449468 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m20:43:29.452492 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 20:43:29.439512 => 20:43:29.451486
[0m20:43:29.453497 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m20:43:29.503303 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m20:43:29.505325 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m20:43:29.507359 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m20:43:29.512696 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m20:43:29.519775 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 20:43:29.454502 => 20:43:29.518769
[0m20:43:29.521806 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '194ba5c8-2864-4af7-9c2f-e7158d1639e2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC12B55970>]}
[0m20:43:29.522816 [info ] [Thread-7 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.09s]
[0m20:43:29.525329 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:43:29.527347 [debug] [Thread-7 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:43:29.529361 [info ] [Thread-7 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m20:43:29.531372 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m20:43:29.532442 [debug] [Thread-7 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:43:29.546998 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:43:29.549422 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 20:43:29.534470 => 20:43:29.548418
[0m20:43:29.552505 [debug] [Thread-7 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:43:29.566561 [debug] [Thread-7 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:43:29.568904 [debug] [Thread-7 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:43:29.571953 [debug] [Thread-7 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:43:29.578066 [debug] [Thread-7 (]: SQL status: OK in 0.0 seconds
[0m20:43:29.583064 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 20:43:29.554546 => 20:43:29.582085
[0m20:43:29.586679 [info ] [Thread-7 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.06s]
[0m20:43:29.589886 [debug] [Thread-7 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:43:29.592067 [debug] [Thread-7 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:43:29.594084 [info ] [Thread-7 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m20:43:29.598125 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m20:43:29.599337 [debug] [Thread-7 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:43:29.613658 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:43:29.616061 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 20:43:29.601366 => 20:43:29.615057
[0m20:43:29.619212 [debug] [Thread-7 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:43:29.629336 [debug] [Thread-7 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:43:29.630342 [debug] [Thread-7 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:43:29.632411 [debug] [Thread-7 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:43:29.639661 [debug] [Thread-7 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:43:29.646248 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 20:43:29.621236 => 20:43:29.644960
[0m20:43:29.649267 [info ] [Thread-7 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.05s]
[0m20:43:29.651352 [debug] [Thread-7 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:43:29.655383 [debug] [MainThread]: On master: ROLLBACK
[0m20:43:29.656462 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:43:29.657483 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:43:29.659503 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:43:29.660510 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:43:29.661517 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:43:29.663603 [info ] [MainThread]: 
[0m20:43:29.666681 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 8.06 seconds (8.06s).
[0m20:43:29.672793 [debug] [MainThread]: Command end result
[0m20:43:29.689589 [info ] [MainThread]: 
[0m20:43:29.691632 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:43:29.694662 [info ] [MainThread]: 
[0m20:43:29.696676 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m20:43:29.700715 [debug] [MainThread]: Command `cli build` succeeded at 20:43:29.700715 after 8.76 seconds
[0m20:43:29.703834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC11DFEC30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC12B5C560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BC12B5D6D0>]}
[0m20:43:29.705851 [debug] [MainThread]: Flushing usage events
[0m06:49:56.461871 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D64A8346B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D64A835970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D64A834320>]}


============================== 06:49:56.461871 | aafa0803-bdcc-46e0-b06d-fe957b111f5e ==============================
[0m06:49:56.461871 [info ] [MainThread]: Running with dbt=1.7.14
[0m06:49:56.462877 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m06:49:56.556820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'aafa0803-bdcc-46e0-b06d-fe957b111f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D64A236FF0>]}
[0m06:49:56.590071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'aafa0803-bdcc-46e0-b06d-fe957b111f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D64A124EC0>]}
[0m06:49:56.592082 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m06:49:56.603236 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m06:49:57.078061 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m06:49:57.078061 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m06:49:57.082125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aafa0803-bdcc-46e0-b06d-fe957b111f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D64A8A5F70>]}
[0m06:49:57.093198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aafa0803-bdcc-46e0-b06d-fe957b111f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D64BA25CA0>]}
[0m06:49:57.094203 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m06:49:57.095210 [info ] [MainThread]: 
[0m06:49:57.096215 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m06:49:57.097221 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m06:49:57.105327 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m06:49:57.121495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aafa0803-bdcc-46e0-b06d-fe957b111f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D64A937590>]}
[0m06:49:57.122501 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:49:57.122501 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:49:57.123507 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m06:49:57.123507 [info ] [MainThread]: 
[0m06:49:57.125519 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m06:49:57.126524 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m06:49:57.126524 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m06:49:57.127530 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m06:49:57.131551 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m06:49:57.132557 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 06:49:57.127530 => 06:49:57.131551
[0m06:49:57.132557 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m06:49:57.160841 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:49:57.161843 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m06:49:57.161843 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m06:49:57.162849 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m06:49:57.190111 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m06:49:57.215362 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m06:49:57.216370 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m06:49:57.216370 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m06:49:57.222473 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m06:49:57.230691 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 06:49:57.132557 => 06:49:57.229686
[0m06:49:57.230691 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aafa0803-bdcc-46e0-b06d-fe957b111f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D64A4FC4D0>]}
[0m06:49:57.231696 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.10s]
[0m06:49:57.231696 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m06:49:57.232702 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m06:49:57.233707 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m06:49:57.233707 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m06:49:57.234713 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m06:49:57.234713 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 06:49:57.234713 => 06:49:57.234713
[0m06:49:57.234713 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m06:49:57.258951 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m06:49:57.258951 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m06:49:57.260963 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m06:49:57.269103 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m06:49:57.271114 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m06:49:57.271114 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m06:49:57.277158 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m06:49:57.281180 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m06:49:57.284243 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:49:57.285250 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 06:49:57.235718 => 06:49:57.285250
[0m06:49:57.286255 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aafa0803-bdcc-46e0-b06d-fe957b111f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D64BA5BAA0>]}
[0m06:49:57.286255 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.05s]
[0m06:49:57.287261 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m06:49:57.288277 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m06:49:57.288277 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m06:49:57.288277 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m06:49:57.289283 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m06:49:57.295314 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m06:49:57.297324 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 06:49:57.289283 => 06:49:57.296319
[0m06:49:57.297324 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m06:49:57.305415 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m06:49:57.306422 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m06:49:57.307430 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m06:49:57.308437 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m06:49:57.310448 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 06:49:57.297324 => 06:49:57.310448
[0m06:49:57.310448 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.02s]
[0m06:49:57.311454 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m06:49:57.311454 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m06:49:57.312461 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m06:49:57.312461 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m06:49:57.313468 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m06:49:57.318501 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m06:49:57.319508 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 06:49:57.313468 => 06:49:57.319508
[0m06:49:57.320531 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m06:49:57.321537 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m06:49:57.322594 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m06:49:57.322594 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m06:49:57.324602 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m06:49:57.325608 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 06:49:57.320531 => 06:49:57.325608
[0m06:49:57.326614 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.01s]
[0m06:49:57.326614 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m06:49:57.327620 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m06:49:57.327620 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m06:49:57.328625 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m06:49:57.328625 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m06:49:57.330642 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m06:49:57.331651 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 06:49:57.328625 => 06:49:57.330642
[0m06:49:57.331651 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m06:49:57.341755 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m06:49:57.342877 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m06:49:57.342877 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m06:49:57.343879 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m06:49:57.345900 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 06:49:57.331651 => 06:49:57.345900
[0m06:49:57.345900 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aafa0803-bdcc-46e0-b06d-fe957b111f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D64BD29E80>]}
[0m06:49:57.346907 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m06:49:57.346907 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m06:49:57.347915 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m06:49:57.347915 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m06:49:57.348923 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m06:49:57.348923 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m06:49:57.352975 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m06:49:57.352975 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 06:49:57.349930 => 06:49:57.352975
[0m06:49:57.353981 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m06:49:57.354987 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m06:49:57.355992 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m06:49:57.355992 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m06:49:57.356997 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m06:49:57.358003 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 06:49:57.353981 => 06:49:57.358003
[0m06:49:57.359009 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m06:49:57.359009 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m06:49:57.359009 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m06:49:57.360014 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m06:49:57.360014 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m06:49:57.360014 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m06:49:57.363086 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m06:49:57.364088 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 06:49:57.361019 => 06:49:57.363086
[0m06:49:57.364088 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m06:49:57.365094 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m06:49:57.366111 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m06:49:57.366111 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m06:49:57.367117 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m06:49:57.368123 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 06:49:57.364088 => 06:49:57.368123
[0m06:49:57.369128 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m06:49:57.369128 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m06:49:57.370133 [debug] [MainThread]: On master: ROLLBACK
[0m06:49:57.370133 [debug] [MainThread]: Opening a new connection, currently in state init
[0m06:49:57.371139 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m06:49:57.371139 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m06:49:57.371139 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m06:49:57.371139 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m06:49:57.372144 [info ] [MainThread]: 
[0m06:49:57.372144 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m06:49:57.373149 [debug] [MainThread]: Command end result
[0m06:49:57.377172 [info ] [MainThread]: 
[0m06:49:57.378178 [info ] [MainThread]: [32mCompleted successfully[0m
[0m06:49:57.378178 [info ] [MainThread]: 
[0m06:49:57.379185 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m06:49:57.379185 [debug] [MainThread]: Command `cli build` succeeded at 06:49:57.379185 after 0.95 seconds
[0m06:49:57.380190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D64BE3BBC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D64BCF3FE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D649A36AB0>]}
[0m06:49:57.380190 [debug] [MainThread]: Flushing usage events
[0m14:23:30.596390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195BE1FE9F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195BE1FE990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195BE1FE840>]}


============================== 14:23:30.599434 | b464e7a9-420f-43d2-a081-cd4a1adadf78 ==============================
[0m14:23:30.599434 [info ] [MainThread]: Running with dbt=1.7.14
[0m14:23:30.601479 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:23:30.896261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b464e7a9-420f-43d2-a081-cd4a1adadf78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195BE2AB830>]}
[0m14:23:31.017593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b464e7a9-420f-43d2-a081-cd4a1adadf78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195BDF2FF20>]}
[0m14:23:31.020625 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:23:31.054729 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m14:23:31.825857 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:23:31.827876 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:23:31.846576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b464e7a9-420f-43d2-a081-cd4a1adadf78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195BE2A8A70>]}
[0m14:23:31.864791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b464e7a9-420f-43d2-a081-cd4a1adadf78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195BE469850>]}
[0m14:23:31.866821 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m14:23:31.873888 [info ] [MainThread]: 
[0m14:23:31.875903 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:23:31.880947 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:23:31.934634 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:23:32.008430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b464e7a9-420f-43d2-a081-cd4a1adadf78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195BE4DA9C0>]}
[0m14:23:32.010446 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:23:32.011455 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:23:32.014485 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:23:32.015494 [info ] [MainThread]: 
[0m14:23:32.023592 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m14:23:32.024603 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:23:32.028650 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:23:32.030678 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:23:32.070225 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:23:32.073253 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:23:32.031689 => 14:23:32.072243
[0m14:23:32.074262 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:23:32.209786 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:23:32.210796 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:23:32.212810 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:23:32.213816 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:23:32.281976 [debug] [Thread-6 (]: SQL status: OK in 0.07000000029802322 seconds
[0m14:23:32.371469 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:23:32.373505 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:23:32.374518 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:23:32.384603 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:23:32.418957 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:23:32.075272 => 14:23:32.418957
[0m14:23:32.421982 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b464e7a9-420f-43d2-a081-cd4a1adadf78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195BE682990>]}
[0m14:23:32.423998 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.39s]
[0m14:23:32.426016 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:23:32.427025 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m14:23:32.430084 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m14:23:32.435214 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m14:23:32.436222 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m14:23:32.438241 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 14:23:32.437232 => 14:23:32.437232
[0m14:23:32.439254 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m14:23:32.560652 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:23:32.561667 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m14:23:32.567766 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:23:32.606385 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m14:23:32.617499 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:23:32.619553 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m14:23:32.631737 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:23:32.647460 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m14:23:32.665763 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:23:32.670851 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 14:23:32.440264 => 14:23:32.669839
[0m14:23:32.672887 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b464e7a9-420f-43d2-a081-cd4a1adadf78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195BE680230>]}
[0m14:23:32.674908 [info ] [Thread-6 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.24s]
[0m14:23:32.676930 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m14:23:32.677939 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:23:32.679994 [info ] [Thread-6 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m14:23:32.683035 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m14:23:32.685067 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:23:32.722872 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:23:32.725923 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 14:23:32.686083 => 14:23:32.724895
[0m14:23:32.726938 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:23:32.776976 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:23:32.778997 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:23:32.780024 [debug] [Thread-6 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:23:32.785085 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:23:32.791227 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 14:23:32.727955 => 14:23:32.790217
[0m14:23:32.795307 [info ] [Thread-6 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.11s]
[0m14:23:32.798352 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:23:32.800394 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:23:32.802507 [info ] [Thread-6 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m14:23:32.805621 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m14:23:32.807648 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:23:32.826979 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:23:32.830018 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 14:23:32.808666 => 14:23:32.829007
[0m14:23:32.833157 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:23:32.843343 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:23:32.845381 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:23:32.846903 [debug] [Thread-6 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:23:32.852518 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:23:32.857576 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 14:23:32.834174 => 14:23:32.856562
[0m14:23:32.860609 [info ] [Thread-6 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.06s]
[0m14:23:32.863730 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:23:32.867789 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m14:23:32.870846 [info ] [Thread-6 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:23:32.873882 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m14:23:32.874893 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:23:32.885074 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:23:32.888114 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:23:32.875904 => 14:23:32.887100
[0m14:23:32.889126 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:23:32.938996 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:23:32.941035 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:23:32.942050 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:23:32.947102 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:23:32.952651 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:23:32.890140 => 14:23:32.952651
[0m14:23:32.955710 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b464e7a9-420f-43d2-a081-cd4a1adadf78', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195BE8999A0>]}
[0m14:23:32.956723 [info ] [Thread-6 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m14:23:32.959754 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:23:32.961770 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:23:32.963810 [info ] [Thread-6 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m14:23:32.966851 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m14:23:32.968890 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:23:32.982057 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:23:32.986276 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 14:23:32.969905 => 14:23:32.985103
[0m14:23:32.987279 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:23:32.996383 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:23:32.999430 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:23:33.001503 [debug] [Thread-6 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:23:33.006579 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:23:33.011625 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 14:23:32.988290 => 14:23:33.010618
[0m14:23:33.013643 [info ] [Thread-6 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.05s]
[0m14:23:33.015668 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:23:33.017705 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:23:33.019728 [info ] [Thread-6 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m14:23:33.022771 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m14:23:33.023781 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:23:33.038975 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:23:33.041496 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 14:23:33.024790 => 14:23:33.040484
[0m14:23:33.043518 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:23:33.051624 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:23:33.053646 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:23:33.055664 [debug] [Thread-6 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:23:33.060731 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:23:33.066867 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 14:23:33.043518 => 14:23:33.066867
[0m14:23:33.070920 [info ] [Thread-6 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.05s]
[0m14:23:33.072956 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:23:33.076999 [debug] [MainThread]: On master: ROLLBACK
[0m14:23:33.078016 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:23:33.080041 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:23:33.081054 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:23:33.082067 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:23:33.084094 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:23:33.087133 [info ] [MainThread]: 
[0m14:23:33.088145 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 1.21 seconds (1.21s).
[0m14:23:33.093264 [debug] [MainThread]: Command end result
[0m14:23:33.109508 [info ] [MainThread]: 
[0m14:23:33.110521 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:23:33.112544 [info ] [MainThread]: 
[0m14:23:33.114575 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m14:23:33.117622 [debug] [MainThread]: Command `cli build` succeeded at 14:23:33.117622 after 2.64 seconds
[0m14:23:33.119647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195BE19F3B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195BE4EBF20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000195BE642C30>]}
[0m14:23:33.121671 [debug] [MainThread]: Flushing usage events
[0m14:24:46.437978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023ABE5ABD10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023ABE2AE420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023ABE48C260>]}


============================== 14:24:46.440000 | aea63229-e8db-4bd9-a668-62e17ec6d3e4 ==============================
[0m14:24:46.440000 [info ] [MainThread]: Running with dbt=1.7.14
[0m14:24:46.442021 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:24:46.713994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'aea63229-e8db-4bd9-a668-62e17ec6d3e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023ABE62B890>]}
[0m14:24:46.835821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'aea63229-e8db-4bd9-a668-62e17ec6d3e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023ABE57F770>]}
[0m14:24:46.837837 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:24:46.866241 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m14:24:46.969689 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:24:46.971712 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:24:46.988921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aea63229-e8db-4bd9-a668-62e17ec6d3e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023ABE5FF890>]}
[0m14:24:47.006691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aea63229-e8db-4bd9-a668-62e17ec6d3e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023ABE832ED0>]}
[0m14:24:47.008230 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m14:24:47.014358 [info ] [MainThread]: 
[0m14:24:47.017397 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:24:47.024529 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:24:47.074373 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:24:47.143444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aea63229-e8db-4bd9-a668-62e17ec6d3e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023ABE830FB0>]}
[0m14:24:47.144461 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:24:47.146499 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:24:47.149587 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:24:47.151632 [info ] [MainThread]: 
[0m14:24:47.157720 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m14:24:47.158732 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:24:47.161765 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:24:47.163823 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:24:47.203961 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:24:47.206994 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:24:47.164834 => 14:24:47.205981
[0m14:24:47.208007 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:24:47.353013 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:24:47.354029 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:24:47.356078 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:24:47.357095 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:24:47.408926 [debug] [Thread-6 (]: SQL status: OK in 0.05000000074505806 seconds
[0m14:24:47.496213 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:24:47.499281 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:24:47.502443 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:24:47.526626 [debug] [Thread-6 (]: SQL status: OK in 0.019999999552965164 seconds
[0m14:24:47.578716 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:24:47.209046 => 14:24:47.578716
[0m14:24:47.582802 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aea63229-e8db-4bd9-a668-62e17ec6d3e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023ABEA02B70>]}
[0m14:24:47.584850 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.42s]
[0m14:24:47.588927 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:24:47.590954 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m14:24:47.594021 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m14:24:47.598126 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m14:24:47.600172 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m14:24:47.602258 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 14:24:47.601228 => 14:24:47.601228
[0m14:24:47.604297 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m14:24:47.738098 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:24:47.739111 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m14:24:47.743147 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:24:47.780191 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m14:24:47.791303 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:24:47.792311 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m14:24:47.804506 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:24:47.818660 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m14:24:47.834892 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:24:47.837923 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 14:24:47.605317 => 14:24:47.836913
[0m14:24:47.839960 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aea63229-e8db-4bd9-a668-62e17ec6d3e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023ABEA00050>]}
[0m14:24:47.841988 [info ] [Thread-6 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.24s]
[0m14:24:47.846064 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m14:24:47.849153 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:24:47.850176 [info ] [Thread-6 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m14:24:47.853226 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m14:24:47.854290 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:24:47.891977 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:24:47.894004 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 14:24:47.855292 => 14:24:47.894004
[0m14:24:47.897104 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:24:47.938472 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:24:47.940586 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:24:47.941605 [debug] [Thread-6 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:24:47.946738 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:24:47.952826 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 14:24:47.898124 => 14:24:47.951820
[0m14:24:47.954845 [info ] [Thread-6 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.10s]
[0m14:24:47.956863 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:24:47.958896 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:24:47.959906 [info ] [Thread-6 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m14:24:47.962992 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m14:24:47.965039 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:24:47.984369 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:24:47.986393 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 14:24:47.967105 => 14:24:47.986393
[0m14:24:47.988418 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:24:47.997635 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:24:48.001748 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:24:48.003847 [debug] [Thread-6 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:24:48.008926 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:24:48.014008 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 14:24:47.989428 => 14:24:48.012994
[0m14:24:48.017103 [info ] [Thread-6 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.06s]
[0m14:24:48.020157 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:24:48.023211 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m14:24:48.025277 [info ] [Thread-6 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:24:48.027314 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m14:24:48.029345 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:24:48.041636 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:24:48.043656 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:24:48.030372 => 14:24:48.043656
[0m14:24:48.045685 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:24:48.092795 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:24:48.095835 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:24:48.096849 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:24:48.103077 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:24:48.109189 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:24:48.046701 => 14:24:48.108180
[0m14:24:48.111225 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'aea63229-e8db-4bd9-a668-62e17ec6d3e4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023ABEA00680>]}
[0m14:24:48.113246 [info ] [Thread-6 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m14:24:48.115269 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:24:48.117318 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:24:48.118328 [info ] [Thread-6 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m14:24:48.122405 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m14:24:48.123425 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:24:48.139724 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:24:48.141762 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 14:24:48.124438 => 14:24:48.140742
[0m14:24:48.143788 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:24:48.151906 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:24:48.154979 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:24:48.157085 [debug] [Thread-6 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:24:48.163293 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:24:48.169429 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 14:24:48.143788 => 14:24:48.169429
[0m14:24:48.172492 [info ] [Thread-6 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.05s]
[0m14:24:48.175628 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:24:48.176650 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:24:48.177679 [info ] [Thread-6 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m14:24:48.180718 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m14:24:48.181727 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:24:48.195945 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:24:48.199008 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 14:24:48.182738 => 14:24:48.197994
[0m14:24:48.201072 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:24:48.210238 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:24:48.212254 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:24:48.213264 [debug] [Thread-6 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:24:48.217816 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:24:48.222865 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 14:24:48.202096 => 14:24:48.221856
[0m14:24:48.225944 [info ] [Thread-6 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.05s]
[0m14:24:48.227975 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:24:48.232066 [debug] [MainThread]: On master: ROLLBACK
[0m14:24:48.235133 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:24:48.236145 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:24:48.237163 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:24:48.239206 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:24:48.240223 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:24:48.243262 [info ] [MainThread]: 
[0m14:24:48.244276 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 1.23 seconds (1.23s).
[0m14:24:48.248331 [debug] [MainThread]: Command end result
[0m14:24:48.260529 [info ] [MainThread]: 
[0m14:24:48.262567 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:24:48.265624 [info ] [MainThread]: 
[0m14:24:48.267670 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m14:24:48.271790 [debug] [MainThread]: Command `cli build` succeeded at 14:24:48.270767 after 1.92 seconds
[0m14:24:48.273821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023ABE2433B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023ABE9C09B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023ABE7DF860>]}
[0m14:24:48.274841 [debug] [MainThread]: Flushing usage events
[0m14:25:22.548429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002497AFB8D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002497AFBB8C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002497A8C6AE0>]}


============================== 14:25:22.550448 | df0c5ed7-7ea8-4564-9c0d-08065b74bf9a ==============================
[0m14:25:22.550448 [info ] [MainThread]: Running with dbt=1.7.14
[0m14:25:22.553477 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m14:25:22.840016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'df0c5ed7-7ea8-4564-9c0d-08065b74bf9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002497AAEFBC0>]}
[0m14:25:22.960510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'df0c5ed7-7ea8-4564-9c0d-08065b74bf9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002497AD6FB00>]}
[0m14:25:22.963548 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:25:22.992837 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m14:25:23.099083 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:25:23.100091 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:25:23.118253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'df0c5ed7-7ea8-4564-9c0d-08065b74bf9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002497B0DE8A0>]}
[0m14:25:23.135484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'df0c5ed7-7ea8-4564-9c0d-08065b74bf9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002497B23EF00>]}
[0m14:25:23.137500 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m14:25:23.144112 [info ] [MainThread]: 
[0m14:25:23.147658 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:25:23.152700 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:25:23.203380 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:25:23.272912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'df0c5ed7-7ea8-4564-9c0d-08065b74bf9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002497B1A9E80>]}
[0m14:25:23.275939 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:25:23.277993 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:25:23.281028 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:25:23.282039 [info ] [MainThread]: 
[0m14:25:23.289120 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m14:25:23.291151 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:25:23.294183 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:25:23.296209 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:25:23.335808 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:25:23.339909 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:25:23.297226 => 14:25:23.338900
[0m14:25:23.341936 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:25:23.485996 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:25:23.488017 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:25:23.489028 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:25:23.490039 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:25:23.555819 [debug] [Thread-6 (]: SQL status: OK in 0.07000000029802322 seconds
[0m14:25:23.644801 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:25:23.647844 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:25:23.649875 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:25:23.656984 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:25:23.695596 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:25:23.342947 => 14:25:23.695596
[0m14:25:23.699664 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'df0c5ed7-7ea8-4564-9c0d-08065b74bf9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002497B40B620>]}
[0m14:25:23.701685 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.41s]
[0m14:25:23.703713 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:25:23.705751 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m14:25:23.709818 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m14:25:23.713887 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m14:25:23.715918 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m14:25:23.717953 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 14:25:23.716936 => 14:25:23.716936
[0m14:25:23.719989 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m14:25:23.842125 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:25:23.844157 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m14:25:23.848191 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:25:23.889383 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m14:25:23.901536 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:25:23.903551 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m14:25:23.916209 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:25:23.930405 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m14:25:23.946583 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:25:23.949616 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 14:25:23.721042 => 14:25:23.949616
[0m14:25:23.951654 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'df0c5ed7-7ea8-4564-9c0d-08065b74bf9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002497B409670>]}
[0m14:25:23.953669 [info ] [Thread-6 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.24s]
[0m14:25:23.955691 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m14:25:23.956700 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:25:23.958715 [info ] [Thread-6 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m14:25:23.960734 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m14:25:23.962764 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:25:23.995246 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:25:23.999294 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 14:25:23.963778 => 14:25:23.998275
[0m14:25:24.002362 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:25:24.042293 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:25:24.045319 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:25:24.046327 [debug] [Thread-6 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:25:24.051376 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:25:24.056933 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 14:25:24.003373 => 14:25:24.056933
[0m14:25:24.059961 [info ] [Thread-6 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.10s]
[0m14:25:24.063007 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:25:24.065032 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:25:24.067053 [info ] [Thread-6 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m14:25:24.069071 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m14:25:24.070081 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:25:24.089304 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:25:24.091325 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 14:25:24.072125 => 14:25:24.091325
[0m14:25:24.093343 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:25:24.103473 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:25:24.106526 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:25:24.107538 [debug] [Thread-6 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:25:24.111576 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:25:24.116611 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 14:25:24.095366 => 14:25:24.116611
[0m14:25:24.119676 [info ] [Thread-6 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.05s]
[0m14:25:24.121710 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:25:24.123738 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m14:25:24.125757 [info ] [Thread-6 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:25:24.128784 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m14:25:24.129804 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:25:24.139978 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:25:24.143006 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:25:24.130834 => 14:25:24.141998
[0m14:25:24.145031 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:25:24.191659 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:25:24.194728 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:25:24.196746 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:25:24.201831 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:25:24.208956 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:25:24.146041 => 14:25:24.207922
[0m14:25:24.213008 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'df0c5ed7-7ea8-4564-9c0d-08065b74bf9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002497B04CD70>]}
[0m14:25:24.215029 [info ] [Thread-6 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.09s]
[0m14:25:24.217047 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:25:24.219066 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:25:24.220073 [info ] [Thread-6 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m14:25:24.223108 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m14:25:24.225200 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:25:24.241476 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:25:24.244522 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 14:25:24.226216 => 14:25:24.243509
[0m14:25:24.246556 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:25:24.254670 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:25:24.257184 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:25:24.258194 [debug] [Thread-6 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:25:24.263297 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:25:24.269388 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 14:25:24.247567 => 14:25:24.268372
[0m14:25:24.271420 [info ] [Thread-6 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.05s]
[0m14:25:24.273435 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:25:24.275454 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:25:24.276460 [info ] [Thread-6 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m14:25:24.279516 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m14:25:24.280530 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:25:24.293702 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:25:24.296765 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 14:25:24.281544 => 14:25:24.295739
[0m14:25:24.298791 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:25:24.306932 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:25:24.308950 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:25:24.310967 [debug] [Thread-6 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:25:24.315025 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:25:24.320093 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 14:25:24.299870 => 14:25:24.320093
[0m14:25:24.323114 [info ] [Thread-6 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.04s]
[0m14:25:24.325133 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:25:24.329189 [debug] [MainThread]: On master: ROLLBACK
[0m14:25:24.330199 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:25:24.332240 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:25:24.333251 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:25:24.335281 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:25:24.336293 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:25:24.339362 [info ] [MainThread]: 
[0m14:25:24.341397 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 1.19 seconds (1.19s).
[0m14:25:24.345471 [debug] [MainThread]: Command end result
[0m14:25:24.357130 [info ] [MainThread]: 
[0m14:25:24.359153 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:25:24.361218 [info ] [MainThread]: 
[0m14:25:24.363257 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m14:25:24.367339 [debug] [MainThread]: Command `cli build` succeeded at 14:25:24.366329 after 1.92 seconds
[0m14:25:24.369362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002497C6FE360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002497B23EE40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002497B23E6F0>]}
[0m14:25:24.371409 [debug] [MainThread]: Flushing usage events
[0m14:26:50.412501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021138628380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021138628050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211383DA060>]}


============================== 14:26:50.414518 | 36e8ba21-a706-44f1-b84b-ad586b9fb614 ==============================
[0m14:26:50.414518 [info ] [MainThread]: Running with dbt=1.7.14
[0m14:26:50.416537 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m14:26:50.692299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '36e8ba21-a706-44f1-b84b-ad586b9fb614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021138003DA0>]}
[0m14:26:50.815555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '36e8ba21-a706-44f1-b84b-ad586b9fb614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211385FE7E0>]}
[0m14:26:50.817569 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:26:50.847957 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m14:26:50.943202 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:26:50.944208 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:26:50.963405 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '36e8ba21-a706-44f1-b84b-ad586b9fb614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021138771B20>]}
[0m14:26:50.979564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '36e8ba21-a706-44f1-b84b-ad586b9fb614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211388658B0>]}
[0m14:26:50.981587 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m14:26:50.987169 [info ] [MainThread]: 
[0m14:26:50.990193 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:26:50.995254 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:26:51.044846 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:26:51.114172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '36e8ba21-a706-44f1-b84b-ad586b9fb614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211388B8D40>]}
[0m14:26:51.116198 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:26:51.117211 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:26:51.120237 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:26:51.121243 [info ] [MainThread]: 
[0m14:26:51.129413 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m14:26:51.132456 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:26:51.136505 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:26:51.138536 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:26:51.180097 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:26:51.184153 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:26:51.139554 => 14:26:51.183141
[0m14:26:51.186176 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:26:51.330332 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:26:51.333375 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:26:51.334565 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:26:51.335574 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:26:51.380080 [debug] [Thread-6 (]: SQL status: OK in 0.03999999910593033 seconds
[0m14:26:51.466093 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:26:51.468110 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:26:51.469117 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:26:51.480225 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:26:51.519748 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:26:51.187186 => 14:26:51.518741
[0m14:26:51.521763 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '36e8ba21-a706-44f1-b84b-ad586b9fb614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021138A808C0>]}
[0m14:26:51.524790 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.39s]
[0m14:26:51.526822 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:26:51.529884 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m14:26:51.531926 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m14:26:51.535991 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m14:26:51.537004 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m14:26:51.539027 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 14:26:51.538015 => 14:26:51.538015
[0m14:26:51.540038 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m14:26:51.689697 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:26:51.690711 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m14:26:51.696819 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:26:51.735277 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m14:26:51.746397 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:26:51.747404 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m14:26:51.757498 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:26:51.771722 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m14:26:51.786873 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:26:51.789900 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 14:26:51.542081 => 14:26:51.789900
[0m14:26:51.792927 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '36e8ba21-a706-44f1-b84b-ad586b9fb614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021138A82930>]}
[0m14:26:51.793937 [info ] [Thread-6 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.26s]
[0m14:26:51.796991 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m14:26:51.799029 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:26:51.800037 [info ] [Thread-6 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m14:26:51.803087 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m14:26:51.806138 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:26:51.840644 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:26:51.843674 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 14:26:51.807152 => 14:26:51.842663
[0m14:26:51.845705 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:26:51.885822 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:26:51.887839 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:26:51.889857 [debug] [Thread-6 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:26:51.893911 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:26:51.901016 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 14:26:51.846713 => 14:26:51.900009
[0m14:26:51.904038 [info ] [Thread-6 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.10s]
[0m14:26:51.906055 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:26:51.907064 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:26:51.908071 [info ] [Thread-6 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m14:26:51.911110 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m14:26:51.912120 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:26:51.930319 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:26:51.934383 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 14:26:51.913127 => 14:26:51.933367
[0m14:26:51.935390 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:26:51.944482 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:26:51.946508 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:26:51.947523 [debug] [Thread-6 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:26:51.952582 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:26:51.957633 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 14:26:51.936398 => 14:26:51.956625
[0m14:26:51.959651 [info ] [Thread-6 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.05s]
[0m14:26:51.962171 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:26:51.965219 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m14:26:51.967255 [info ] [Thread-6 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:26:51.969279 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m14:26:51.971314 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:26:51.979959 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:26:51.982984 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:26:51.972320 => 14:26:51.981974
[0m14:26:51.983992 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:26:52.026450 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:26:52.029488 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:26:52.030510 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:26:52.036627 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:26:52.042681 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:26:51.985000 => 14:26:52.042681
[0m14:26:52.045707 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '36e8ba21-a706-44f1-b84b-ad586b9fb614', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021138BAE240>]}
[0m14:26:52.046728 [info ] [Thread-6 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m14:26:52.048745 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:26:52.050767 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:26:52.052784 [info ] [Thread-6 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m14:26:52.054804 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m14:26:52.055816 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:26:52.072220 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:26:52.074236 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 14:26:52.056828 => 14:26:52.074236
[0m14:26:52.076255 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:26:52.085474 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:26:52.088528 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:26:52.090565 [debug] [Thread-6 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:26:52.095622 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:26:52.101698 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 14:26:52.077288 => 14:26:52.100683
[0m14:26:52.104737 [info ] [Thread-6 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.05s]
[0m14:26:52.107851 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:26:52.109877 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:26:52.111909 [info ] [Thread-6 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m14:26:52.114951 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m14:26:52.116971 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:26:52.131164 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:26:52.134217 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 14:26:52.117988 => 14:26:52.133196
[0m14:26:52.135232 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:26:52.145396 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:26:52.148421 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:26:52.149434 [debug] [Thread-6 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:26:52.156580 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:26:52.162687 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 14:26:52.137308 => 14:26:52.161668
[0m14:26:52.165733 [info ] [Thread-6 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.05s]
[0m14:26:52.168807 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:26:52.172851 [debug] [MainThread]: On master: ROLLBACK
[0m14:26:52.174877 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:26:52.175890 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:26:52.177922 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:26:52.179450 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:26:52.180969 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:26:52.184069 [info ] [MainThread]: 
[0m14:26:52.186093 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 1.19 seconds (1.19s).
[0m14:26:52.190234 [debug] [MainThread]: Command end result
[0m14:26:52.211778 [info ] [MainThread]: 
[0m14:26:52.214391 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:26:52.216902 [info ] [MainThread]: 
[0m14:26:52.218435 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m14:26:52.222514 [debug] [MainThread]: Command `cli build` succeeded at 14:26:52.222007 after 1.90 seconds
[0m14:26:52.225090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211385FF470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021139D6DF70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000211386AEB10>]}
[0m14:26:52.226115 [debug] [MainThread]: Flushing usage events
[0m14:28:59.040781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F75F68ECF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F75F68F560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F75F68F7D0>]}


============================== 14:28:59.042821 | a2890f5e-e23b-438e-bf5f-221fdc12c302 ==============================
[0m14:28:59.042821 [info ] [MainThread]: Running with dbt=1.7.14
[0m14:28:59.044352 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:28:59.324823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a2890f5e-e23b-438e-bf5f-221fdc12c302', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F75F59C8C0>]}
[0m14:28:59.456416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a2890f5e-e23b-438e-bf5f-221fdc12c302', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F75F7500B0>]}
[0m14:28:59.459453 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:28:59.492479 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m14:28:59.612787 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:28:59.613794 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:28:59.633313 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a2890f5e-e23b-438e-bf5f-221fdc12c302', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F75F8E88C0>]}
[0m14:28:59.649039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a2890f5e-e23b-438e-bf5f-221fdc12c302', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F75F946B10>]}
[0m14:28:59.650560 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m14:28:59.656640 [info ] [MainThread]: 
[0m14:28:59.659666 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:28:59.665809 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:28:59.715872 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:28:59.785753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a2890f5e-e23b-438e-bf5f-221fdc12c302', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F75F2F6840>]}
[0m14:28:59.787776 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:28:59.788789 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:28:59.791820 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:28:59.792829 [info ] [MainThread]: 
[0m14:28:59.803020 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m14:28:59.805042 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:28:59.809116 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:28:59.811136 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:28:59.850688 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:28:59.852706 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:28:59.812146 => 14:28:59.851698
[0m14:28:59.854724 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:28:59.995798 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:28:59.997858 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:28:59.999909 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:29:00.001931 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:29:00.048546 [debug] [Thread-6 (]: SQL status: OK in 0.05000000074505806 seconds
[0m14:29:00.132713 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:29:00.134761 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:29:00.135776 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:29:00.146872 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:29:00.183828 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:28:59.855740 => 14:29:00.183828
[0m14:29:00.186859 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a2890f5e-e23b-438e-bf5f-221fdc12c302', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F75FB0E5D0>]}
[0m14:29:00.188883 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.38s]
[0m14:29:00.191929 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:29:00.193960 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m14:29:00.197038 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m14:29:00.200100 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m14:29:00.202127 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m14:29:00.204155 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 14:29:00.203142 => 14:29:00.203142
[0m14:29:00.205170 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m14:29:00.338375 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:29:00.340427 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m14:29:00.344482 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:29:00.385213 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m14:29:00.397434 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:29:00.399474 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m14:29:00.415660 [debug] [Thread-6 (]: SQL status: OK in 0.019999999552965164 seconds
[0m14:29:00.431893 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m14:29:00.449627 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:29:00.452664 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 14:29:00.206180 => 14:29:00.452664
[0m14:29:00.456744 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a2890f5e-e23b-438e-bf5f-221fdc12c302', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F75FB0C0E0>]}
[0m14:29:00.458771 [info ] [Thread-6 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.26s]
[0m14:29:00.461820 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m14:29:00.464886 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:29:00.467932 [info ] [Thread-6 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m14:29:00.473085 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m14:29:00.475132 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:29:00.514961 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:29:00.521338 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 14:29:00.476141 => 14:29:00.520297
[0m14:29:00.524405 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:29:00.574480 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:29:00.578041 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:29:00.580127 [debug] [Thread-6 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:29:00.586245 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:29:00.593329 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 14:29:00.525435 => 14:29:00.592316
[0m14:29:00.596415 [info ] [Thread-6 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.13s]
[0m14:29:00.599466 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:29:00.601523 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:29:00.602538 [info ] [Thread-6 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m14:29:00.606596 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m14:29:00.608629 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:29:00.629012 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:29:00.633110 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 14:29:00.609651 => 14:29:00.632096
[0m14:29:00.635160 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:29:00.645415 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:29:00.647443 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:29:00.649468 [debug] [Thread-6 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:29:00.654520 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:29:00.660651 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 14:29:00.636175 => 14:29:00.659621
[0m14:29:00.663704 [info ] [Thread-6 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.06s]
[0m14:29:00.666758 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:29:00.669808 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m14:29:00.672903 [info ] [Thread-6 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:29:00.674936 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m14:29:00.677981 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:29:00.686089 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:29:00.689140 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:29:00.679001 => 14:29:00.688131
[0m14:29:00.690149 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:29:00.733292 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:29:00.736339 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:29:00.737350 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:29:00.742395 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:29:00.748473 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:29:00.691157 => 14:29:00.747445
[0m14:29:00.750490 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a2890f5e-e23b-438e-bf5f-221fdc12c302', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F75F7D9D00>]}
[0m14:29:00.752506 [info ] [Thread-6 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m14:29:00.754523 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:29:00.756547 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:29:00.757556 [info ] [Thread-6 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m14:29:00.759571 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m14:29:00.761593 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:29:00.777846 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:29:00.779890 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 14:29:00.763663 => 14:29:00.778879
[0m14:29:00.781921 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:29:00.790023 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:29:00.792037 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:29:00.793046 [debug] [Thread-6 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:29:00.799161 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:29:00.805228 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 14:29:00.782938 => 14:29:00.804219
[0m14:29:00.807248 [info ] [Thread-6 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.05s]
[0m14:29:00.809271 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:29:00.811288 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:29:00.812295 [info ] [Thread-6 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m14:29:00.814311 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m14:29:00.816331 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:29:00.832546 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:29:00.834574 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 14:29:00.817340 => 14:29:00.833555
[0m14:29:00.836615 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:29:00.845743 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:29:00.847762 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:29:00.849778 [debug] [Thread-6 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:29:00.854877 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:29:00.859927 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 14:29:00.837628 => 14:29:00.858918
[0m14:29:00.862957 [info ] [Thread-6 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.05s]
[0m14:29:00.866016 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:29:00.870111 [debug] [MainThread]: On master: ROLLBACK
[0m14:29:00.872143 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:29:00.874159 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:29:00.875670 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:29:00.876678 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:29:00.877686 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:29:00.880709 [info ] [MainThread]: 
[0m14:29:00.881722 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 1.22 seconds (1.22s).
[0m14:29:00.885776 [debug] [MainThread]: Command end result
[0m14:29:00.898945 [info ] [MainThread]: 
[0m14:29:00.900987 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:29:00.903008 [info ] [MainThread]: 
[0m14:29:00.905039 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m14:29:00.910173 [debug] [MainThread]: Command `cli build` succeeded at 14:29:00.910173 after 1.96 seconds
[0m14:29:00.912196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F75F990590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F75FCAC8C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F75FCADD90>]}
[0m14:29:00.914214 [debug] [MainThread]: Flushing usage events
[0m14:29:34.618677 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A66420170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A6680F4A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A6680C830>]}


============================== 14:29:34.621714 | 0a298cd3-44c0-49a5-b964-5c396ce783e8 ==============================
[0m14:29:34.621714 [info ] [MainThread]: Running with dbt=1.7.14
[0m14:29:34.623731 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:29:34.902764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0a298cd3-44c0-49a5-b964-5c396ce783e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A66A4F530>]}
[0m14:29:35.024339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0a298cd3-44c0-49a5-b964-5c396ce783e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A668FF8C0>]}
[0m14:29:35.027394 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:29:35.057887 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m14:29:35.156641 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:29:35.157651 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:29:35.176943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0a298cd3-44c0-49a5-b964-5c396ce783e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A66A4FE60>]}
[0m14:29:35.192608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0a298cd3-44c0-49a5-b964-5c396ce783e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A66BB2B40>]}
[0m14:29:35.195185 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m14:29:35.202298 [info ] [MainThread]: 
[0m14:29:35.204827 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:29:35.210448 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:29:35.258005 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:29:35.325836 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0a298cd3-44c0-49a5-b964-5c396ce783e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A66BB0E00>]}
[0m14:29:35.328904 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:29:35.330940 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:29:35.333983 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:29:35.336005 [info ] [MainThread]: 
[0m14:29:35.343077 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m14:29:35.345098 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:29:35.348163 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:29:35.350185 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:29:35.387644 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:29:35.389661 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:29:35.351202 => 14:29:35.389661
[0m14:29:35.391693 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:29:35.538737 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:29:35.539749 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:29:35.541768 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:29:35.573205 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:29:35.620542 [debug] [Thread-6 (]: SQL status: OK in 0.05000000074505806 seconds
[0m14:29:35.704889 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:29:35.706393 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:29:35.708411 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:29:35.718507 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:29:35.754924 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:29:35.392701 => 14:29:35.753917
[0m14:29:35.757438 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0a298cd3-44c0-49a5-b964-5c396ce783e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A66D7AAE0>]}
[0m14:29:35.758458 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.41s]
[0m14:29:35.761492 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:29:35.763538 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m14:29:35.766573 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m14:29:35.769609 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m14:29:35.770619 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m14:29:35.773665 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 14:29:35.772639 => 14:29:35.772639
[0m14:29:35.775688 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m14:29:35.908261 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:29:35.909269 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m14:29:35.914362 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:29:35.951852 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m14:29:35.964070 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:29:35.967118 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m14:29:35.979314 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:29:35.992439 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m14:29:36.009169 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:29:36.012189 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 14:29:35.776701 => 14:29:36.011183
[0m14:29:36.014211 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0a298cd3-44c0-49a5-b964-5c396ce783e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A66D797F0>]}
[0m14:29:36.016228 [info ] [Thread-6 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.25s]
[0m14:29:36.018260 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m14:29:36.019270 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:29:36.021287 [info ] [Thread-6 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m14:29:36.023303 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m14:29:36.024315 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:29:36.060928 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:29:36.064053 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 14:29:36.025326 => 14:29:36.064053
[0m14:29:36.066168 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:29:36.107310 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:29:36.110345 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:29:36.111356 [debug] [Thread-6 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:29:36.117411 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:29:36.123455 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 14:29:36.067186 => 14:29:36.122447
[0m14:29:36.125506 [info ] [Thread-6 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.10s]
[0m14:29:36.127529 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:29:36.129554 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:29:36.131588 [info ] [Thread-6 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m14:29:36.134628 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m14:29:36.135636 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:29:36.154902 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:29:36.156940 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 14:29:36.136646 => 14:29:36.155927
[0m14:29:36.157953 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:29:36.168135 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:29:36.171214 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:29:36.173257 [debug] [Thread-6 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:29:36.177295 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:29:36.183416 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 14:29:36.158961 => 14:29:36.182406
[0m14:29:36.185434 [info ] [Thread-6 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.05s]
[0m14:29:36.187464 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:29:36.189480 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m14:29:36.190488 [info ] [Thread-6 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:29:36.192505 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m14:29:36.194525 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:29:36.206245 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:29:36.208266 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:29:36.195536 => 14:29:36.207253
[0m14:29:36.209275 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:29:36.252941 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:29:36.254958 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:29:36.256476 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:29:36.261014 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:29:36.268138 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:29:36.210285 => 14:29:36.267122
[0m14:29:36.270203 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0a298cd3-44c0-49a5-b964-5c396ce783e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A6806B8C0>]}
[0m14:29:36.272220 [info ] [Thread-6 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.08s]
[0m14:29:36.274237 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:29:36.275245 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:29:36.277263 [info ] [Thread-6 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m14:29:36.279298 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m14:29:36.280308 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:29:36.294462 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:29:36.296493 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 14:29:36.281321 => 14:29:36.296493
[0m14:29:36.298520 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:29:36.308124 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:29:36.310170 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:29:36.311180 [debug] [Thread-6 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:29:36.316231 [debug] [Thread-6 (]: SQL status: OK in 0.0 seconds
[0m14:29:36.321280 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 14:29:36.299536 => 14:29:36.320270
[0m14:29:36.323308 [info ] [Thread-6 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.05s]
[0m14:29:36.325348 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:29:36.327397 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:29:36.329417 [info ] [Thread-6 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m14:29:36.332468 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m14:29:36.334501 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:29:36.347670 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:29:36.349687 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 14:29:36.335518 => 14:29:36.348679
[0m14:29:36.350696 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:29:36.359839 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:29:36.361860 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:29:36.363894 [debug] [Thread-6 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:29:36.369992 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:29:36.376075 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 14:29:36.351708 => 14:29:36.375061
[0m14:29:36.378093 [info ] [Thread-6 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.05s]
[0m14:29:36.380116 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:29:36.384155 [debug] [MainThread]: On master: ROLLBACK
[0m14:29:36.385193 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:29:36.387223 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:29:36.388234 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:29:36.389244 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:29:36.391266 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:29:36.394301 [info ] [MainThread]: 
[0m14:29:36.397347 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 1.19 seconds (1.19s).
[0m14:29:36.402450 [debug] [MainThread]: Command end result
[0m14:29:36.418228 [info ] [MainThread]: 
[0m14:29:36.420243 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:29:36.421250 [info ] [MainThread]: 
[0m14:29:36.423269 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m14:29:36.426309 [debug] [MainThread]: Command `cli build` succeeded at 14:29:36.426309 after 1.91 seconds
[0m14:29:36.428335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A669294C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A6680C830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019A68069E50>]}
[0m14:29:36.430360 [debug] [MainThread]: Flushing usage events
[0m14:30:32.044374 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B412B9A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B412B9C40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B412B9760>]}


============================== 14:30:32.046911 | 96cb6f61-c850-4a21-89ef-a41f74f5a2f1 ==============================
[0m14:30:32.046911 [info ] [MainThread]: Running with dbt=1.7.14
[0m14:30:32.048476 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:30:32.439037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '96cb6f61-c850-4a21-89ef-a41f74f5a2f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B413523F0>]}
[0m14:30:32.568701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '96cb6f61-c850-4a21-89ef-a41f74f5a2f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B4119EAB0>]}
[0m14:30:32.573850 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:30:32.610700 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m14:30:32.720746 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m14:30:32.722270 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m14:30:32.739699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '96cb6f61-c850-4a21-89ef-a41f74f5a2f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B4128EA50>]}
[0m14:30:32.754966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '96cb6f61-c850-4a21-89ef-a41f74f5a2f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B414F5A60>]}
[0m14:30:32.756514 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m14:30:32.762597 [info ] [MainThread]: 
[0m14:30:32.765659 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:30:32.770750 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:30:32.824504 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m14:30:32.899646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '96cb6f61-c850-4a21-89ef-a41f74f5a2f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B4154D010>]}
[0m14:30:32.902719 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:30:32.904984 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:30:32.908799 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:30:32.911466 [info ] [MainThread]: 
[0m14:30:32.920133 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m14:30:32.922676 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m14:30:32.926785 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:30:32.928317 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:30:32.965681 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:30:32.968735 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:30:32.929339 => 14:30:32.967716
[0m14:30:32.970283 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:30:33.171241 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:30:33.173823 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:30:33.175884 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:30:33.177955 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m14:30:33.258452 [debug] [Thread-6 (]: SQL status: OK in 0.07999999821186066 seconds
[0m14:30:33.372190 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:30:33.376387 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:30:33.377948 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:30:33.404263 [debug] [Thread-6 (]: SQL status: OK in 0.019999999552965164 seconds
[0m14:30:33.476244 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:30:32.971309 => 14:30:33.475220
[0m14:30:33.481348 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '96cb6f61-c850-4a21-89ef-a41f74f5a2f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B4170E870>]}
[0m14:30:33.486476 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.55s]
[0m14:30:33.490723 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:30:33.493803 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m14:30:33.497952 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m14:30:33.502104 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m14:30:33.505389 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m14:30:33.509617 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 14:30:33.507468 => 14:30:33.508607
[0m14:30:33.512762 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m14:30:33.714206 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:30:33.716809 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m14:30:33.724729 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:30:33.798204 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m14:30:33.822731 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:30:33.824803 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m14:30:33.858568 [debug] [Thread-6 (]: SQL status: OK in 0.029999999329447746 seconds
[0m14:30:33.943614 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m14:30:34.019398 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:30:34.031180 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 14:30:33.514905 => 14:30:34.029100
[0m14:30:34.042256 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '96cb6f61-c850-4a21-89ef-a41f74f5a2f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B4170C5F0>]}
[0m14:30:34.050641 [info ] [Thread-6 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.54s]
[0m14:30:34.058506 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m14:30:34.064275 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:30:34.070014 [info ] [Thread-6 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m14:30:34.078419 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m14:30:34.082592 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:30:34.143748 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:30:34.149604 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 14:30:34.086234 => 14:30:34.148030
[0m14:30:34.151701 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:30:34.203636 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:30:34.207267 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:30:34.209461 [debug] [Thread-6 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:30:34.216823 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:30:34.230306 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 14:30:34.153293 => 14:30:34.229265
[0m14:30:34.233880 [info ] [Thread-6 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.16s]
[0m14:30:34.243352 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:30:34.249147 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:30:34.254525 [info ] [Thread-6 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m14:30:34.263204 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m14:30:34.269564 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:30:34.314320 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:30:34.324409 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 14:30:34.273776 => 14:30:34.320198
[0m14:30:34.328049 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:30:34.356006 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:30:34.365673 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:30:34.369312 [debug] [Thread-6 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:30:34.385151 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:30:34.399983 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 14:30:34.330789 => 14:30:34.396772
[0m14:30:34.405629 [info ] [Thread-6 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.15s]
[0m14:30:34.410804 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:30:34.414916 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m14:30:34.417988 [info ] [Thread-6 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m14:30:34.424319 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m14:30:34.427965 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:30:34.448852 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:30:34.453083 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:30:34.430039 => 14:30:34.450895
[0m14:30:34.458330 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:30:34.539737 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:30:34.546389 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:30:34.549089 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m14:30:34.558982 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:30:34.570950 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:30:34.460404 => 14:30:34.569901
[0m14:30:34.577853 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '96cb6f61-c850-4a21-89ef-a41f74f5a2f1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B419297C0>]}
[0m14:30:34.583655 [info ] [Thread-6 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.15s]
[0m14:30:34.591547 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:30:34.598593 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:30:34.602829 [info ] [Thread-6 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m14:30:34.609235 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m14:30:34.613462 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:30:34.642152 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:30:34.648972 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 14:30:34.615552 => 14:30:34.646888
[0m14:30:34.653252 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:30:34.668038 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:30:34.671754 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:30:34.674395 [debug] [Thread-6 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:30:34.682823 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:30:34.691483 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 14:30:34.655299 => 14:30:34.690345
[0m14:30:34.696268 [info ] [Thread-6 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.09s]
[0m14:30:34.701781 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:30:34.707019 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:30:34.711178 [info ] [Thread-6 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m14:30:34.717505 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m14:30:34.721679 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:30:34.750366 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:30:34.759894 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 14:30:34.725484 => 14:30:34.756669
[0m14:30:34.764601 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:30:34.779333 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:30:34.784093 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:30:34.787727 [debug] [Thread-6 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:30:34.797793 [debug] [Thread-6 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:30:34.811450 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 14:30:34.767274 => 14:30:34.809361
[0m14:30:34.817705 [info ] [Thread-6 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.10s]
[0m14:30:34.824187 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:30:34.834779 [debug] [MainThread]: On master: ROLLBACK
[0m14:30:34.837924 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:30:34.840005 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:30:34.842660 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:30:34.844206 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:30:34.845765 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:30:34.850027 [info ] [MainThread]: 
[0m14:30:34.852642 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 2.09 seconds (2.09s).
[0m14:30:34.859001 [debug] [MainThread]: Command end result
[0m14:30:34.888850 [info ] [MainThread]: 
[0m14:30:34.891950 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:30:34.895176 [info ] [MainThread]: 
[0m14:30:34.897758 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m14:30:34.902536 [debug] [MainThread]: Command `cli build` succeeded at 14:30:34.901515 after 2.95 seconds
[0m14:30:34.905132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B40B673B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B41408110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018B4140B800>]}
[0m14:30:34.908228 [debug] [MainThread]: Flushing usage events
[0m17:50:31.707067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D180B8C80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D180B98E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D180B8800>]}


============================== 17:50:31.708075 | 33c04d3a-10fd-4617-8477-b03c12fcb662 ==============================
[0m17:50:31.708075 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:50:31.709084 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:50:31.797276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '33c04d3a-10fd-4617-8477-b03c12fcb662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D180450A0>]}
[0m17:50:31.834815 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '33c04d3a-10fd-4617-8477-b03c12fcb662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D1915C7A0>]}
[0m17:50:31.835834 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:50:31.852036 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:50:32.638197 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:50:32.639204 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:50:32.642229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '33c04d3a-10fd-4617-8477-b03c12fcb662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D191C9430>]}
[0m17:50:32.658447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '33c04d3a-10fd-4617-8477-b03c12fcb662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D1924E390>]}
[0m17:50:32.659457 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m17:50:32.660467 [info ] [MainThread]: 
[0m17:50:32.661495 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:50:32.662506 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:50:32.671586 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m17:50:32.691833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '33c04d3a-10fd-4617-8477-b03c12fcb662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D18027BF0>]}
[0m17:50:32.692842 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:50:32.692842 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:50:32.693848 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:50:32.693848 [info ] [MainThread]: 
[0m17:50:32.695862 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:50:32.695862 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m17:50:32.696870 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:50:32.696870 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:50:32.701929 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:50:32.702936 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:50:32.697884 => 17:50:32.702936
[0m17:50:32.702936 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:50:32.733318 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:50:32.734330 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:50:32.734330 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:50:32.735339 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:50:32.763136 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m17:50:32.788404 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:50:32.789417 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:50:32.790442 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:50:32.797512 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:50:32.806121 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:50:32.702936 => 17:50:32.806121
[0m17:50:32.807128 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '33c04d3a-10fd-4617-8477-b03c12fcb662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D1926EE70>]}
[0m17:50:32.807128 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.11s]
[0m17:50:32.808135 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:50:32.808135 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m17:50:32.809149 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m17:50:32.809149 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:50:32.810162 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m17:50:32.810162 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 17:50:32.810162 => 17:50:32.810162
[0m17:50:32.810162 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m17:50:32.837501 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:50:32.838513 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m17:50:32.839525 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:50:32.847613 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m17:50:32.850642 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:50:32.850642 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:50:32.858807 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:50:32.862851 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:50:32.866887 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:50:32.867896 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 17:50:32.810162 => 17:50:32.867896
[0m17:50:32.867896 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '33c04d3a-10fd-4617-8477-b03c12fcb662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D1926CD70>]}
[0m17:50:32.868902 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.06s]
[0m17:50:32.868902 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m17:50:32.869911 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:50:32.869911 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:50:32.869911 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:50:32.870924 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:50:32.878007 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:50:32.879014 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:50:32.870924 => 17:50:32.878007
[0m17:50:32.879014 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:50:32.889202 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:50:32.891226 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:50:32.891226 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:50:32.892240 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:50:32.894259 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:50:32.879014 => 17:50:32.894259
[0m17:50:32.894259 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.02s]
[0m17:50:32.895268 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:50:32.895268 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:50:32.895268 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:50:32.896277 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:50:32.896277 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:50:32.900315 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:50:32.901325 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:50:32.896277 => 17:50:32.901325
[0m17:50:32.901325 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:50:32.903346 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:50:32.904435 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:50:32.905374 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:50:32.906896 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:50:32.907909 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:50:32.902334 => 17:50:32.907909
[0m17:50:32.908915 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.01s]
[0m17:50:32.909936 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:50:32.910945 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:50:32.910945 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m17:50:32.911955 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m17:50:32.911955 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:50:32.914997 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:50:32.916011 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:50:32.912966 => 17:50:32.916011
[0m17:50:32.916011 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:50:32.927134 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:50:32.929156 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:50:32.929156 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m17:50:32.931198 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:50:32.933240 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:50:32.917021 => 17:50:32.933240
[0m17:50:32.934254 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '33c04d3a-10fd-4617-8477-b03c12fcb662', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D19663D40>]}
[0m17:50:32.935270 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m17:50:32.936322 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:50:32.937336 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:50:32.937336 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m17:50:32.938347 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m17:50:32.938347 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:50:32.946503 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:50:32.948553 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 17:50:32.938347 => 17:50:32.947528
[0m17:50:32.948553 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:50:32.952748 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:50:32.953769 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:50:32.954795 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:50:32.957850 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:50:32.959884 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 17:50:32.949570 => 17:50:32.959884
[0m17:50:32.960910 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.02s]
[0m17:50:32.962941 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:50:32.962941 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:50:32.963967 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m17:50:32.964984 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m17:50:32.964984 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:50:32.970118 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:50:32.971128 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 17:50:32.964984 => 17:50:32.971128
[0m17:50:32.971128 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:50:32.973145 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:50:32.973145 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:50:32.974159 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:50:32.976184 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:50:32.977205 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 17:50:32.971128 => 17:50:32.977205
[0m17:50:32.978225 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m17:50:32.979232 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:50:32.981318 [debug] [MainThread]: On master: ROLLBACK
[0m17:50:32.981318 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:50:32.982321 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:50:32.982321 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:50:32.983350 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:50:32.983350 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:50:32.984374 [info ] [MainThread]: 
[0m17:50:32.984374 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.32 seconds (0.32s).
[0m17:50:32.985395 [debug] [MainThread]: Command end result
[0m17:50:32.994008 [info ] [MainThread]: 
[0m17:50:32.994514 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:50:32.995025 [info ] [MainThread]: 
[0m17:50:32.995536 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m17:50:32.996644 [debug] [MainThread]: Command `cli build` succeeded at 17:50:32.996644 after 1.33 seconds
[0m17:50:32.997147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D19665460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D193C7E60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024D18083E00>]}
[0m17:50:32.997654 [debug] [MainThread]: Flushing usage events
[0m17:51:17.566985 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CBE5C470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CBE5D910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CBE5C290>]}


============================== 17:51:17.567995 | 21422562-4c46-497b-acda-afc05fd79abd ==============================
[0m17:51:17.567995 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:51:17.567995 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:51:17.652539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '21422562-4c46-497b-acda-afc05fd79abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CBDE4E00>]}
[0m17:51:17.688971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '21422562-4c46-497b-acda-afc05fd79abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CBE757C0>]}
[0m17:51:17.691005 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:51:17.698077 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:51:17.738594 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:51:17.739602 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:51:17.742643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '21422562-4c46-497b-acda-afc05fd79abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CBEC4710>]}
[0m17:51:17.748721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '21422562-4c46-497b-acda-afc05fd79abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CC01D670>]}
[0m17:51:17.749731 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m17:51:17.750744 [info ] [MainThread]: 
[0m17:51:17.751755 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:51:17.753778 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:51:17.758868 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m17:51:17.771975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '21422562-4c46-497b-acda-afc05fd79abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CBE76720>]}
[0m17:51:17.772984 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:51:17.772984 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:51:17.774022 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:51:17.774022 [info ] [MainThread]: 
[0m17:51:17.777053 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:51:17.777053 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m17:51:17.778063 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:51:17.778063 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:51:17.783118 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:51:17.784126 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:51:17.778063 => 17:51:17.783118
[0m17:51:17.784126 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:51:17.820842 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:51:17.821851 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:51:17.822859 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:51:17.823868 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:51:17.863304 [debug] [Thread-1 (]: SQL status: OK in 0.03999999910593033 seconds
[0m17:51:17.890769 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:51:17.892825 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:51:17.892825 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:51:17.913574 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m17:51:17.926404 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:51:17.784126 => 17:51:17.926404
[0m17:51:17.929121 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '21422562-4c46-497b-acda-afc05fd79abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CBF9A150>]}
[0m17:51:17.929631 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.15s]
[0m17:51:17.930139 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:51:17.930651 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m17:51:17.931687 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m17:51:17.932709 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:51:17.933218 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m17:51:17.933733 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 17:51:17.933218 => 17:51:17.933218
[0m17:51:17.934244 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m17:51:17.995058 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:51:17.997144 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m17:51:18.001858 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:51:18.017328 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m17:51:18.022542 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:51:18.023056 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:51:18.041528 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m17:51:18.047730 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:51:18.055106 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:51:18.056134 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 17:51:17.934244 => 17:51:18.056134
[0m17:51:18.056644 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '21422562-4c46-497b-acda-afc05fd79abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CBFC7AA0>]}
[0m17:51:18.057153 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.12s]
[0m17:51:18.058170 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m17:51:18.058682 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:51:18.059201 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:51:18.060760 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:51:18.061779 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:51:18.074748 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:51:18.076830 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:51:18.061779 => 17:51:18.076830
[0m17:51:18.077342 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:51:18.088178 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:51:18.090269 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:51:18.090785 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:51:18.092326 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:51:18.094943 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:51:18.077865 => 17:51:18.094943
[0m17:51:18.096524 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.04s]
[0m17:51:18.097545 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:51:18.098058 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:51:18.098571 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:51:18.099592 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:51:18.100105 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:51:18.106237 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:51:18.108293 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:51:18.100105 => 17:51:18.108293
[0m17:51:18.109326 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:51:18.113505 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:51:18.115570 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:51:18.116084 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:51:18.118145 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:51:18.119681 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:51:18.109844 => 17:51:18.119173
[0m17:51:18.120191 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m17:51:18.121216 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:51:18.121729 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:51:18.122241 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m17:51:18.123265 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m17:51:18.123780 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:51:18.127503 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:51:18.129608 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:51:18.123780 => 17:51:18.129096
[0m17:51:18.130119 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:51:18.146067 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:51:18.149135 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:51:18.149641 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m17:51:18.151698 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:51:18.154252 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:51:18.130630 => 17:51:18.154252
[0m17:51:18.155272 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '21422562-4c46-497b-acda-afc05fd79abd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CD45B7D0>]}
[0m17:51:18.156330 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m17:51:18.158002 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:51:18.160086 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:51:18.161115 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m17:51:18.162644 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m17:51:18.163157 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:51:18.173838 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:51:18.175358 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 17:51:18.163673 => 17:51:18.174849
[0m17:51:18.175358 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:51:18.179580 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:51:18.181133 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:51:18.181644 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:51:18.184218 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:51:18.186784 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 17:51:18.175871 => 17:51:18.186274
[0m17:51:18.187802 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.03s]
[0m17:51:18.189408 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:51:18.189918 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:51:18.190426 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m17:51:18.191446 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m17:51:18.191957 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:51:18.198625 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:51:18.200159 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 17:51:18.192470 => 17:51:18.199649
[0m17:51:18.200671 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:51:18.203827 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:51:18.205353 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:51:18.205883 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:51:18.209597 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:51:18.211784 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 17:51:18.200671 => 17:51:18.211268
[0m17:51:18.212827 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.02s]
[0m17:51:18.213864 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:51:18.216452 [debug] [MainThread]: On master: ROLLBACK
[0m17:51:18.216978 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:51:18.217487 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:51:18.217995 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:51:18.218506 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:51:18.218506 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:51:18.219586 [info ] [MainThread]: 
[0m17:51:18.220094 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.47 seconds (0.47s).
[0m17:51:18.221111 [debug] [MainThread]: Command end result
[0m17:51:18.233415 [info ] [MainThread]: 
[0m17:51:18.234435 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:51:18.234970 [info ] [MainThread]: 
[0m17:51:18.235481 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m17:51:18.236506 [debug] [MainThread]: Command `cli build` succeeded at 17:51:18.236506 after 0.70 seconds
[0m17:51:18.237016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CB555160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CBD4EA80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CBD4FFB0>]}
[0m17:51:18.237527 [debug] [MainThread]: Flushing usage events
[0m17:51:50.753272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F253689E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F25368BF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F25368920>]}


============================== 17:51:50.753272 | 283dbd25-dbe0-4d0b-960a-6dd9593fdcf4 ==============================
[0m17:51:50.753272 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:51:50.754280 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m17:51:50.840004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '283dbd25-dbe0-4d0b-960a-6dd9593fdcf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F24F96420>]}
[0m17:51:50.875894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '283dbd25-dbe0-4d0b-960a-6dd9593fdcf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F24CEA0C0>]}
[0m17:51:50.876901 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:51:50.895249 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:51:51.732733 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m17:51:51.733785 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m17:51:51.737823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '283dbd25-dbe0-4d0b-960a-6dd9593fdcf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F25387F20>]}
[0m17:51:51.748934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '283dbd25-dbe0-4d0b-960a-6dd9593fdcf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F264F9E50>]}
[0m17:51:51.748934 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m17:51:51.750962 [info ] [MainThread]: 
[0m17:51:51.751975 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:51:51.752989 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:51:51.764160 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m17:51:51.783355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '283dbd25-dbe0-4d0b-960a-6dd9593fdcf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F264FA450>]}
[0m17:51:51.784363 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:51:51.784363 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:51:51.785382 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:51:51.785382 [info ] [MainThread]: 
[0m17:51:51.788424 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:51:51.789434 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m17:51:51.790446 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:51:51.790446 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:51:51.795641 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:51:51.796648 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:51:51.790446 => 17:51:51.795641
[0m17:51:51.796648 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:51:51.826068 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:51:51.827075 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:51:51.827075 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:51:51.827075 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:51:51.859438 [debug] [Thread-1 (]: SQL status: OK in 0.029999999329447746 seconds
[0m17:51:51.885800 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:51:51.886813 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:51:51.887828 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:51:51.894919 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:51:51.904499 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:51:51.796648 => 17:51:51.904499
[0m17:51:51.905506 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '283dbd25-dbe0-4d0b-960a-6dd9593fdcf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F264D02F0>]}
[0m17:51:51.905506 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.12s]
[0m17:51:51.906513 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:51:51.906513 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m17:51:51.907521 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m17:51:51.907521 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:51:51.907521 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m17:51:51.908532 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 17:51:51.908532 => 17:51:51.908532
[0m17:51:51.908532 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m17:51:51.935873 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:51:51.935873 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m17:51:51.936880 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:51:51.947008 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m17:51:51.951177 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:51:51.952180 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:51:51.964365 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:51:51.967896 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:51:51.971935 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:51:51.972943 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 17:51:51.908532 => 17:51:51.972943
[0m17:51:51.973954 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '283dbd25-dbe0-4d0b-960a-6dd9593fdcf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F2653C260>]}
[0m17:51:51.973954 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.07s]
[0m17:51:51.974966 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m17:51:51.974966 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:51:51.974966 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:51:51.975977 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:51:51.975977 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:51:51.983088 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:51:51.984107 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:51:51.975977 => 17:51:51.984107
[0m17:51:51.985122 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:51:51.994298 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:51:51.995314 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:51:51.995314 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:51:51.996330 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:51:51.998359 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:51:51.985122 => 17:51:51.997346
[0m17:51:51.998359 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.02s]
[0m17:51:51.999381 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:51:51.999381 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:51:52.000396 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:51:52.000396 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:51:52.001410 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:51:52.005470 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:51:52.006485 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:51:52.001410 => 17:51:52.005470
[0m17:51:52.006485 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:51:52.007500 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:51:52.008540 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:51:52.008540 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:51:52.009550 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:51:52.010559 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:51:52.006485 => 17:51:52.010559
[0m17:51:52.011623 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.01s]
[0m17:51:52.011623 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:51:52.012647 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:51:52.012647 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m17:51:52.013660 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m17:51:52.013660 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:51:52.015740 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:51:52.015740 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:51:52.013660 => 17:51:52.015740
[0m17:51:52.016756 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:51:52.027919 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:51:52.028930 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:51:52.029941 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m17:51:52.031979 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:51:52.034019 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:51:52.016756 => 17:51:52.034019
[0m17:51:52.035037 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '283dbd25-dbe0-4d0b-960a-6dd9593fdcf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F2667E2A0>]}
[0m17:51:52.035037 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m17:51:52.036051 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:51:52.037068 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:51:52.037068 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m17:51:52.038082 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m17:51:52.038082 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:51:52.043176 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:51:52.045203 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 17:51:52.038082 => 17:51:52.044190
[0m17:51:52.045203 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:51:52.047230 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:51:52.048241 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:51:52.048241 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:51:52.050268 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:51:52.052301 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 17:51:52.045203 => 17:51:52.052301
[0m17:51:52.053380 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.02s]
[0m17:51:52.054383 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:51:52.054383 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:51:52.055396 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m17:51:52.055396 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m17:51:52.056404 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:51:52.059434 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:51:52.060442 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 17:51:52.056404 => 17:51:52.060442
[0m17:51:52.061453 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:51:52.063478 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:51:52.064488 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:51:52.064488 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:51:52.066521 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:51:52.068030 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 17:51:52.061453 => 17:51:52.068030
[0m17:51:52.069052 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m17:51:52.070103 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:51:52.071118 [debug] [MainThread]: On master: ROLLBACK
[0m17:51:52.071118 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:51:52.072131 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:51:52.072131 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:51:52.072131 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:51:52.073141 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:51:52.073141 [info ] [MainThread]: 
[0m17:51:52.074149 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.32 seconds (0.32s).
[0m17:51:52.075158 [debug] [MainThread]: Command end result
[0m17:51:52.085340 [info ] [MainThread]: 
[0m17:51:52.086407 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:51:52.087409 [info ] [MainThread]: 
[0m17:51:52.087409 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m17:51:52.089434 [debug] [MainThread]: Command `cli build` succeeded at 17:51:52.089434 after 1.38 seconds
[0m17:51:52.089434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F24BD2AB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F24FEC0E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025F2682A270>]}
[0m17:51:52.090446 [debug] [MainThread]: Flushing usage events
[0m18:11:20.471359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189F2A592B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189F2A596D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189F2A59220>]}


============================== 18:11:20.471359 | e707894e-6064-4afa-81e3-a962616cb330 ==============================
[0m18:11:20.471359 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:11:20.472373 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:11:20.558519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e707894e-6064-4afa-81e3-a962616cb330', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189F2206630>]}
[0m18:11:20.593916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e707894e-6064-4afa-81e3-a962616cb330', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189F2A59520>]}
[0m18:11:20.594924 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:11:20.606055 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:11:21.533144 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:11:21.534152 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:11:21.538182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e707894e-6064-4afa-81e3-a962616cb330', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189F2B2EC30>]}
[0m18:11:21.550317 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e707894e-6064-4afa-81e3-a962616cb330', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189F2C19B50>]}
[0m18:11:21.550317 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:11:21.552346 [info ] [MainThread]: 
[0m18:11:21.553368 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:11:21.554379 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:11:21.566558 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:11:21.587794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e707894e-6064-4afa-81e3-a962616cb330', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189F2977BF0>]}
[0m18:11:21.588813 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:11:21.588813 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:11:21.589828 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:11:21.589828 [info ] [MainThread]: 
[0m18:11:21.592873 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:11:21.592873 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:11:21.593897 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:11:21.593897 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:11:21.597946 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:11:21.598958 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:11:21.593897 => 18:11:21.598958
[0m18:11:21.598958 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:11:21.628353 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:11:21.629363 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:11:21.629363 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:11:21.629363 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:11:21.630377 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:11:21.630377 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Generate_Model_Notebook() missing 1 required positional argument: 'sql'
[0m18:11:21.631391 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:11:21.598958 => 18:11:21.631391
[0m18:11:21.702819 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Generate_Model_Notebook() missing 1 required positional argument: 'sql'
[0m18:11:21.703836 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e707894e-6064-4afa-81e3-a962616cb330', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189F2B56CC0>]}
[0m18:11:21.703836 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.11s]
[0m18:11:21.704854 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:11:21.704854 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:11:21.705866 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:11:21.706876 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:11:21.706876 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:11:21.706876 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:11:21.706876 => 18:11:21.706876
[0m18:11:21.707888 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:11:21.735241 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:11:21.735241 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:11:21.736248 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:11:21.736248 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Generate_Model_Notebook() missing 1 required positional argument: 'sql'
[0m18:11:21.736248 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:11:21.707888 => 18:11:21.736248
[0m18:11:21.739273 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Generate_Model_Notebook() missing 1 required positional argument: 'sql'
[0m18:11:21.739273 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e707894e-6064-4afa-81e3-a962616cb330', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189F3D7EA80>]}
[0m18:11:21.740281 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.03s]
[0m18:11:21.740281 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:11:21.741288 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:11:21.741288 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m18:11:21.742301 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:11:21.742301 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:11:21.743314 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m18:11:21.743314 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:11:21.744326 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:11:21.744326 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m18:11:21.745339 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:11:21.745339 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:11:21.746389 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m18:11:21.746389 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:11:21.746389 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:11:21.747404 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m18:11:21.747404 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:11:21.748415 [debug] [MainThread]: On master: ROLLBACK
[0m18:11:21.749425 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:11:21.749425 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:11:21.749425 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:11:21.750436 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:11:21.750436 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:11:21.751459 [info ] [MainThread]: 
[0m18:11:21.751459 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m18:11:21.752468 [debug] [MainThread]: Command end result
[0m18:11:21.758544 [info ] [MainThread]: 
[0m18:11:21.759554 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m18:11:21.759554 [info ] [MainThread]: 
[0m18:11:21.760691 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Generate_Model_Notebook() missing 1 required positional argument: 'sql'
[0m18:11:21.761695 [info ] [MainThread]: 
[0m18:11:21.761695 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Generate_Model_Notebook() missing 1 required positional argument: 'sql'
[0m18:11:21.761695 [info ] [MainThread]: 
[0m18:11:21.762708 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m18:11:21.763723 [debug] [MainThread]: Command `cli build` failed at 18:11:21.763723 after 1.34 seconds
[0m18:11:21.764746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189F2B567E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189F2B54CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189F2AD8A40>]}
[0m18:11:21.764746 [debug] [MainThread]: Flushing usage events
[0m18:33:52.344861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BB5B44CF50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BB5B44D400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BB5B44CDD0>]}


============================== 18:33:52.345867 | 3d488eca-ff2c-4b96-9ef5-f4f8295f3090 ==============================
[0m18:33:52.345867 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:33:52.345867 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:33:52.436342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3d488eca-ff2c-4b96-9ef5-f4f8295f3090', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BB5B367E00>]}
[0m18:33:52.471785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3d488eca-ff2c-4b96-9ef5-f4f8295f3090', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BB5B45E5A0>]}
[0m18:33:52.472799 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:33:52.489004 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:33:53.174188 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:33:53.174188 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:33:53.178226 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3d488eca-ff2c-4b96-9ef5-f4f8295f3090', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BB5B4B50D0>]}
[0m18:33:53.193454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3d488eca-ff2c-4b96-9ef5-f4f8295f3090', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BB5C636A20>]}
[0m18:33:53.193454 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:33:53.195474 [info ] [MainThread]: 
[0m18:33:53.196487 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:33:53.196487 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:33:53.206103 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:33:53.225395 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d488eca-ff2c-4b96-9ef5-f4f8295f3090', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BB5B45C470>]}
[0m18:33:53.226405 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:33:53.226405 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:33:53.227414 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:33:53.227414 [info ] [MainThread]: 
[0m18:33:53.229440 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:33:53.230454 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:33:53.231467 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:33:53.231467 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:33:53.236531 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:33:53.237540 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:33:53.231467 => 18:33:53.237540
[0m18:33:53.238553 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:33:53.279174 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:33:53.280188 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:33:53.280188 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:33:53.281202 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:33:53.293474 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:33:53.293474 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:33:53.294478 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:33:53.238553 => 18:33:53.293474
[0m18:33:53.421759 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:33:53.422775 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d488eca-ff2c-4b96-9ef5-f4f8295f3090', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BB5C5ADE20>]}
[0m18:33:53.423790 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.19s]
[0m18:33:53.424797 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:33:53.424797 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:33:53.425812 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:33:53.426824 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:33:53.426824 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:33:53.426824 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:33:53.426824 => 18:33:53.426824
[0m18:33:53.427839 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:33:53.463407 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:33:53.463407 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:33:53.465427 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:33:53.465427 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:33:53.466438 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:33:53.427839 => 18:33:53.465427
[0m18:33:53.468953 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:33:53.468953 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d488eca-ff2c-4b96-9ef5-f4f8295f3090', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BB5C7658E0>]}
[0m18:33:53.469967 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.04s]
[0m18:33:53.469967 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:33:53.470978 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:33:53.470978 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m18:33:53.471987 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:33:53.473002 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:33:53.473002 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m18:33:53.474013 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:33:53.475061 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:33:53.475061 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m18:33:53.476080 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:33:53.476080 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:33:53.476080 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m18:33:53.477182 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:33:53.477182 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:33:53.478185 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m18:33:53.478185 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:33:53.480221 [debug] [MainThread]: On master: ROLLBACK
[0m18:33:53.480221 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:33:53.480221 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:33:53.481230 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:33:53.481230 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:33:53.481230 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:33:53.482242 [info ] [MainThread]: 
[0m18:33:53.482242 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m18:33:53.484259 [debug] [MainThread]: Command end result
[0m18:33:53.491447 [info ] [MainThread]: 
[0m18:33:53.491447 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m18:33:53.492461 [info ] [MainThread]: 
[0m18:33:53.492461 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:33:53.493486 [info ] [MainThread]: 
[0m18:33:53.493486 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:33:53.493486 [info ] [MainThread]: 
[0m18:33:53.494499 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m18:33:53.495514 [debug] [MainThread]: Command `cli build` failed at 18:33:53.495514 after 1.20 seconds
[0m18:33:53.496532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BB5C784350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BB5C5DDD30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BB5AD8A3F0>]}
[0m18:33:53.496532 [debug] [MainThread]: Flushing usage events
[0m18:36:37.694126 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAD3B0D1F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAD3B0CB30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAD3A53770>]}


============================== 18:36:37.697170 | 940fe773-cc3b-45c8-a62f-a7a06572f09f ==============================
[0m18:36:37.697170 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:36:37.699221 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m18:36:37.997820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '940fe773-cc3b-45c8-a62f-a7a06572f09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAD3BAB470>]}
[0m18:36:38.119220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '940fe773-cc3b-45c8-a62f-a7a06572f09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAD3A7D070>]}
[0m18:36:38.123272 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:36:38.151580 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:36:38.246202 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:36:38.247228 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:36:38.266441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '940fe773-cc3b-45c8-a62f-a7a06572f09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAD3B989B0>]}
[0m18:36:38.282076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '940fe773-cc3b-45c8-a62f-a7a06572f09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAD3D49E20>]}
[0m18:36:38.284117 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:36:38.291251 [info ] [MainThread]: 
[0m18:36:38.294310 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:36:38.299361 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:36:38.346978 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:36:38.417435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '940fe773-cc3b-45c8-a62f-a7a06572f09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAD3CCDBE0>]}
[0m18:36:38.419464 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:36:38.421494 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:36:38.424529 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:36:38.426554 [info ] [MainThread]: 
[0m18:36:38.433645 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m18:36:38.435668 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:36:38.438706 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:36:38.440728 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:36:38.481359 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:36:38.484396 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:36:38.441738 => 18:36:38.483384
[0m18:36:38.485412 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:36:38.622847 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:36:38.624860 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:36:38.625868 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:36:38.627883 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:36:38.631926 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:36:38.633941 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:36:38.634948 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:36:38.487440 => 18:36:38.634948
[0m18:36:38.646085 [debug] [Thread-6 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:36:38.648101 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '940fe773-cc3b-45c8-a62f-a7a06572f09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAD3F24E60>]}
[0m18:36:38.650117 [error] [Thread-6 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.21s]
[0m18:36:38.652144 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:36:38.655200 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m18:36:38.658250 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:36:38.662319 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:36:38.664345 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m18:36:38.666371 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 18:36:38.666371 => 18:36:38.666371
[0m18:36:38.668391 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m18:36:38.792117 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:36:38.794137 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:36:38.797212 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:36:38.799232 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:36:38.801258 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 18:36:38.669403 => 18:36:38.801258
[0m18:36:38.809338 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:36:38.810347 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '940fe773-cc3b-45c8-a62f-a7a06572f09f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAD402BA40>]}
[0m18:36:38.812392 [error] [Thread-6 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.15s]
[0m18:36:38.814412 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m18:36:38.816438 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:36:38.818476 [info ] [Thread-6 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m18:36:38.822542 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:36:38.824570 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:36:38.826600 [info ] [Thread-6 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m18:36:38.829661 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:36:38.831683 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m18:36:38.832691 [info ] [Thread-6 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m18:36:38.834712 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:36:38.836731 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:36:38.837743 [info ] [Thread-6 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m18:36:38.839778 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:36:38.841813 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:36:38.842848 [info ] [Thread-6 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m18:36:38.844876 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:36:38.849952 [debug] [MainThread]: On master: ROLLBACK
[0m18:36:38.851971 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:36:38.855031 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:36:38.856049 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:36:38.858157 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:36:38.860212 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:36:38.863261 [info ] [MainThread]: 
[0m18:36:38.865320 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.57 seconds (0.57s).
[0m18:36:38.868367 [debug] [MainThread]: Command end result
[0m18:36:38.886252 [info ] [MainThread]: 
[0m18:36:38.889377 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m18:36:38.892429 [info ] [MainThread]: 
[0m18:36:38.894990 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:36:38.897033 [info ] [MainThread]: 
[0m18:36:38.898559 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:36:38.901123 [info ] [MainThread]: 
[0m18:36:38.902655 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m18:36:38.907800 [debug] [MainThread]: Command `cli build` failed at 18:36:38.906784 after 1.31 seconds
[0m18:36:38.909824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAD31F8620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAD3DA5910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001AAD3938D40>]}
[0m18:36:38.911858 [debug] [MainThread]: Flushing usage events
[0m18:37:44.107918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D738DAA330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D73BAF6C00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D73BE6DD60>]}


============================== 18:37:44.108926 | f73741ae-7211-4a81-b6dd-18b87230ed3f ==============================
[0m18:37:44.108926 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:37:44.109946 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:37:44.204810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f73741ae-7211-4a81-b6dd-18b87230ed3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D73AB32960>]}
[0m18:37:44.240691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f73741ae-7211-4a81-b6dd-18b87230ed3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D73BE6DEE0>]}
[0m18:37:44.241701 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:37:44.258973 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:37:45.170786 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:37:45.171794 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:37:45.174819 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f73741ae-7211-4a81-b6dd-18b87230ed3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D73CFA6C30>]}
[0m18:37:45.190010 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f73741ae-7211-4a81-b6dd-18b87230ed3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D73CFFDB80>]}
[0m18:37:45.190010 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:37:45.192050 [info ] [MainThread]: 
[0m18:37:45.192050 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:37:45.193057 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:37:45.201127 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:37:45.222589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f73741ae-7211-4a81-b6dd-18b87230ed3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D73D055AF0>]}
[0m18:37:45.222589 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:37:45.223598 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:37:45.224621 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:37:45.224621 [info ] [MainThread]: 
[0m18:37:45.226651 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:37:45.226651 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:37:45.227660 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:37:45.227660 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:37:45.232703 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:37:45.232703 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:37:45.228673 => 18:37:45.232703
[0m18:37:45.233712 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:37:45.265231 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:37:45.265231 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:37:45.265231 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:37:45.266245 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:37:45.277380 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:37:45.278394 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:37:45.278394 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:37:45.233712 => 18:37:45.278394
[0m18:37:45.440870 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:37:45.441886 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f73741ae-7211-4a81-b6dd-18b87230ed3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D73CFCE120>]}
[0m18:37:45.441886 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.21s]
[0m18:37:45.442900 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:37:45.442900 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:37:45.443912 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:37:45.444931 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:37:45.444931 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:37:45.445945 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:37:45.445945 => 18:37:45.445945
[0m18:37:45.445945 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:37:45.477635 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:37:45.478645 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:37:45.479655 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:37:45.479655 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:37:45.480666 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:37:45.445945 => 18:37:45.480666
[0m18:37:45.483712 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:37:45.484739 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f73741ae-7211-4a81-b6dd-18b87230ed3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D73CFCD970>]}
[0m18:37:45.485780 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.04s]
[0m18:37:45.486794 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:37:45.486794 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:37:45.487813 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m18:37:45.487813 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:37:45.488888 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:37:45.488888 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m18:37:45.489891 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:37:45.489891 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:37:45.490901 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m18:37:45.490901 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:37:45.491913 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:37:45.491913 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m18:37:45.492926 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:37:45.492926 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:37:45.493943 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m18:37:45.493943 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:37:45.494956 [debug] [MainThread]: On master: ROLLBACK
[0m18:37:45.494956 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:37:45.494956 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:37:45.495973 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:37:45.495973 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:37:45.495973 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:37:45.497007 [info ] [MainThread]: 
[0m18:37:45.497007 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.30 seconds (0.30s).
[0m18:37:45.498020 [debug] [MainThread]: Command end result
[0m18:37:45.504087 [info ] [MainThread]: 
[0m18:37:45.505102 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m18:37:45.505102 [info ] [MainThread]: 
[0m18:37:45.505102 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:37:45.506116 [info ] [MainThread]: 
[0m18:37:45.506116 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:37:45.507134 [info ] [MainThread]: 
[0m18:37:45.507134 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m18:37:45.508145 [debug] [MainThread]: Command `cli build` failed at 18:37:45.508145 after 1.47 seconds
[0m18:37:45.508145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D738680560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D738DAA330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D73B43E630>]}
[0m18:37:45.509156 [debug] [MainThread]: Flushing usage events
[0m18:38:12.683288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014129F1E630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014129E34F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014129F1C800>]}


============================== 18:38:12.686356 | ddcb47da-a2ad-466e-a05d-812ad6406028 ==============================
[0m18:38:12.686356 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:38:12.689439 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m18:38:13.022133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ddcb47da-a2ad-466e-a05d-812ad6406028', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014129F1C800>]}
[0m18:38:13.145272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ddcb47da-a2ad-466e-a05d-812ad6406028', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001412A06A390>]}
[0m18:38:13.147288 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:38:13.177658 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:38:13.272466 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:38:13.274485 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:38:13.292686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ddcb47da-a2ad-466e-a05d-812ad6406028', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014129AED8B0>]}
[0m18:38:13.308851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ddcb47da-a2ad-466e-a05d-812ad6406028', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001412A14A330>]}
[0m18:38:13.309886 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:38:13.317991 [info ] [MainThread]: 
[0m18:38:13.322045 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:38:13.327098 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:38:13.373781 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:38:13.445220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ddcb47da-a2ad-466e-a05d-812ad6406028', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001412A14B830>]}
[0m18:38:13.447277 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:38:13.449301 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:38:13.452351 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:38:13.454397 [info ] [MainThread]: 
[0m18:38:13.464591 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m18:38:13.466612 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:38:13.469641 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:38:13.470651 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:38:13.510496 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:38:13.515609 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:38:13.471661 => 18:38:13.513553
[0m18:38:13.516621 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:38:13.684464 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:38:13.686520 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:38:13.688557 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:38:13.691713 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:38:13.699905 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:38:13.702986 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:38:13.705052 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:38:13.518648 => 18:38:13.704032
[0m18:38:13.725646 [debug] [Thread-6 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:38:13.728726 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddcb47da-a2ad-466e-a05d-812ad6406028', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001412A32D6A0>]}
[0m18:38:13.730754 [error] [Thread-6 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.26s]
[0m18:38:13.732782 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:38:13.734807 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m18:38:13.737842 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:38:13.740920 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:38:13.742955 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m18:38:13.745482 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 18:38:13.744977 => 18:38:13.744977
[0m18:38:13.746493 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m18:38:13.897086 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:38:13.899113 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:38:13.902152 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:38:13.903164 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:38:13.905192 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 18:38:13.747505 => 18:38:13.904178
[0m18:38:13.911287 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:38:13.913310 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ddcb47da-a2ad-466e-a05d-812ad6406028', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001412A437710>]}
[0m18:38:13.915330 [error] [Thread-6 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.17s]
[0m18:38:13.918394 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m18:38:13.919407 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:38:13.922601 [info ] [Thread-6 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m18:38:13.925635 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:38:13.926648 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:38:13.928673 [info ] [Thread-6 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m18:38:13.930699 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:38:13.932728 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m18:38:13.934746 [info ] [Thread-6 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m18:38:13.935754 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:38:13.937772 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:38:13.939824 [info ] [Thread-6 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m18:38:13.940834 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:38:13.942859 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:38:13.945937 [info ] [Thread-6 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m18:38:13.949032 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:38:13.959475 [debug] [MainThread]: On master: ROLLBACK
[0m18:38:13.963587 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:38:13.966691 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:38:13.969878 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:38:13.972975 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:38:13.974027 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:38:13.978118 [info ] [MainThread]: 
[0m18:38:13.983352 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.66 seconds (0.66s).
[0m18:38:13.988587 [debug] [MainThread]: Command end result
[0m18:38:14.011199 [info ] [MainThread]: 
[0m18:38:14.014267 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m18:38:14.017422 [info ] [MainThread]: 
[0m18:38:14.020523 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:38:14.023588 [info ] [MainThread]: 
[0m18:38:14.027205 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:38:14.031410 [info ] [MainThread]: 
[0m18:38:14.033482 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m18:38:14.039614 [debug] [MainThread]: Command `cli build` failed at 18:38:14.037573 after 1.46 seconds
[0m18:38:14.041667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014129B28E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000141298C0A10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001412960A000>]}
[0m18:38:14.043693 [debug] [MainThread]: Flushing usage events
[0m18:39:11.334298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E2FC1C080>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E2FC1C6E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E2FC1E7E0>]}


============================== 18:39:11.336326 | ad659bc3-f445-4309-99fc-783be46892e1 ==============================
[0m18:39:11.336326 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:39:11.338352 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m18:39:11.632048 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ad659bc3-f445-4309-99fc-783be46892e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E2FA3C830>]}
[0m18:39:11.754403 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ad659bc3-f445-4309-99fc-783be46892e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E2FB8D4C0>]}
[0m18:39:11.756422 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:39:11.786807 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:39:11.889302 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:39:11.890812 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:39:11.908973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ad659bc3-f445-4309-99fc-783be46892e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E2FDD75F0>]}
[0m18:39:11.926233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ad659bc3-f445-4309-99fc-783be46892e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E2FEA6B70>]}
[0m18:39:11.928272 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:39:11.934335 [info ] [MainThread]: 
[0m18:39:11.937365 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:39:11.942443 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:39:12.000204 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:39:12.072391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ad659bc3-f445-4309-99fc-783be46892e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E2FEA4D70>]}
[0m18:39:12.075440 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:39:12.076456 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:39:12.079504 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:39:12.081533 [info ] [MainThread]: 
[0m18:39:12.093826 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m18:39:12.096859 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:39:12.099897 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:39:12.101925 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:39:12.144923 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:39:12.149047 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:39:12.102937 => 18:39:12.148014
[0m18:39:12.151100 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:39:12.309972 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:39:12.313058 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:39:12.315083 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:39:12.316100 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:39:32.314339 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:39:32.316403 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:39:32.319464 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:39:12.153161 => 18:39:32.318441
[0m18:39:32.331719 [debug] [Thread-6 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:39:32.334795 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad659bc3-f445-4309-99fc-783be46892e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E30031430>]}
[0m18:39:32.336845 [error] [Thread-6 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 20.23s]
[0m18:39:32.339925 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:39:32.341958 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m18:39:32.343983 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:39:32.350117 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:39:32.352167 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m18:39:32.354216 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 18:39:32.354216 => 18:39:32.354216
[0m18:39:32.356328 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m18:39:32.501727 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:39:32.502748 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:40:35.209108 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:40:35.211126 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:40:35.217210 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 18:39:32.358354 => 18:40:35.216184
[0m18:40:35.226407 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:40:35.228425 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ad659bc3-f445-4309-99fc-783be46892e1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E2FFB7D70>]}
[0m18:40:35.229436 [error] [Thread-6 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 62.88s]
[0m18:40:35.231458 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m18:40:35.233477 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:40:35.235514 [info ] [Thread-6 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m18:40:35.237537 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:40:35.238553 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:40:35.240579 [info ] [Thread-6 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m18:40:35.242601 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:40:35.244627 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m18:40:35.245637 [info ] [Thread-6 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m18:40:35.247654 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:40:35.249741 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:40:35.251773 [info ] [Thread-6 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m18:40:35.253279 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:40:35.255307 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:40:35.257341 [info ] [Thread-6 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m18:40:35.260389 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:40:35.264439 [debug] [MainThread]: On master: ROLLBACK
[0m18:40:35.265490 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:40:35.267513 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:40:35.268523 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:40:35.269533 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:40:35.271552 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:40:35.273583 [info ] [MainThread]: 
[0m18:40:35.275607 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 1 minutes and 23.34 seconds (83.34s).
[0m18:40:35.278646 [debug] [MainThread]: Command end result
[0m18:40:35.295013 [info ] [MainThread]: 
[0m18:40:35.297033 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m18:40:35.299049 [info ] [MainThread]: 
[0m18:40:35.300580 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:40:35.302112 [info ] [MainThread]: 
[0m18:40:35.304157 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:40:35.305683 [info ] [MainThread]: 
[0m18:40:35.307709 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m18:40:35.312799 [debug] [MainThread]: Command `cli build` failed at 18:40:35.312799 after 84.08 seconds
[0m18:40:35.316886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E2F37F500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E2FFB7DD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E2FFB7E00>]}
[0m18:40:35.318931 [debug] [MainThread]: Flushing usage events
[0m18:40:55.718422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028283367560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028283367050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028283366450>]}


============================== 18:40:55.721507 | 01b2298a-2a49-41c1-a469-7e833c10930e ==============================
[0m18:40:55.721507 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:40:55.723522 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m18:40:56.026703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '01b2298a-2a49-41c1-a469-7e833c10930e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000282834ABC20>]}
[0m18:40:56.160603 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '01b2298a-2a49-41c1-a469-7e833c10930e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002828351B2F0>]}
[0m18:40:56.163655 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:40:56.195125 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:40:56.297191 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:40:56.299210 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:40:56.318561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '01b2298a-2a49-41c1-a469-7e833c10930e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000282834E6180>]}
[0m18:40:56.336793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '01b2298a-2a49-41c1-a469-7e833c10930e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000282835E34A0>]}
[0m18:40:56.338817 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:40:56.347934 [info ] [MainThread]: 
[0m18:40:56.351999 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:40:56.358118 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:40:56.431931 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:40:56.516494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '01b2298a-2a49-41c1-a469-7e833c10930e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002828354F9B0>]}
[0m18:40:56.518516 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:40:56.520568 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:40:56.523620 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:40:56.525664 [info ] [MainThread]: 
[0m18:40:56.535874 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m18:40:56.536880 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:40:56.539912 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:40:56.541934 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:40:56.579091 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:40:56.583256 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:40:56.542945 => 18:40:56.582244
[0m18:40:56.585284 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:40:56.726741 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:40:56.727753 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:40:56.729771 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:40:56.731804 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:41:52.305208 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:41:52.307464 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:41:52.311507 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:40:56.586303 => 18:41:52.310495
[0m18:41:52.325754 [debug] [Thread-6 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:41:52.327776 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01b2298a-2a49-41c1-a469-7e833c10930e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000282837AFA70>]}
[0m18:41:52.330839 [error] [Thread-6 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 55.79s]
[0m18:41:52.334919 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:41:52.336954 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m18:41:52.338976 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:41:52.342009 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:41:52.344033 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m18:41:52.345046 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 18:41:52.345046 => 18:41:52.345046
[0m18:41:52.347075 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m18:41:52.517897 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:41:52.522060 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:43:27.067593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B0CE96CDA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B0CE758920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B0CE8DF170>]}


============================== 18:43:27.070616 | a4fafe99-4b1c-47bb-ab2b-bb3fc3b93a4c ==============================
[0m18:43:27.070616 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:43:27.071622 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m18:43:27.358742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a4fafe99-4b1c-47bb-ab2b-bb3fc3b93a4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B0CEA07C20>]}
[0m18:43:27.487033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a4fafe99-4b1c-47bb-ab2b-bb3fc3b93a4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B0CE53F320>]}
[0m18:43:27.491088 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:43:27.522529 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:43:27.628779 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:43:27.630796 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:43:27.651083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a4fafe99-4b1c-47bb-ab2b-bb3fc3b93a4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B0CEAF8290>]}
[0m18:43:27.673407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a4fafe99-4b1c-47bb-ab2b-bb3fc3b93a4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B0CEB96240>]}
[0m18:43:27.674930 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:43:27.683081 [info ] [MainThread]: 
[0m18:43:27.688172 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:43:27.694290 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:43:27.743001 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:43:27.814770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a4fafe99-4b1c-47bb-ab2b-bb3fc3b93a4c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B0CE8DCAA0>]}
[0m18:43:27.818873 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:43:27.819882 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:43:27.822907 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:43:27.824947 [info ] [MainThread]: 
[0m18:43:27.836249 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m18:43:27.839304 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:43:27.843395 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:43:27.845426 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:43:27.885494 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:43:27.888575 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:43:27.846442 => 18:43:27.888062
[0m18:43:27.890624 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:43:28.043684 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:43:28.045702 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:43:28.047738 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:43:28.049764 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:51:01.263953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E637B9C470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E637B9DDC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E637B9C050>]}


============================== 18:51:01.263953 | 78033958-1b70-4b4b-8a4f-14ddf816f670 ==============================
[0m18:51:01.263953 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:51:01.264964 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:51:01.356582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '78033958-1b70-4b4b-8a4f-14ddf816f670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E637B27E00>]}
[0m18:51:01.392028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '78033958-1b70-4b4b-8a4f-14ddf816f670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E634B0A5A0>]}
[0m18:51:01.393040 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:51:01.400104 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:51:01.442691 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:51:01.442691 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:51:01.447781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '78033958-1b70-4b4b-8a4f-14ddf816f670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E637CD99D0>]}
[0m18:51:01.456914 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '78033958-1b70-4b4b-8a4f-14ddf816f670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E637D5D6D0>]}
[0m18:51:01.457416 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:51:01.458425 [info ] [MainThread]: 
[0m18:51:01.459438 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:51:01.460454 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:51:01.464494 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:51:01.478637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '78033958-1b70-4b4b-8a4f-14ddf816f670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E637866BD0>]}
[0m18:51:01.479652 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:51:01.479652 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:51:01.479652 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:51:01.480664 [info ] [MainThread]: 
[0m18:51:01.483696 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:51:01.483696 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:51:01.484712 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:51:01.484712 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:51:01.489255 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:51:01.490264 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:51:01.484712 => 18:51:01.490264
[0m18:51:01.490264 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:51:01.526790 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:51:01.527799 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:51:01.527799 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:51:01.528807 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:51:01.541937 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:51:01.541937 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:51:01.542948 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:51:01.491273 => 18:51:01.542948
[0m18:51:01.549041 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:51:01.550592 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '78033958-1b70-4b4b-8a4f-14ddf816f670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E637D2E120>]}
[0m18:51:01.551101 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.07s]
[0m18:51:01.551614 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:51:01.552123 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:51:01.552634 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:51:01.553668 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:51:01.554178 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:51:01.554710 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:51:01.554178 => 18:51:01.554178
[0m18:51:01.554710 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:51:01.588431 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:51:01.588938 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:51:01.590463 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:51:01.590463 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:51:01.590972 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:51:01.555218 => 18:51:01.590972
[0m18:51:01.594024 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:51:01.594539 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '78033958-1b70-4b4b-8a4f-14ddf816f670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E638F028A0>]}
[0m18:51:01.595048 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.04s]
[0m18:51:01.596067 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:51:01.596067 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:51:01.596576 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m18:51:01.597592 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:51:01.597592 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:51:01.598101 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m18:51:01.598609 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:51:01.599175 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:51:01.599677 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m18:51:01.600343 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:51:01.600845 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:51:01.600845 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m18:51:01.601355 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:51:01.601867 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:51:01.601867 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m18:51:01.602378 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:51:01.603906 [debug] [MainThread]: On master: ROLLBACK
[0m18:51:01.604412 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:51:01.604920 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:51:01.604920 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:51:01.605428 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:51:01.605428 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:51:01.605937 [info ] [MainThread]: 
[0m18:51:01.606446 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m18:51:01.606953 [debug] [MainThread]: Command end result
[0m18:51:01.612542 [info ] [MainThread]: 
[0m18:51:01.612542 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m18:51:01.613053 [info ] [MainThread]: 
[0m18:51:01.613564 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:51:01.614075 [info ] [MainThread]: 
[0m18:51:01.614636 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:51:01.615139 [info ] [MainThread]: 
[0m18:51:01.615650 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m18:51:01.616680 [debug] [MainThread]: Command `cli build` failed at 18:51:01.616680 after 0.38 seconds
[0m18:51:01.616680 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E638E0B1D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E637866BD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E637B9D400>]}
[0m18:51:01.617190 [debug] [MainThread]: Flushing usage events
[0m18:51:42.957561 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F8F7BD4F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F8F7BC6B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F8F7BD2B0>]}


============================== 18:51:42.958575 | 7c4c5e84-2070-4bab-81d3-a26127883e32 ==============================
[0m18:51:42.958575 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:51:42.960606 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m18:51:43.050444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7c4c5e84-2070-4bab-81d3-a26127883e32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F9085C1D0>]}
[0m18:51:43.087998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7c4c5e84-2070-4bab-81d3-a26127883e32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F8F7BDCD0>]}
[0m18:51:43.089007 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:51:43.097121 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:51:43.141661 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:51:43.141661 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:51:43.144688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7c4c5e84-2070-4bab-81d3-a26127883e32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F909E39B0>]}
[0m18:51:43.153846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7c4c5e84-2070-4bab-81d3-a26127883e32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F9094DD00>]}
[0m18:51:43.154859 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:51:43.155868 [info ] [MainThread]: 
[0m18:51:43.156882 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:51:43.158388 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:51:43.162464 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:51:43.175638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7c4c5e84-2070-4bab-81d3-a26127883e32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F908833B0>]}
[0m18:51:43.176650 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:51:43.176650 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:51:43.177658 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:51:43.177658 [info ] [MainThread]: 
[0m18:51:43.179674 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:51:43.180686 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:51:43.180686 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:51:43.181702 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:51:43.187862 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:51:43.188877 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:51:43.181702 => 18:51:43.188877
[0m18:51:43.189895 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:51:43.224970 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:51:43.225479 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:51:43.225991 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:51:43.226497 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:51:43.279196 [debug] [Thread-1 (]: SQL status: OK in 0.05000000074505806 seconds
[0m18:51:43.314661 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:51:43.316754 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:51:43.317266 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:51:43.337358 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:51:43.338374 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'str' object has no attribute 'contains'
[0m18:51:43.339394 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:51:43.189895 => 18:51:43.339394
[0m18:51:43.348596 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'str' object has no attribute 'contains'
[0m18:51:43.349614 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7c4c5e84-2070-4bab-81d3-a26127883e32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F9098F230>]}
[0m18:51:43.350123 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.17s]
[0m18:51:43.351139 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:51:43.351534 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:51:43.352562 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:51:43.353601 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:51:43.354619 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:51:43.355130 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:51:43.354619 => 18:51:43.354619
[0m18:51:43.355640 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:51:43.390344 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:51:43.391892 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:51:43.396016 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:51:43.408885 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m18:51:43.414108 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:51:43.415231 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m18:51:43.420006 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      
[0m18:51:43.421027 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'str' object has no attribute 'contains'
[0m18:51:43.421535 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:51:43.355640 => 18:51:43.421535
[0m18:51:43.450049 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  'str' object has no attribute 'contains'
[0m18:51:43.450049 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7c4c5e84-2070-4bab-81d3-a26127883e32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F9098C620>]}
[0m18:51:43.451581 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.10s]
[0m18:51:43.452608 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:51:43.453117 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:51:43.453117 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m18:51:43.454135 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:51:43.454648 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:51:43.455167 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m18:51:43.456449 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:51:43.456955 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:51:43.457995 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m18:51:43.458509 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:51:43.459020 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:51:43.459528 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m18:51:43.460575 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:51:43.461088 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:51:43.462133 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m18:51:43.462649 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:51:43.464182 [debug] [MainThread]: On master: ROLLBACK
[0m18:51:43.464691 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:51:43.465201 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:51:43.465201 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:51:43.465713 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:51:43.466221 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:51:43.467242 [info ] [MainThread]: 
[0m18:51:43.467752 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.31 seconds (0.31s).
[0m18:51:43.468772 [debug] [MainThread]: Command end result
[0m18:51:43.481699 [info ] [MainThread]: 
[0m18:51:43.482704 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m18:51:43.482704 [info ] [MainThread]: 
[0m18:51:43.483875 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'str' object has no attribute 'contains'
[0m18:51:43.483875 [info ] [MainThread]: 
[0m18:51:43.484878 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'str' object has no attribute 'contains'
[0m18:51:43.484878 [info ] [MainThread]: 
[0m18:51:43.485882 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m18:51:43.486895 [debug] [MainThread]: Command `cli build` failed at 18:51:43.486895 after 0.56 seconds
[0m18:51:43.487404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F8F149EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F8F719850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020F8F54BE00>]}
[0m18:51:43.487917 [debug] [MainThread]: Flushing usage events
[0m18:52:31.851584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418DDC290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418DDDD60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418DDD820>]}


============================== 18:52:31.852605 | a92164c3-165c-4841-bd8a-a714799cc18e ==============================
[0m18:52:31.852605 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:52:31.852605 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:52:31.943939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a92164c3-165c-4841-bd8a-a714799cc18e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022419E7C410>]}
[0m18:52:31.996933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a92164c3-165c-4841-bd8a-a714799cc18e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418D9F140>]}
[0m18:52:31.998459 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:52:32.014195 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:52:32.986473 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:52:32.987491 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:52:32.993606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a92164c3-165c-4841-bd8a-a714799cc18e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418DEC5C0>]}
[0m18:52:33.013889 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a92164c3-165c-4841-bd8a-a714799cc18e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022419F6D9A0>]}
[0m18:52:33.014904 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:52:33.016967 [info ] [MainThread]: 
[0m18:52:33.017982 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:52:33.018991 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:52:33.032234 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:52:33.053526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a92164c3-165c-4841-bd8a-a714799cc18e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022419FC9730>]}
[0m18:52:33.053526 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:52:33.054543 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:52:33.055569 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:52:33.056594 [info ] [MainThread]: 
[0m18:52:33.058622 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:52:33.059634 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:52:33.060647 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:52:33.060647 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:52:33.066195 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:52:33.067202 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:52:33.060647 => 18:52:33.067202
[0m18:52:33.067202 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:52:33.099732 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:52:33.100744 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:52:33.100744 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:52:33.101753 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:52:33.140284 [debug] [Thread-1 (]: SQL status: OK in 0.03999999910593033 seconds
[0m18:52:33.172130 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:52:33.173142 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:52:33.173142 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:52:33.180212 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:52:33.189362 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:52:33.067202 => 18:52:33.189362
[0m18:52:33.190370 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a92164c3-165c-4841-bd8a-a714799cc18e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418DDC590>]}
[0m18:52:33.190370 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.13s]
[0m18:52:33.191378 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:52:33.192385 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:52:33.192385 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:52:33.193395 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:52:33.193395 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:52:33.194411 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:52:33.194411 => 18:52:33.194411
[0m18:52:33.194411 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:52:33.220750 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:52:33.220750 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:52:33.223795 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:52:33.231928 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m18:52:33.234955 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:52:33.234955 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m18:52:33.237984 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:52:33.242025 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m18:52:33.245047 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:52:33.246073 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:52:33.194411 => 18:52:33.246073
[0m18:52:33.247087 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a92164c3-165c-4841-bd8a-a714799cc18e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002241A0FDEB0>]}
[0m18:52:33.247087 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.05s]
[0m18:52:33.249116 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:52:33.249116 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:52:33.250158 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m18:52:33.251174 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m18:52:33.252186 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:52:33.262359 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:52:33.264388 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 18:52:33.252186 => 18:52:33.263373
[0m18:52:33.264388 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:52:33.274505 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:52:33.276562 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:52:33.277575 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:52:33.281104 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:52:33.282637 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 18:52:33.264388 => 18:52:33.282123
[0m18:52:33.283677 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m18:52:33.284711 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:52:33.285218 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:52:33.285731 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m18:52:33.286749 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m18:52:33.286749 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:52:33.292550 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:52:33.294591 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 18:52:33.287257 => 18:52:33.294085
[0m18:52:33.295099 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:52:33.297134 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:52:33.298660 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:52:33.299169 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:52:33.302757 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:52:33.304291 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 18:52:33.295099 => 18:52:33.304291
[0m18:52:33.305311 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m18:52:33.305819 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:52:33.306894 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:52:33.309545 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m18:52:33.310425 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m18:52:33.310928 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:52:33.312946 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:52:33.313973 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:52:33.310928 => 18:52:33.313973
[0m18:52:33.314481 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:52:33.328734 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:52:33.330287 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:52:33.330801 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m18:52:33.334421 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:52:33.336977 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:52:33.314988 => 18:52:33.336977
[0m18:52:33.338031 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a92164c3-165c-4841-bd8a-a714799cc18e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002241A27F050>]}
[0m18:52:33.339048 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m18:52:33.339572 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:52:33.340591 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:52:33.341101 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m18:52:33.341611 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m18:52:33.342120 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:52:33.346257 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:52:33.347283 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 18:52:33.342628 => 18:52:33.347283
[0m18:52:33.347793 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:52:33.348807 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:52:33.349830 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:52:33.350338 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:52:33.353995 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:52:33.355533 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 18:52:33.347793 => 18:52:33.355533
[0m18:52:33.356553 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m18:52:33.357061 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:52:33.357568 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:52:33.358079 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m18:52:33.358588 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m18:52:33.358588 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:52:33.361632 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:52:33.362141 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 18:52:33.359098 => 18:52:33.362141
[0m18:52:33.362648 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:52:33.364176 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:52:33.365700 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:52:33.366209 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:52:33.368247 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:52:33.369270 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 18:52:33.362648 => 18:52:33.368758
[0m18:52:33.369780 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m18:52:33.370292 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:52:33.371308 [debug] [MainThread]: On master: ROLLBACK
[0m18:52:33.371308 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:52:33.371814 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:52:33.371814 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:52:33.372322 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:52:33.372322 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:52:33.372828 [info ] [MainThread]: 
[0m18:52:33.373335 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.35 seconds (0.35s).
[0m18:52:33.374349 [debug] [MainThread]: Command end result
[0m18:52:33.378404 [info ] [MainThread]: 
[0m18:52:33.378910 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:52:33.379418 [info ] [MainThread]: 
[0m18:52:33.379926 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m18:52:33.380434 [debug] [MainThread]: Command `cli build` succeeded at 18:52:33.380434 after 1.58 seconds
[0m18:52:33.380941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002241A003620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022415C6A6C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022418D9FCE0>]}
[0m18:52:33.381448 [debug] [MainThread]: Flushing usage events
[0m18:52:48.693508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3D544AA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3D544890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3D5446E0>]}


============================== 18:52:48.694515 | b557caec-6f75-4ef2-a7ee-e5917c2c4e90 ==============================
[0m18:52:48.694515 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:52:48.694515 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:52:48.778981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b557caec-6f75-4ef2-a7ee-e5917c2c4e90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3CB0DC10>]}
[0m18:52:48.819483 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b557caec-6f75-4ef2-a7ee-e5917c2c4e90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3C9DD670>]}
[0m18:52:48.821500 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:52:48.828577 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:52:48.875043 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:52:48.875043 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:52:48.879080 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b557caec-6f75-4ef2-a7ee-e5917c2c4e90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3CD17F80>]}
[0m18:52:48.887231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b557caec-6f75-4ef2-a7ee-e5917c2c4e90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3E712900>]}
[0m18:52:48.888241 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:52:48.889256 [info ] [MainThread]: 
[0m18:52:48.890271 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:52:48.891283 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:52:48.895316 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:52:48.909484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b557caec-6f75-4ef2-a7ee-e5917c2c4e90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3D175610>]}
[0m18:52:48.910491 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:52:48.910491 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:52:48.910491 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:52:48.911499 [info ] [MainThread]: 
[0m18:52:48.913517 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:52:48.913517 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:52:48.914528 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:52:48.914528 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:52:48.921806 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:52:48.923849 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:52:48.915701 => 18:52:48.923849
[0m18:52:48.924869 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:52:48.985486 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:52:48.987504 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:52:48.987504 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:52:48.988516 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:52:49.035135 [debug] [Thread-1 (]: SQL status: OK in 0.05000000074505806 seconds
[0m18:52:49.061192 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:52:49.063215 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:52:49.064233 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:52:49.080556 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m18:52:49.092788 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:52:48.924869 => 18:52:49.092788
[0m18:52:49.093810 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b557caec-6f75-4ef2-a7ee-e5917c2c4e90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3E852AB0>]}
[0m18:52:49.094819 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.18s]
[0m18:52:49.094819 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:52:49.095831 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:52:49.095831 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:52:49.096859 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:52:49.096859 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:52:49.097871 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:52:49.096859 => 18:52:49.096859
[0m18:52:49.097871 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:52:49.134614 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:52:49.135624 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:52:49.138671 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:52:49.148827 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m18:52:49.153380 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:52:49.153907 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m18:52:49.159003 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:52:49.163600 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m18:52:49.169684 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:52:49.170696 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:52:49.097871 => 18:52:49.170696
[0m18:52:49.171707 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b557caec-6f75-4ef2-a7ee-e5917c2c4e90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3E730530>]}
[0m18:52:49.171707 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.07s]
[0m18:52:49.173726 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:52:49.174242 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:52:49.174755 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m18:52:49.175266 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m18:52:49.175776 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:52:49.187087 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:52:49.189148 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 18:52:49.175776 => 18:52:49.188620
[0m18:52:49.189697 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:52:49.199941 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:52:49.203012 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:52:49.203522 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:52:49.206585 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:52:49.209684 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 18:52:49.190212 => 18:52:49.209174
[0m18:52:49.210700 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m18:52:49.211208 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:52:49.211719 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:52:49.212227 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m18:52:49.212737 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m18:52:49.213247 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:52:49.219412 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:52:49.221456 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 18:52:49.213247 => 18:52:49.220944
[0m18:52:49.222065 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:52:49.226402 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:52:49.227452 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:52:49.227962 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:52:49.231013 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:52:49.232538 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 18:52:49.222596 => 18:52:49.232538
[0m18:52:49.233560 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m18:52:49.234581 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:52:49.235087 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:52:49.236109 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m18:52:49.236621 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m18:52:49.237128 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:52:49.240258 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:52:49.241273 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:52:49.237128 => 18:52:49.240767
[0m18:52:49.241782 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:52:49.256556 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:52:49.257577 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:52:49.258085 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m18:52:49.261182 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:52:49.263742 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:52:49.241782 => 18:52:49.263230
[0m18:52:49.264253 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b557caec-6f75-4ef2-a7ee-e5917c2c4e90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3EA06330>]}
[0m18:52:49.264761 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.03s]
[0m18:52:49.265269 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:52:49.266840 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:52:49.267866 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m18:52:49.268887 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m18:52:49.269489 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:52:49.275600 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:52:49.277134 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 18:52:49.269993 => 18:52:49.277134
[0m18:52:49.277642 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:52:49.280218 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:52:49.281256 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:52:49.281256 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:52:49.284866 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:52:49.286431 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 18:52:49.278155 => 18:52:49.286431
[0m18:52:49.286939 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.02s]
[0m18:52:49.287448 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:52:49.287962 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:52:49.288475 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m18:52:49.289508 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m18:52:49.290027 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:52:49.293619 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:52:49.295670 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 18:52:49.290540 => 18:52:49.295670
[0m18:52:49.296271 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:52:49.298308 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:52:49.299327 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:52:49.299837 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:52:49.302596 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:52:49.304120 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 18:52:49.296271 => 18:52:49.303612
[0m18:52:49.304629 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.02s]
[0m18:52:49.305657 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:52:49.306689 [debug] [MainThread]: On master: ROLLBACK
[0m18:52:49.307199 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:52:49.307707 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:52:49.308220 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:52:49.308220 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:52:49.308731 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:52:49.309244 [info ] [MainThread]: 
[0m18:52:49.309757 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.42 seconds (0.42s).
[0m18:52:49.310267 [debug] [MainThread]: Command end result
[0m18:52:49.318628 [info ] [MainThread]: 
[0m18:52:49.319666 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:52:49.320690 [info ] [MainThread]: 
[0m18:52:49.321202 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m18:52:49.323253 [debug] [MainThread]: Command `cli build` succeeded at 18:52:49.322742 after 0.65 seconds
[0m18:52:49.323763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3EA349B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3D175610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010C3E8A3AD0>]}
[0m18:52:49.323763 [debug] [MainThread]: Flushing usage events
[0m18:53:03.873520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FF8ACB00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FF8AD850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FF8ACF80>]}


============================== 18:53:03.874528 | 58b2f1e3-bd9d-468e-998b-ca15b80eaf66 ==============================
[0m18:53:03.874528 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:53:03.874528 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m18:53:03.975209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '58b2f1e3-bd9d-468e-998b-ca15b80eaf66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FF77D910>]}
[0m18:53:04.022143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '58b2f1e3-bd9d-468e-998b-ca15b80eaf66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FF81A270>]}
[0m18:53:04.026210 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:53:04.035363 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:53:04.094743 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:53:04.095757 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:53:04.102841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '58b2f1e3-bd9d-468e-998b-ca15b80eaf66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FF8BD0A0>]}
[0m18:53:04.111965 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '58b2f1e3-bd9d-468e-998b-ca15b80eaf66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FFA6D9D0>]}
[0m18:53:04.112980 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:53:04.115001 [info ] [MainThread]: 
[0m18:53:04.116017 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:53:04.118055 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:53:04.126466 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:53:04.143440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '58b2f1e3-bd9d-468e-998b-ca15b80eaf66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FF8AD370>]}
[0m18:53:04.145468 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:53:04.146482 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:53:04.147495 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:53:04.147495 [info ] [MainThread]: 
[0m18:53:04.154662 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:53:04.154662 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:53:04.156686 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:53:04.156686 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:53:04.162786 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:53:04.164830 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:53:04.156686 => 18:53:04.164830
[0m18:53:04.165856 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:53:04.207232 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:53:04.207232 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:53:04.208251 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:53:04.208251 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:53:04.211321 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:53:04.211321 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:53:04.212341 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:53:04.165856 => 18:53:04.212341
[0m18:53:04.413355 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:53:04.413355 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '58b2f1e3-bd9d-468e-998b-ca15b80eaf66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FFA16270>]}
[0m18:53:04.414370 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.26s]
[0m18:53:04.415380 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:53:04.416420 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:53:04.417438 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:53:04.418451 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:53:04.419480 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:53:04.419993 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:53:04.419993 => 18:53:04.419993
[0m18:53:04.420502 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:53:04.454005 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:53:04.455029 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:53:04.457070 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:53:04.457070 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:53:04.458085 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:53:04.420502 => 18:53:04.457070
[0m18:53:04.461121 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:53:04.461121 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '58b2f1e3-bd9d-468e-998b-ca15b80eaf66', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FF9E8440>]}
[0m18:53:04.462159 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.04s]
[0m18:53:04.463170 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:53:04.463170 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:53:04.464240 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m18:53:04.464240 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:53:04.464240 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:53:04.465244 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m18:53:04.465244 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:53:04.466253 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:53:04.466253 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m18:53:04.467268 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:53:04.467268 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:53:04.468279 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m18:53:04.468279 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:53:04.469293 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:53:04.469293 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m18:53:04.470307 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:53:04.471318 [debug] [MainThread]: On master: ROLLBACK
[0m18:53:04.471318 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:53:04.472331 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:53:04.472331 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:53:04.473342 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:53:04.473342 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:53:04.473342 [info ] [MainThread]: 
[0m18:53:04.474351 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.36 seconds (0.36s).
[0m18:53:04.475359 [debug] [MainThread]: Command end result
[0m18:53:04.480434 [info ] [MainThread]: 
[0m18:53:04.481443 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m18:53:04.482447 [info ] [MainThread]: 
[0m18:53:04.482950 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:53:04.482950 [info ] [MainThread]: 
[0m18:53:04.483973 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:53:04.483973 [info ] [MainThread]: 
[0m18:53:04.485000 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m18:53:04.487028 [debug] [MainThread]: Command `cli build` failed at 18:53:04.486014 after 0.64 seconds
[0m18:53:04.487028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FEE0E630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FF97E690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000276FF9A7650>]}
[0m18:53:04.488045 [debug] [MainThread]: Flushing usage events
[0m18:53:27.523429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001614560E390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001614560C980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001614560D7F0>]}


============================== 18:53:27.525448 | 5bd51be1-da50-43f2-bcfd-d8d9cb8bc838 ==============================
[0m18:53:27.525448 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:53:27.527476 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:53:27.806810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5bd51be1-da50-43f2-bcfd-d8d9cb8bc838', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016145697A40>]}
[0m18:53:27.929211 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5bd51be1-da50-43f2-bcfd-d8d9cb8bc838', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000161450404A0>]}
[0m18:53:27.932235 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:53:27.966822 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:53:28.075422 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:53:28.076444 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:53:28.094651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5bd51be1-da50-43f2-bcfd-d8d9cb8bc838', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016145664D10>]}
[0m18:53:28.110311 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5bd51be1-da50-43f2-bcfd-d8d9cb8bc838', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001614589A720>]}
[0m18:53:28.112328 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:53:28.119958 [info ] [MainThread]: 
[0m18:53:28.123008 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:53:28.128079 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:53:28.177734 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:53:28.248742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5bd51be1-da50-43f2-bcfd-d8d9cb8bc838', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016144ED9730>]}
[0m18:53:28.250767 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:53:28.252795 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:53:28.255854 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:53:28.257891 [info ] [MainThread]: 
[0m18:53:28.266012 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m18:53:28.268038 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:53:28.271066 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:53:28.272075 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:53:28.310260 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:53:28.312285 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:53:28.273086 => 18:53:28.312285
[0m18:53:28.314317 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:53:28.457813 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:53:28.458824 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:53:28.460845 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:53:28.461852 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:56:44.974568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEF1C2D1C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEF1C2D280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEF1C2CB90>]}


============================== 18:56:44.975580 | 5e0ec89d-8116-4718-aade-c3020f9e3597 ==============================
[0m18:56:44.975580 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:56:44.976592 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m18:56:45.074046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5e0ec89d-8116-4718-aade-c3020f9e3597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEF1BB7E00>]}
[0m18:56:45.110055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5e0ec89d-8116-4718-aade-c3020f9e3597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEF1D957C0>]}
[0m18:56:45.111063 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:56:45.121310 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:56:45.168985 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:56:45.168985 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:56:45.173016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5e0ec89d-8116-4718-aade-c3020f9e3597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEF1C94590>]}
[0m18:56:45.179070 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5e0ec89d-8116-4718-aade-c3020f9e3597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEF1DEE120>]}
[0m18:56:45.179070 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:56:45.182144 [info ] [MainThread]: 
[0m18:56:45.183155 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:56:45.185321 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:56:45.191426 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:56:45.205579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5e0ec89d-8116-4718-aade-c3020f9e3597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEF2E197F0>]}
[0m18:56:45.206585 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:56:45.207595 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:56:45.207595 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:56:45.208603 [info ] [MainThread]: 
[0m18:56:45.212695 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:56:45.212695 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:56:45.213706 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:56:45.213706 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:56:45.218785 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:56:45.219802 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:56:45.214728 => 18:56:45.219802
[0m18:56:45.220813 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:56:45.253927 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:56:45.254951 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:56:45.254951 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:56:45.255457 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:56:45.257511 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:56:45.258019 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:56:45.258019 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:56:45.220813 => 18:56:45.258019
[0m18:56:45.265768 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:56:45.266280 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5e0ec89d-8116-4718-aade-c3020f9e3597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEF1D95E80>]}
[0m18:56:45.267299 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.05s]
[0m18:56:45.268321 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:56:45.268830 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:56:45.269855 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:56:45.270919 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:56:45.271434 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:56:45.271950 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:56:45.271434 => 18:56:45.271434
[0m18:56:45.271950 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:56:45.305831 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:56:45.306838 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:56:45.307847 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:56:45.307847 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:56:45.308855 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:56:45.272463 => 18:56:45.307847
[0m18:56:45.310872 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:56:45.310872 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5e0ec89d-8116-4718-aade-c3020f9e3597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEF2F41C70>]}
[0m18:56:45.311880 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.04s]
[0m18:56:45.311880 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:56:45.312887 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:56:45.312887 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m18:56:45.312887 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:56:45.313900 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:56:45.313900 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m18:56:45.313900 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:56:45.314912 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:56:45.314912 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m18:56:45.315924 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:56:45.315924 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:56:45.317010 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m18:56:45.318013 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:56:45.318013 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:56:45.318013 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m18:56:45.319027 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:56:45.320043 [debug] [MainThread]: On master: ROLLBACK
[0m18:56:45.320043 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:56:45.321061 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:56:45.321061 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:56:45.321061 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:56:45.322073 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:56:45.322073 [info ] [MainThread]: 
[0m18:56:45.322073 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m18:56:45.323082 [debug] [MainThread]: Command end result
[0m18:56:45.327118 [info ] [MainThread]: 
[0m18:56:45.328127 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m18:56:45.328127 [info ] [MainThread]: 
[0m18:56:45.328127 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:56:45.329151 [info ] [MainThread]: 
[0m18:56:45.329151 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:56:45.329151 [info ] [MainThread]: 
[0m18:56:45.330160 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m18:56:45.330160 [debug] [MainThread]: Command `cli build` failed at 18:56:45.330160 after 0.39 seconds
[0m18:56:45.331169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEF1C96FF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEF0D56D80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FEF1CFF110>]}
[0m18:56:45.331169 [debug] [MainThread]: Flushing usage events
[0m18:57:13.604030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271B877A270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BB763E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BB89DA60>]}


============================== 18:57:13.605039 | 4abde5c7-3e3c-422f-98f7-866ee84abd31 ==============================
[0m18:57:13.605039 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:57:13.605039 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m18:57:13.701009 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4abde5c7-3e3c-422f-98f7-866ee84abd31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BB91AB70>]}
[0m18:57:13.739136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4abde5c7-3e3c-422f-98f7-866ee84abd31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BB80AC30>]}
[0m18:57:13.740145 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:57:13.749244 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:57:13.797924 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:57:13.797924 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:57:13.801955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4abde5c7-3e3c-422f-98f7-866ee84abd31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BCAC3650>]}
[0m18:57:13.810050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4abde5c7-3e3c-422f-98f7-866ee84abd31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BCA2DEB0>]}
[0m18:57:13.810050 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:57:13.812123 [info ] [MainThread]: 
[0m18:57:13.814140 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:57:13.815153 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:57:13.822262 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:57:13.837501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4abde5c7-3e3c-422f-98f7-866ee84abd31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BCA86FC0>]}
[0m18:57:13.838511 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:57:13.838511 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:57:13.838511 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:57:13.839519 [info ] [MainThread]: 
[0m18:57:13.841539 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:57:13.842550 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:57:13.842550 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:57:13.843582 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:57:13.849653 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:57:13.850661 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:57:13.843582 => 18:57:13.850661
[0m18:57:13.851670 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:57:13.891971 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:57:13.892980 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:57:13.892980 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:57:13.893990 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:57:13.895001 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:57:13.896026 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:57:13.897032 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:57:13.851670 => 18:57:13.896026
[0m18:57:13.902086 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:57:13.903105 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4abde5c7-3e3c-422f-98f7-866ee84abd31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BB89E120>]}
[0m18:57:13.904286 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.06s]
[0m18:57:13.905294 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:57:13.905294 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:57:13.906310 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:57:13.908341 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:57:13.908341 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:57:13.908341 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:57:13.908341 => 18:57:13.908341
[0m18:57:13.909355 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:57:13.944487 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:57:13.946047 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:57:13.948120 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:57:13.949153 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:57:13.949670 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:57:13.909355 => 18:57:13.949670
[0m18:57:13.955537 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:57:13.957092 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4abde5c7-3e3c-422f-98f7-866ee84abd31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BCBB0770>]}
[0m18:57:13.957605 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.05s]
[0m18:57:13.959161 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:57:13.959675 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:57:13.960698 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m18:57:13.961213 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:57:13.961726 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:57:13.962254 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m18:57:13.962771 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:57:13.963298 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:57:13.963810 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m18:57:13.964324 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:57:13.964836 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:57:13.965351 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m18:57:13.965875 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:57:13.966413 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:57:13.966413 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m18:57:13.967439 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:57:13.969487 [debug] [MainThread]: On master: ROLLBACK
[0m18:57:13.970001 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:57:13.970512 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:57:13.971029 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:57:13.971543 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:57:13.972066 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:57:13.972606 [info ] [MainThread]: 
[0m18:57:13.973114 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m18:57:13.973622 [debug] [MainThread]: Command end result
[0m18:57:13.984463 [info ] [MainThread]: 
[0m18:57:13.984974 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m18:57:13.985489 [info ] [MainThread]: 
[0m18:57:13.985489 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:57:13.986007 [info ] [MainThread]: 
[0m18:57:13.986517 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:57:13.986517 [info ] [MainThread]: 
[0m18:57:13.987032 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m18:57:13.989609 [debug] [MainThread]: Command `cli build` failed at 18:57:13.989089 after 0.41 seconds
[0m18:57:13.990120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BB7C8FE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271BB7C91F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000271B877A270>]}
[0m18:57:13.990632 [debug] [MainThread]: Flushing usage events
[0m18:58:05.005270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CD6B5D370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CD6B5C710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CD6B5D2B0>]}


============================== 18:58:05.005776 | e4763fee-df08-4b4d-b311-bbb8325adaef ==============================
[0m18:58:05.005776 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:58:05.005776 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m18:58:05.090882 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e4763fee-df08-4b4d-b311-bbb8325adaef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CD6194950>]}
[0m18:58:05.126364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e4763fee-df08-4b4d-b311-bbb8325adaef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CD6A25D90>]}
[0m18:58:05.127374 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:58:05.133448 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:58:05.175885 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:58:05.175885 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:58:05.179916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e4763fee-df08-4b4d-b311-bbb8325adaef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CD6BC73E0>]}
[0m18:58:05.186994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e4763fee-df08-4b4d-b311-bbb8325adaef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CD7CED820>]}
[0m18:58:05.188002 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:58:05.190040 [info ] [MainThread]: 
[0m18:58:05.190040 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:58:05.191053 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:58:05.195103 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:58:05.208261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e4763fee-df08-4b4d-b311-bbb8325adaef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CD7D4A030>]}
[0m18:58:05.208261 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:58:05.208261 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:58:05.209266 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:58:05.209266 [info ] [MainThread]: 
[0m18:58:05.211287 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:58:05.212295 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:58:05.212295 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:58:05.212295 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:58:05.217360 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:58:05.220502 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:58:05.213307 => 18:58:05.219485
[0m18:58:05.220502 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:58:05.255960 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:58:05.255960 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:58:05.257076 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:58:05.257076 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:58:05.259086 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:58:05.259086 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:58:05.260094 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:58:05.220502 => 18:58:05.259086
[0m18:58:05.266235 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:58:05.267284 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e4763fee-df08-4b4d-b311-bbb8325adaef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CD7CBECF0>]}
[0m18:58:05.267284 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.05s]
[0m18:58:05.268298 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:58:05.269310 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:58:05.269310 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:58:05.270321 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:58:05.270834 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:58:05.271346 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:58:05.270834 => 18:58:05.270834
[0m18:58:05.271346 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:58:05.305980 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:58:05.306999 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:58:05.308526 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:58:05.309033 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:58:05.309540 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:58:05.271859 => 18:58:05.309540
[0m18:58:05.312679 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:58:05.313190 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e4763fee-df08-4b4d-b311-bbb8325adaef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CD7E73080>]}
[0m18:58:05.313705 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.04s]
[0m18:58:05.314217 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:58:05.314729 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:58:05.315253 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m18:58:05.315767 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:58:05.316277 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:58:05.316277 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m18:58:05.316787 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:58:05.317300 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:58:05.317810 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m18:58:05.318323 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:58:05.318831 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:58:05.319346 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m18:58:05.319862 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:58:05.320379 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:58:05.320890 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m18:58:05.321400 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:58:05.322423 [debug] [MainThread]: On master: ROLLBACK
[0m18:58:05.322423 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:58:05.322931 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:58:05.323439 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:58:05.323439 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:58:05.323949 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:58:05.324464 [info ] [MainThread]: 
[0m18:58:05.325490 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m18:58:05.325997 [debug] [MainThread]: Command end result
[0m18:58:05.330582 [info ] [MainThread]: 
[0m18:58:05.331088 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m18:58:05.331595 [info ] [MainThread]: 
[0m18:58:05.331595 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:58:05.332101 [info ] [MainThread]: 
[0m18:58:05.332101 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m18:58:05.332607 [info ] [MainThread]: 
[0m18:58:05.333114 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m18:58:05.333622 [debug] [MainThread]: Command `cli build` failed at 18:58:05.333622 after 0.35 seconds
[0m18:58:05.334128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CD6AEA5A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CD7BFCC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025CD5D36A50>]}
[0m18:58:05.334128 [debug] [MainThread]: Flushing usage events
[0m18:59:46.106262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295DB35E510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295DB35DC70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295DB35EF00>]}


============================== 18:59:46.107281 | d8a6ac75-cb2b-4553-a0a3-0c57b0e98ed7 ==============================
[0m18:59:46.107281 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:59:46.107281 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m18:59:46.193199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd8a6ac75-cb2b-4553-a0a3-0c57b0e98ed7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295DB3EE780>]}
[0m18:59:46.230668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd8a6ac75-cb2b-4553-a0a3-0c57b0e98ed7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295DB0C7140>]}
[0m18:59:46.231675 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:59:46.238753 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:59:46.287372 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:59:46.288380 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:59:46.291406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd8a6ac75-cb2b-4553-a0a3-0c57b0e98ed7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295DC5D7B30>]}
[0m18:59:46.298492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd8a6ac75-cb2b-4553-a0a3-0c57b0e98ed7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295DC5B6DE0>]}
[0m18:59:46.298492 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m18:59:46.299504 [info ] [MainThread]: 
[0m18:59:46.300517 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:59:46.301524 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:59:46.305566 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m18:59:46.319816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd8a6ac75-cb2b-4553-a0a3-0c57b0e98ed7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295DA5969C0>]}
[0m18:59:46.320830 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:59:46.320830 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:59:46.320830 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:59:46.321838 [info ] [MainThread]: 
[0m18:59:46.323853 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:59:46.323853 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m18:59:46.324863 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:59:46.324863 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:59:46.329927 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:59:46.330935 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:59:46.325896 => 18:59:46.329927
[0m18:59:46.330935 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:59:46.362303 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:59:46.363312 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:59:46.364323 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:59:46.364323 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:59:46.366341 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:59:46.366341 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\\n "cells": [\\n  {\\n   "cell_type":...
[0m18:59:46.367348 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:59:46.330935 => 18:59:46.367348
[0m18:59:46.373468 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\\n "cells": [\\n  {\\n   "cell_type":...
[0m18:59:46.374481 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8a6ac75-cb2b-4553-a0a3-0c57b0e98ed7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295DC4E8920>]}
[0m18:59:46.374481 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.05s]
[0m18:59:46.375500 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:59:46.375500 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:59:46.376511 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m18:59:46.377523 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:59:46.377523 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:59:46.378540 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:59:46.378540 => 18:59:46.378540
[0m18:59:46.378540 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:59:46.410041 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:59:46.411051 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:59:46.412062 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m18:59:46.412062 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\\n "cells": [\\n  {\\n   "cell_type":...
[0m18:59:46.413079 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:59:46.378540 => 18:59:46.413079
[0m18:59:46.416236 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\\n "cells": [\\n  {\\n   "cell_type":...
[0m18:59:46.416236 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd8a6ac75-cb2b-4553-a0a3-0c57b0e98ed7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295DC7426F0>]}
[0m18:59:46.417243 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.04s]
[0m18:59:46.418262 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:59:46.418262 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:59:46.419276 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m18:59:46.419276 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:59:46.419276 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:59:46.420288 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m18:59:46.420288 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:59:46.421309 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:59:46.422329 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m18:59:46.422329 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:59:46.423340 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:59:46.423340 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m18:59:46.423340 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:59:46.424355 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:59:46.424355 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m18:59:46.425372 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:59:46.426386 [debug] [MainThread]: On master: ROLLBACK
[0m18:59:46.426386 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:59:46.427396 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:59:46.427396 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:59:46.427396 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:59:46.428406 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:59:46.428406 [info ] [MainThread]: 
[0m18:59:46.429418 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m18:59:46.430432 [debug] [MainThread]: Command end result
[0m18:59:46.437582 [info ] [MainThread]: 
[0m18:59:46.438590 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m18:59:46.438590 [info ] [MainThread]: 
[0m18:59:46.438590 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\\n "cells": [\\n  {\\n   "cell_type":...
[0m18:59:46.439597 [info ] [MainThread]: 
[0m18:59:46.439597 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\\n "cells": [\\n  {\\n   "cell_type":...
[0m18:59:46.439597 [info ] [MainThread]: 
[0m18:59:46.440605 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m18:59:46.441616 [debug] [MainThread]: Command `cli build` failed at 18:59:46.441616 after 0.37 seconds
[0m18:59:46.441616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295DB3DDDC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295DB3DD280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000295DB3DCB60>]}
[0m18:59:46.442628 [debug] [MainThread]: Flushing usage events
[0m19:01:18.256694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A9F47CE90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A9F47D130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A9F47CDA0>]}


============================== 19:01:18.256694 | 424441e8-d86c-4634-83ce-50ac30a578e3 ==============================
[0m19:01:18.256694 [info ] [MainThread]: Running with dbt=1.7.14
[0m19:01:18.257706 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m19:01:18.340693 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '424441e8-d86c-4634-83ce-50ac30a578e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A9EA07FB0>]}
[0m19:01:18.377103 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '424441e8-d86c-4634-83ce-50ac30a578e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A9F22B170>]}
[0m19:01:18.378122 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:01:18.387220 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m19:01:18.428684 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:01:18.429694 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:01:18.432723 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '424441e8-d86c-4634-83ce-50ac30a578e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AA05DEC30>]}
[0m19:01:18.438791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '424441e8-d86c-4634-83ce-50ac30a578e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AA060D880>]}
[0m19:01:18.439800 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m19:01:18.440809 [info ] [MainThread]: 
[0m19:01:18.441818 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:01:18.442825 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:01:18.446875 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:01:18.461043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '424441e8-d86c-4634-83ce-50ac30a578e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AA066AE10>]}
[0m19:01:18.461043 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:01:18.461043 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:01:18.462054 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:01:18.462054 [info ] [MainThread]: 
[0m19:01:18.464075 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m19:01:18.465082 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m19:01:18.465082 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:01:18.466092 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:01:18.470138 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:01:18.471145 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:01:18.466092 => 19:01:18.470138
[0m19:01:18.471145 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:01:18.502497 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:01:18.504515 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:01:18.504515 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:01:18.505524 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:01:18.507549 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:01:18.508563 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m19:01:18.508563 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:01:18.471145 => 19:01:18.508563
[0m19:01:18.518729 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m19:01:18.519738 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '424441e8-d86c-4634-83ce-50ac30a578e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A9F4FA3F0>]}
[0m19:01:18.520753 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.05s]
[0m19:01:18.521771 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:01:18.521771 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m19:01:18.523789 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m19:01:18.524798 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m19:01:18.524798 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m19:01:18.524798 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 19:01:18.524798 => 19:01:18.524798
[0m19:01:18.525807 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m19:01:18.562470 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m19:01:18.563489 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m19:01:18.566551 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m19:01:18.566551 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m19:01:18.567561 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 19:01:18.525807 => 19:01:18.567561
[0m19:01:18.571603 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m19:01:18.572612 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '424441e8-d86c-4634-83ce-50ac30a578e3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AA07E28D0>]}
[0m19:01:18.573619 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.05s]
[0m19:01:18.574629 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m19:01:18.574629 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:01:18.575730 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m19:01:18.576732 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:01:18.576732 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:01:18.577751 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m19:01:18.577751 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:01:18.578763 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m19:01:18.579785 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m19:01:18.580800 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:01:18.580800 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:01:18.581813 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m19:01:18.581813 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:01:18.582827 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:01:18.582827 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m19:01:18.583838 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:01:18.584850 [debug] [MainThread]: On master: ROLLBACK
[0m19:01:18.585861 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:01:18.585861 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:01:18.585861 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:01:18.586870 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:01:18.586870 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:01:18.587883 [info ] [MainThread]: 
[0m19:01:18.587883 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m19:01:18.588892 [debug] [MainThread]: Command end result
[0m19:01:18.598020 [info ] [MainThread]: 
[0m19:01:18.599023 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m19:01:18.599528 [info ] [MainThread]: 
[0m19:01:18.599528 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m19:01:18.600038 [info ] [MainThread]: 
[0m19:01:18.600545 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Notebook does not appear to be JSON: '{\n "cells": [\n  {\n   "cell_type": "m...
[0m19:01:18.601058 [info ] [MainThread]: 
[0m19:01:18.601058 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m19:01:18.602075 [debug] [MainThread]: Command `cli build` failed at 19:01:18.602075 after 0.37 seconds
[0m19:01:18.603090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AA058B860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A9EE24A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022A9F4E4B60>]}
[0m19:01:18.603606 [debug] [MainThread]: Flushing usage events
[0m19:01:24.641757 [debug] [MainThread]: Error sending anonymous usage statistics. Disabling tracking for this execution. If you wish to permanently disable tracking, see: https://docs.getdbt.com/reference/global-configs#send-anonymous-usage-stats.
[0m19:01:38.879114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B730C1490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B730C0140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B730C0950>]}


============================== 19:01:38.882147 | f4de06fe-59db-4f0f-bcef-9d05d915dbb8 ==============================
[0m19:01:38.882147 [info ] [MainThread]: Running with dbt=1.7.14
[0m19:01:38.885256 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:01:39.195762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f4de06fe-59db-4f0f-bcef-9d05d915dbb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B73051EE0>]}
[0m19:01:39.322429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f4de06fe-59db-4f0f-bcef-9d05d915dbb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B7308FE30>]}
[0m19:01:39.325456 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:01:39.357928 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m19:01:39.464449 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:01:39.466471 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:01:39.485703 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f4de06fe-59db-4f0f-bcef-9d05d915dbb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B732E4950>]}
[0m19:01:39.501344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f4de06fe-59db-4f0f-bcef-9d05d915dbb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B73346C90>]}
[0m19:01:39.502873 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m19:01:39.509475 [info ] [MainThread]: 
[0m19:01:39.512031 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:01:39.519172 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:01:39.569440 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:01:39.637551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4de06fe-59db-4f0f-bcef-9d05d915dbb8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B72766D20>]}
[0m19:01:39.639571 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:01:39.640581 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:01:39.643615 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:01:39.645638 [info ] [MainThread]: 
[0m19:01:39.654837 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m19:01:39.657864 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m19:01:39.660899 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:01:39.661910 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:01:39.698900 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:01:39.700928 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:01:39.662944 => 19:01:39.699916
[0m19:01:39.702948 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:01:39.843624 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:01:39.844635 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:01:39.846668 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:01:39.847685 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m19:07:04.678855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002038A2A0260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002038A2A1C40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002038A2A0950>]}


============================== 19:07:04.682913 | 7a94c095-7bd4-4275-9cd4-627bce12bb47 ==============================
[0m19:07:04.682913 [info ] [MainThread]: Running with dbt=1.7.14
[0m19:07:04.686051 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:07:04.988093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7a94c095-7bd4-4275-9cd4-627bce12bb47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002038A3BFEC0>]}
[0m19:07:05.123429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7a94c095-7bd4-4275-9cd4-627bce12bb47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002038A1DB2C0>]}
[0m19:07:05.129661 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:07:05.175741 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m19:07:06.170602 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:07:06.172636 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:07:06.192006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7a94c095-7bd4-4275-9cd4-627bce12bb47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002038A4938F0>]}
[0m19:07:06.220518 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7a94c095-7bd4-4275-9cd4-627bce12bb47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002038A4C60F0>]}
[0m19:07:06.222558 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m19:07:06.229648 [info ] [MainThread]: 
[0m19:07:06.232188 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:07:06.237277 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:07:06.292095 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m19:07:06.371384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7a94c095-7bd4-4275-9cd4-627bce12bb47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002038A4C7A10>]}
[0m19:07:06.373410 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:07:06.374426 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:07:06.376453 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:07:06.378472 [info ] [MainThread]: 
[0m19:07:06.387677 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m19:07:06.389702 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m19:07:06.392746 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:07:06.394766 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:07:06.433372 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:07:06.435394 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:07:06.395774 => 19:07:06.435394
[0m19:07:06.437424 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:07:06.584124 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:07:06.587201 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:07:06.589254 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:07:06.591280 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:29:46.263063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002756A65EB70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002756A65DBE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002756A65DE80>]}


============================== 20:29:46.265080 | f48c96fc-743a-44e2-8b65-4657ee451368 ==============================
[0m20:29:46.265080 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:29:46.268129 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m20:29:46.548188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f48c96fc-743a-44e2-8b65-4657ee451368', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002756A780620>]}
[0m20:29:46.666625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f48c96fc-743a-44e2-8b65-4657ee451368', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002756A81FA10>]}
[0m20:29:46.670156 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:29:46.705085 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:29:47.415058 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:29:47.417079 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:29:47.434746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f48c96fc-743a-44e2-8b65-4657ee451368', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002756A7E80B0>]}
[0m20:29:47.451941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f48c96fc-743a-44e2-8b65-4657ee451368', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002756A8E78C0>]}
[0m20:29:47.452950 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:29:47.459016 [info ] [MainThread]: 
[0m20:29:47.462042 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:29:47.467091 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:29:47.521889 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:29:47.593757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f48c96fc-743a-44e2-8b65-4657ee451368', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002756A7E8FB0>]}
[0m20:29:47.595778 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:29:47.597805 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:29:47.599829 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:29:47.601844 [info ] [MainThread]: 
[0m20:29:47.608944 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m20:29:47.611990 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:29:47.615024 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:29:47.618083 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:29:47.655780 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:29:47.659828 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:29:47.619108 => 20:29:47.657801
[0m20:29:47.660842 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:29:47.792217 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:29:47.794237 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:29:47.795249 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:29:47.797268 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:29:47.853550 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:29:47.855572 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: ModelNotebook.GetSparkSqlCell() takes 1 positional argument but 3 were given
[0m20:29:47.857596 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:29:47.661853 => 20:29:47.856582
[0m20:29:47.927483 [debug] [Thread-6 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  ModelNotebook.GetSparkSqlCell() takes 1 positional argument but 3 were given
[0m20:29:47.929514 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f48c96fc-743a-44e2-8b65-4657ee451368', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002756AA79550>]}
[0m20:29:47.931530 [error] [Thread-6 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.32s]
[0m20:29:47.933543 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:29:47.935558 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m20:29:47.937575 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m20:29:47.940606 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m20:29:47.941623 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m20:29:47.944752 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 20:29:47.943747 => 20:29:47.943747
[0m20:29:47.945774 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m20:29:48.060288 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:29:48.061298 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:29:48.064322 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:29:48.066357 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: ModelNotebook.GetSparkSqlCell() takes 1 positional argument but 3 were given
[0m20:29:48.067367 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 20:29:47.947797 => 20:29:48.067367
[0m20:29:48.073414 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  ModelNotebook.GetSparkSqlCell() takes 1 positional argument but 3 were given
[0m20:29:48.074423 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f48c96fc-743a-44e2-8b65-4657ee451368', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002756AA78350>]}
[0m20:29:48.076444 [error] [Thread-6 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.14s]
[0m20:29:48.078477 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m20:29:48.080655 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:29:48.082666 [info ] [Thread-6 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m20:29:48.084689 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:29:48.086714 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:29:48.087722 [info ] [Thread-6 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m20:29:48.089741 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:29:48.091763 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m20:29:48.092773 [info ] [Thread-6 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m20:29:48.094792 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:29:48.096828 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:29:48.097837 [info ] [Thread-6 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m20:29:48.099853 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:29:48.100862 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:29:48.101870 [info ] [Thread-6 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m20:29:48.103890 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:29:48.107942 [debug] [MainThread]: On master: ROLLBACK
[0m20:29:48.109974 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:29:48.113080 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:29:48.115120 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:29:48.116146 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:29:48.118157 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:29:48.120194 [info ] [MainThread]: 
[0m20:29:48.122230 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.66 seconds (0.66s).
[0m20:29:48.125268 [debug] [MainThread]: Command end result
[0m20:29:48.138490 [info ] [MainThread]: 
[0m20:29:48.140509 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m20:29:48.142599 [info ] [MainThread]: 
[0m20:29:48.143611 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  ModelNotebook.GetSparkSqlCell() takes 1 positional argument but 3 were given
[0m20:29:48.146143 [info ] [MainThread]: 
[0m20:29:48.147669 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  ModelNotebook.GetSparkSqlCell() takes 1 positional argument but 3 were given
[0m20:29:48.149196 [info ] [MainThread]: 
[0m20:29:48.150729 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m20:29:48.154311 [debug] [MainThread]: Command `cli build` failed at 20:29:48.153293 after 1.99 seconds
[0m20:29:48.156346 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002756A7E8950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027569825520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002756AA8ABD0>]}
[0m20:29:48.158412 [debug] [MainThread]: Flushing usage events
[0m20:30:31.548077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B423D5C10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B423D49B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B423D4B00>]}


============================== 20:30:31.551111 | 7f25732e-6fb4-4af2-a563-6671ffec2b64 ==============================
[0m20:30:31.551111 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:30:31.552617 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m20:30:31.824514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7f25732e-6fb4-4af2-a563-6671ffec2b64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B4245FE90>]}
[0m20:30:31.944992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7f25732e-6fb4-4af2-a563-6671ffec2b64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B4258E150>]}
[0m20:30:31.948079 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:30:31.979576 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:30:32.081581 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:30:32.083597 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:30:32.100765 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7f25732e-6fb4-4af2-a563-6671ffec2b64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B423A8440>]}
[0m20:30:32.118962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7f25732e-6fb4-4af2-a563-6671ffec2b64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B42650050>]}
[0m20:30:32.120482 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:30:32.126609 [info ] [MainThread]: 
[0m20:30:32.129648 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:30:32.134707 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:30:32.183913 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:30:32.254473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7f25732e-6fb4-4af2-a563-6671ffec2b64', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B4245F7A0>]}
[0m20:30:32.255484 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:30:32.257515 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:30:32.260631 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:30:32.262652 [info ] [MainThread]: 
[0m20:30:32.271785 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m20:30:32.275891 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:30:32.279976 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:30:32.283057 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:30:32.321695 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:30:32.323733 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:30:32.285086 => 20:30:32.323733
[0m20:30:32.325757 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:30:32.456910 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:30:32.458938 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:30:32.459950 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:30:32.461967 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:35:13.069415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CF760BB90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CF760AB10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CF760B080>]}


============================== 20:35:13.071440 | 313dbc00-739e-437f-80aa-f8cccf65ce86 ==============================
[0m20:35:13.071440 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:35:13.073513 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:35:13.346715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '313dbc00-739e-437f-80aa-f8cccf65ce86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CF76BAF30>]}
[0m20:35:13.467486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '313dbc00-739e-437f-80aa-f8cccf65ce86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CF75C7530>]}
[0m20:35:13.470518 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:35:13.500873 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:35:13.605041 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:35:13.607087 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:35:13.627352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '313dbc00-739e-437f-80aa-f8cccf65ce86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CF7559100>]}
[0m20:35:13.644054 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '313dbc00-739e-437f-80aa-f8cccf65ce86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CF78B75F0>]}
[0m20:35:13.645066 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:35:13.653172 [info ] [MainThread]: 
[0m20:35:13.655189 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:35:13.660235 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:35:13.710318 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:35:13.782353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '313dbc00-739e-437f-80aa-f8cccf65ce86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CF78B5790>]}
[0m20:35:13.784384 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:35:13.785400 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:35:13.787420 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:35:13.789459 [info ] [MainThread]: 
[0m20:35:13.795544 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m20:35:13.797568 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:35:13.799589 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:35:13.801609 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:35:13.838111 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:35:13.841162 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:35:13.802616 => 20:35:13.840147
[0m20:35:13.842197 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:35:13.979118 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:35:13.981276 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:35:13.983290 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:35:13.985314 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:35:14.025794 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:35:14.027811 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: 'ModelNotebook' object has no attribute 'GetSparkSqlCell'
[0m20:35:14.029828 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:35:13.844286 => 20:35:14.028819
[0m20:35:14.038953 [debug] [Thread-6 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'ModelNotebook' object has no attribute 'GetSparkSqlCell'
[0m20:35:14.039961 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '313dbc00-739e-437f-80aa-f8cccf65ce86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CF7A44BC0>]}
[0m20:35:14.043002 [error] [Thread-6 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.24s]
[0m20:35:14.045032 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:35:14.048081 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m20:35:14.050125 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m20:35:14.053192 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m20:35:14.054201 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m20:35:14.056227 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 20:35:14.055216 => 20:35:14.055216
[0m20:35:14.057237 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m20:35:14.174751 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:35:14.176771 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:35:14.179832 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:35:14.181869 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: 'ModelNotebook' object has no attribute 'GetSparkSqlCell'
[0m20:35:14.183890 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 20:35:14.058244 => 20:35:14.182877
[0m20:35:14.189950 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  'ModelNotebook' object has no attribute 'GetSparkSqlCell'
[0m20:35:14.190958 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '313dbc00-739e-437f-80aa-f8cccf65ce86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CF7A44E00>]}
[0m20:35:14.192974 [error] [Thread-6 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.14s]
[0m20:35:14.194994 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m20:35:14.196003 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:35:14.198018 [info ] [Thread-6 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m20:35:14.200036 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:35:14.201048 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:35:14.203110 [info ] [Thread-6 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m20:35:14.204120 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:35:14.206143 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m20:35:14.208159 [info ] [Thread-6 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m20:35:14.211208 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:35:14.213245 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:35:14.215279 [info ] [Thread-6 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m20:35:14.217388 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:35:14.220427 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:35:14.223467 [info ] [Thread-6 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m20:35:14.224477 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:35:14.228528 [debug] [MainThread]: On master: ROLLBACK
[0m20:35:14.230551 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:35:14.233612 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:35:14.235665 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:35:14.236677 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:35:14.238704 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:35:14.240725 [info ] [MainThread]: 
[0m20:35:14.242747 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.59 seconds (0.59s).
[0m20:35:14.245807 [debug] [MainThread]: Command end result
[0m20:35:14.262181 [info ] [MainThread]: 
[0m20:35:14.264263 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m20:35:14.266317 [info ] [MainThread]: 
[0m20:35:14.267880 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'ModelNotebook' object has no attribute 'GetSparkSqlCell'
[0m20:35:14.269429 [info ] [MainThread]: 
[0m20:35:14.271473 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'ModelNotebook' object has no attribute 'GetSparkSqlCell'
[0m20:35:14.273515 [info ] [MainThread]: 
[0m20:35:14.274547 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m20:35:14.278728 [debug] [MainThread]: Command `cli build` failed at 20:35:14.277619 after 1.30 seconds
[0m20:35:14.281775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CF74C2240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CF781F950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027CF781E600>]}
[0m20:35:14.285019 [debug] [MainThread]: Flushing usage events
[0m20:35:27.862913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3EF64470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3EF65040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3EF658B0>]}


============================== 20:35:27.865938 | dd6c939f-45b7-4517-a92f-cd53f1b13876 ==============================
[0m20:35:27.865938 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:35:27.866946 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:35:28.144482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'dd6c939f-45b7-4517-a92f-cd53f1b13876', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3E668E60>]}
[0m20:35:28.264381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'dd6c939f-45b7-4517-a92f-cd53f1b13876', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3EF66270>]}
[0m20:35:28.267407 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:35:28.297773 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:35:28.400624 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:35:28.401633 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:35:28.420845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dd6c939f-45b7-4517-a92f-cd53f1b13876', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3EFAF560>]}
[0m20:35:28.436056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dd6c939f-45b7-4517-a92f-cd53f1b13876', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3F1E70B0>]}
[0m20:35:28.438073 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:35:28.445171 [info ] [MainThread]: 
[0m20:35:28.449218 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:35:28.456303 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:35:28.503925 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:35:28.570774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dd6c939f-45b7-4517-a92f-cd53f1b13876', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3F1E5370>]}
[0m20:35:28.571783 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:35:28.573806 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:35:28.575889 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:35:28.577904 [info ] [MainThread]: 
[0m20:35:28.587049 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m20:35:28.589075 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:35:28.592131 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:35:28.593148 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:35:28.630576 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:35:28.633601 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:35:28.594163 => 20:35:28.632591
[0m20:35:28.634607 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:35:28.769644 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:35:28.771660 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:35:28.772668 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:35:28.774698 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:35:59.762902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001446C540170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001446D08A150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001446D0881A0>]}


============================== 20:35:59.764918 | d940e18a-dec0-444d-8ef7-b1ef1b752168 ==============================
[0m20:35:59.764918 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:35:59.766933 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:36:00.041893 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd940e18a-dec0-444d-8ef7-b1ef1b752168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001446D3D7410>]}
[0m20:36:00.161288 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd940e18a-dec0-444d-8ef7-b1ef1b752168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001446D322CC0>]}
[0m20:36:00.164312 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:36:00.194762 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:36:00.302698 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:36:00.303706 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:36:00.322916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd940e18a-dec0-444d-8ef7-b1ef1b752168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001446D362C90>]}
[0m20:36:00.338575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd940e18a-dec0-444d-8ef7-b1ef1b752168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001446D5D3590>]}
[0m20:36:00.340104 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:36:00.347772 [info ] [MainThread]: 
[0m20:36:00.350800 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:36:00.355848 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:36:00.403403 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:36:00.470869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd940e18a-dec0-444d-8ef7-b1ef1b752168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001446D5D17F0>]}
[0m20:36:00.472895 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:36:00.474919 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:36:00.476948 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:36:00.479066 [info ] [MainThread]: 
[0m20:36:00.487159 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m20:36:00.489182 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:36:00.492260 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:36:00.494286 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:36:00.531965 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:36:00.533984 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:36:00.495312 => 20:36:00.532974
[0m20:36:00.534992 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:36:00.667979 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:36:00.668985 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:36:00.669993 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:36:00.672007 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:36:22.212109 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:36:22.214162 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: ModelNotebook.AddCell() missing 1 required positional argument: 'cell_source'
[0m20:36:22.218242 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:36:00.536003 => 20:36:22.217232
[0m20:36:22.226806 [debug] [Thread-6 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  ModelNotebook.AddCell() missing 1 required positional argument: 'cell_source'
[0m20:36:22.228824 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd940e18a-dec0-444d-8ef7-b1ef1b752168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001446D7A3740>]}
[0m20:36:22.230845 [error] [Thread-6 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 21.74s]
[0m20:36:22.233896 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:36:22.235949 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m20:36:22.238987 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m20:36:22.241007 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m20:36:22.243029 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m20:36:22.245060 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 20:36:22.244047 => 20:36:22.244047
[0m20:36:22.247128 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m20:36:22.364288 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:36:22.365296 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:36:46.973019 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:36:46.975056 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: ModelNotebook.AddCell() missing 1 required positional argument: 'cell_source'
[0m20:36:46.977094 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 20:36:22.249163 => 20:36:46.976076
[0m20:36:46.984267 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  ModelNotebook.AddCell() missing 1 required positional argument: 'cell_source'
[0m20:36:46.985280 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd940e18a-dec0-444d-8ef7-b1ef1b752168', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001446D5F3440>]}
[0m20:36:46.987321 [error] [Thread-6 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 24.75s]
[0m20:36:46.989359 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m20:36:46.991433 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:36:46.992449 [info ] [Thread-6 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m20:36:46.994479 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:36:46.996512 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:36:46.997534 [info ] [Thread-6 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m20:36:47.001679 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:36:47.003712 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m20:36:47.005732 [info ] [Thread-6 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m20:36:47.006743 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:36:47.008768 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:36:47.011818 [info ] [Thread-6 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m20:36:47.013871 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:36:47.016991 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:36:47.019034 [info ] [Thread-6 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m20:36:47.022077 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:36:47.026177 [debug] [MainThread]: On master: ROLLBACK
[0m20:36:47.028213 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:36:47.030275 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:36:47.031287 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:36:47.033329 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:36:47.035366 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:36:47.038409 [info ] [MainThread]: 
[0m20:36:47.039917 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 46.69 seconds (46.69s).
[0m20:36:47.044066 [debug] [MainThread]: Command end result
[0m20:36:47.060955 [info ] [MainThread]: 
[0m20:36:47.063009 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m20:36:47.066133 [info ] [MainThread]: 
[0m20:36:47.068706 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  ModelNotebook.AddCell() missing 1 required positional argument: 'cell_source'
[0m20:36:47.070749 [info ] [MainThread]: 
[0m20:36:47.072781 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  ModelNotebook.AddCell() missing 1 required positional argument: 'cell_source'
[0m20:36:47.074819 [info ] [MainThread]: 
[0m20:36:47.075845 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m20:36:47.081956 [debug] [MainThread]: Command `cli build` failed at 20:36:47.080935 after 47.41 seconds
[0m20:36:47.086069 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001446CCEDE80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001446D5F2750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001446D5F3BC0>]}
[0m20:36:47.088089 [debug] [MainThread]: Flushing usage events
[0m20:38:40.526063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029CE45BA0C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029CE45B88F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029CE45B8D70>]}


============================== 20:38:40.527065 | ac247d63-4e04-4237-b88a-a6f3f2b0a885 ==============================
[0m20:38:40.527065 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:38:40.527567 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:38:40.611477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ac247d63-4e04-4237-b88a-a6f3f2b0a885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029CE32D6540>]}
[0m20:38:40.647991 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ac247d63-4e04-4237-b88a-a6f3f2b0a885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029CE2F3E4B0>]}
[0m20:38:40.650008 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:38:40.665211 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:38:41.438196 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:38:41.439203 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:38:41.443254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ac247d63-4e04-4237-b88a-a6f3f2b0a885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029CE4639E20>]}
[0m20:38:41.466575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ac247d63-4e04-4237-b88a-a6f3f2b0a885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029CE477D6D0>]}
[0m20:38:41.466575 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:38:41.468595 [info ] [MainThread]: 
[0m20:38:41.469609 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:38:41.469609 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:38:41.481810 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:38:41.503033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ac247d63-4e04-4237-b88a-a6f3f2b0a885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029CE47E2C00>]}
[0m20:38:41.504040 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:38:41.504040 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:38:41.504040 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:38:41.505047 [info ] [MainThread]: 
[0m20:38:41.506552 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m20:38:41.506552 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:38:41.507568 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:38:41.508594 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:38:41.514686 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:38:41.515712 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:38:41.508594 => 20:38:41.515712
[0m20:38:41.516732 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:38:41.554198 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:38:41.555206 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:38:41.555206 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:38:41.555206 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:38:41.626206 [debug] [Thread-1 (]: SQL status: OK in 0.07000000029802322 seconds
[0m20:38:41.651490 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m20:38:41.652499 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:38:41.652499 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m20:38:41.664624 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:38:41.673702 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:38:41.516732 => 20:38:41.672694
[0m20:38:41.673702 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ac247d63-4e04-4237-b88a-a6f3f2b0a885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029CE47A0DD0>]}
[0m20:38:41.674712 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.17s]
[0m20:38:41.675746 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:38:41.675746 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m20:38:41.676759 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m20:38:41.676759 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m20:38:41.677777 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m20:38:41.677777 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 20:38:41.677777 => 20:38:41.677777
[0m20:38:41.678788 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m20:38:41.704053 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:38:41.704053 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:38:41.709118 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:38:41.718219 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m20:38:41.721257 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:38:41.721257 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m20:38:41.734874 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:38:41.737914 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m20:38:41.742973 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:38:41.745038 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 20:38:41.678788 => 20:38:41.745038
[0m20:38:41.746055 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ac247d63-4e04-4237-b88a-a6f3f2b0a885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029CE492B320>]}
[0m20:38:41.746055 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.07s]
[0m20:38:41.748089 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m20:38:41.748089 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:38:41.749099 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m20:38:41.750114 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m20:38:41.750114 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:38:41.757702 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:38:41.759736 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 20:38:41.750114 => 20:38:41.759736
[0m20:38:41.759736 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:38:41.768860 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:38:41.769870 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:38:41.770883 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:38:41.774923 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:38:41.777974 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 20:38:41.760747 => 20:38:41.776953
[0m20:38:41.778984 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m20:38:41.781005 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:38:41.782130 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:38:41.783133 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m20:38:41.784150 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m20:38:41.784150 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:38:41.793282 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:38:41.794295 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 20:38:41.784150 => 20:38:41.794295
[0m20:38:41.794295 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:38:41.796318 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:38:41.798447 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:38:41.798447 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:38:41.803505 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:38:41.805526 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 20:38:41.795307 => 20:38:41.805526
[0m20:38:41.806533 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m20:38:41.807545 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:38:41.808558 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m20:38:41.808558 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m20:38:41.809570 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m20:38:41.810583 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m20:38:41.812622 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m20:38:41.813667 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 20:38:41.810583 => 20:38:41.813667
[0m20:38:41.814678 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m20:38:41.825894 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m20:38:41.826909 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m20:38:41.827417 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m20:38:41.831435 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:38:41.833499 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 20:38:41.814678 => 20:38:41.833499
[0m20:38:41.834532 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ac247d63-4e04-4237-b88a-a6f3f2b0a885', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029CE4A51790>]}
[0m20:38:41.835051 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m20:38:41.836080 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:38:41.837114 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:38:41.837624 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m20:38:41.838646 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m20:38:41.839155 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:38:41.843731 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:38:41.845284 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 20:38:41.839155 => 20:38:41.844773
[0m20:38:41.845792 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:38:41.847825 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:38:41.848840 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:38:41.849348 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:38:41.853942 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:38:41.855469 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 20:38:41.845792 => 20:38:41.854962
[0m20:38:41.857010 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.02s]
[0m20:38:41.858038 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:38:41.858548 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:38:41.859058 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m20:38:41.859580 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m20:38:41.860091 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:38:41.863142 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:38:41.864155 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 20:38:41.860602 => 20:38:41.863649
[0m20:38:41.864155 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:38:41.865682 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:38:41.866695 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:38:41.867204 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:38:41.870245 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:38:41.871259 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 20:38:41.864664 => 20:38:41.871259
[0m20:38:41.871766 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m20:38:41.872272 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:38:41.873292 [debug] [MainThread]: On master: ROLLBACK
[0m20:38:41.873808 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:38:41.873808 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:38:41.874317 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:38:41.874869 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:38:41.875381 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:38:41.875897 [info ] [MainThread]: 
[0m20:38:41.876403 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.41 seconds (0.41s).
[0m20:38:41.877418 [debug] [MainThread]: Command end result
[0m20:38:41.887594 [info ] [MainThread]: 
[0m20:38:41.888103 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:38:41.888611 [info ] [MainThread]: 
[0m20:38:41.889117 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m20:38:41.890697 [debug] [MainThread]: Command `cli build` succeeded at 20:38:41.890184 after 1.41 seconds
[0m20:38:41.890697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029CE330F350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029CE3502C30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029CE45CBDA0>]}
[0m20:38:41.891205 [debug] [MainThread]: Flushing usage events
[0m20:41:21.917291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C157A72F00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C1576E4F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C157A71730>]}


============================== 20:41:21.920342 | 3485d624-aff1-4877-b168-3617449bc78f ==============================
[0m20:41:21.920342 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:41:21.922359 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:41:22.206021 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3485d624-aff1-4877-b168-3617449bc78f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C157B398E0>]}
[0m20:41:22.325472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3485d624-aff1-4877-b168-3617449bc78f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C15768B140>]}
[0m20:41:22.328504 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:41:22.355856 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:41:22.464248 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:41:22.465258 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:41:22.487752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3485d624-aff1-4877-b168-3617449bc78f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C157CAA2D0>]}
[0m20:41:22.505982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3485d624-aff1-4877-b168-3617449bc78f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C157CFD370>]}
[0m20:41:22.507516 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:41:22.518311 [info ] [MainThread]: 
[0m20:41:22.522429 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:41:22.529525 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:41:22.578135 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:41:22.649567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3485d624-aff1-4877-b168-3617449bc78f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C157CFF5F0>]}
[0m20:41:22.651606 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:41:22.653633 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:41:22.656145 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:41:22.657153 [info ] [MainThread]: 
[0m20:41:22.665253 [debug] [Thread-7 (]: Began running node model.testproj.my_first_dbt_model
[0m20:41:22.666262 [info ] [Thread-7 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:41:22.669283 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:41:22.671301 [debug] [Thread-7 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:41:22.708877 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:41:22.711939 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:41:22.672313 => 20:41:22.710912
[0m20:41:22.713982 [debug] [Thread-7 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:41:22.851799 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:41:22.853820 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:41:22.855832 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:41:22.857346 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m20:41:29.323922 [debug] [Thread-7 (]: SQL status: OK in 6.46999979019165 seconds
[0m20:41:29.425023 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m20:41:29.428057 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:41:29.429066 [debug] [Thread-7 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m20:41:29.457620 [debug] [Thread-7 (]: SQL status: OK in 0.029999999329447746 seconds
[0m20:41:29.525875 [debug] [Thread-7 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:41:22.716029 => 20:41:29.524839
[0m20:41:29.532030 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3485d624-aff1-4877-b168-3617449bc78f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C157E9D610>]}
[0m20:41:29.535106 [info ] [Thread-7 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 6.86s]
[0m20:41:29.539236 [debug] [Thread-7 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:41:29.543318 [debug] [Thread-7 (]: Began running node seed.testproj.sample
[0m20:41:29.549469 [info ] [Thread-7 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m20:41:29.556755 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m20:41:29.560852 [debug] [Thread-7 (]: Began compiling node seed.testproj.sample
[0m20:41:29.564395 [debug] [Thread-7 (]: Timing info for seed.testproj.sample (compile): 20:41:29.563376 => 20:41:29.563376
[0m20:41:29.567454 [debug] [Thread-7 (]: Began executing node seed.testproj.sample
[0m20:41:29.699055 [debug] [Thread-7 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:41:29.701085 [debug] [Thread-7 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:41:29.723590 [debug] [Thread-7 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:41:29.762171 [debug] [Thread-7 (]: Inserting batches of 500 records
[0m20:41:29.773329 [debug] [Thread-7 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:41:29.774342 [debug] [Thread-7 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m20:41:29.810915 [debug] [Thread-7 (]: SQL status: OK in 0.029999999329447746 seconds
[0m20:41:29.826101 [debug] [Thread-7 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m20:41:29.840244 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:41:29.843280 [debug] [Thread-7 (]: Timing info for seed.testproj.sample (execute): 20:41:29.569539 => 20:41:29.843280
[0m20:41:29.846440 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3485d624-aff1-4877-b168-3617449bc78f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C1580F8EF0>]}
[0m20:41:29.848501 [info ] [Thread-7 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.29s]
[0m20:41:29.850532 [debug] [Thread-7 (]: Finished running node seed.testproj.sample
[0m20:41:29.852550 [debug] [Thread-7 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:41:29.853558 [info ] [Thread-7 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m20:41:29.856069 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m20:41:29.858086 [debug] [Thread-7 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:41:29.892639 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:41:29.895666 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 20:41:29.858086 => 20:41:29.894654
[0m20:41:29.897694 [debug] [Thread-7 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:41:29.939256 [debug] [Thread-7 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:41:29.941272 [debug] [Thread-7 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:41:29.943295 [debug] [Thread-7 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:41:29.965195 [debug] [Thread-7 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:41:29.975366 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 20:41:29.898713 => 20:41:29.973339
[0m20:41:29.978427 [info ] [Thread-7 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.12s]
[0m20:41:29.982605 [debug] [Thread-7 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:41:29.985637 [debug] [Thread-7 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:41:29.987677 [info ] [Thread-7 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m20:41:29.989702 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m20:41:29.991720 [debug] [Thread-7 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:41:30.010985 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:41:30.016138 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 20:41:29.992730 => 20:41:30.014091
[0m20:41:30.017155 [debug] [Thread-7 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:41:30.027298 [debug] [Thread-7 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:41:30.029331 [debug] [Thread-7 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:41:30.031344 [debug] [Thread-7 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:41:30.049166 [debug] [Thread-7 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:41:30.054240 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 20:41:30.019192 => 20:41:30.054240
[0m20:41:30.057267 [info ] [Thread-7 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.07s]
[0m20:41:30.059292 [debug] [Thread-7 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:41:30.061860 [debug] [Thread-7 (]: Began running node model.testproj.my_second_dbt_model
[0m20:41:30.063379 [info ] [Thread-7 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m20:41:30.065408 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m20:41:30.067426 [debug] [Thread-7 (]: Began compiling node model.testproj.my_second_dbt_model
[0m20:41:30.076601 [debug] [Thread-7 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m20:41:30.079644 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (compile): 20:41:30.068438 => 20:41:30.078635
[0m20:41:30.081713 [debug] [Thread-7 (]: Began executing node model.testproj.my_second_dbt_model
[0m20:41:30.124910 [debug] [Thread-7 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m20:41:30.126928 [debug] [Thread-7 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m20:41:30.127941 [debug] [Thread-7 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m20:41:30.146161 [debug] [Thread-7 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:41:30.152225 [debug] [Thread-7 (]: Timing info for model.testproj.my_second_dbt_model (execute): 20:41:30.082735 => 20:41:30.152225
[0m20:41:30.155252 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3485d624-aff1-4877-b168-3617449bc78f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C15925A690>]}
[0m20:41:30.156261 [info ] [Thread-7 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.09s]
[0m20:41:30.158277 [debug] [Thread-7 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:41:30.160293 [debug] [Thread-7 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:41:30.162322 [info ] [Thread-7 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m20:41:30.164350 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m20:41:30.166370 [debug] [Thread-7 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:41:30.179543 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:41:30.182599 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 20:41:30.167395 => 20:41:30.181569
[0m20:41:30.183611 [debug] [Thread-7 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:41:30.191685 [debug] [Thread-7 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:41:30.194716 [debug] [Thread-7 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:41:30.195727 [debug] [Thread-7 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:41:30.216046 [debug] [Thread-7 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:41:30.221100 [debug] [Thread-7 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 20:41:30.184619 => 20:41:30.221100
[0m20:41:30.224129 [info ] [Thread-7 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.06s]
[0m20:41:30.226149 [debug] [Thread-7 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:41:30.227159 [debug] [Thread-7 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:41:30.229192 [info ] [Thread-7 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m20:41:30.231205 [debug] [Thread-7 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m20:41:30.233221 [debug] [Thread-7 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:41:30.246387 [debug] [Thread-7 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:41:30.249456 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 20:41:30.233221 => 20:41:30.248429
[0m20:41:30.251499 [debug] [Thread-7 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:41:30.259606 [debug] [Thread-7 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:41:30.261635 [debug] [Thread-7 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:41:30.262646 [debug] [Thread-7 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:41:30.281974 [debug] [Thread-7 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:41:30.287051 [debug] [Thread-7 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 20:41:30.252510 => 20:41:30.286042
[0m20:41:30.289110 [info ] [Thread-7 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.06s]
[0m20:41:30.291132 [debug] [Thread-7 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:41:30.296206 [debug] [MainThread]: On master: ROLLBACK
[0m20:41:30.297216 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:41:30.299237 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:41:30.300247 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:41:30.301255 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:41:30.302265 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:41:30.305313 [info ] [MainThread]: 
[0m20:41:30.307340 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 7.78 seconds (7.78s).
[0m20:41:30.311400 [debug] [MainThread]: Command end result
[0m20:41:30.326659 [info ] [MainThread]: 
[0m20:41:30.328692 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:41:30.331745 [info ] [MainThread]: 
[0m20:41:30.332768 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m20:41:30.336847 [debug] [MainThread]: Command `cli build` succeeded at 20:41:30.336847 after 8.51 seconds
[0m20:41:30.338874 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C159221EE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C159222F00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C1592213D0>]}
[0m20:41:30.339884 [debug] [MainThread]: Flushing usage events
[0m20:42:19.036115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002932409DE20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000293240C5EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000293240C6A20>]}


============================== 20:42:19.038132 | 9f907a62-6eb8-4d7f-b9b3-e9e8c56e70a2 ==============================
[0m20:42:19.038132 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:42:19.040163 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:42:19.314807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9f907a62-6eb8-4d7f-b9b3-e9e8c56e70a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002932430FFE0>]}
[0m20:42:19.434059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9f907a62-6eb8-4d7f-b9b3-e9e8c56e70a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000293240EBFB0>]}
[0m20:42:19.436079 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:42:19.465429 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:42:19.567134 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:42:19.569147 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:42:19.587352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9f907a62-6eb8-4d7f-b9b3-e9e8c56e70a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029324249EE0>]}
[0m20:42:19.601974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9f907a62-6eb8-4d7f-b9b3-e9e8c56e70a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000293243EACC0>]}
[0m20:42:19.603992 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:42:19.611136 [info ] [MainThread]: 
[0m20:42:19.615193 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:42:19.620283 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:42:19.667820 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:42:19.736716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9f907a62-6eb8-4d7f-b9b3-e9e8c56e70a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002932190A330>]}
[0m20:42:19.737732 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:42:19.739773 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:42:19.741798 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:42:19.743836 [info ] [MainThread]: 
[0m20:42:19.751955 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m20:42:19.753998 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:42:19.757040 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:42:19.759053 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:42:19.797825 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:42:19.800341 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:42:19.760068 => 20:42:19.798917
[0m20:42:19.801349 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:42:19.936003 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:42:19.938034 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:42:19.939044 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:42:19.941067 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:42:58.387032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C34493E5A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C34493ED80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C34493F320>]}


============================== 20:42:58.389049 | 004d380d-7a52-41ad-8340-074042343246 ==============================
[0m20:42:58.389049 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:42:58.391095 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m20:42:58.660016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '004d380d-7a52-41ad-8340-074042343246', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C3449F7530>]}
[0m20:42:58.779182 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '004d380d-7a52-41ad-8340-074042343246', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C344A8C230>]}
[0m20:42:58.782223 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:42:58.810606 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:42:58.908502 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:42:58.910553 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:42:58.929798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '004d380d-7a52-41ad-8340-074042343246', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C344AF9C40>]}
[0m20:42:58.947599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '004d380d-7a52-41ad-8340-074042343246', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C344BA5E80>]}
[0m20:42:58.948608 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:42:58.955169 [info ] [MainThread]: 
[0m20:42:58.958253 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:42:58.968544 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:42:59.018430 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:42:59.086534 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '004d380d-7a52-41ad-8340-074042343246', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C344BA5010>]}
[0m20:42:59.088551 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:42:59.089560 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:42:59.091577 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:42:59.092585 [info ] [MainThread]: 
[0m20:42:59.099690 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m20:42:59.101716 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:42:59.104761 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:42:59.106787 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:42:59.144904 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:42:59.149005 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:42:59.107797 => 20:42:59.147995
[0m20:42:59.150019 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:42:59.285003 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:42:59.287025 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:42:59.288036 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:42:59.290054 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:46:47.285996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019454454AD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019454454470>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194544573E0>]}


============================== 20:46:47.288011 | 6587efec-7557-4fb6-b00f-d7c3ccddb55f ==============================
[0m20:46:47.288011 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:46:47.290028 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m20:46:47.562662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6587efec-7557-4fb6-b00f-d7c3ccddb55f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019454587440>]}
[0m20:46:47.684721 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6587efec-7557-4fb6-b00f-d7c3ccddb55f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019453B494C0>]}
[0m20:46:47.686735 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:46:47.716069 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:46:47.811369 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:46:47.813407 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:46:47.831575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6587efec-7557-4fb6-b00f-d7c3ccddb55f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019453DFE270>]}
[0m20:46:47.849776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6587efec-7557-4fb6-b00f-d7c3ccddb55f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194546E7530>]}
[0m20:46:47.851795 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:46:47.858421 [info ] [MainThread]: 
[0m20:46:47.861969 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:46:47.867019 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:46:47.917201 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:46:47.986030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6587efec-7557-4fb6-b00f-d7c3ccddb55f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194544740E0>]}
[0m20:46:47.988049 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:46:47.989059 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:46:47.992190 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:46:47.993198 [info ] [MainThread]: 
[0m20:46:48.002293 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m20:46:48.004308 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:46:48.006342 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:46:48.009399 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:46:48.046848 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:46:48.049880 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:46:48.010415 => 20:46:48.048870
[0m20:46:48.051898 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:46:48.186380 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:46:48.188395 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:46:48.189402 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:46:48.191430 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:46:52.425242 [debug] [Thread-6 (]: SQL status: OK in 4.230000019073486 seconds
[0m20:46:52.525986 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m20:46:52.529030 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:46:52.531059 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m20:46:54.337397 [debug] [Thread-6 (]: SQL status: OK in 1.809999942779541 seconds
[0m20:46:54.377067 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:46:48.052940 => 20:46:54.376050
[0m20:46:54.380117 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6587efec-7557-4fb6-b00f-d7c3ccddb55f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019454477F20>]}
[0m20:46:54.382139 [info ] [Thread-6 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 6.37s]
[0m20:46:54.384227 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:46:54.386240 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m20:46:54.388259 [info ] [Thread-6 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m20:46:54.391288 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m20:46:54.392296 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m20:46:54.394314 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 20:46:54.393303 => 20:46:54.393303
[0m20:46:54.395322 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m20:46:54.513754 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:46:54.515785 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:46:56.619120 [debug] [Thread-6 (]: SQL status: OK in 2.0999999046325684 seconds
[0m20:46:56.658719 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m20:46:56.669897 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:46:56.671417 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m20:47:00.398806 [debug] [Thread-6 (]: SQL status: OK in 3.7300000190734863 seconds
[0m20:47:00.418118 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m20:47:00.435333 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:47:00.441431 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 20:46:54.396334 => 20:47:00.440418
[0m20:47:00.444511 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6587efec-7557-4fb6-b00f-d7c3ccddb55f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019455BA3A70>]}
[0m20:47:00.447574 [info ] [Thread-6 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 6.06s]
[0m20:47:00.450629 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m20:47:00.452665 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:47:00.454715 [info ] [Thread-6 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m20:47:00.456746 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m20:47:00.457776 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:47:00.495493 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:47:00.497573 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 20:47:00.458842 => 20:47:00.496557
[0m20:47:00.498587 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:47:00.540639 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:47:00.543689 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:47:00.545724 [debug] [Thread-6 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:47:00.563963 [debug] [Thread-6 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:47:00.571028 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 20:47:00.499597 => 20:47:00.570021
[0m20:47:00.573045 [info ] [Thread-6 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.12s]
[0m20:47:00.576082 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:47:00.578126 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:47:00.579136 [info ] [Thread-6 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m20:47:00.582205 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m20:47:00.584227 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:47:00.603453 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:47:00.605481 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 20:47:00.585237 => 20:47:00.604467
[0m20:47:00.607504 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:47:00.617752 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:47:00.620801 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:47:00.621821 [debug] [Thread-6 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:47:00.643237 [debug] [Thread-6 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:47:00.650349 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 20:47:00.608516 => 20:47:00.649334
[0m20:47:00.653395 [info ] [Thread-6 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.07s]
[0m20:47:00.656564 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:47:00.659599 [debug] [Thread-6 (]: Began running node model.testproj.my_second_dbt_model
[0m20:47:00.661621 [info ] [Thread-6 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m20:47:00.665688 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m20:47:00.667737 [debug] [Thread-6 (]: Began compiling node model.testproj.my_second_dbt_model
[0m20:47:00.676901 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m20:47:00.680956 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (compile): 20:47:00.668751 => 20:47:00.679941
[0m20:47:00.681964 [debug] [Thread-6 (]: Began executing node model.testproj.my_second_dbt_model
[0m20:47:00.727251 [debug] [Thread-6 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m20:47:00.729270 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m20:47:00.730281 [debug] [Thread-6 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m20:47:00.750866 [debug] [Thread-6 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:47:00.757952 [debug] [Thread-6 (]: Timing info for model.testproj.my_second_dbt_model (execute): 20:47:00.683988 => 20:47:00.756940
[0m20:47:00.759986 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6587efec-7557-4fb6-b00f-d7c3ccddb55f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019454A10680>]}
[0m20:47:00.762003 [info ] [Thread-6 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.10s]
[0m20:47:00.764035 [debug] [Thread-6 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:47:00.766055 [debug] [Thread-6 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:47:00.769101 [info ] [Thread-6 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m20:47:00.773163 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m20:47:00.775210 [debug] [Thread-6 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:47:00.792586 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:47:00.794722 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 20:47:00.777264 => 20:47:00.793623
[0m20:47:00.796753 [debug] [Thread-6 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:47:00.805888 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:47:00.810086 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:47:00.812216 [debug] [Thread-6 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:47:00.831627 [debug] [Thread-6 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:47:00.838746 [debug] [Thread-6 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 20:47:00.797763 => 20:47:00.837729
[0m20:47:00.841819 [info ] [Thread-6 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.07s]
[0m20:47:00.844879 [debug] [Thread-6 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:47:00.845911 [debug] [Thread-6 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:47:00.847934 [info ] [Thread-6 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m20:47:00.853051 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m20:47:00.854066 [debug] [Thread-6 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:47:00.867259 [debug] [Thread-6 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:47:00.869283 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 20:47:00.855159 => 20:47:00.869283
[0m20:47:00.871317 [debug] [Thread-6 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:47:00.880017 [debug] [Thread-6 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:47:00.882549 [debug] [Thread-6 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:47:00.883559 [debug] [Thread-6 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:47:00.901832 [debug] [Thread-6 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:47:00.907894 [debug] [Thread-6 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 20:47:00.871317 => 20:47:00.906885
[0m20:47:00.909915 [info ] [Thread-6 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.06s]
[0m20:47:00.912967 [debug] [Thread-6 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:47:00.916487 [debug] [MainThread]: On master: ROLLBACK
[0m20:47:00.918541 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:47:00.920562 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:47:00.921572 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:47:00.922581 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:47:00.924598 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:47:00.926621 [info ] [MainThread]: 
[0m20:47:00.929677 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 13.07 seconds (13.07s).
[0m20:47:00.933737 [debug] [MainThread]: Command end result
[0m20:47:00.948949 [info ] [MainThread]: 
[0m20:47:00.950974 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:47:00.953015 [info ] [MainThread]: 
[0m20:47:00.955040 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m20:47:00.958069 [debug] [MainThread]: Command `cli build` succeeded at 20:47:00.957060 after 13.76 seconds
[0m20:47:00.961118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000019454436480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194549E9940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000194549EBCE0>]}
[0m20:47:00.963171 [debug] [MainThread]: Flushing usage events
[0m20:48:44.149818 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001552BEF1970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001552BE06300>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001552BE04FB0>]}


============================== 20:48:44.152839 | 0cc93afd-bbea-42af-ae63-70d9f3c7ce67 ==============================
[0m20:48:44.152839 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:48:44.154892 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:48:44.429212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0cc93afd-bbea-42af-ae63-70d9f3c7ce67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001552BEBEA20>]}
[0m20:48:44.549043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0cc93afd-bbea-42af-ae63-70d9f3c7ce67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001552C0972F0>]}
[0m20:48:44.551074 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:48:44.580520 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:48:44.690207 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:48:44.692223 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:48:44.709387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0cc93afd-bbea-42af-ae63-70d9f3c7ce67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001552BE801A0>]}
[0m20:48:44.726634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0cc93afd-bbea-42af-ae63-70d9f3c7ce67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001552C11DEB0>]}
[0m20:48:44.727641 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:48:44.734254 [info ] [MainThread]: 
[0m20:48:44.736797 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:48:44.742865 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:48:44.793139 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:48:44.863040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0cc93afd-bbea-42af-ae63-70d9f3c7ce67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001552AF72570>]}
[0m20:48:44.865059 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:48:44.866070 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:48:44.868087 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:48:44.871193 [info ] [MainThread]: 
[0m20:48:44.881360 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m20:48:44.884409 [info ] [Thread-6 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:48:44.886944 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:48:44.888964 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:48:44.927433 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:48:44.929449 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:48:44.889977 => 20:48:44.928442
[0m20:48:44.930460 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:48:45.065791 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:48:45.066798 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:48:45.067806 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:48:45.068813 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m20:49:21.843209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002107CA94D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002107CA94E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002107CA95310>]}


============================== 20:49:21.843719 | 07e560fa-237d-4c81-b042-6894c50c3424 ==============================
[0m20:49:21.843719 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:49:21.844734 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m20:49:21.929731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '07e560fa-237d-4c81-b042-6894c50c3424', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002107CB1BE90>]}
[0m20:49:21.967200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '07e560fa-237d-4c81-b042-6894c50c3424', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002107C7CB530>]}
[0m20:49:21.968208 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:49:21.976298 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:49:22.023862 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:49:22.024874 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:49:22.028923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '07e560fa-237d-4c81-b042-6894c50c3424', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002107DBAEB70>]}
[0m20:49:22.036019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '07e560fa-237d-4c81-b042-6894c50c3424', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002107DC2DE80>]}
[0m20:49:22.036019 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:49:22.038041 [info ] [MainThread]: 
[0m20:49:22.039055 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:49:22.040068 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:49:22.045228 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:49:22.060671 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07e560fa-237d-4c81-b042-6894c50c3424', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002107CAF9250>]}
[0m20:49:22.061732 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:49:22.061732 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:49:22.061732 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:49:22.062784 [info ] [MainThread]: 
[0m20:49:22.064841 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m20:49:22.065855 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:49:22.066873 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:49:22.066873 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:49:22.071923 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:49:22.072932 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:49:22.067884 => 20:49:22.071923
[0m20:49:22.072932 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:49:22.103314 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:49:22.104323 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:49:22.104323 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:49:22.104323 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:49:22.154421 [debug] [Thread-1 (]: SQL status: OK in 0.05000000074505806 seconds
[0m20:49:22.172589 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m20:49:22.173597 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:49:22.174610 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m20:49:22.186752 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:49:22.194812 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:49:22.072932 => 20:49:22.194812
[0m20:49:22.195821 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07e560fa-237d-4c81-b042-6894c50c3424', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002107DDDC650>]}
[0m20:49:22.195821 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.13s]
[0m20:49:22.196832 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:49:22.196832 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m20:49:22.197841 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m20:49:22.198850 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m20:49:22.198850 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m20:49:22.198850 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 20:49:22.198850 => 20:49:22.198850
[0m20:49:22.199859 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m20:49:22.226176 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:49:22.226176 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:49:22.230210 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:49:22.240348 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m20:49:22.244415 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:49:22.244415 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m20:49:22.264682 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:49:22.269727 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m20:49:22.276866 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:49:22.277869 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 20:49:22.199859 => 20:49:22.277869
[0m20:49:22.278880 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07e560fa-237d-4c81-b042-6894c50c3424', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002107DDDFD10>]}
[0m20:49:22.279899 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.08s]
[0m20:49:22.280920 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m20:49:22.280920 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:49:22.281936 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m20:49:22.282950 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m20:49:22.282950 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:49:22.294157 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:49:22.295165 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 20:49:22.283965 => 20:49:22.295165
[0m20:49:22.295165 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:49:22.306296 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:49:22.308441 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:49:22.309444 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:49:22.314495 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:49:22.316031 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 20:49:22.296172 => 20:49:22.316031
[0m20:49:22.317065 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m20:49:22.317577 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:49:22.318089 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:49:22.318596 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m20:49:22.319619 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m20:49:22.320134 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:49:22.325260 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:49:22.326277 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 20:49:22.320134 => 20:49:22.326277
[0m20:49:22.326783 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:49:22.329333 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:49:22.330347 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:49:22.330855 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:49:22.335420 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:49:22.336434 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 20:49:22.327298 => 20:49:22.335927
[0m20:49:22.336941 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m20:49:22.337447 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:49:22.338475 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m20:49:22.338985 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m20:49:22.339495 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m20:49:22.339495 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m20:49:22.341538 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m20:49:22.343109 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 20:49:22.340006 => 20:49:22.343109
[0m20:49:22.343618 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m20:49:22.354343 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m20:49:22.355357 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m20:49:22.355864 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m20:49:22.359414 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:49:22.360933 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 20:49:22.344123 => 20:49:22.360933
[0m20:49:22.361946 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07e560fa-237d-4c81-b042-6894c50c3424', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002107DBAE000>]}
[0m20:49:22.362452 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m20:49:22.362960 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:49:22.363467 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:49:22.363978 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m20:49:22.364997 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m20:49:22.365507 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:49:22.368042 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:49:22.368548 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 20:49:22.365507 => 20:49:22.368548
[0m20:49:22.369073 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:49:22.370087 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:49:22.370593 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:49:22.371100 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:49:22.374656 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:49:22.376199 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 20:49:22.369073 => 20:49:22.376199
[0m20:49:22.376713 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m20:49:22.377221 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:49:22.377731 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:49:22.378240 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m20:49:22.379261 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m20:49:22.379261 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:49:22.382332 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:49:22.383356 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 20:49:22.379771 => 20:49:22.383356
[0m20:49:22.383868 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:49:22.385419 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:49:22.385929 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:49:22.386443 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:49:22.390013 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:49:22.390523 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 20:49:22.383868 => 20:49:22.390523
[0m20:49:22.391547 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m20:49:22.392059 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:49:22.393075 [debug] [MainThread]: On master: ROLLBACK
[0m20:49:22.393075 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:49:22.393583 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:49:22.393583 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:49:22.394090 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:49:22.394090 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:49:22.394598 [info ] [MainThread]: 
[0m20:49:22.395105 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.36 seconds (0.36s).
[0m20:49:22.396118 [debug] [MainThread]: Command end result
[0m20:49:22.401717 [info ] [MainThread]: 
[0m20:49:22.402223 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:49:22.402223 [info ] [MainThread]: 
[0m20:49:22.402732 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m20:49:22.403746 [debug] [MainThread]: Command `cli build` succeeded at 20:49:22.403238 after 0.58 seconds
[0m20:49:22.403746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002107C7C9EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002107C3FE4B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002107C658860>]}
[0m20:49:22.404252 [debug] [MainThread]: Flushing usage events
[0m20:58:00.289696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFCFBE4D10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFCFBE5070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFCFBE4BF0>]}


============================== 20:58:00.289696 | cb5db7e5-8d6a-4109-bd46-911e91af043b ==============================
[0m20:58:00.289696 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:58:00.290735 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:58:00.377568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cb5db7e5-8d6a-4109-bd46-911e91af043b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFCFC488F0>]}
[0m20:58:00.412996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cb5db7e5-8d6a-4109-bd46-911e91af043b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFCFAA1970>]}
[0m20:58:00.415015 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:58:00.427139 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:58:01.119064 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:58:01.119064 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:58:01.123096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cb5db7e5-8d6a-4109-bd46-911e91af043b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFD0D2B470>]}
[0m20:58:01.134203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cb5db7e5-8d6a-4109-bd46-911e91af043b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFD0D7DE80>]}
[0m20:58:01.135211 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:58:01.136216 [info ] [MainThread]: 
[0m20:58:01.137329 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:58:01.138340 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:58:01.150541 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:58:01.168705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cb5db7e5-8d6a-4109-bd46-911e91af043b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFD0D7DD60>]}
[0m20:58:01.168705 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:58:01.169713 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:58:01.169713 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:58:01.169713 [info ] [MainThread]: 
[0m20:58:01.172764 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m20:58:01.172764 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:58:01.173787 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:58:01.173787 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:58:01.179886 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:58:01.180897 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:58:01.173787 => 20:58:01.180897
[0m20:58:01.181903 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:58:01.211228 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:58:01.212254 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:58:01.212254 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:58:01.213262 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:58:01.214279 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:58:01.214279 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: ModelNotebook.__init__() got an unexpected keyword argument 'type'
[0m20:58:01.215309 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:58:01.181903 => 20:58:01.215309
[0m20:58:01.289170 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  ModelNotebook.__init__() got an unexpected keyword argument 'type'
[0m20:58:01.290180 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cb5db7e5-8d6a-4109-bd46-911e91af043b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFD0D52A80>]}
[0m20:58:01.290180 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.12s]
[0m20:58:01.291200 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:58:01.291200 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m20:58:01.292707 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m20:58:01.292707 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m20:58:01.292707 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m20:58:01.293717 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 20:58:01.293717 => 20:58:01.293717
[0m20:58:01.293717 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m20:58:01.321114 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:58:01.322124 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:58:01.322124 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:58:01.322124 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: ModelNotebook.__init__() got an unexpected keyword argument 'type'
[0m20:58:01.323139 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 20:58:01.293717 => 20:58:01.323139
[0m20:58:01.325178 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  ModelNotebook.__init__() got an unexpected keyword argument 'type'
[0m20:58:01.325178 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cb5db7e5-8d6a-4109-bd46-911e91af043b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFD0F527E0>]}
[0m20:58:01.326188 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.03s]
[0m20:58:01.326188 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m20:58:01.327198 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:58:01.327198 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m20:58:01.328209 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:58:01.328209 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:58:01.328209 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m20:58:01.329219 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:58:01.329219 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m20:58:01.330233 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m20:58:01.330233 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:58:01.331245 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:58:01.331245 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m20:58:01.331245 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:58:01.332258 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:58:01.332258 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m20:58:01.332258 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:58:01.333270 [debug] [MainThread]: On master: ROLLBACK
[0m20:58:01.333270 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:58:01.334279 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:58:01.334279 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:58:01.334279 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:58:01.334279 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:58:01.335288 [info ] [MainThread]: 
[0m20:58:01.335288 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m20:58:01.336296 [debug] [MainThread]: Command end result
[0m20:58:01.341352 [info ] [MainThread]: 
[0m20:58:01.342365 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m20:58:01.342365 [info ] [MainThread]: 
[0m20:58:01.343384 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  ModelNotebook.__init__() got an unexpected keyword argument 'type'
[0m20:58:01.343384 [info ] [MainThread]: 
[0m20:58:01.344394 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  ModelNotebook.__init__() got an unexpected keyword argument 'type'
[0m20:58:01.344394 [info ] [MainThread]: 
[0m20:58:01.345408 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m20:58:01.346426 [debug] [MainThread]: Command `cli build` failed at 20:58:01.346426 after 1.10 seconds
[0m20:58:01.346426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFCF917A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFCED96FF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFCF485760>]}
[0m20:58:01.347446 [debug] [MainThread]: Flushing usage events
[0m20:58:22.721046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022983AB4EF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022983AB6450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022983AB6420>]}


============================== 20:58:22.722054 | f7d2f77e-1b87-4761-8699-e2d3ad57706b ==============================
[0m20:58:22.722054 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:58:22.722054 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m20:58:22.807159 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f7d2f77e-1b87-4761-8699-e2d3ad57706b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022984B4CB00>]}
[0m20:58:22.846685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f7d2f77e-1b87-4761-8699-e2d3ad57706b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002298385A930>]}
[0m20:58:22.847706 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:58:22.853756 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:58:22.894388 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:58:22.895390 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:58:22.899432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f7d2f77e-1b87-4761-8699-e2d3ad57706b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022983AE7FE0>]}
[0m20:58:22.905486 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f7d2f77e-1b87-4761-8699-e2d3ad57706b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022984C3DD00>]}
[0m20:58:22.905486 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:58:22.907521 [info ] [MainThread]: 
[0m20:58:22.908546 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:58:22.909561 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:58:22.914628 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:58:22.927764 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f7d2f77e-1b87-4761-8699-e2d3ad57706b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022984C97290>]}
[0m20:58:22.927764 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:58:22.928774 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:58:22.928774 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:58:22.929782 [info ] [MainThread]: 
[0m20:58:22.931804 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m20:58:22.932817 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:58:22.932817 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:58:22.933828 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:58:22.937859 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:58:22.938881 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:58:22.933828 => 20:58:22.938881
[0m20:58:22.938881 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:58:22.973467 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:58:22.975493 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:58:22.975493 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:58:22.976505 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:58:23.060236 [debug] [Thread-1 (]: SQL status: OK in 0.07999999821186066 seconds
[0m20:58:23.086214 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m20:58:23.088873 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:58:23.089380 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m20:58:23.103651 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:58:23.114414 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:58:22.938881 => 20:58:23.114414
[0m20:58:23.115438 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7d2f77e-1b87-4761-8699-e2d3ad57706b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022984C80AD0>]}
[0m20:58:23.115947 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.18s]
[0m20:58:23.116453 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:58:23.116962 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m20:58:23.117471 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m20:58:23.117984 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m20:58:23.119006 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m20:58:23.119517 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 20:58:23.119006 => 20:58:23.119006
[0m20:58:23.119517 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m20:58:23.145514 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:58:23.146021 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:58:23.150592 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:58:23.158719 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m20:58:23.161251 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:58:23.161251 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m20:58:23.175064 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:58:23.180132 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m20:58:23.184189 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:58:23.185203 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 20:58:23.120029 => 20:58:23.184189
[0m20:58:23.185203 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7d2f77e-1b87-4761-8699-e2d3ad57706b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022984C83290>]}
[0m20:58:23.186216 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.07s]
[0m20:58:23.186216 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m20:58:23.187232 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:58:23.187232 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m20:58:23.188241 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m20:58:23.188241 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:58:23.195290 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:58:23.196297 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 20:58:23.188241 => 20:58:23.195290
[0m20:58:23.196297 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:58:23.204404 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:58:23.205413 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:58:23.205413 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:58:23.209465 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:58:23.211492 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 20:58:23.196297 => 20:58:23.211492
[0m20:58:23.212513 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m20:58:23.213545 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:58:23.213545 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:58:23.214575 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m20:58:23.214575 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m20:58:23.215588 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:58:23.219640 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:58:23.220652 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 20:58:23.215588 => 20:58:23.220652
[0m20:58:23.220652 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:58:23.222677 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:58:23.222677 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:58:23.222677 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:58:23.226715 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:58:23.227723 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 20:58:23.220652 => 20:58:23.227723
[0m20:58:23.228731 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.01s]
[0m20:58:23.228731 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:58:23.229763 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m20:58:23.229763 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m20:58:23.230775 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m20:58:23.230775 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m20:58:23.232809 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m20:58:23.233822 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 20:58:23.231790 => 20:58:23.233822
[0m20:58:23.233822 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m20:58:23.244986 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m20:58:23.246003 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m20:58:23.246003 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m20:58:23.250036 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:58:23.251045 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 20:58:23.233822 => 20:58:23.251045
[0m20:58:23.252056 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f7d2f77e-1b87-4761-8699-e2d3ad57706b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229850FAB70>]}
[0m20:58:23.252056 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m20:58:23.253064 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:58:23.254074 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:58:23.254074 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m20:58:23.254074 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m20:58:23.255085 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:58:23.257104 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:58:23.258111 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 20:58:23.255085 => 20:58:23.258111
[0m20:58:23.258111 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:58:23.260166 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:58:23.260166 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:58:23.261176 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:58:23.264202 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:58:23.265213 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 20:58:23.259118 => 20:58:23.265213
[0m20:58:23.266224 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m20:58:23.266224 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:58:23.267236 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:58:23.267236 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m20:58:23.268251 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m20:58:23.268251 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:58:23.271282 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:58:23.271282 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 20:58:23.268251 => 20:58:23.271282
[0m20:58:23.271282 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:58:23.273304 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:58:23.275352 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:58:23.275352 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:58:23.279412 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:58:23.280423 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 20:58:23.272290 => 20:58:23.280423
[0m20:58:23.280423 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m20:58:23.281436 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:58:23.282443 [debug] [MainThread]: On master: ROLLBACK
[0m20:58:23.282443 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:58:23.283457 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:58:23.283457 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:58:23.283457 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:58:23.284467 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:58:23.284467 [info ] [MainThread]: 
[0m20:58:23.284467 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.38 seconds (0.38s).
[0m20:58:23.285475 [debug] [MainThread]: Command end result
[0m20:58:23.290541 [info ] [MainThread]: 
[0m20:58:23.290541 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:58:23.291550 [info ] [MainThread]: 
[0m20:58:23.291550 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m20:58:23.292555 [debug] [MainThread]: Command `cli build` succeeded at 20:58:23.292555 after 0.60 seconds
[0m20:58:23.292555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022984F8C200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022984F8D220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229839E62D0>]}
[0m20:58:23.292555 [debug] [MainThread]: Flushing usage events
[0m20:59:29.704520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B160E04B60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B160E06420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B160E04CE0>]}


============================== 20:59:29.704520 | 65e46d64-e8e7-4181-b1ed-c08f2a0032ce ==============================
[0m20:59:29.704520 [info ] [MainThread]: Running with dbt=1.7.14
[0m20:59:29.705526 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m20:59:29.792778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '65e46d64-e8e7-4181-b1ed-c08f2a0032ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B1606C6E10>]}
[0m20:59:29.827175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '65e46d64-e8e7-4181-b1ed-c08f2a0032ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B160D08E60>]}
[0m20:59:29.828184 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m20:59:29.835282 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m20:59:29.884912 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m20:59:29.885918 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m20:59:29.889953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '65e46d64-e8e7-4181-b1ed-c08f2a0032ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B161F6DF10>]}
[0m20:59:29.896005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '65e46d64-e8e7-4181-b1ed-c08f2a0032ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B161FF9F40>]}
[0m20:59:29.896005 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m20:59:29.898031 [info ] [MainThread]: 
[0m20:59:29.898031 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m20:59:29.899058 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m20:59:29.903106 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m20:59:29.917273 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '65e46d64-e8e7-4181-b1ed-c08f2a0032ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B160E89250>]}
[0m20:59:29.918280 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:59:29.918280 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:59:29.919287 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m20:59:29.919287 [info ] [MainThread]: 
[0m20:59:29.921301 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m20:59:29.921301 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m20:59:29.922310 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m20:59:29.922310 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m20:59:29.926352 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m20:59:29.927361 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 20:59:29.922310 => 20:59:29.927361
[0m20:59:29.927361 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m20:59:29.957677 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:59:29.958683 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:59:29.958683 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m20:59:29.959734 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:59:30.011558 [debug] [Thread-1 (]: SQL status: OK in 0.05000000074505806 seconds
[0m20:59:30.030756 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m20:59:30.031766 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m20:59:30.031766 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m20:59:30.043890 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m20:59:30.053008 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 20:59:29.928370 => 20:59:30.053008
[0m20:59:30.054022 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65e46d64-e8e7-4181-b1ed-c08f2a0032ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B161FE2180>]}
[0m20:59:30.054022 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.13s]
[0m20:59:30.055037 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m20:59:30.055037 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m20:59:30.056050 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m20:59:30.056050 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m20:59:30.057061 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m20:59:30.057061 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 20:59:30.057061 => 20:59:30.057061
[0m20:59:30.058074 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m20:59:30.084109 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:59:30.085120 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m20:59:30.090175 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:59:30.100390 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m20:59:30.103463 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m20:59:30.104472 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m20:59:30.127806 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:59:30.132397 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m20:59:30.137481 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:59:30.138509 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 20:59:30.058074 => 20:59:30.138509
[0m20:59:30.139532 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65e46d64-e8e7-4181-b1ed-c08f2a0032ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B162144AA0>]}
[0m20:59:30.140044 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.08s]
[0m20:59:30.141061 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m20:59:30.141568 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:59:30.142077 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m20:59:30.143137 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m20:59:30.143650 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:59:30.153382 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:59:30.154397 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 20:59:30.143650 => 20:59:30.154397
[0m20:59:30.154905 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:59:30.162536 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:59:30.163560 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m20:59:30.164079 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:59:30.184016 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m20:59:30.185540 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 20:59:30.154905 => 20:59:30.185540
[0m20:59:30.186046 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.04s]
[0m20:59:30.186555 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m20:59:30.187065 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:59:30.187572 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m20:59:30.188080 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m20:59:30.188080 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:59:30.192151 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:59:30.193165 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 20:59:30.188603 => 20:59:30.192657
[0m20:59:30.193165 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:59:30.194691 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:59:30.195198 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m20:59:30.195705 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:59:30.199766 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:59:30.200783 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 20:59:30.193673 => 20:59:30.200783
[0m20:59:30.201294 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.01s]
[0m20:59:30.202309 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m20:59:30.202309 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m20:59:30.202820 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m20:59:30.203857 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m20:59:30.203857 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m20:59:30.205380 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m20:59:30.206399 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 20:59:30.204365 => 20:59:30.205889
[0m20:59:30.206399 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m20:59:30.217159 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m20:59:30.218172 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m20:59:30.218172 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m20:59:30.221738 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:59:30.223260 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 20:59:30.206912 => 20:59:30.223260
[0m20:59:30.223792 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '65e46d64-e8e7-4181-b1ed-c08f2a0032ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B16243C680>]}
[0m20:59:30.224300 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m20:59:30.224807 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m20:59:30.225315 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:59:30.225822 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m20:59:30.226335 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m20:59:30.226845 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:59:30.229380 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:59:30.230394 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 20:59:30.226845 => 20:59:30.229887
[0m20:59:30.230394 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:59:30.231921 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:59:30.232934 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m20:59:30.232934 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m20:59:30.236508 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:59:30.237522 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 20:59:30.230904 => 20:59:30.237522
[0m20:59:30.238029 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m20:59:30.238539 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m20:59:30.239049 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:59:30.239049 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m20:59:30.239561 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m20:59:30.240073 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:59:30.243146 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:59:30.245203 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 20:59:30.240585 => 20:59:30.244690
[0m20:59:30.245714 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:59:30.247234 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:59:30.248251 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m20:59:30.248251 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m20:59:30.251812 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m20:59:30.252827 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 20:59:30.245714 => 20:59:30.252827
[0m20:59:30.253335 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m20:59:30.253842 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m20:59:30.254856 [debug] [MainThread]: On master: ROLLBACK
[0m20:59:30.255362 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:59:30.255362 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m20:59:30.255871 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m20:59:30.255871 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m20:59:30.256377 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m20:59:30.256885 [info ] [MainThread]: 
[0m20:59:30.256885 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.36 seconds (0.36s).
[0m20:59:30.257901 [debug] [MainThread]: Command end result
[0m20:59:30.262440 [info ] [MainThread]: 
[0m20:59:30.262947 [info ] [MainThread]: [32mCompleted successfully[0m
[0m20:59:30.263455 [info ] [MainThread]: 
[0m20:59:30.263455 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m20:59:30.265522 [debug] [MainThread]: Command `cli build` succeeded at 20:59:30.264520 after 0.59 seconds
[0m20:59:30.265522 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B161F207D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B162048A40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B161F6F4A0>]}
[0m20:59:30.265522 [debug] [MainThread]: Flushing usage events
[0m21:02:22.712903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E7DC44B00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E7DC46420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E7DC443B0>]}


============================== 21:02:22.713909 | 3c38e733-d583-48c3-9632-5773c1e0fbf0 ==============================
[0m21:02:22.713909 [info ] [MainThread]: Running with dbt=1.7.14
[0m21:02:22.713909 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m21:02:22.797806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3c38e733-d583-48c3-9632-5773c1e0fbf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E7ECDF4D0>]}
[0m21:02:22.834287 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3c38e733-d583-48c3-9632-5773c1e0fbf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E7DCA4D40>]}
[0m21:02:22.835295 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m21:02:22.842357 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m21:02:22.885902 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:02:22.886912 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:02:22.889940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3c38e733-d583-48c3-9632-5773c1e0fbf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E7CFB2900>]}
[0m21:02:22.896011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3c38e733-d583-48c3-9632-5773c1e0fbf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E7EDC5F70>]}
[0m21:02:22.897019 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m21:02:22.898027 [info ] [MainThread]: 
[0m21:02:22.899036 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m21:02:22.900043 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m21:02:22.904088 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m21:02:22.917278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3c38e733-d583-48c3-9632-5773c1e0fbf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E7EDC5AC0>]}
[0m21:02:22.917278 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:02:22.918286 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m21:02:22.918286 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m21:02:22.919298 [info ] [MainThread]: 
[0m21:02:22.921314 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m21:02:22.921314 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m21:02:22.922323 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m21:02:22.922323 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m21:02:22.926372 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m21:02:22.927379 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 21:02:22.922323 => 21:02:22.927379
[0m21:02:22.927379 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m21:02:22.958775 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:02:22.959783 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m21:02:22.960796 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m21:02:22.960796 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:02:23.012499 [debug] [Thread-1 (]: SQL status: OK in 0.05000000074505806 seconds
[0m21:02:23.031195 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m21:02:23.032202 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m21:02:23.032202 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m21:02:23.044320 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m21:02:23.053460 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 21:02:22.928389 => 21:02:23.053460
[0m21:02:23.054468 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c38e733-d583-48c3-9632-5773c1e0fbf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E7EF7BA10>]}
[0m21:02:23.054468 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.13s]
[0m21:02:23.055482 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m21:02:23.055482 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m21:02:23.056490 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m21:02:23.056490 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m21:02:23.057498 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m21:02:23.057498 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 21:02:23.057498 => 21:02:23.057498
[0m21:02:23.057498 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m21:02:23.083873 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m21:02:23.083873 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,` id` bigint)
    
    
    
    
    
  
[0m21:02:23.087910 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m21:02:23.098559 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m21:02:23.101595 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m21:02:23.102606 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m21:02:23.124500 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m21:02:23.129065 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m21:02:23.134141 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m21:02:23.135159 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 21:02:23.058507 => 21:02:23.135159
[0m21:02:23.135159 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c38e733-d583-48c3-9632-5773c1e0fbf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E7EFA0050>]}
[0m21:02:23.136160 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.08s]
[0m21:02:23.137161 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m21:02:23.137161 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m21:02:23.137161 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m21:02:23.138161 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m21:02:23.138664 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m21:02:23.147322 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m21:02:23.148842 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 21:02:23.139167 => 21:02:23.148335
[0m21:02:23.149352 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m21:02:23.158528 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m21:02:23.160555 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m21:02:23.161069 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m21:02:23.175331 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m21:02:23.176874 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 21:02:23.149352 => 21:02:23.176874
[0m21:02:23.178414 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.04s]
[0m21:02:23.179430 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m21:02:23.179942 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m21:02:23.180448 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m21:02:23.180954 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m21:02:23.181461 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m21:02:23.186046 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m21:02:23.188085 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 21:02:23.181461 => 21:02:23.188085
[0m21:02:23.188594 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m21:02:23.191142 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m21:02:23.192156 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m21:02:23.192662 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m21:02:23.197790 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m21:02:23.199313 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 21:02:23.189102 => 21:02:23.199313
[0m21:02:23.199820 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m21:02:23.200325 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m21:02:23.200832 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m21:02:23.201339 [info ] [Thread-1 (]: 5 of 7 START sql view model datalake.my_second_dbt_model ....................... [RUN]
[0m21:02:23.202355 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m21:02:23.202355 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m21:02:23.204384 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m21:02:23.205399 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 21:02:23.202863 => 21:02:23.205399
[0m21:02:23.205905 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m21:02:23.216650 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m21:02:23.217665 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m21:02:23.218177 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
create or replace view datalake.my_second_dbt_model
  
  
  as
    -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1

[0m21:02:23.222245 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m21:02:23.223259 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 21:02:23.205905 => 21:02:23.223259
[0m21:02:23.224273 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3c38e733-d583-48c3-9632-5773c1e0fbf0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E7F1347A0>]}
[0m21:02:23.224780 [info ] [Thread-1 (]: 5 of 7 OK created sql view model datalake.my_second_dbt_model .................. [[32mOK[0m in 0.02s]
[0m21:02:23.225286 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m21:02:23.225794 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m21:02:23.226301 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m21:02:23.226813 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m21:02:23.227322 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m21:02:23.229871 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m21:02:23.230884 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 21:02:23.227322 => 21:02:23.230884
[0m21:02:23.231390 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m21:02:23.232912 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m21:02:23.233420 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m21:02:23.233420 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m21:02:23.236978 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m21:02:23.237991 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 21:02:23.231390 => 21:02:23.237991
[0m21:02:23.238496 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m21:02:23.239513 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m21:02:23.239513 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m21:02:23.240023 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m21:02:23.241050 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m21:02:23.241050 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m21:02:23.244643 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m21:02:23.245663 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 21:02:23.241562 => 21:02:23.245663
[0m21:02:23.246171 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m21:02:23.247186 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m21:02:23.248205 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m21:02:23.248205 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m21:02:23.251763 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m21:02:23.252779 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 21:02:23.246171 => 21:02:23.252779
[0m21:02:23.253286 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m21:02:23.253792 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m21:02:23.254807 [debug] [MainThread]: On master: ROLLBACK
[0m21:02:23.254807 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:02:23.255313 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m21:02:23.255313 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m21:02:23.255824 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:02:23.255824 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m21:02:23.256332 [info ] [MainThread]: 
[0m21:02:23.256839 [info ] [MainThread]: Finished running 1 incremental model, 1 seed, 4 tests, 1 view model in 0 hours 0 minutes and 0.36 seconds (0.36s).
[0m21:02:23.257852 [debug] [MainThread]: Command end result
[0m21:02:23.262931 [info ] [MainThread]: 
[0m21:02:23.263438 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:02:23.263945 [info ] [MainThread]: 
[0m21:02:23.263945 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m21:02:23.264960 [debug] [MainThread]: Command `cli build` succeeded at 21:02:23.264453 after 0.58 seconds
[0m21:02:23.264960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E7DB2EBD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E7D8783E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E7DB07CE0>]}
[0m21:02:23.265467 [debug] [MainThread]: Flushing usage events
[0m21:02:29.285102 [debug] [MainThread]: Error sending anonymous usage statistics. Disabling tracking for this execution. If you wish to permanently disable tracking, see: https://docs.getdbt.com/reference/global-configs#send-anonymous-usage-stats.
[0m08:09:45.379579 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6C0EE1490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6C0EE00E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6C0EE11C0>]}


============================== 08:09:45.380588 | fb05f60c-a7a1-4b4e-870e-df024d543fec ==============================
[0m08:09:45.380588 [info ] [MainThread]: Running with dbt=1.7.14
[0m08:09:45.381595 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m08:09:45.474727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fb05f60c-a7a1-4b4e-870e-df024d543fec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6C0E65DC0>]}
[0m08:09:45.509160 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fb05f60c-a7a1-4b4e-870e-df024d543fec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6C0EC7CB0>]}
[0m08:09:45.510176 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m08:09:45.528367 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m08:09:45.529379 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m08:09:45.530389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'fb05f60c-a7a1-4b4e-870e-df024d543fec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6BF3133B0>]}
[0m08:09:47.060526 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fb05f60c-a7a1-4b4e-870e-df024d543fec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6C13C0BC0>]}
[0m08:09:47.079289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fb05f60c-a7a1-4b4e-870e-df024d543fec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6C13B0F20>]}
[0m08:09:47.079289 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m08:09:47.081303 [info ] [MainThread]: 
[0m08:09:47.082312 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m08:09:47.083319 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m08:09:47.091399 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m08:09:47.110641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fb05f60c-a7a1-4b4e-870e-df024d543fec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6C110A570>]}
[0m08:09:47.111649 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:09:47.111649 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m08:09:47.111649 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m08:09:47.112663 [info ] [MainThread]: 
[0m08:09:47.114693 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m08:09:47.114693 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m08:09:47.115701 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m08:09:47.115701 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m08:09:47.120764 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m08:09:47.121772 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 08:09:47.116709 => 08:09:47.121772
[0m08:09:47.121772 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m08:09:47.159352 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:09:47.160359 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m08:09:47.160359 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m08:09:47.160359 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m08:09:47.170511 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m08:09:47.170511 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:09:47.171520 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 08:09:47.121772 => 08:09:47.171520
[0m08:09:47.274338 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:09:47.274338 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb05f60c-a7a1-4b4e-870e-df024d543fec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6C1415970>]}
[0m08:09:47.275345 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model datalake.my_first_dbt_model ........ [[31mERROR[0m in 0.16s]
[0m08:09:47.276353 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m08:09:47.276353 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m08:09:47.277379 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m08:09:47.277889 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m08:09:47.278400 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m08:09:47.278400 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 08:09:47.278400 => 08:09:47.278400
[0m08:09:47.278400 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m08:09:47.307774 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m08:09:47.308786 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m08:09:47.308786 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m08:09:47.309797 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:09:47.309797 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 08:09:47.278400 => 08:09:47.309797
[0m08:09:47.311810 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:09:47.312817 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fb05f60c-a7a1-4b4e-870e-df024d543fec', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6C15BAD80>]}
[0m08:09:47.312817 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.04s]
[0m08:09:47.313824 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m08:09:47.313824 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m08:09:47.314840 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m08:09:47.314840 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m08:09:47.314840 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m08:09:47.315852 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m08:09:47.315852 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m08:09:47.316897 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m08:09:47.316897 [info ] [Thread-1 (]: 5 of 7 SKIP relation datalake.my_second_dbt_model .............................. [[33mSKIP[0m]
[0m08:09:47.316897 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m08:09:47.317911 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m08:09:47.317911 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m08:09:47.318923 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m08:09:47.318923 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m08:09:47.318923 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m08:09:47.319935 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m08:09:47.320947 [debug] [MainThread]: On master: ROLLBACK
[0m08:09:47.320947 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:09:47.320947 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m08:09:47.320947 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m08:09:47.321959 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:09:47.321959 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m08:09:47.321959 [info ] [MainThread]: 
[0m08:09:47.322972 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.24 seconds (0.24s).
[0m08:09:47.323987 [debug] [MainThread]: Command end result
[0m08:09:47.329033 [info ] [MainThread]: 
[0m08:09:47.330047 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m08:09:47.330047 [info ] [MainThread]: 
[0m08:09:47.331207 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:09:47.331207 [info ] [MainThread]: 
[0m08:09:47.332210 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:09:47.332210 [info ] [MainThread]: 
[0m08:09:47.333220 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m08:09:47.334233 [debug] [MainThread]: Command `cli build` failed at 08:09:47.334233 after 1.98 seconds
[0m08:09:47.335250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6C12A20F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6BF3133B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6C0FB7830>]}
[0m08:09:47.335250 [debug] [MainThread]: Flushing usage events
[0m08:11:54.844322 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011EC55E10A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011EC55E0D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011EC55E11F0>]}


============================== 08:11:54.845355 | b8d594b3-a4ee-4673-8448-93521eba10c2 ==============================
[0m08:11:54.845355 [info ] [MainThread]: Running with dbt=1.7.14
[0m08:11:54.845355 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m08:11:54.928297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b8d594b3-a4ee-4673-8448-93521eba10c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011EC55A3AD0>]}
[0m08:11:54.962822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b8d594b3-a4ee-4673-8448-93521eba10c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011EC55C7140>]}
[0m08:11:54.963839 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m08:11:54.972433 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m08:11:54.973440 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m08:11:54.973440 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b8d594b3-a4ee-4673-8448-93521eba10c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011EC56E27B0>]}
[0m08:11:55.694271 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b8d594b3-a4ee-4673-8448-93521eba10c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011EC5AC24E0>]}
[0m08:11:55.704447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b8d594b3-a4ee-4673-8448-93521eba10c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011EC5A99370>]}
[0m08:11:55.705459 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m08:11:55.705459 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b8d594b3-a4ee-4673-8448-93521eba10c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011EC5824770>]}
[0m08:11:55.706966 [info ] [MainThread]: 
[0m08:11:55.707976 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m08:11:55.708979 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m08:11:55.712512 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m08:11:55.726667 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b8d594b3-a4ee-4673-8448-93521eba10c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011EC5AD29C0>]}
[0m08:11:55.726667 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:11:55.727673 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m08:11:55.727673 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m08:11:55.728678 [info ] [MainThread]: 
[0m08:11:55.731799 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m08:11:55.732907 [info ] [Thread-1 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m08:11:55.733911 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m08:11:55.734931 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m08:11:55.734931 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 08:11:55.734931 => 08:11:55.734931
[0m08:11:55.735941 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m08:11:55.767316 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:11:55.768332 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m08:11:55.768844 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m08:11:55.768844 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m08:11:55.770364 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m08:11:55.770364 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:11:55.771379 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 08:11:55.735941 => 08:11:55.770364
[0m08:11:55.776424 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:11:55.777436 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b8d594b3-a4ee-4673-8448-93521eba10c2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011EC5989FA0>]}
[0m08:11:55.777436 [error] [Thread-1 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.04s]
[0m08:11:55.778443 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m08:11:55.779460 [debug] [MainThread]: On master: ROLLBACK
[0m08:11:55.780488 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:11:55.780488 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m08:11:55.780488 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m08:11:55.781496 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:11:55.781496 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m08:11:55.782503 [info ] [MainThread]: 
[0m08:11:55.782503 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.07 seconds (0.07s).
[0m08:11:55.782503 [debug] [MainThread]: Command end result
[0m08:11:55.788550 [info ] [MainThread]: 
[0m08:11:55.789591 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m08:11:55.789591 [info ] [MainThread]: 
[0m08:11:55.790600 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:11:55.790600 [info ] [MainThread]: 
[0m08:11:55.790600 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m08:11:55.791608 [debug] [MainThread]: Command `cli seed` failed at 08:11:55.791608 after 0.97 seconds
[0m08:11:55.792616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011EC57E2840>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011EC54E5430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000011EC584F4A0>]}
[0m08:11:55.792616 [debug] [MainThread]: Flushing usage events
[0m08:18:16.854351 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26C77E9C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26C77E2A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26C77DFA0>]}


============================== 08:18:16.858391 | bfd6ef28-f575-4fed-87f9-eb4060f790db ==============================
[0m08:18:16.858391 [info ] [MainThread]: Running with dbt=1.7.14
[0m08:18:16.860411 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m08:18:17.130046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bfd6ef28-f575-4fed-87f9-eb4060f790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26C747DA0>]}
[0m08:18:17.259988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bfd6ef28-f575-4fed-87f9-eb4060f790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26C5BC2F0>]}
[0m08:18:17.264046 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m08:18:17.302011 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m08:18:17.305068 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m08:18:17.306076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'bfd6ef28-f575-4fed-87f9-eb4060f790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26C683770>]}
[0m08:18:20.915467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bfd6ef28-f575-4fed-87f9-eb4060f790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26CB442F0>]}
[0m08:18:20.942029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bfd6ef28-f575-4fed-87f9-eb4060f790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26CC7B6B0>]}
[0m08:18:20.943038 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m08:18:20.945072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bfd6ef28-f575-4fed-87f9-eb4060f790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26CC15640>]}
[0m08:18:20.951125 [info ] [MainThread]: 
[0m08:18:20.953146 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m08:18:20.959237 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m08:18:21.016603 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m08:18:21.096110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bfd6ef28-f575-4fed-87f9-eb4060f790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26CCB7B00>]}
[0m08:18:21.098157 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:18:21.100181 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m08:18:21.102196 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m08:18:21.104211 [info ] [MainThread]: 
[0m08:18:21.111274 [debug] [Thread-7 (]: Began running node seed.testproj.sample
[0m08:18:21.113312 [info ] [Thread-7 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m08:18:21.115328 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m08:18:21.117347 [debug] [Thread-7 (]: Began compiling node seed.testproj.sample
[0m08:18:21.119365 [debug] [Thread-7 (]: Timing info for seed.testproj.sample (compile): 08:18:21.118357 => 08:18:21.118357
[0m08:18:21.120374 [debug] [Thread-7 (]: Began executing node seed.testproj.sample
[0m08:18:21.262088 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:18:21.264181 [debug] [Thread-7 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m08:18:21.266197 [debug] [Thread-7 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m08:18:21.268226 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m08:18:21.270247 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m08:18:21.272272 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:18:21.274295 [debug] [Thread-7 (]: Timing info for seed.testproj.sample (execute): 08:18:21.121384 => 08:18:21.274295
[0m08:18:21.418816 [debug] [Thread-7 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:18:21.420843 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bfd6ef28-f575-4fed-87f9-eb4060f790db', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26CCD8620>]}
[0m08:18:21.422869 [error] [Thread-7 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.31s]
[0m08:18:21.424886 [debug] [Thread-7 (]: Finished running node seed.testproj.sample
[0m08:18:21.430044 [debug] [MainThread]: On master: ROLLBACK
[0m08:18:21.432067 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:18:21.434143 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m08:18:21.435154 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m08:18:21.436164 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:18:21.438188 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m08:18:21.440217 [info ] [MainThread]: 
[0m08:18:21.442242 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.49 seconds (0.49s).
[0m08:18:21.445290 [debug] [MainThread]: Command end result
[0m08:18:21.457464 [info ] [MainThread]: 
[0m08:18:21.460510 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m08:18:21.462599 [info ] [MainThread]: 
[0m08:18:21.464614 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:18:21.466646 [info ] [MainThread]: 
[0m08:18:21.468674 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m08:18:21.472745 [debug] [MainThread]: Command `cli seed` failed at 08:18:21.471720 after 4.72 seconds
[0m08:18:21.474770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26DE72360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26DE725A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001E26DE72570>]}
[0m08:18:21.476797 [debug] [MainThread]: Flushing usage events
[0m08:19:29.808931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283EE53CEF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283EE53F6E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283EE53DBB0>]}


============================== 08:19:29.810957 | 9b0d4656-1537-4a1a-8401-4ce9a1662a13 ==============================
[0m08:19:29.810957 [info ] [MainThread]: Running with dbt=1.7.14
[0m08:19:29.813042 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m08:19:30.086702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9b0d4656-1537-4a1a-8401-4ce9a1662a13', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283EE6635F0>]}
[0m08:19:30.205005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9b0d4656-1537-4a1a-8401-4ce9a1662a13', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283EE2BB170>]}
[0m08:19:30.208055 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m08:19:30.235324 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m08:19:30.238397 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m08:19:30.240417 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '9b0d4656-1537-4a1a-8401-4ce9a1662a13', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283EE5B4D70>]}
[0m08:19:33.160210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9b0d4656-1537-4a1a-8401-4ce9a1662a13', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283EEA60680>]}
[0m08:19:33.186506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9b0d4656-1537-4a1a-8401-4ce9a1662a13', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283EEAFD7F0>]}
[0m08:19:33.187514 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m08:19:33.189535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b0d4656-1537-4a1a-8401-4ce9a1662a13', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283EE7E9DF0>]}
[0m08:19:33.195624 [info ] [MainThread]: 
[0m08:19:33.199689 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m08:19:33.206836 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m08:19:33.256570 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m08:19:33.325387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b0d4656-1537-4a1a-8401-4ce9a1662a13', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283EE80C5F0>]}
[0m08:19:33.327426 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:19:33.328437 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m08:19:33.331494 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m08:19:33.333539 [info ] [MainThread]: 
[0m08:19:33.342125 [debug] [Thread-7 (]: Began running node seed.testproj.sample
[0m08:19:33.343154 [info ] [Thread-7 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m08:19:33.346183 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m08:19:33.347194 [debug] [Thread-7 (]: Began compiling node seed.testproj.sample
[0m08:19:33.350232 [debug] [Thread-7 (]: Timing info for seed.testproj.sample (compile): 08:19:33.348207 => 08:19:33.349219
[0m08:19:33.351274 [debug] [Thread-7 (]: Began executing node seed.testproj.sample
[0m08:19:33.491546 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:19:33.493570 [debug] [Thread-7 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m08:19:33.495619 [debug] [Thread-7 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m08:19:33.497678 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m08:19:33.501725 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m08:19:33.503754 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:19:33.505771 [debug] [Thread-7 (]: Timing info for seed.testproj.sample (execute): 08:19:33.352288 => 08:19:33.504764
[0m08:19:33.518971 [debug] [Thread-7 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:19:33.520991 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b0d4656-1537-4a1a-8401-4ce9a1662a13', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283EEA56210>]}
[0m08:19:33.523011 [error] [Thread-7 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.18s]
[0m08:19:33.525030 [debug] [Thread-7 (]: Finished running node seed.testproj.sample
[0m08:19:33.530100 [debug] [MainThread]: On master: ROLLBACK
[0m08:19:33.532138 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:19:33.534179 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m08:19:33.535193 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m08:19:33.537218 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:19:33.538230 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m08:19:33.540774 [info ] [MainThread]: 
[0m08:19:33.542289 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.34 seconds (0.34s).
[0m08:19:33.544305 [debug] [MainThread]: Command end result
[0m08:19:33.556427 [info ] [MainThread]: 
[0m08:19:33.557437 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m08:19:33.559456 [info ] [MainThread]: 
[0m08:19:33.560463 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:19:33.562482 [info ] [MainThread]: 
[0m08:19:33.564513 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m08:19:33.568572 [debug] [MainThread]: Command `cli seed` failed at 08:19:33.567564 after 3.85 seconds
[0m08:19:33.570589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283EE5A1E20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283EEA827B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000283EEA63A40>]}
[0m08:19:33.571613 [debug] [MainThread]: Flushing usage events
[0m08:20:18.671563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130558D550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130558D040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021302262B10>]}


============================== 08:20:18.673594 | 3dcb0794-a81c-4fb6-a06b-f8cbac7a38e6 ==============================
[0m08:20:18.673594 [info ] [MainThread]: Running with dbt=1.7.14
[0m08:20:18.675610 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m08:20:18.955616 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3dcb0794-a81c-4fb6-a06b-f8cbac7a38e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000213055426F0>]}
[0m08:20:19.078045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3dcb0794-a81c-4fb6-a06b-f8cbac7a38e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002130567C2F0>]}
[0m08:20:19.080059 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m08:20:19.109398 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m08:20:19.112432 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m08:20:19.114501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3dcb0794-a81c-4fb6-a06b-f8cbac7a38e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000213055427E0>]}
[0m08:20:22.053857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3dcb0794-a81c-4fb6-a06b-f8cbac7a38e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021305B47D10>]}
[0m08:20:22.078976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3dcb0794-a81c-4fb6-a06b-f8cbac7a38e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021306BAEE70>]}
[0m08:20:22.079987 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m08:20:22.082018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3dcb0794-a81c-4fb6-a06b-f8cbac7a38e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021305A36E70>]}
[0m08:20:22.087054 [info ] [MainThread]: 
[0m08:20:22.090080 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m08:20:22.095130 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m08:20:22.144442 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m08:20:22.215686 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3dcb0794-a81c-4fb6-a06b-f8cbac7a38e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021305A8F0B0>]}
[0m08:20:22.217701 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:20:22.218710 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m08:20:22.221761 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m08:20:22.223792 [info ] [MainThread]: 
[0m08:20:22.231877 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m08:20:22.233918 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m08:20:22.236982 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m08:20:22.239009 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m08:20:22.240018 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 08:20:22.240018 => 08:20:22.240018
[0m08:20:22.242034 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m08:20:22.380732 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:20:22.381740 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m08:20:22.383757 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m08:20:22.384767 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m08:20:35.574994 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m08:20:35.578040 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:20:35.584143 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 08:20:22.243040 => 08:20:35.583128
[0m08:20:35.595326 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:20:35.598357 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3dcb0794-a81c-4fb6-a06b-f8cbac7a38e6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021306BDAE70>]}
[0m08:20:35.600386 [error] [Thread-6 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 13.36s]
[0m08:20:35.603423 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m08:20:35.610590 [debug] [MainThread]: On master: ROLLBACK
[0m08:20:35.611600 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:20:35.613617 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m08:20:35.614626 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m08:20:35.616644 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:20:35.617654 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m08:20:35.619670 [info ] [MainThread]: 
[0m08:20:35.621688 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 13.53 seconds (13.53s).
[0m08:20:35.624817 [debug] [MainThread]: Command end result
[0m08:20:35.638962 [info ] [MainThread]: 
[0m08:20:35.641001 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m08:20:35.644112 [info ] [MainThread]: 
[0m08:20:35.646142 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m08:20:35.648165 [info ] [MainThread]: 
[0m08:20:35.649173 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m08:20:35.654217 [debug] [MainThread]: Command `cli seed` failed at 08:20:35.653210 after 17.07 seconds
[0m08:20:35.656755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021305415520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021305AA8110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021306CE4A10>]}
[0m08:20:35.658790 [debug] [MainThread]: Flushing usage events
[0m08:23:02.790229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFA8FD490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFA8FC4A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFA414EC0>]}


============================== 08:23:02.792592 | 067a9177-f307-43da-993c-dfefa7550969 ==============================
[0m08:23:02.792592 [info ] [MainThread]: Running with dbt=1.7.14
[0m08:23:02.794611 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m08:23:03.059525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '067a9177-f307-43da-993c-dfefa7550969', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFA92DA60>]}
[0m08:23:03.180375 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '067a9177-f307-43da-993c-dfefa7550969', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFA92E870>]}
[0m08:23:03.183399 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m08:23:03.214245 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m08:23:03.217271 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m08:23:03.219289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '067a9177-f307-43da-993c-dfefa7550969', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFA9BD1F0>]}
[0m08:23:06.223316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '067a9177-f307-43da-993c-dfefa7550969', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFAE65A90>]}
[0m08:23:06.249668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '067a9177-f307-43da-993c-dfefa7550969', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFAD21370>]}
[0m08:23:06.250676 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m08:23:06.252689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '067a9177-f307-43da-993c-dfefa7550969', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFAE65A60>]}
[0m08:23:06.257722 [info ] [MainThread]: 
[0m08:23:06.260751 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m08:23:06.266865 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m08:23:06.314416 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m08:23:06.386321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '067a9177-f307-43da-993c-dfefa7550969', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFAD53E60>]}
[0m08:23:06.387329 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:23:06.389346 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m08:23:06.391374 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m08:23:06.393400 [info ] [MainThread]: 
[0m08:23:06.400518 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m08:23:06.402540 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m08:23:06.407072 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m08:23:06.409125 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m08:23:06.411142 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 08:23:06.410133 => 08:23:06.410133
[0m08:23:06.412150 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m08:23:06.552289 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:23:06.553296 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m08:23:06.555319 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m08:23:06.556331 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m08:23:13.621139 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m08:23:13.622144 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m08:23:13.624159 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 08:23:06.413158 => 08:23:13.624159
[0m08:23:13.636362 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m08:23:13.638375 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '067a9177-f307-43da-993c-dfefa7550969', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFAEB34A0>]}
[0m08:23:13.639382 [error] [Thread-6 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 7.23s]
[0m08:23:13.642426 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m08:23:13.647480 [debug] [MainThread]: On master: ROLLBACK
[0m08:23:13.648491 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:23:13.650505 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m08:23:13.651515 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m08:23:13.653537 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:23:13.654544 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m08:23:13.656578 [info ] [MainThread]: 
[0m08:23:13.658593 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 7.40 seconds (7.40s).
[0m08:23:13.660612 [debug] [MainThread]: Command end result
[0m08:23:13.676877 [info ] [MainThread]: 
[0m08:23:13.678897 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m08:23:13.680915 [info ] [MainThread]: 
[0m08:23:13.681922 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m08:23:13.683940 [info ] [MainThread]: 
[0m08:23:13.685964 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m08:23:13.691053 [debug] [MainThread]: Command `cli seed` failed at 08:23:13.690045 after 10.99 seconds
[0m08:23:13.693091 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFA8B2840>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFBFFE420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FFFBFFDC10>]}
[0m08:23:13.695112 [debug] [MainThread]: Flushing usage events
[0m08:24:32.431079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027C3D0C1CA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027C3D0C1A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027C3D0C1A30>]}


============================== 08:24:32.435137 | a344b86d-934c-4ded-b3ed-d29b055e9ad2 ==============================
[0m08:24:32.435137 [info ] [MainThread]: Running with dbt=1.7.14
[0m08:24:32.437196 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m08:24:32.713659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a344b86d-934c-4ded-b3ed-d29b055e9ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027C3CFA06E0>]}
[0m08:24:32.838113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a344b86d-934c-4ded-b3ed-d29b055e9ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027C3D295190>]}
[0m08:24:32.841140 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m08:24:32.871511 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m08:24:32.873536 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m08:24:32.875552 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a344b86d-934c-4ded-b3ed-d29b055e9ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027C3D1FFC20>]}
[0m08:24:35.896986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a344b86d-934c-4ded-b3ed-d29b055e9ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027C3D698BF0>]}
[0m08:24:35.923397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a344b86d-934c-4ded-b3ed-d29b055e9ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027C3D7D80B0>]}
[0m08:24:35.924407 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m08:24:35.926424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a344b86d-934c-4ded-b3ed-d29b055e9ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027C3D389070>]}
[0m08:24:35.935606 [info ] [MainThread]: 
[0m08:24:35.941682 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m08:24:35.945722 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m08:24:35.994478 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m08:24:36.067980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a344b86d-934c-4ded-b3ed-d29b055e9ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027C3B983E00>]}
[0m08:24:36.070013 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:24:36.072033 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m08:24:36.074064 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m08:24:36.076086 [info ] [MainThread]: 
[0m08:24:36.083154 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m08:24:36.087224 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m08:24:36.091292 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m08:24:36.093335 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m08:24:36.095350 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 08:24:36.094342 => 08:24:36.094342
[0m08:24:36.097388 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m08:24:36.239667 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:24:36.241694 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m08:24:36.242703 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m08:24:36.243714 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m08:24:39.554658 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m08:24:39.556693 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m08:24:39.558729 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 08:24:36.098409 => 08:24:39.557704
[0m08:24:39.572633 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m08:24:39.575707 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a344b86d-934c-4ded-b3ed-d29b055e9ad2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027C3D79A3F0>]}
[0m08:24:39.577735 [error] [Thread-6 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 3.49s]
[0m08:24:39.579759 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m08:24:39.589972 [debug] [MainThread]: On master: ROLLBACK
[0m08:24:39.592007 [debug] [MainThread]: Opening a new connection, currently in state init
[0m08:24:39.593027 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m08:24:39.595058 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m08:24:39.596067 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m08:24:39.597080 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m08:24:39.600154 [info ] [MainThread]: 
[0m08:24:39.601171 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 3.66 seconds (3.66s).
[0m08:24:39.604214 [debug] [MainThread]: Command end result
[0m08:24:39.620011 [info ] [MainThread]: 
[0m08:24:39.622572 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m08:24:39.624613 [info ] [MainThread]: 
[0m08:24:39.626661 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m08:24:39.628188 [info ] [MainThread]: 
[0m08:24:39.630301 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m08:24:39.634395 [debug] [MainThread]: Command `cli seed` failed at 08:24:39.633885 after 7.30 seconds
[0m08:24:39.636937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027C3D0C1A30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027C3E835BB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027C3E835AC0>]}
[0m08:24:39.638455 [debug] [MainThread]: Flushing usage events
[0m09:01:25.449612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002281AF4D340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002281AF4C440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002281AF4FA70>]}


============================== 09:01:25.452743 | 7c1cbc9a-0ca4-4f86-9c70-c3e29c9e9e6e ==============================
[0m09:01:25.452743 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:01:25.453757 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m09:01:25.758715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7c1cbc9a-0ca4-4f86-9c70-c3e29c9e9e6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002281AF03F20>]}
[0m09:01:25.887389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7c1cbc9a-0ca4-4f86-9c70-c3e29c9e9e6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002281AF02570>]}
[0m09:01:25.891466 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:01:25.938563 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:01:25.941602 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:01:25.944732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '7c1cbc9a-0ca4-4f86-9c70-c3e29c9e9e6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002281A5F5730>]}
[0m09:01:30.235924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7c1cbc9a-0ca4-4f86-9c70-c3e29c9e9e6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002281AC6BE60>]}
[0m09:01:30.251099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7c1cbc9a-0ca4-4f86-9c70-c3e29c9e9e6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002281B492C30>]}
[0m09:01:30.253124 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:01:30.254645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7c1cbc9a-0ca4-4f86-9c70-c3e29c9e9e6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002281B1E3680>]}
[0m09:01:30.260716 [info ] [MainThread]: 
[0m09:01:30.264837 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:01:30.269925 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:01:30.322862 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:01:30.404128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7c1cbc9a-0ca4-4f86-9c70-c3e29c9e9e6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002281B475970>]}
[0m09:01:30.406152 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:01:30.407163 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:01:30.410216 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:01:30.411223 [info ] [MainThread]: 
[0m09:01:30.418307 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m09:01:30.419315 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m09:01:30.422348 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m09:01:30.424376 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m09:01:30.426405 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 09:01:30.425390 => 09:01:30.425390
[0m09:01:30.428451 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m09:01:30.580624 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:01:30.581634 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:01:30.583682 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:01:30.584714 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m09:01:36.420033 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:01:36.422053 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:01:36.424071 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 09:01:30.429465 => 09:01:36.423062
[0m09:01:36.540492 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:01:36.542524 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7c1cbc9a-0ca4-4f86-9c70-c3e29c9e9e6e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002281C677D10>]}
[0m09:01:36.544555 [error] [Thread-6 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 6.12s]
[0m09:01:36.546572 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m09:01:36.551661 [debug] [MainThread]: On master: ROLLBACK
[0m09:01:36.553687 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:01:36.554695 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m09:01:36.555715 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m09:01:36.556724 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:01:36.558742 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:01:36.560764 [info ] [MainThread]: 
[0m09:01:36.562786 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 6.30 seconds (6.30s).
[0m09:01:36.565869 [debug] [MainThread]: Command end result
[0m09:01:36.580663 [info ] [MainThread]: 
[0m09:01:36.581666 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m09:01:36.583694 [info ] [MainThread]: 
[0m09:01:36.584705 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:01:36.586723 [info ] [MainThread]: 
[0m09:01:36.587734 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m09:01:36.591809 [debug] [MainThread]: Command `cli seed` failed at 09:01:36.591809 after 11.24 seconds
[0m09:01:36.594858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002281AAA2750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002281C6B1370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002281C6B13A0>]}
[0m09:01:36.595883 [debug] [MainThread]: Flushing usage events
[0m09:03:28.829186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029771161880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029771160140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029771163C50>]}


============================== 09:03:28.832249 | 190e8743-6d91-4226-a94a-d37d50ed8de7 ==============================
[0m09:03:28.832249 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:03:28.834284 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m09:03:29.091373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '190e8743-6d91-4226-a94a-d37d50ed8de7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029771053830>]}
[0m09:03:29.211259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '190e8743-6d91-4226-a94a-d37d50ed8de7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000297711BDB50>]}
[0m09:03:29.213274 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:03:29.243658 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:03:29.245670 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:03:29.247688 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '190e8743-6d91-4226-a94a-d37d50ed8de7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000297712BCAA0>]}
[0m09:03:32.210510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '190e8743-6d91-4226-a94a-d37d50ed8de7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000297712BC740>]}
[0m09:03:32.236923 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '190e8743-6d91-4226-a94a-d37d50ed8de7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000297715DB8F0>]}
[0m09:03:32.237932 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:03:32.239950 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '190e8743-6d91-4226-a94a-d37d50ed8de7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000297712ED100>]}
[0m09:03:32.244991 [info ] [MainThread]: 
[0m09:03:32.249073 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:03:32.254123 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:03:32.304410 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:03:32.375566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '190e8743-6d91-4226-a94a-d37d50ed8de7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029771675970>]}
[0m09:03:32.376575 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:03:32.378597 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:03:32.381635 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:03:32.383668 [info ] [MainThread]: 
[0m09:03:32.390766 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m09:03:32.391777 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m09:03:32.395838 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m09:03:32.397874 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m09:03:32.402456 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 09:03:32.398901 => 09:03:32.398901
[0m09:03:32.404493 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m09:03:32.545280 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:03:32.547305 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:03:32.548317 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:03:32.550346 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m09:03:36.885218 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:03:36.887263 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:03:36.889292 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 09:03:32.405521 => 09:03:36.888274
[0m09:03:36.901452 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:03:36.902477 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '190e8743-6d91-4226-a94a-d37d50ed8de7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002977179F950>]}
[0m09:03:36.904992 [error] [Thread-6 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 4.51s]
[0m09:03:36.907013 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m09:03:36.912078 [debug] [MainThread]: On master: ROLLBACK
[0m09:03:36.914100 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:03:36.915113 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m09:03:36.917153 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m09:03:36.918203 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:03:36.920234 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:03:36.922254 [info ] [MainThread]: 
[0m09:03:36.924277 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 4.68 seconds (4.68s).
[0m09:03:36.926306 [debug] [MainThread]: Command end result
[0m09:03:36.943212 [info ] [MainThread]: 
[0m09:03:36.944230 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m09:03:36.946255 [info ] [MainThread]: 
[0m09:03:36.948273 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:03:36.956581 [info ] [MainThread]: 
[0m09:03:36.959660 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m09:03:36.965794 [debug] [MainThread]: Command `cli seed` failed at 09:03:36.964779 after 8.23 seconds
[0m09:03:36.968835 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029771116270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029772861B80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029772860980>]}
[0m09:03:36.970883 [debug] [MainThread]: Flushing usage events
[0m09:06:02.408644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B2990FFFB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B298C55D00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B298C545C0>]}


============================== 09:06:02.411683 | 70c9d60d-98a4-4f96-824e-37ab4fbf31c1 ==============================
[0m09:06:02.411683 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:06:02.413704 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:06:02.692506 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '70c9d60d-98a4-4f96-824e-37ab4fbf31c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B2991327B0>]}
[0m09:06:02.830935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '70c9d60d-98a4-4f96-824e-37ab4fbf31c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B29897DBB0>]}
[0m09:06:02.834008 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:06:02.861420 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:06:02.864471 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:06:02.867521 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '70c9d60d-98a4-4f96-824e-37ab4fbf31c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B2991F77A0>]}
[0m09:06:05.882078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '70c9d60d-98a4-4f96-824e-37ab4fbf31c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B29951D670>]}
[0m09:06:05.899378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '70c9d60d-98a4-4f96-824e-37ab4fbf31c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B29921BB00>]}
[0m09:06:05.901396 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:06:05.903413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '70c9d60d-98a4-4f96-824e-37ab4fbf31c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B2992BD520>]}
[0m09:06:05.908457 [info ] [MainThread]: 
[0m09:06:05.911607 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:06:05.920824 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:06:05.971908 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:06:06.042697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '70c9d60d-98a4-4f96-824e-37ab4fbf31c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B29921B8F0>]}
[0m09:06:06.044721 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:06:06.045731 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:06:06.047751 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:06:06.049783 [info ] [MainThread]: 
[0m09:06:06.056880 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m09:06:06.057892 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m09:06:06.061947 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m09:06:06.064003 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m09:06:06.067089 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 09:06:06.065017 => 09:06:06.065017
[0m09:06:06.068108 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m09:06:06.207805 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:06:06.209823 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:06:06.211342 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:06:06.212353 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m09:06:09.825774 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:06:09.826790 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:06:09.829834 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 09:06:06.069131 => 09:06:09.828825
[0m09:06:09.841077 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:06:09.843653 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '70c9d60d-98a4-4f96-824e-37ab4fbf31c1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B29A847EF0>]}
[0m09:06:09.845689 [error] [Thread-6 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 3.78s]
[0m09:06:09.848823 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m09:06:09.855010 [debug] [MainThread]: On master: ROLLBACK
[0m09:06:09.856536 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:06:09.858582 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m09:06:09.860120 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m09:06:09.862204 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:06:09.863220 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:06:09.866818 [info ] [MainThread]: 
[0m09:06:09.868361 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 3.96 seconds (3.96s).
[0m09:06:09.870922 [debug] [MainThread]: Command end result
[0m09:06:09.885276 [info ] [MainThread]: 
[0m09:06:09.888324 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m09:06:09.889333 [info ] [MainThread]: 
[0m09:06:09.891351 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:06:09.892362 [info ] [MainThread]: 
[0m09:06:09.894387 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m09:06:09.898427 [debug] [MainThread]: Command `cli seed` failed at 09:06:09.898427 after 7.58 seconds
[0m09:06:09.901517 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B298F99DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B29A806C00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B29A807E90>]}
[0m09:06:09.903553 [debug] [MainThread]: Flushing usage events
[0m09:07:10.382691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018FDD373050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018FDD371B20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018FDD371C10>]}


============================== 09:07:10.385730 | 07d4fd86-d24d-4c15-8f72-9681ba4de5a2 ==============================
[0m09:07:10.385730 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:07:10.387758 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:07:10.656760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '07d4fd86-d24d-4c15-8f72-9681ba4de5a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018FDD3F0980>]}
[0m09:07:10.774877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '07d4fd86-d24d-4c15-8f72-9681ba4de5a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018FDD3BD5B0>]}
[0m09:07:10.776896 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:07:10.806259 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:07:10.808275 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:07:10.810299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '07d4fd86-d24d-4c15-8f72-9681ba4de5a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018FDD252990>]}
[0m09:07:13.868250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '07d4fd86-d24d-4c15-8f72-9681ba4de5a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018FDD8AEE70>]}
[0m09:07:13.883473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '07d4fd86-d24d-4c15-8f72-9681ba4de5a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018FDD8E41A0>]}
[0m09:07:13.885495 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:07:13.887514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07d4fd86-d24d-4c15-8f72-9681ba4de5a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018FDD942B40>]}
[0m09:07:13.892556 [info ] [MainThread]: 
[0m09:07:13.896677 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:07:13.901747 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:07:13.950389 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:07:14.022877 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07d4fd86-d24d-4c15-8f72-9681ba4de5a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018FDD95B9E0>]}
[0m09:07:14.024903 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:07:14.026926 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:07:14.028985 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:07:14.032099 [info ] [MainThread]: 
[0m09:07:14.039204 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m09:07:14.041228 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m09:07:14.043253 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m09:07:14.045279 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m09:07:14.047342 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 09:07:14.046292 => 09:07:14.046292
[0m09:07:14.048352 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m09:07:14.187574 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:07:14.189595 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:07:14.191616 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:07:14.193647 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m09:07:14.305637 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:07:14.307670 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:07:14.309686 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 09:07:14.050378 => 09:07:14.308678
[0m09:07:14.318768 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:07:14.320784 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07d4fd86-d24d-4c15-8f72-9681ba4de5a2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018FDD419D90>]}
[0m09:07:14.322833 [error] [Thread-6 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.28s]
[0m09:07:14.324849 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m09:07:14.329960 [debug] [MainThread]: On master: ROLLBACK
[0m09:07:14.331997 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:07:14.334038 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m09:07:14.335050 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m09:07:14.337133 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:07:14.338147 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:07:14.340163 [info ] [MainThread]: 
[0m09:07:14.342178 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.44 seconds (0.44s).
[0m09:07:14.344197 [debug] [MainThread]: Command end result
[0m09:07:14.358560 [info ] [MainThread]: 
[0m09:07:14.360583 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m09:07:14.362613 [info ] [MainThread]: 
[0m09:07:14.364645 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:07:14.366769 [info ] [MainThread]: 
[0m09:07:14.368785 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m09:07:14.372850 [debug] [MainThread]: Command `cli seed` failed at 09:07:14.371835 after 4.08 seconds
[0m09:07:14.373858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018FDD97FC20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018FDD97FDA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018FDD70BF80>]}
[0m09:07:14.375875 [debug] [MainThread]: Flushing usage events
[0m09:07:38.970349 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234ECACD010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234ECACD640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234ECACD130>]}


============================== 09:07:38.973380 | 6ccbb87e-a078-4bac-8557-9fe04176f355 ==============================
[0m09:07:38.973380 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:07:38.975406 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:07:39.239694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6ccbb87e-a078-4bac-8557-9fe04176f355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234ECB28650>]}
[0m09:07:39.358840 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6ccbb87e-a078-4bac-8557-9fe04176f355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234ECC5BA40>]}
[0m09:07:39.361938 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:07:39.391801 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:07:39.394839 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:07:39.396878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6ccbb87e-a078-4bac-8557-9fe04176f355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234ECBC3440>]}
[0m09:07:42.355175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6ccbb87e-a078-4bac-8557-9fe04176f355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234ECD3BFB0>]}
[0m09:07:42.372951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6ccbb87e-a078-4bac-8557-9fe04176f355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234ECC820F0>]}
[0m09:07:42.373961 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:07:42.375980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6ccbb87e-a078-4bac-8557-9fe04176f355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234ECE1F9E0>]}
[0m09:07:42.382060 [info ] [MainThread]: 
[0m09:07:42.384575 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:07:42.389649 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:07:42.457106 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:07:42.529927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6ccbb87e-a078-4bac-8557-9fe04176f355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234ED07A9C0>]}
[0m09:07:42.533019 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:07:42.535055 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:07:42.537575 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:07:42.538584 [info ] [MainThread]: 
[0m09:07:42.545658 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m09:07:42.547704 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m09:07:42.549721 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m09:07:42.551739 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m09:07:42.553765 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 09:07:42.552749 => 09:07:42.552749
[0m09:07:42.554786 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m09:07:42.704208 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:07:42.706224 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:07:42.707226 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:07:42.708736 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m09:07:42.820646 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:07:42.822660 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:07:42.823668 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 09:07:42.555804 => 09:07:42.823668
[0m09:07:42.834919 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:07:42.836440 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6ccbb87e-a078-4bac-8557-9fe04176f355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234EE35FE30>]}
[0m09:07:42.837456 [error] [Thread-6 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 0.29s]
[0m09:07:42.839473 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m09:07:42.844578 [debug] [MainThread]: On master: ROLLBACK
[0m09:07:42.846604 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:07:42.847626 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m09:07:42.848635 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m09:07:42.850667 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:07:42.851677 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:07:42.853702 [info ] [MainThread]: 
[0m09:07:42.855722 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.47 seconds (0.47s).
[0m09:07:42.857809 [debug] [MainThread]: Command end result
[0m09:07:42.871566 [info ] [MainThread]: 
[0m09:07:42.873617 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m09:07:42.874635 [info ] [MainThread]: 
[0m09:07:42.876662 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:07:42.877682 [info ] [MainThread]: 
[0m09:07:42.879712 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m09:07:42.881740 [debug] [MainThread]: Command `cli seed` failed at 09:07:42.881740 after 4.00 seconds
[0m09:07:42.882748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234ECACF6E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234ECACD010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000234ED177920>]}
[0m09:07:42.884253 [debug] [MainThread]: Flushing usage events
[0m09:09:20.006310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ACB761C980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ACB761CCB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ACB761FFE0>]}


============================== 09:09:20.009341 | 16f85580-e6c1-4b54-af22-58830f5b79c7 ==============================
[0m09:09:20.009341 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:09:20.011358 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'invocation_command': 'dbt ', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:09:20.300490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '16f85580-e6c1-4b54-af22-58830f5b79c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ACB6EDD5E0>]}
[0m09:09:20.418742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '16f85580-e6c1-4b54-af22-58830f5b79c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ACB7513200>]}
[0m09:09:20.420757 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:09:20.450254 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:09:20.452273 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:09:20.454301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '16f85580-e6c1-4b54-af22-58830f5b79c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ACB74819A0>]}
[0m09:09:23.387435 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '16f85580-e6c1-4b54-af22-58830f5b79c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ACB7BDBFB0>]}
[0m09:09:23.403649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '16f85580-e6c1-4b54-af22-58830f5b79c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ACB7B46B40>]}
[0m09:09:23.404658 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:09:23.406674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '16f85580-e6c1-4b54-af22-58830f5b79c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ACB7858380>]}
[0m09:09:23.411726 [info ] [MainThread]: 
[0m09:09:23.414802 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:09:23.418870 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:09:39.867011 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:09:45.612283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '16f85580-e6c1-4b54-af22-58830f5b79c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ACB7CB8680>]}
[0m09:09:45.614315 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:09:45.616351 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:09:45.618417 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:09:45.620446 [info ] [MainThread]: 
[0m09:09:45.627534 [debug] [Thread-7 (]: Began running node seed.testproj.sample
[0m09:09:45.629567 [info ] [Thread-7 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m09:09:45.633657 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m09:09:45.635677 [debug] [Thread-7 (]: Began compiling node seed.testproj.sample
[0m09:09:45.637708 [debug] [Thread-7 (]: Timing info for seed.testproj.sample (compile): 09:09:45.636695 => 09:09:45.636695
[0m09:09:45.638739 [debug] [Thread-7 (]: Began executing node seed.testproj.sample
[0m09:09:45.785634 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:09:45.786666 [debug] [Thread-7 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:09:45.787677 [debug] [Thread-7 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:09:45.789694 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m09:10:10.192961 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:10:10.195003 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:10:10.200066 [debug] [Thread-7 (]: Timing info for seed.testproj.sample (execute): 09:09:45.639751 => 09:10:10.199057
[0m09:10:10.211273 [debug] [Thread-7 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:10:10.213288 [debug] [Thread-7 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '16f85580-e6c1-4b54-af22-58830f5b79c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ACB8EE1C10>]}
[0m09:10:10.215304 [error] [Thread-7 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 24.58s]
[0m09:10:10.217320 [debug] [Thread-7 (]: Finished running node seed.testproj.sample
[0m09:10:10.223405 [debug] [MainThread]: On master: ROLLBACK
[0m09:10:10.225435 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:10:10.227474 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m09:10:10.229517 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m09:10:10.230540 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:10:10.231569 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:10:10.234659 [info ] [MainThread]: 
[0m09:10:10.236700 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 46.82 seconds (46.82s).
[0m09:10:10.238731 [debug] [MainThread]: Command end result
[0m09:10:10.253019 [info ] [MainThread]: 
[0m09:10:10.255036 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m09:10:10.256046 [info ] [MainThread]: 
[0m09:10:10.258068 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't encode character '\U0001f4cc' in position 93: character maps to <undefined>
[0m09:10:10.260104 [info ] [MainThread]: 
[0m09:10:10.262152 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m09:10:10.268280 [debug] [MainThread]: Command `cli seed` failed at 09:10:10.267266 after 50.35 seconds
[0m09:10:10.271369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ACB74A6F00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ACB8D641D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001ACB8D65610>]}
[0m09:10:10.273396 [debug] [MainThread]: Flushing usage events
[0m09:11:48.343578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208D845A510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208D845A570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208D845A0F0>]}


============================== 09:11:48.346603 | 6a2b8a43-af28-4c89-b771-2ae09c0a7b88 ==============================
[0m09:11:48.346603 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:11:48.348622 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m09:11:48.615865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6a2b8a43-af28-4c89-b771-2ae09c0a7b88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208D84A3B30>]}
[0m09:11:48.739714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6a2b8a43-af28-4c89-b771-2ae09c0a7b88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208D863AEA0>]}
[0m09:11:48.741739 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:11:48.772819 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:11:48.775873 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:11:48.776884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6a2b8a43-af28-4c89-b771-2ae09c0a7b88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208D8609A90>]}
[0m09:11:51.716215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6a2b8a43-af28-4c89-b771-2ae09c0a7b88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208D89F8A10>]}
[0m09:11:51.735669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6a2b8a43-af28-4c89-b771-2ae09c0a7b88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208D860BB30>]}
[0m09:11:51.737703 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:11:51.739795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6a2b8a43-af28-4c89-b771-2ae09c0a7b88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208D85A2EA0>]}
[0m09:11:51.745894 [info ] [MainThread]: 
[0m09:11:51.748944 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:11:51.754053 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:11:56.439817 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:11:58.646092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6a2b8a43-af28-4c89-b771-2ae09c0a7b88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208D8BBD2B0>]}
[0m09:11:58.649149 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:11:58.652204 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:11:58.654226 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:11:58.656248 [info ] [MainThread]: 
[0m09:11:58.666473 [debug] [Thread-7 (]: Began running node seed.testproj.sample
[0m09:11:58.669530 [info ] [Thread-7 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m09:11:58.672686 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m09:11:58.675713 [debug] [Thread-7 (]: Began compiling node seed.testproj.sample
[0m09:11:58.677743 [debug] [Thread-7 (]: Timing info for seed.testproj.sample (compile): 09:11:58.676723 => 09:11:58.676723
[0m09:11:58.678756 [debug] [Thread-7 (]: Began executing node seed.testproj.sample
[0m09:11:58.837588 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:11:58.838655 [debug] [Thread-7 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:11:58.840748 [debug] [Thread-7 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:11:58.842806 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m09:14:03.098974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20E99D2E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20E99C2C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20DE54E00>]}


============================== 09:14:03.100994 | 4d0c0aad-1804-42b0-a3cc-5041a5ba911f ==============================
[0m09:14:03.100994 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:14:03.103014 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:14:03.368974 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '4d0c0aad-1804-42b0-a3cc-5041a5ba911f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20E827D40>]}
[0m09:14:03.490264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '4d0c0aad-1804-42b0-a3cc-5041a5ba911f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20E9FDE20>]}
[0m09:14:03.492284 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:14:03.523747 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:14:03.527851 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:14:03.530920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4d0c0aad-1804-42b0-a3cc-5041a5ba911f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20E8FDD30>]}
[0m09:14:06.461445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4d0c0aad-1804-42b0-a3cc-5041a5ba911f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20EDBBE30>]}
[0m09:14:06.487301 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4d0c0aad-1804-42b0-a3cc-5041a5ba911f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20ECEECC0>]}
[0m09:14:06.488330 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:14:06.490348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4d0c0aad-1804-42b0-a3cc-5041a5ba911f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A20EEB1370>]}
[0m09:14:06.497482 [info ] [MainThread]: 
[0m09:14:06.500530 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:14:06.505637 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:14:10.497433 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:14:12.444258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4d0c0aad-1804-42b0-a3cc-5041a5ba911f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A210062CC0>]}
[0m09:14:12.446813 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:14:12.447320 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:14:12.450348 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:14:12.451357 [info ] [MainThread]: 
[0m09:14:12.458434 [debug] [Thread-7 (]: Began running node seed.testproj.sample
[0m09:14:12.460461 [info ] [Thread-7 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m09:14:12.464689 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m09:14:12.466723 [debug] [Thread-7 (]: Began compiling node seed.testproj.sample
[0m09:14:12.468748 [debug] [Thread-7 (]: Timing info for seed.testproj.sample (compile): 09:14:12.467737 => 09:14:12.467737
[0m09:14:12.469757 [debug] [Thread-7 (]: Began executing node seed.testproj.sample
[0m09:14:12.619809 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:14:12.620825 [debug] [Thread-7 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:14:12.622855 [debug] [Thread-7 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:14:12.623866 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m09:15:13.620639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A2911D610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A2911C2F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A2911C110>]}


============================== 09:15:13.622652 | 9b563b7d-44ac-4057-8bd3-fbb1266611d0 ==============================
[0m09:15:13.622652 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:15:13.624667 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m09:15:13.889438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9b563b7d-44ac-4057-8bd3-fbb1266611d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A29212510>]}
[0m09:15:14.008739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9b563b7d-44ac-4057-8bd3-fbb1266611d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A291DF950>]}
[0m09:15:14.011779 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:15:14.041636 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:15:14.043653 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:15:14.044661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '9b563b7d-44ac-4057-8bd3-fbb1266611d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A29212750>]}
[0m09:15:16.988203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9b563b7d-44ac-4057-8bd3-fbb1266611d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A2927B1A0>]}
[0m09:15:17.014525 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9b563b7d-44ac-4057-8bd3-fbb1266611d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A29278050>]}
[0m09:15:17.015532 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:15:17.017548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b563b7d-44ac-4057-8bd3-fbb1266611d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A295C8320>]}
[0m09:15:17.022587 [info ] [MainThread]: 
[0m09:15:17.025610 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:15:17.031766 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:15:23.548170 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:15:28.407225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b563b7d-44ac-4057-8bd3-fbb1266611d0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015A29383290>]}
[0m09:15:28.409269 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:15:28.411340 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:15:28.414391 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:15:28.417447 [info ] [MainThread]: 
[0m09:15:28.426696 [debug] [Thread-7 (]: Began running node seed.testproj.sample
[0m09:15:28.429771 [info ] [Thread-7 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m09:15:28.433861 [debug] [Thread-7 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m09:15:28.435903 [debug] [Thread-7 (]: Began compiling node seed.testproj.sample
[0m09:15:28.437943 [debug] [Thread-7 (]: Timing info for seed.testproj.sample (compile): 09:15:28.436927 => 09:15:28.436927
[0m09:15:28.440064 [debug] [Thread-7 (]: Began executing node seed.testproj.sample
[0m09:15:28.600746 [debug] [Thread-7 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:15:28.601753 [debug] [Thread-7 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:15:28.603775 [debug] [Thread-7 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:15:28.604789 [debug] [Thread-7 (]: Opening a new connection, currently in state init
[0m09:17:09.815513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0483B2A80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0483B2420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0483B2000>]}


============================== 09:17:09.817562 | 218f6983-af4c-4aee-abb5-e7fc12dcf439 ==============================
[0m09:17:09.817562 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:17:09.819586 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m09:17:10.088934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '218f6983-af4c-4aee-abb5-e7fc12dcf439', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0484F60F0>]}
[0m09:17:10.216751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '218f6983-af4c-4aee-abb5-e7fc12dcf439', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C048564FB0>]}
[0m09:17:10.218785 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:17:10.249285 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:17:10.251304 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:17:10.253324 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '218f6983-af4c-4aee-abb5-e7fc12dcf439', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C048437560>]}
[0m09:17:13.155690 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '218f6983-af4c-4aee-abb5-e7fc12dcf439', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0487AF740>]}
[0m09:17:13.183155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '218f6983-af4c-4aee-abb5-e7fc12dcf439', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0487AE570>]}
[0m09:17:13.185188 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:17:13.186202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '218f6983-af4c-4aee-abb5-e7fc12dcf439', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C04845BA40>]}
[0m09:17:13.194341 [info ] [MainThread]: 
[0m09:17:13.199482 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:17:13.207138 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:17:13.257291 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:17:13.332894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '218f6983-af4c-4aee-abb5-e7fc12dcf439', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C04892E780>]}
[0m09:17:13.333902 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:17:13.335926 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:17:13.337954 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:17:13.339969 [info ] [MainThread]: 
[0m09:17:13.347091 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m09:17:13.349109 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m09:17:13.351131 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m09:17:13.353148 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m09:17:13.355166 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 09:17:13.354158 => 09:17:13.354158
[0m09:17:13.356233 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m09:17:13.497752 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:17:13.499775 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:17:13.500783 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:17:13.502801 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m09:17:26.693962 [debug] [Thread-6 (]: SQL status: OK in 13.1899995803833 seconds
[0m09:17:26.734724 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m09:17:26.744817 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:17:26.746835 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m09:17:31.905064 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      
[0m09:17:31.907090 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m09:17:31.909115 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 09:17:13.357237 => 09:17:31.909115
[0m09:17:31.933465 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m09:17:31.935494 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '218f6983-af4c-4aee-abb5-e7fc12dcf439', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C049B47DA0>]}
[0m09:17:31.937518 [error] [Thread-6 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 18.59s]
[0m09:17:31.940560 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m09:17:31.946676 [debug] [MainThread]: On master: ROLLBACK
[0m09:17:31.949757 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:17:31.951824 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m09:17:31.953867 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m09:17:31.954879 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:17:31.956906 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:17:31.958961 [info ] [MainThread]: 
[0m09:17:31.961050 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 18.76 seconds (18.76s).
[0m09:17:31.964109 [debug] [MainThread]: Command end result
[0m09:17:31.979460 [info ] [MainThread]: 
[0m09:17:31.981489 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m09:17:31.982997 [info ] [MainThread]: 
[0m09:17:31.984008 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m09:17:31.986038 [info ] [MainThread]: 
[0m09:17:31.987569 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m09:17:31.994304 [debug] [MainThread]: Command `cli seed` failed at 09:17:31.993277 after 22.27 seconds
[0m09:17:31.997337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C0483B3FE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C049B0AAB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C049B0A5A0>]}
[0m09:17:31.999375 [debug] [MainThread]: Flushing usage events
[0m09:24:27.741386 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024890D0D370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024890D0C140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024890D0F7A0>]}


============================== 09:24:27.743408 | 2f823dab-9890-49a9-a0c6-231ca422d9c7 ==============================
[0m09:24:27.743408 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:24:27.745425 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m09:24:28.022339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2f823dab-9890-49a9-a0c6-231ca422d9c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024890DCD340>]}
[0m09:24:28.141625 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2f823dab-9890-49a9-a0c6-231ca422d9c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024890C6D970>]}
[0m09:24:28.144664 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:24:28.173068 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:24:28.175093 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:24:28.177609 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2f823dab-9890-49a9-a0c6-231ca422d9c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024890D6C980>]}
[0m09:24:31.133090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2f823dab-9890-49a9-a0c6-231ca422d9c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024890D6D670>]}
[0m09:24:31.148263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2f823dab-9890-49a9-a0c6-231ca422d9c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002489121D310>]}
[0m09:24:31.150283 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:24:31.152298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f823dab-9890-49a9-a0c6-231ca422d9c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024890E72C60>]}
[0m09:24:31.157355 [info ] [MainThread]: 
[0m09:24:31.160441 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:24:31.167597 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:24:31.215769 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:24:31.286637 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2f823dab-9890-49a9-a0c6-231ca422d9c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002488E2AC470>]}
[0m09:24:31.288658 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:24:31.290695 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:24:31.292724 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:24:31.294750 [info ] [MainThread]: 
[0m09:24:31.302934 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m09:24:31.303946 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m09:24:31.306974 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m09:24:31.308990 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m09:24:31.311013 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 09:24:31.310000 => 09:24:31.310000
[0m09:24:31.313090 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m09:24:31.450055 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:24:31.451082 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:24:31.453110 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:24:31.454118 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m09:24:38.476263 [debug] [Thread-6 (]: SQL status: OK in 7.019999980926514 seconds
[0m09:24:38.514351 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m09:24:38.526568 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:24:38.528628 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m09:24:39.799561 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      
[0m09:24:39.802662 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: 'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m09:24:39.803676 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 09:24:31.314101 => 09:24:39.803676
[0m09:24:39.812761 [debug] [Thread-6 (]: Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m09:24:39.813769 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2f823dab-9890-49a9-a0c6-231ca422d9c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002489235AA50>]}
[0m09:24:39.815792 [error] [Thread-6 (]: 1 of 1 ERROR loading seed file datalake.sample ................................. [[31mERROR[0m in 8.51s]
[0m09:24:39.818899 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m09:24:39.824989 [debug] [MainThread]: On master: ROLLBACK
[0m09:24:39.827059 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:24:39.828083 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m09:24:39.829106 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m09:24:39.831136 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:24:39.832155 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:24:39.835226 [info ] [MainThread]: 
[0m09:24:39.836235 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 8.67 seconds (8.67s).
[0m09:24:39.839290 [debug] [MainThread]: Command end result
[0m09:24:39.854535 [info ] [MainThread]: 
[0m09:24:39.855541 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m09:24:39.857558 [info ] [MainThread]: 
[0m09:24:39.859591 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  'charmap' codec can't decode byte 0x9d in position 134: character maps to <undefined>
[0m09:24:39.861624 [info ] [MainThread]: 
[0m09:24:39.862706 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m09:24:39.866761 [debug] [MainThread]: Command `cli seed` failed at 09:24:39.865740 after 12.22 seconds
[0m09:24:39.869813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024890D0CFE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024892448AA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024892448260>]}
[0m09:24:39.870839 [debug] [MainThread]: Flushing usage events
[0m09:25:17.802908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B831F3C5F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B831F3FC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B8315EC470>]}


============================== 09:25:17.804928 | 81d19626-9400-4ace-8b4b-5f604d0cfdbe ==============================
[0m09:25:17.804928 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:25:17.806960 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m09:25:18.111673 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '81d19626-9400-4ace-8b4b-5f604d0cfdbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B831EED670>]}
[0m09:25:18.237604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '81d19626-9400-4ace-8b4b-5f604d0cfdbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B831EED670>]}
[0m09:25:18.240639 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:25:18.271693 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:25:18.274770 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:25:18.276807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '81d19626-9400-4ace-8b4b-5f604d0cfdbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B83205ACF0>]}
[0m09:25:21.275880 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '81d19626-9400-4ace-8b4b-5f604d0cfdbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B8324CD670>]}
[0m09:25:21.291093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '81d19626-9400-4ace-8b4b-5f604d0cfdbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B83235BD10>]}
[0m09:25:21.294168 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:25:21.296195 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '81d19626-9400-4ace-8b4b-5f604d0cfdbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B831FF95B0>]}
[0m09:25:21.303322 [info ] [MainThread]: 
[0m09:25:21.306373 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:25:21.310428 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:25:21.358245 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:25:21.430004 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '81d19626-9400-4ace-8b4b-5f604d0cfdbe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B83242B9E0>]}
[0m09:25:21.432049 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:25:21.434082 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:25:21.436121 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:25:21.438149 [info ] [MainThread]: 
[0m09:25:21.444245 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m09:25:21.446264 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m09:25:21.448286 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m09:25:21.450305 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m09:25:21.452322 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 09:25:21.451314 => 09:25:21.451314
[0m09:25:21.454410 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m09:25:21.601375 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:25:21.602397 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:25:21.603407 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:25:21.605433 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m09:25:38.568665 [debug] [Thread-6 (]: SQL status: OK in 16.959999084472656 seconds
[0m09:25:38.610523 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m09:25:38.620639 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:25:38.622667 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m09:27:24.272716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7D5FB1490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7D5FB0B00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7D5FB1D00>]}


============================== 09:27:24.274736 | 6caece53-f8d8-4ceb-b107-69801891f3bf ==============================
[0m09:27:24.274736 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:27:24.276760 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m09:27:24.539490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6caece53-f8d8-4ceb-b107-69801891f3bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7D600F7A0>]}
[0m09:27:24.654861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6caece53-f8d8-4ceb-b107-69801891f3bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7D5F5B470>]}
[0m09:27:24.657894 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:27:24.687721 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:27:24.690742 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:27:24.692760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6caece53-f8d8-4ceb-b107-69801891f3bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7D60688F0>]}
[0m09:27:27.600570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6caece53-f8d8-4ceb-b107-69801891f3bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7D6507680>]}
[0m09:27:27.615746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6caece53-f8d8-4ceb-b107-69801891f3bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7D6400DA0>]}
[0m09:27:27.616765 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:27:27.618802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6caece53-f8d8-4ceb-b107-69801891f3bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7D6233200>]}
[0m09:27:27.624870 [info ] [MainThread]: 
[0m09:27:27.627935 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:27:27.635069 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:27:27.682827 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:27:27.753145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6caece53-f8d8-4ceb-b107-69801891f3bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7D645EC00>]}
[0m09:27:27.755166 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:27:27.756174 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:27:27.759198 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:27:27.761228 [info ] [MainThread]: 
[0m09:27:27.769333 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m09:27:27.771347 [info ] [Thread-6 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m09:27:27.773363 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m09:27:27.775376 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m09:27:27.777390 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 09:27:27.776383 => 09:27:27.776383
[0m09:27:27.779408 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m09:27:27.919024 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:27:27.920032 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:27:27.922047 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:27:27.923060 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m09:27:34.096233 [debug] [Thread-6 (]: SQL status: OK in 6.170000076293945 seconds
[0m09:27:34.137786 [debug] [Thread-6 (]: Inserting batches of 500 records
[0m09:27:34.147888 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:27:34.149904 [debug] [Thread-6 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m09:27:43.273664 [debug] [Thread-6 (]: SQL status: OK in 9.119999885559082 seconds
[0m09:27:43.293089 [debug] [Thread-6 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m09:27:43.341815 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:27:43.345877 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (execute): 09:27:27.780432 => 09:27:43.345877
[0m09:27:43.348918 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6caece53-f8d8-4ceb-b107-69801891f3bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7D79BFB30>]}
[0m09:27:43.350952 [info ] [Thread-6 (]: 1 of 1 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 15.58s]
[0m09:27:43.352985 [debug] [Thread-6 (]: Finished running node seed.testproj.sample
[0m09:27:43.358116 [debug] [MainThread]: On master: ROLLBACK
[0m09:27:43.361210 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:27:43.363272 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m09:27:43.365316 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m09:27:43.367370 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:27:43.368387 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:27:43.371430 [info ] [MainThread]: 
[0m09:27:43.373487 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 15.74 seconds (15.74s).
[0m09:27:43.375517 [debug] [MainThread]: Command end result
[0m09:27:43.392824 [info ] [MainThread]: 
[0m09:27:43.394860 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:27:43.396893 [info ] [MainThread]: 
[0m09:27:43.399930 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m09:27:43.404043 [debug] [MainThread]: Command `cli seed` succeeded at 09:27:43.404043 after 19.22 seconds
[0m09:27:43.408119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7D5FB00B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7D65F1EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F7D65F31D0>]}
[0m09:27:43.410144 [debug] [MainThread]: Flushing usage events
[0m09:29:13.341740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002415B3E12E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002415B3E1220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002415B3E0140>]}


============================== 09:29:13.342751 | ce32c2d8-a34f-48c3-8014-366ca3cef679 ==============================
[0m09:29:13.342751 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:29:13.342751 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:29:13.430731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ce32c2d8-a34f-48c3-8014-366ca3cef679', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002415B3605F0>]}
[0m09:29:13.465163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ce32c2d8-a34f-48c3-8014-366ca3cef679', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002415B3C75C0>]}
[0m09:29:13.466168 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:29:13.473238 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:29:13.474245 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:29:13.474245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'ce32c2d8-a34f-48c3-8014-366ca3cef679', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024159E283B0>]}
[0m09:29:14.189615 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ce32c2d8-a34f-48c3-8014-366ca3cef679', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002415B3E3C20>]}
[0m09:29:14.198739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ce32c2d8-a34f-48c3-8014-366ca3cef679', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002415B589970>]}
[0m09:29:14.199748 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:29:14.199748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ce32c2d8-a34f-48c3-8014-366ca3cef679', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002415B6793A0>]}
[0m09:29:14.200785 [info ] [MainThread]: 
[0m09:29:14.201798 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:29:14.202806 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:29:14.206842 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:29:14.219999 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ce32c2d8-a34f-48c3-8014-366ca3cef679', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002415B8CEA50>]}
[0m09:29:14.221013 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:29:14.221013 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:29:14.222024 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:29:14.222024 [info ] [MainThread]: 
[0m09:29:14.225057 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m09:29:14.226093 [info ] [Thread-1 (]: 1 of 1 START seed file datalake.sample ......................................... [RUN]
[0m09:29:14.226093 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m09:29:14.227123 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m09:29:14.227123 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 09:29:14.227123 => 09:29:14.227123
[0m09:29:14.228134 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m09:29:14.259481 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:29:14.260570 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:29:14.260570 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:29:14.261574 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:29:14.313084 [debug] [Thread-1 (]: SQL status: OK in 0.05000000074505806 seconds
[0m09:29:14.324201 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m09:29:14.328262 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:29:14.329273 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m09:29:14.346448 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m09:29:14.351515 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m09:29:14.365758 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:29:14.366820 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 09:29:14.228134 => 09:29:14.366820
[0m09:29:14.367823 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ce32c2d8-a34f-48c3-8014-366ca3cef679', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002415B89EAE0>]}
[0m09:29:14.367823 [info ] [Thread-1 (]: 1 of 1 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.14s]
[0m09:29:14.368836 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m09:29:14.369851 [debug] [MainThread]: On master: ROLLBACK
[0m09:29:14.370861 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:29:14.370861 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m09:29:14.370861 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m09:29:14.371871 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:29:14.371871 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:29:14.371871 [info ] [MainThread]: 
[0m09:29:14.372882 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.17 seconds (0.17s).
[0m09:29:14.372882 [debug] [MainThread]: Command end result
[0m09:29:14.378952 [info ] [MainThread]: 
[0m09:29:14.379975 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:29:14.379975 [info ] [MainThread]: 
[0m09:29:14.380999 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m09:29:14.382017 [debug] [MainThread]: Command `cli seed` succeeded at 09:29:14.380999 after 1.07 seconds
[0m09:29:14.382017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002415B330320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002415B589E20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002415B8E84D0>]}
[0m09:29:14.382017 [debug] [MainThread]: Flushing usage events
[0m09:29:27.835939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1B091730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1B091340>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1B0916D0>]}


============================== 09:29:27.835939 | 517d65e2-5a1a-4690-bd63-9a22e2a7858f ==============================
[0m09:29:27.835939 [info ] [MainThread]: Running with dbt=1.7.14
[0m09:29:27.836966 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m09:29:27.918563 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '517d65e2-5a1a-4690-bd63-9a22e2a7858f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1B110EC0>]}
[0m09:29:27.953013 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '517d65e2-5a1a-4690-bd63-9a22e2a7858f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1AF9AC30>]}
[0m09:29:27.954021 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m09:29:27.962126 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m09:29:27.963153 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:29:27.964171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '517d65e2-5a1a-4690-bd63-9a22e2a7858f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1B1CD0D0>]}
[0m09:29:28.692743 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '517d65e2-5a1a-4690-bd63-9a22e2a7858f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1B2A7EC0>]}
[0m09:29:28.710951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '517d65e2-5a1a-4690-bd63-9a22e2a7858f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1B5001A0>]}
[0m09:29:28.710951 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 446 macros, 0 groups, 0 semantic models
[0m09:29:28.712965 [info ] [MainThread]: 
[0m09:29:28.712965 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m09:29:28.713974 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m09:29:28.719019 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_datalake'
[0m09:29:28.734194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '517d65e2-5a1a-4690-bd63-9a22e2a7858f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1B551970>]}
[0m09:29:28.734194 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:29:28.735207 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:29:28.735207 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m09:29:28.736215 [info ] [MainThread]: 
[0m09:29:28.738255 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m09:29:28.739268 [info ] [Thread-1 (]: 1 of 7 START sql incremental model datalake.my_first_dbt_model ................. [RUN]
[0m09:29:28.739268 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m09:29:28.740276 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m09:29:28.744326 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m09:29:28.746346 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 09:29:28.740276 => 09:29:28.746346
[0m09:29:28.746346 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m09:29:28.776799 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:29:28.776799 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m09:29:28.777809 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m09:29:28.777809 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m09:29:28.832635 [debug] [Thread-1 (]: SQL status: OK in 0.05000000074505806 seconds
[0m09:29:28.859944 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m09:29:28.861977 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m09:29:28.863002 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table datalake.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m09:29:28.875162 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m09:29:28.883259 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 09:29:28.746346 => 09:29:28.883259
[0m09:29:28.884276 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '517d65e2-5a1a-4690-bd63-9a22e2a7858f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1B6EA750>]}
[0m09:29:28.885286 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model datalake.my_first_dbt_model ............ [[32mOK[0m in 0.15s]
[0m09:29:28.885286 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m09:29:28.886300 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m09:29:28.886300 [info ] [Thread-1 (]: 2 of 7 START seed file datalake.sample ......................................... [RUN]
[0m09:29:28.887319 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m09:29:28.887319 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m09:29:28.888335 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 09:29:28.888335 => 09:29:28.888335
[0m09:29:28.888335 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m09:29:28.914191 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:29:28.914191 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table datalake.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m09:29:28.918232 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m09:29:28.927406 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m09:29:28.931471 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m09:29:28.931471 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into datalake.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m09:29:28.945656 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m09:29:28.949789 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m09:29:28.952812 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:29:28.953822 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 09:29:28.888335 => 09:29:28.953822
[0m09:29:28.954835 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '517d65e2-5a1a-4690-bd63-9a22e2a7858f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1B6EA540>]}
[0m09:29:28.954835 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file datalake.sample ..................................... [[32mINSERT 2[0m in 0.07s]
[0m09:29:28.955848 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m09:29:28.955848 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m09:29:28.955848 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m09:29:28.956863 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m09:29:28.956863 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m09:29:28.966083 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m09:29:28.967095 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 09:29:28.956863 => 09:29:28.967095
[0m09:29:28.967095 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m09:29:28.975172 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m09:29:28.976181 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m09:29:28.976181 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m09:29:28.987353 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m09:29:28.988370 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 09:29:28.967095 => 09:29:28.988370
[0m09:29:28.989391 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m09:29:28.990411 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m09:29:28.990411 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m09:29:28.991426 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m09:29:28.991426 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m09:29:28.991426 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m09:29:28.997570 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m09:29:28.999076 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 09:29:28.992440 => 09:29:28.998572
[0m09:29:28.999076 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m09:29:29.001097 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m09:29:29.002104 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m09:29:29.002104 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m09:29:29.007156 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m09:29:29.008164 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 09:29:28.999076 => 09:29:29.008164
[0m09:29:29.008164 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m09:29:29.009174 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m09:29:29.009174 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m09:29:29.010185 [info ] [Thread-1 (]: 5 of 7 START sql incremental model datalake.my_second_dbt_model ................ [RUN]
[0m09:29:29.010185 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m09:29:29.011199 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m09:29:29.013238 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m09:29:29.013238 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 09:29:29.011199 => 09:29:29.013238
[0m09:29:29.014247 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m09:29:29.019342 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m09:29:29.019342 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/
drop view if exists datalake.my_second_dbt_model
[0m09:29:29.023410 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m09:29:29.033570 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m09:29:29.034580 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m09:29:29.034580 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    
        create or replace table datalake.my_second_dbt_model
      
      
    using delta
      
      
      
      
      
      

      as
      -- Use the `ref` function to select from other models

select *
from datalake.my_first_dbt_model
where id = 1
  
[0m09:29:29.047754 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m09:29:29.050814 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 09:29:29.014247 => 09:29:29.050814
[0m09:29:29.051830 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '517d65e2-5a1a-4690-bd63-9a22e2a7858f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1B8E7410>]}
[0m09:29:29.051830 [info ] [Thread-1 (]: 5 of 7 OK created sql incremental model datalake.my_second_dbt_model ........... [[32mOK[0m in 0.04s]
[0m09:29:29.052843 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m09:29:29.052843 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m09:29:29.053858 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m09:29:29.053858 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m09:29:29.054877 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m09:29:29.057919 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m09:29:29.059943 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 09:29:29.054877 => 09:29:29.058928
[0m09:29:29.059943 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m09:29:29.061991 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m09:29:29.064036 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m09:29:29.064036 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from datalake.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m09:29:29.069088 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m09:29:29.070098 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 09:29:29.059943 => 09:29:29.069088
[0m09:29:29.070098 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.02s]
[0m09:29:29.071108 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m09:29:29.071108 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m09:29:29.071108 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m09:29:29.072121 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m09:29:29.072121 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m09:29:29.075176 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m09:29:29.076184 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 09:29:29.073156 => 09:29:29.075176
[0m09:29:29.076184 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m09:29:29.077194 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m09:29:29.078203 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m09:29:29.078203 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from datalake.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m09:29:29.082246 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m09:29:29.083253 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 09:29:29.076184 => 09:29:29.083253
[0m09:29:29.084265 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m09:29:29.084265 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m09:29:29.085278 [debug] [MainThread]: On master: ROLLBACK
[0m09:29:29.086289 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:29:29.086289 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m09:29:29.086289 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m09:29:29.086289 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m09:29:29.087300 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m09:29:29.087300 [info ] [MainThread]: 
[0m09:29:29.088329 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.37 seconds (0.37s).
[0m09:29:29.088329 [debug] [MainThread]: Command end result
[0m09:29:29.096431 [info ] [MainThread]: 
[0m09:29:29.097452 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:29:29.097452 [info ] [MainThread]: 
[0m09:29:29.097452 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m09:29:29.098959 [debug] [MainThread]: Command `cli build` succeeded at 09:29:29.098456 after 1.29 seconds
[0m09:29:29.098959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1B2A7EC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1B4AC200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002AA1B091D00>]}
[0m09:29:29.098959 [debug] [MainThread]: Flushing usage events
[0m14:25:56.082728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4CFA323F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4CF8A8D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4CFA32B40>]}


============================== 14:25:56.085764 | c288ef09-d03a-4ea3-8a11-2f327cf1a485 ==============================
[0m14:25:56.085764 [info ] [MainThread]: Running with dbt=1.7.14
[0m14:25:56.086793 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'C:\\Users\\jramp\\source\\gitjr\\dbt-fabricsparknb\\testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt build', 'send_anonymous_usage_stats': 'True'}
[0m14:25:56.090829 [info ] [MainThread]: Error importing adapter: No module named 'dbt.adapters.fabricsparknb'
[0m14:25:56.090829 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Could not find adapter type fabricsparknb!
[0m14:25:56.091839 [debug] [MainThread]: Command `dbt build` failed at 14:25:56.091839 after 0.09 seconds
[0m14:25:56.091839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4CCB06600>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4CF6E5DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A4CD38FCB0>]}
[0m14:25:56.091839 [debug] [MainThread]: Flushing usage events
[0m14:26:58.956816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000251AFC2D430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000251AFC2D9A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000251AFC2D340>]}


============================== 14:26:58.957832 | 777c3af7-f35b-4000-9fc5-db8c3cd11eac ==============================
[0m14:26:58.957832 [info ] [MainThread]: Running with dbt=1.7.14
[0m14:26:58.957832 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m14:26:59.042415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '777c3af7-f35b-4000-9fc5-db8c3cd11eac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000251AF8E5DC0>]}
[0m14:26:59.074618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '777c3af7-f35b-4000-9fc5-db8c3cd11eac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000251AFBBBAA0>]}
[0m14:26:59.076629 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:26:59.088784 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m14:26:59.089789 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:26:59.090797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '777c3af7-f35b-4000-9fc5-db8c3cd11eac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000251AFAF7980>]}
[0m14:27:00.213586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '777c3af7-f35b-4000-9fc5-db8c3cd11eac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000251B0DBC200>]}
[0m14:27:00.221154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '777c3af7-f35b-4000-9fc5-db8c3cd11eac', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000251B10EE270>]}
[0m14:27:00.221658 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m14:27:00.222669 [info ] [MainThread]: 
[0m14:27:00.223680 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:27:00.224689 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:27:00.232756 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m14:27:00.233767 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m14:27:00.239842 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro create_schema
[0m14:27:00.239842 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */
  /*{"project_root": "testproj"}*/
  
      /*FABRICSPARKNB_ALERT: Schema Does NOT exist and automatic schema creation in Fabric Lakehouse not allowed. Please create the schema datalake manually*/ select 1
    
[0m14:27:00.240847 [info ] [MainThread]: 
[0m14:27:00.241853 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.02 seconds (0.02s).
[0m14:27:00.241853 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */
    /*{"project_root": "testproj"}*/
    
        /*FABRICSPARKNB_ALERT: Schema Does NOT exist and automatic schema creation in Fabric Lakehouse not allowed. Please create the schema datalake manually*/ select 1
      
[0m14:27:00.242859 [debug] [MainThread]: Command `cli build` failed at 14:27:00.242859 after 1.31 seconds
[0m14:27:00.243865 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000251AFC2C650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000251B0E60440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000251B0E80A40>]}
[0m14:27:00.243865 [debug] [MainThread]: Flushing usage events
[0m14:29:09.346429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C195D17F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C195D1A30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C195D16D0>]}


============================== 14:29:09.346429 | 09aa5af2-2d26-4bc7-994c-c6798d358c7e ==============================
[0m14:29:09.346429 [info ] [MainThread]: Running with dbt=1.7.14
[0m14:29:09.347441 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:29:09.426000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '09aa5af2-2d26-4bc7-994c-c6798d358c7e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C195D2B70>]}
[0m14:29:09.458708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '09aa5af2-2d26-4bc7-994c-c6798d358c7e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C18B87860>]}
[0m14:29:09.460718 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:29:09.467057 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m14:29:09.468062 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:29:09.468062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '09aa5af2-2d26-4bc7-994c-c6798d358c7e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C19591DF0>]}
[0m14:29:10.133158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '09aa5af2-2d26-4bc7-994c-c6798d358c7e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C1A9F2600>]}
[0m14:29:10.140197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '09aa5af2-2d26-4bc7-994c-c6798d358c7e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C1AA8B890>]}
[0m14:29:10.141205 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m14:29:10.142213 [info ] [MainThread]: 
[0m14:29:10.143218 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:29:10.144222 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:29:10.149538 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m14:29:10.149538 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m14:29:10.155592 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro create_schema
[0m14:29:10.155592 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */
  /*{"project_root": "testproj"}*/
  
      /*FABRICSPARKNB_ALERT: Schema Does NOT exist and automatic schema creation in Fabric Lakehouse not allowed. Please create the schema datalake manually*/ select 1
    
[0m14:29:10.156602 [info ] [MainThread]: 
[0m14:29:10.156602 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.01 seconds (0.01s).
[0m14:29:10.157611 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */
    /*{"project_root": "testproj"}*/
    
        /*FABRICSPARKNB_ALERT: Schema Does NOT exist and automatic schema creation in Fabric Lakehouse not allowed. Please create the schema datalake manually*/ select 1
      
[0m14:29:10.158617 [debug] [MainThread]: Command `cli build` failed at 14:29:10.158617 after 0.84 seconds
[0m14:29:10.158617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C195D28D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C1A66E960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015C1A8D06E0>]}
[0m14:29:10.159621 [debug] [MainThread]: Flushing usage events
[0m14:29:52.038390 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018D483CEFC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018D483CF4D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018D483CF350>]}


============================== 14:29:52.038893 | 8a92558e-d181-4fad-938e-7759ede7f9b8 ==============================
[0m14:29:52.038893 [info ] [MainThread]: Running with dbt=1.7.14
[0m14:29:52.038893 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m14:29:52.117261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8a92558e-d181-4fad-938e-7759ede7f9b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018D480720C0>]}
[0m14:29:52.149774 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8a92558e-d181-4fad-938e-7759ede7f9b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018D4835CD70>]}
[0m14:29:52.151786 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:29:52.158236 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m14:29:52.159240 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:29:52.160245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '8a92558e-d181-4fad-938e-7759ede7f9b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018D494AE2A0>]}
[0m14:29:52.826326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8a92558e-d181-4fad-938e-7759ede7f9b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018D49834E90>]}
[0m14:29:52.843627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8a92558e-d181-4fad-938e-7759ede7f9b8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018D497FC2F0>]}
[0m14:29:52.843627 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m14:29:52.845638 [info ] [MainThread]: 
[0m14:29:52.845638 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:29:52.846643 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:29:52.851669 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_schemas, now create__datalake)
[0m14:29:52.852674 [debug] [ThreadPool]: Creating schema "schema: "datalake"
"
[0m14:29:52.857986 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro create_schema
[0m14:29:52.858991 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */
  /*{"project_root": "testproj"}*/
  
      /*FABRICSPARKNB_ALERT: Schema Does NOT exist and automatic schema creation in Fabric Lakehouse not allowed. Please create the schema datalake manually*/ select 1
    
[0m14:29:52.859996 [info ] [MainThread]: 
[0m14:29:52.859996 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.01 seconds (0.01s).
[0m14:29:52.861001 [error] [MainThread]: Encountered an error:
Runtime Error
  Runtime Error
    /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "create__datalake"} */
    /*{"project_root": "testproj"}*/
    
        /*FABRICSPARKNB_ALERT: Schema Does NOT exist and automatic schema creation in Fabric Lakehouse not allowed. Please create the schema datalake manually*/ select 1
      
[0m14:29:52.862006 [debug] [MainThread]: Command `cli build` failed at 14:29:52.862006 after 0.85 seconds
[0m14:29:52.862006 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018D483CE000>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018D4963D040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018D4960D190>]}
[0m14:29:52.862006 [debug] [MainThread]: Flushing usage events
[0m14:32:51.521055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DDECF7D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DDECEE70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DDECF1D0>]}


============================== 14:32:51.521055 | 807008cb-54bb-4bb2-91dc-e80795bbcc25 ==============================
[0m14:32:51.521055 [info ] [MainThread]: Running with dbt=1.7.14
[0m14:32:51.522060 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m14:32:51.598335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '807008cb-54bb-4bb2-91dc-e80795bbcc25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DDE0BCE0>]}
[0m14:32:51.632169 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '807008cb-54bb-4bb2-91dc-e80795bbcc25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DDA6A2D0>]}
[0m14:32:51.633175 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m14:32:51.640258 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m14:32:51.640258 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m14:32:51.641264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '807008cb-54bb-4bb2-91dc-e80795bbcc25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DDBDA930>]}
[0m14:32:52.309439 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '807008cb-54bb-4bb2-91dc-e80795bbcc25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DF326FF0>]}
[0m14:32:52.317527 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '807008cb-54bb-4bb2-91dc-e80795bbcc25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DF318B00>]}
[0m14:32:52.318034 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m14:32:52.318973 [info ] [MainThread]: 
[0m14:32:52.320479 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m14:32:52.320981 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m14:32:52.326012 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales2'
[0m14:32:52.341500 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales2, now list_None_lakesales)
[0m14:32:52.351648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '807008cb-54bb-4bb2-91dc-e80795bbcc25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DE0042F0>]}
[0m14:32:52.352655 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:32:52.352655 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:32:52.353662 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m14:32:52.353662 [info ] [MainThread]: 
[0m14:32:52.356683 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m14:32:52.357189 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m14:32:52.357696 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m14:32:52.357696 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m14:32:52.362757 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m14:32:52.364774 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 14:32:52.357696 => 14:32:52.363765
[0m14:32:52.364774 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m14:32:52.392987 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:32:52.393994 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:32:52.393994 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m14:32:52.393994 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:32:52.462084 [debug] [Thread-1 (]: SQL status: OK in 0.07000000029802322 seconds
[0m14:32:52.478720 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m14:32:52.480730 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m14:32:52.480730 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m14:32:52.492840 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:32:52.500914 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 14:32:52.364774 => 14:32:52.500914
[0m14:32:52.500914 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '807008cb-54bb-4bb2-91dc-e80795bbcc25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DF1BCC20>]}
[0m14:32:52.501919 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model lakesales.my_first_dbt_model ........... [[32mOK[0m in 0.14s]
[0m14:32:52.501919 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m14:32:52.502929 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m14:32:52.502929 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m14:32:52.503942 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m14:32:52.503942 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m14:32:52.504955 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 14:32:52.504955 => 14:32:52.504955
[0m14:32:52.504955 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m14:32:52.519101 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:32:52.519101 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m14:32:52.524133 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:32:52.540224 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:32:52.540726 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table lakesales.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m14:32:52.554308 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:32:52.562377 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m14:32:52.564406 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m14:32:52.565413 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into lakesales.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m14:32:52.577476 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:32:52.581030 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m14:32:52.583050 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:32:52.584057 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 14:32:52.504955 => 14:32:52.584057
[0m14:32:52.584057 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '807008cb-54bb-4bb2-91dc-e80795bbcc25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DF48A540>]}
[0m14:32:52.585063 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file lakesales.sample .................................... [[32mINSERT 2[0m in 0.08s]
[0m14:32:52.586072 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m14:32:52.586072 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:32:52.586072 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m14:32:52.587583 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m14:32:52.587583 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:32:52.594074 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:32:52.596129 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 14:32:52.588087 => 14:32:52.596129
[0m14:32:52.596129 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:32:52.604172 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:32:52.605177 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m14:32:52.605177 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:32:52.618247 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:32:52.621295 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 14:32:52.597132 => 14:32:52.620280
[0m14:32:52.622316 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.04s]
[0m14:32:52.623334 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m14:32:52.623334 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:32:52.624358 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m14:32:52.625376 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m14:32:52.625376 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:32:52.630446 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:32:52.633486 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 14:32:52.625376 => 14:32:52.632471
[0m14:32:52.633486 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:32:52.635516 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:32:52.637564 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m14:32:52.637564 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:32:52.642819 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:32:52.644848 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 14:32:52.634500 => 14:32:52.644848
[0m14:32:52.645865 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m14:32:52.645865 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m14:32:52.646913 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m14:32:52.648476 [info ] [Thread-1 (]: 5 of 7 START sql incremental model lakesales2.my_second_dbt_model .............. [RUN]
[0m14:32:52.650558 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m14:32:52.650558 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m14:32:52.653586 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m14:32:52.655609 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 14:32:52.650558 => 14:32:52.654595
[0m14:32:52.655609 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m14:32:52.658689 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:32:52.659697 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_second_dbt_model__dbt_tmp as
      -- Use the `ref` function to select from other models

select *
from lakesales.my_first_dbt_model
where id = 1
  
[0m14:32:52.663729 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:32:52.668265 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m14:32:52.668771 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m14:32:52.668771 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales2.my_second_dbt_model
    select `id` from my_second_dbt_model__dbt_tmp


[0m14:32:52.679864 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m14:32:52.682897 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 14:32:52.655609 => 14:32:52.682897
[0m14:32:52.683907 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '807008cb-54bb-4bb2-91dc-e80795bbcc25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DF6F87D0>]}
[0m14:32:52.683907 [info ] [Thread-1 (]: 5 of 7 OK created sql incremental model lakesales2.my_second_dbt_model ......... [[32mOK[0m in 0.03s]
[0m14:32:52.684913 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m14:32:52.685923 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:32:52.685923 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m14:32:52.687434 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m14:32:52.687936 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:32:52.691487 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:32:52.692493 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 14:32:52.687936 => 14:32:52.692493
[0m14:32:52.692493 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:32:52.694510 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:32:52.694510 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m14:32:52.695521 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales2.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m14:32:52.699564 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:32:52.700570 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 14:32:52.692493 => 14:32:52.700570
[0m14:32:52.701576 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m14:32:52.701576 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m14:32:52.701576 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:32:52.702583 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m14:32:52.702583 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m14:32:52.703618 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:32:52.705632 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:32:52.706638 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 14:32:52.703618 => 14:32:52.706638
[0m14:32:52.707146 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:32:52.708165 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:32:52.708672 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m14:32:52.708672 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales2.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m14:32:52.714735 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m14:32:52.715746 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 14:32:52.707146 => 14:32:52.715746
[0m14:32:52.716754 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m14:32:52.717261 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m14:32:52.718840 [debug] [MainThread]: On master: ROLLBACK
[0m14:32:52.718840 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:32:52.719853 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m14:32:52.719853 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m14:32:52.719853 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m14:32:52.719853 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m14:32:52.720869 [info ] [MainThread]: 
[0m14:32:52.720869 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.40 seconds (0.40s).
[0m14:32:52.721885 [debug] [MainThread]: Command end result
[0m14:32:52.728938 [info ] [MainThread]: 
[0m14:32:52.728938 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:32:52.729951 [info ] [MainThread]: 
[0m14:32:52.729951 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m14:32:52.730959 [debug] [MainThread]: Command `cli build` succeeded at 14:32:52.730959 after 1.23 seconds
[0m14:32:52.730959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DF19D580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DF335F10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000229DF30E420>]}
[0m14:32:52.730959 [debug] [MainThread]: Flushing usage events
[0m17:19:05.574695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253651BD520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253651BD700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253651BD3A0>]}


============================== 17:19:05.574695 | 9a3528c4-e750-470d-aeed-2f504c8a0355 ==============================
[0m17:19:05.574695 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:19:05.575713 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m17:19:05.653604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9a3528c4-e750-470d-aeed-2f504c8a0355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002536625A930>]}
[0m17:19:05.685539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9a3528c4-e750-470d-aeed-2f504c8a0355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253651D0560>]}
[0m17:19:05.686544 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:19:05.700488 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:19:05.701492 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m17:19:05.701492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '9a3528c4-e750-470d-aeed-2f504c8a0355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025366258350>]}
[0m17:19:06.876966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9a3528c4-e750-470d-aeed-2f504c8a0355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002536653DD60>]}
[0m17:19:06.889549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9a3528c4-e750-470d-aeed-2f504c8a0355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253664109E0>]}
[0m17:19:06.889549 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m17:19:06.891565 [info ] [MainThread]: 
[0m17:19:06.892144 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:19:06.892672 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:19:06.901731 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales2'
[0m17:19:06.915010 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales2, now list_None_lakesales)
[0m17:19:06.924398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9a3528c4-e750-470d-aeed-2f504c8a0355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025366323740>]}
[0m17:19:06.925364 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:19:06.925886 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:19:06.925886 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:19:06.926892 [info ] [MainThread]: 
[0m17:19:06.928904 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:19:06.929910 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m17:19:06.929910 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:19:06.930915 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:19:06.934937 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:19:06.936948 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:19:06.930915 => 17:19:06.935943
[0m17:19:06.936948 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:19:06.965141 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:19:06.966147 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:19:06.966652 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:19:06.966652 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:19:07.040440 [debug] [Thread-1 (]: SQL status: OK in 0.07000000029802322 seconds
[0m17:19:07.057130 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:19:07.059483 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:19:07.059483 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:19:07.072523 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:19:07.080426 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:19:06.936948 => 17:19:07.080426
[0m17:19:07.080426 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a3528c4-e750-470d-aeed-2f504c8a0355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253662FB230>]}
[0m17:19:07.081431 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model lakesales.my_first_dbt_model ........... [[32mOK[0m in 0.15s]
[0m17:19:07.082435 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:19:07.082435 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m17:19:07.082435 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m17:19:07.083441 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:19:07.083441 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m17:19:07.084446 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 17:19:07.084446 => 17:19:07.084446
[0m17:19:07.084446 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m17:19:07.098988 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:19:07.099997 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m17:19:07.104030 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:19:07.122899 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:19:07.123149 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table lakesales.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m17:19:07.139299 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m17:19:07.148962 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m17:19:07.150971 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:19:07.151977 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into lakesales.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:19:07.166736 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:19:07.171784 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:19:07.174251 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:19:07.174251 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 17:19:07.085451 => 17:19:07.174251
[0m17:19:07.175709 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a3528c4-e750-470d-aeed-2f504c8a0355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253668ED280>]}
[0m17:19:07.176764 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file lakesales.sample .................................... [[32mINSERT 2[0m in 0.09s]
[0m17:19:07.177779 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m17:19:07.178789 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:19:07.179802 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:19:07.180815 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:19:07.180815 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:19:07.191179 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:19:07.193191 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:19:07.180815 => 17:19:07.193191
[0m17:19:07.193191 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:19:07.202532 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:19:07.204588 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:19:07.205595 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:19:07.221599 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m17:19:07.224930 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:19:07.194197 => 17:19:07.224930
[0m17:19:07.226943 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.05s]
[0m17:19:07.227839 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:19:07.228384 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:19:07.228384 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:19:07.229391 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:19:07.229391 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:19:07.235419 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:19:07.236442 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:19:07.229391 => 17:19:07.236442
[0m17:19:07.237449 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:19:07.239430 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:19:07.239959 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:19:07.239959 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:19:07.244530 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:19:07.245540 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:19:07.237449 => 17:19:07.245540
[0m17:19:07.246046 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m17:19:07.246550 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:19:07.247056 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:19:07.247561 [info ] [Thread-1 (]: 5 of 7 START sql incremental model lakesales2.my_second_dbt_model .............. [RUN]
[0m17:19:07.248066 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m17:19:07.248066 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:19:07.250086 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:19:07.250590 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:19:07.248571 => 17:19:07.250590
[0m17:19:07.251096 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:19:07.252610 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:19:07.253115 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_second_dbt_model__dbt_tmp as
      -- Use the `ref` function to select from other models

select *
from lakesales.my_first_dbt_model
where id = 1
  
[0m17:19:07.257551 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:19:07.264949 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:19:07.265965 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:19:07.265965 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales2.my_second_dbt_model
    select `id` from my_second_dbt_model__dbt_tmp


[0m17:19:07.280112 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:19:07.282639 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:19:07.251096 => 17:19:07.282134
[0m17:19:07.283144 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9a3528c4-e750-470d-aeed-2f504c8a0355', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025366975190>]}
[0m17:19:07.283652 [info ] [Thread-1 (]: 5 of 7 OK created sql incremental model lakesales2.my_second_dbt_model ......... [[32mOK[0m in 0.04s]
[0m17:19:07.284165 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:19:07.284672 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:19:07.285179 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m17:19:07.285685 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m17:19:07.286192 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:19:07.289713 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:19:07.291536 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 17:19:07.286192 => 17:19:07.291536
[0m17:19:07.292042 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:19:07.293987 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:19:07.295048 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:19:07.295048 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales2.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:19:07.300098 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:19:07.301612 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 17:19:07.292042 => 17:19:07.301612
[0m17:19:07.303279 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.02s]
[0m17:19:07.305316 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:19:07.305916 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:19:07.305997 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m17:19:07.307062 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m17:19:07.307572 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:19:07.312073 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:19:07.314243 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 17:19:07.308083 => 17:19:07.313076
[0m17:19:07.314243 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:19:07.316250 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:19:07.317252 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:19:07.317252 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales2.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:19:07.322685 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:19:07.323703 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 17:19:07.314243 => 17:19:07.323703
[0m17:19:07.323703 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.02s]
[0m17:19:07.325705 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:19:07.325705 [debug] [MainThread]: On master: ROLLBACK
[0m17:19:07.326707 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:19:07.326707 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:19:07.326707 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:19:07.326707 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:19:07.327771 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:19:07.327771 [info ] [MainThread]: 
[0m17:19:07.327771 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.44 seconds (0.44s).
[0m17:19:07.328774 [debug] [MainThread]: Command end result
[0m17:19:07.334890 [info ] [MainThread]: 
[0m17:19:07.334890 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:19:07.335393 [info ] [MainThread]: 
[0m17:19:07.335897 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m17:19:07.336402 [debug] [MainThread]: Command `cli build` succeeded at 17:19:07.336402 after 1.79 seconds
[0m17:19:07.336906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253651BD220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253651BD3A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000253651BD250>]}
[0m17:19:07.336906 [debug] [MainThread]: Flushing usage events
[0m18:02:20.953005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFEDD8170>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFF060890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFF062120>]}


============================== 18:02:20.953005 | be6ad104-10d9-4e75-bd57-4a855a1f4a88 ==============================
[0m18:02:20.953005 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:02:20.954010 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m18:02:21.030917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'be6ad104-10d9-4e75-bd57-4a855a1f4a88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFF088860>]}
[0m18:02:21.062916 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'be6ad104-10d9-4e75-bd57-4a855a1f4a88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFF062870>]}
[0m18:02:21.064927 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:02:21.079535 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:02:21.080540 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m18:02:21.080540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'be6ad104-10d9-4e75-bd57-4a855a1f4a88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFF169430>]}
[0m18:02:22.263578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'be6ad104-10d9-4e75-bd57-4a855a1f4a88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFF4EEFF0>]}
[0m18:02:22.286972 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'be6ad104-10d9-4e75-bd57-4a855a1f4a88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFF297890>]}
[0m18:02:22.286972 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m18:02:22.288000 [info ] [MainThread]: 
[0m18:02:22.289005 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:02:22.290011 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:02:22.299301 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m18:02:22.312777 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m18:02:22.321839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be6ad104-10d9-4e75-bd57-4a855a1f4a88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFEEE7800>]}
[0m18:02:22.322845 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:02:22.322845 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:02:22.323722 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:02:22.324231 [info ] [MainThread]: 
[0m18:02:22.327254 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:02:22.327254 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m18:02:22.328260 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:02:22.328418 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:02:22.332458 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:02:22.334481 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:02:22.328418 => 18:02:22.333463
[0m18:02:22.334481 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:02:22.361456 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:02:22.362461 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:02:22.362461 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:02:22.363467 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:02:22.430586 [debug] [Thread-1 (]: SQL status: OK in 0.07000000029802322 seconds
[0m18:02:22.447115 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:02:22.449125 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:02:22.450130 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:02:22.463892 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:02:22.472958 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:02:22.334481 => 18:02:22.472958
[0m18:02:22.473968 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be6ad104-10d9-4e75-bd57-4a855a1f4a88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFF2CAB40>]}
[0m18:02:22.473968 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model lakesales.my_first_dbt_model ........... [[32mOK[0m in 0.15s]
[0m18:02:22.474977 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:02:22.474977 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:02:22.475984 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m18:02:22.475984 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:02:22.476991 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:02:22.476991 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:02:22.476991 => 18:02:22.476991
[0m18:02:22.476991 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:02:22.490363 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:02:22.490363 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m18:02:22.494391 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:02:22.510538 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:02:22.511048 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table lakesales.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m18:02:22.521648 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:02:22.529723 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m18:02:22.531732 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:02:22.532736 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into lakesales.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m18:02:22.544562 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:02:22.548103 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m18:02:22.549125 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:02:22.550131 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:02:22.477749 => 18:02:22.550131
[0m18:02:22.550131 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be6ad104-10d9-4e75-bd57-4a855a1f4a88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFF653650>]}
[0m18:02:22.550131 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file lakesales.sample .................................... [[32mINSERT 2[0m in 0.07s]
[0m18:02:22.551136 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:02:22.551136 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:02:22.552142 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m18:02:22.552142 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m18:02:22.553148 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:02:22.559378 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:02:22.560389 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 18:02:22.553148 => 18:02:22.560389
[0m18:02:22.561123 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:02:22.568707 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:02:22.569712 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:02:22.569712 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:02:22.582930 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:02:22.584941 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 18:02:22.561123 => 18:02:22.584941
[0m18:02:22.585947 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m18:02:22.585947 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:02:22.586953 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:02:22.586953 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m18:02:22.587960 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m18:02:22.587960 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:02:22.593129 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:02:22.594139 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 18:02:22.587960 => 18:02:22.594139
[0m18:02:22.595150 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:02:22.596203 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:02:22.598215 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:02:22.598215 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:02:22.603242 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:02:22.604248 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 18:02:22.595700 => 18:02:22.604248
[0m18:02:22.605254 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m18:02:22.606259 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:02:22.607264 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:02:22.607264 [info ] [Thread-1 (]: 5 of 7 START sql incremental model lakesales2.my_second_dbt_model .............. [RUN]
[0m18:02:22.608273 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m18:02:22.608273 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:02:22.611322 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:02:22.612430 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:02:22.608273 => 18:02:22.612430
[0m18:02:22.612430 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:02:22.617575 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:02:22.618586 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_second_dbt_model__dbt_tmp as
      -- Use the `ref` function to select from other models

select *
from lakesales.my_first_dbt_model
where id = 1
  
[0m18:02:22.625708 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:02:22.631951 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:02:22.632998 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:02:22.634003 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales2.my_second_dbt_model
    select `id` from my_second_dbt_model__dbt_tmp


[0m18:02:22.647047 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:02:22.649069 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:02:22.613468 => 18:02:22.649069
[0m18:02:22.650078 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'be6ad104-10d9-4e75-bd57-4a855a1f4a88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFF89F410>]}
[0m18:02:22.650583 [info ] [Thread-1 (]: 5 of 7 OK created sql incremental model lakesales2.my_second_dbt_model ......... [[32mOK[0m in 0.04s]
[0m18:02:22.651592 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:02:22.652601 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:02:22.653105 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m18:02:22.653611 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m18:02:22.654115 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:02:22.657165 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:02:22.659193 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 18:02:22.654115 => 18:02:22.659193
[0m18:02:22.659700 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:02:22.662225 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:02:22.663438 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:02:22.663438 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales2.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:02:22.669492 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:02:22.670502 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 18:02:22.660205 => 18:02:22.670502
[0m18:02:22.671006 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.02s]
[0m18:02:22.671511 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:02:22.672015 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:02:22.672541 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m18:02:22.673047 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m18:02:22.674054 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:02:22.676062 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:02:22.677064 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 18:02:22.674054 => 18:02:22.677064
[0m18:02:22.677064 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:02:22.678512 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:02:22.679515 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:02:22.679515 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales2.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:02:22.684078 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:02:22.684580 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 18:02:22.677064 => 18:02:22.684580
[0m18:02:22.685582 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m18:02:22.685582 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:02:22.686905 [debug] [MainThread]: On master: ROLLBACK
[0m18:02:22.686905 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:02:22.686905 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:02:22.687908 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:02:22.687908 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:02:22.688410 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:02:22.688410 [info ] [MainThread]: 
[0m18:02:22.688916 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.40 seconds (0.40s).
[0m18:02:22.689925 [debug] [MainThread]: Command end result
[0m18:02:22.695846 [info ] [MainThread]: 
[0m18:02:22.696355 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:02:22.696355 [info ] [MainThread]: 
[0m18:02:22.696862 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m18:02:22.697368 [debug] [MainThread]: Command `cli build` succeeded at 18:02:22.697368 after 1.77 seconds
[0m18:02:22.697872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFF4803B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFF1BF4D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BFFF3F5A60>]}
[0m18:02:22.698379 [debug] [MainThread]: Flushing usage events
[0m18:22:28.935314 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001539C971190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001539C9712E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001539C970320>]}


============================== 18:22:28.936318 | 5a6a96cc-924f-4ac6-9397-6f6c11c10f77 ==============================
[0m18:22:28.936318 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:22:28.936318 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:22:29.022184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5a6a96cc-924f-4ac6-9397-6f6c11c10f77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001539C998DA0>]}
[0m18:22:29.055393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5a6a96cc-924f-4ac6-9397-6f6c11c10f77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001539C8ECCB0>]}
[0m18:22:29.057403 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:22:29.070375 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:22:29.070894 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m18:22:29.071903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5a6a96cc-924f-4ac6-9397-6f6c11c10f77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001539CA0C6B0>]}
[0m18:22:30.267981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5a6a96cc-924f-4ac6-9397-6f6c11c10f77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001539DCC9940>]}
[0m18:22:30.286175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5a6a96cc-924f-4ac6-9397-6f6c11c10f77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001539DBB6990>]}
[0m18:22:30.287181 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m18:22:30.290205 [info ] [MainThread]: 
[0m18:22:30.291212 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:22:30.292219 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:22:30.305404 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales2'
[0m18:22:30.318620 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales2, now list_None_lakesales)
[0m18:22:30.332777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5a6a96cc-924f-4ac6-9397-6f6c11c10f77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001539CAA8D70>]}
[0m18:22:30.333783 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:22:30.333783 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:22:30.334789 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:22:30.334789 [info ] [MainThread]: 
[0m18:22:30.338813 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:22:30.339818 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m18:22:30.339818 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:22:30.340823 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:22:30.344874 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:22:30.345879 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:22:30.340823 => 18:22:30.345879
[0m18:22:30.345879 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:22:30.377427 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:22:30.377427 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:22:30.378433 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:22:30.378433 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:22:30.459689 [debug] [Thread-1 (]: SQL status: OK in 0.07999999821186066 seconds
[0m18:22:30.478838 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:22:30.479845 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:22:30.479845 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:22:30.491963 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:22:30.500010 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:22:30.346885 => 18:22:30.500010
[0m18:22:30.500010 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5a6a96cc-924f-4ac6-9397-6f6c11c10f77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001539DC910D0>]}
[0m18:22:30.501018 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model lakesales.my_first_dbt_model ........... [[32mOK[0m in 0.16s]
[0m18:22:30.501018 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:22:30.502024 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:22:30.502024 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m18:22:30.503033 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:22:30.503033 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:22:30.503033 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:22:30.503033 => 18:22:30.503033
[0m18:22:30.504062 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:22:30.518712 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:22:30.518712 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m18:22:30.524830 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:22:30.542497 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:22:30.543004 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table lakesales.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m18:22:30.559161 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m18:22:30.572315 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m18:22:30.574330 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:22:30.575337 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into lakesales.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m18:22:30.597585 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m18:22:30.601660 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m18:22:30.606714 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:22:30.608753 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:22:30.504062 => 18:22:30.608753
[0m18:22:30.609767 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5a6a96cc-924f-4ac6-9397-6f6c11c10f77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001539DEF5CA0>]}
[0m18:22:30.610778 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file lakesales.sample .................................... [[32mINSERT 2[0m in 0.11s]
[0m18:22:30.611791 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:22:30.612800 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:22:30.613808 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m18:22:30.614877 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m18:22:30.614877 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:22:30.624971 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:22:30.628007 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 18:22:30.614877 => 18:22:30.628007
[0m18:22:30.629127 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:22:30.644672 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:22:30.647717 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:22:30.648733 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:22:30.667314 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m18:22:30.669334 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 18:22:30.630129 => 18:22:30.668829
[0m18:22:30.669864 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.06s]
[0m18:22:30.670668 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:22:30.671192 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:22:30.671192 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m18:22:30.672209 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m18:22:30.672209 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:22:30.676751 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:22:30.677760 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 18:22:30.672714 => 18:22:30.677760
[0m18:22:30.678264 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:22:30.679273 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:22:30.680282 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:22:30.680786 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:22:30.685326 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:22:30.686366 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 18:22:30.678264 => 18:22:30.686366
[0m18:22:30.687294 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m18:22:30.688016 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:22:30.688523 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:22:30.689029 [info ] [Thread-1 (]: 5 of 7 START sql incremental model lakesales2.my_second_dbt_model .............. [RUN]
[0m18:22:30.690042 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m18:22:30.690547 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:22:30.692566 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:22:30.693574 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:22:30.690547 => 18:22:30.693069
[0m18:22:30.693574 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:22:30.695593 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:22:30.696097 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_second_dbt_model__dbt_tmp as
      -- Use the `ref` function to select from other models

select *
from lakesales.my_first_dbt_model
where id = 1
  
[0m18:22:30.700641 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:22:30.705289 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:22:30.705794 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:22:30.706300 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales2.my_second_dbt_model
    select `id` from my_second_dbt_model__dbt_tmp


[0m18:22:30.717383 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:22:30.718390 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:22:30.694079 => 18:22:30.718390
[0m18:22:30.719400 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5a6a96cc-924f-4ac6-9397-6f6c11c10f77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001539DB777D0>]}
[0m18:22:30.719400 [info ] [Thread-1 (]: 5 of 7 OK created sql incremental model lakesales2.my_second_dbt_model ......... [[32mOK[0m in 0.03s]
[0m18:22:30.720755 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:22:30.721335 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:22:30.721838 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m18:22:30.722846 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m18:22:30.722846 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:22:30.725867 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:22:30.726874 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 18:22:30.722846 => 18:22:30.726874
[0m18:22:30.726874 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:22:30.727882 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:22:30.728889 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:22:30.729894 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales2.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:22:30.733927 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:22:30.734935 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 18:22:30.726874 => 18:22:30.734935
[0m18:22:30.735943 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m18:22:30.736993 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:22:30.737680 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:22:30.738183 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m18:22:30.738183 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m18:22:30.739193 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:22:30.742227 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:22:30.742738 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 18:22:30.739193 => 18:22:30.742738
[0m18:22:30.743249 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:22:30.744258 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:22:30.745267 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:22:30.745267 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales2.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:22:30.750312 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:22:30.751322 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 18:22:30.743249 => 18:22:30.751322
[0m18:22:30.752335 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m18:22:30.753347 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:22:30.754566 [debug] [MainThread]: On master: ROLLBACK
[0m18:22:30.754566 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:22:30.755583 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:22:30.755583 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:22:30.755583 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:22:30.756596 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:22:30.756596 [info ] [MainThread]: 
[0m18:22:30.757605 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.47 seconds (0.47s).
[0m18:22:30.758615 [debug] [MainThread]: Command end result
[0m18:22:30.764684 [info ] [MainThread]: 
[0m18:22:30.764684 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:22:30.765695 [info ] [MainThread]: 
[0m18:22:30.765695 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m18:22:30.766706 [debug] [MainThread]: Command `cli build` succeeded at 18:22:30.766706 after 1.85 seconds
[0m18:22:30.766706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001539C5AA450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001539C685F70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153991BC680>]}
[0m18:22:30.767717 [debug] [MainThread]: Flushing usage events
[0m18:46:01.744230 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3FEEFC7A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3FEEFE630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3FEEFD3D0>]}


============================== 18:46:01.745235 | 80212027-ca7b-47a7-9759-74e8c8b2af25 ==============================
[0m18:46:01.745235 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:46:01.745235 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m18:46:01.823924 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '80212027-ca7b-47a7-9759-74e8c8b2af25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3FE6F2C00>]}
[0m18:46:01.855828 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '80212027-ca7b-47a7-9759-74e8c8b2af25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38000A8A0>]}
[0m18:46:01.856884 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:46:01.869010 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:46:01.870015 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m18:46:01.870015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '80212027-ca7b-47a7-9759-74e8c8b2af25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38005D700>]}
[0m18:46:03.032072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '80212027-ca7b-47a7-9759-74e8c8b2af25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3801E6FF0>]}
[0m18:46:03.054856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '80212027-ca7b-47a7-9759-74e8c8b2af25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38038F380>]}
[0m18:46:03.054856 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m18:46:03.056882 [info ] [MainThread]: 
[0m18:46:03.056882 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:46:03.057896 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:46:03.069054 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales2'
[0m18:46:03.081397 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales2, now list_None_lakesales)
[0m18:46:03.092484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '80212027-ca7b-47a7-9759-74e8c8b2af25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3FE4292E0>]}
[0m18:46:03.093560 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:46:03.094564 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:46:03.094564 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:46:03.095575 [info ] [MainThread]: 
[0m18:46:03.097862 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:46:03.098870 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m18:46:03.099877 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:46:03.099877 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:46:03.105910 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:46:03.106915 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:46:03.099877 => 18:46:03.106915
[0m18:46:03.107922 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:46:03.143197 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:46:03.145231 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:46:03.145231 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m18:46:03.145231 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:46:03.243855 [debug] [Thread-1 (]: SQL status: OK in 0.10000000149011612 seconds
[0m18:46:03.263712 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m18:46:03.265237 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m18:46:03.265742 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m18:46:03.277370 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:46:03.284993 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:46:03.107922 => 18:46:03.284993
[0m18:46:03.286008 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '80212027-ca7b-47a7-9759-74e8c8b2af25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38055AE70>]}
[0m18:46:03.287022 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model lakesales.my_first_dbt_model ........... [[32mOK[0m in 0.19s]
[0m18:46:03.287022 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:46:03.288037 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:46:03.288037 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m18:46:03.289051 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:46:03.289051 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:46:03.289051 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:46:03.289051 => 18:46:03.289051
[0m18:46:03.290082 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:46:03.303380 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:46:03.304386 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m18:46:03.308408 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:46:03.323628 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:46:03.324637 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table lakesales.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m18:46:03.339501 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:46:03.348238 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m18:46:03.351439 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m18:46:03.351439 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into lakesales.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m18:46:03.363589 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:46:03.367627 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m18:46:03.369642 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:46:03.369642 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:46:03.290082 => 18:46:03.369642
[0m18:46:03.370656 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '80212027-ca7b-47a7-9759-74e8c8b2af25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B38055B9E0>]}
[0m18:46:03.370656 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file lakesales.sample .................................... [[32mINSERT 2[0m in 0.08s]
[0m18:46:03.371672 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:46:03.371672 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:46:03.372686 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m18:46:03.373692 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m18:46:03.373692 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:46:03.383078 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:46:03.384090 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 18:46:03.374195 => 18:46:03.384090
[0m18:46:03.384090 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:46:03.392247 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:46:03.393262 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:46:03.393262 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:46:03.407203 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:46:03.408209 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 18:46:03.385168 => 18:46:03.408209
[0m18:46:03.409216 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.04s]
[0m18:46:03.409216 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:46:03.410222 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:46:03.410222 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m18:46:03.411255 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m18:46:03.411255 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:46:03.415607 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:46:03.416613 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 18:46:03.411255 => 18:46:03.416613
[0m18:46:03.416613 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:46:03.418625 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:46:03.418625 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:46:03.419635 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:46:03.423668 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:46:03.424673 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 18:46:03.416613 => 18:46:03.424673
[0m18:46:03.425679 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m18:46:03.425679 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:46:03.426685 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:46:03.426685 [info ] [Thread-1 (]: 5 of 7 START sql incremental model lakesales2.my_second_dbt_model .............. [RUN]
[0m18:46:03.427692 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m18:46:03.427692 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:46:03.430214 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:46:03.430214 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:46:03.427692 => 18:46:03.430214
[0m18:46:03.431230 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:46:03.433399 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:46:03.433399 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_second_dbt_model__dbt_tmp as
      -- Use the `ref` function to select from other models

select *
from lakesales.my_first_dbt_model
where id = 1
  
[0m18:46:03.437423 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:46:03.441446 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m18:46:03.442452 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m18:46:03.442452 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales2.my_second_dbt_model
    select `id` from my_second_dbt_model__dbt_tmp


[0m18:46:03.455160 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m18:46:03.458205 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:46:03.431230 => 18:46:03.458205
[0m18:46:03.460231 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '80212027-ca7b-47a7-9759-74e8c8b2af25', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3808A73E0>]}
[0m18:46:03.460231 [info ] [Thread-1 (]: 5 of 7 OK created sql incremental model lakesales2.my_second_dbt_model ......... [[32mOK[0m in 0.03s]
[0m18:46:03.461284 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:46:03.462292 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:46:03.462292 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m18:46:03.463302 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m18:46:03.463302 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:46:03.466376 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:46:03.467383 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 18:46:03.463302 => 18:46:03.467383
[0m18:46:03.468388 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:46:03.469393 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:46:03.470398 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:46:03.470398 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales2.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m18:46:03.474425 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:46:03.475430 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 18:46:03.468388 => 18:46:03.475430
[0m18:46:03.476434 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m18:46:03.476434 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:46:03.477440 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:46:03.477440 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m18:46:03.477440 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m18:46:03.478446 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:46:03.480851 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:46:03.481287 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 18:46:03.478446 => 18:46:03.481287
[0m18:46:03.481287 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:46:03.483295 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:46:03.483295 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:46:03.484300 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales2.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m18:46:03.488321 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m18:46:03.489326 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 18:46:03.482290 => 18:46:03.489326
[0m18:46:03.489326 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m18:46:03.490332 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:46:03.491339 [debug] [MainThread]: On master: ROLLBACK
[0m18:46:03.491339 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:46:03.491339 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m18:46:03.491339 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m18:46:03.492344 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:46:03.492344 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m18:46:03.492344 [info ] [MainThread]: 
[0m18:46:03.493349 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.44 seconds (0.44s).
[0m18:46:03.493349 [debug] [MainThread]: Command end result
[0m18:46:03.499423 [info ] [MainThread]: 
[0m18:46:03.499423 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:46:03.499423 [info ] [MainThread]: 
[0m18:46:03.499423 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m18:46:03.500428 [debug] [MainThread]: Command `cli build` succeeded at 18:46:03.500428 after 1.78 seconds
[0m18:46:03.500428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3FD68A960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B380185790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B3801E5820>]}
[0m18:46:03.501433 [debug] [MainThread]: Flushing usage events
[0m18:49:38.091078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018AF4592DB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018AF4592C00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018AF4592F00>]}


============================== 18:49:38.095099 | 234aacbe-f494-4f03-a146-356cf55ad7c9 ==============================
[0m18:49:38.095099 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:49:38.096105 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt docs generate', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m18:49:38.100133 [info ] [MainThread]: Error importing adapter: No module named 'dbt.adapters.fabricsparknb'
[0m18:49:38.100133 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Could not find adapter type fabricsparknb!
[0m18:49:38.101138 [debug] [MainThread]: Command `dbt docs generate` failed at 18:49:38.101138 after 0.10 seconds
[0m18:49:38.102143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018AF4211EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018AF4592DB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018AF4591CA0>]}
[0m18:49:38.102143 [debug] [MainThread]: Flushing usage events
[0m18:51:10.869724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AB7BAC4D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AB7BAE090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AB7BAD070>]}


============================== 18:51:10.873764 | 0b443858-2db2-4449-9c83-7dc7771617ea ==============================
[0m18:51:10.873764 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:51:10.873764 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt docs generate', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m18:51:11.082291 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricspark.connector to DEBUG
[0m18:51:11.083294 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m18:51:11.083294 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m18:51:11.083294 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m18:51:11.148796 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricsparknb.connector to DEBUG
[0m18:51:11.148796 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m18:51:11.148796 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m18:51:11.149803 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m18:51:11.240452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0b443858-2db2-4449-9c83-7dc7771617ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AB8382150>]}
[0m18:51:11.273639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0b443858-2db2-4449-9c83-7dc7771617ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AB834A390>]}
[0m18:51:11.274643 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:51:11.286724 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:51:11.759613 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:51:11.759613 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:51:11.763646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0b443858-2db2-4449-9c83-7dc7771617ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AB9588920>]}
[0m18:51:11.765662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0b443858-2db2-4449-9c83-7dc7771617ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AB95CF1A0>]}
[0m18:51:11.765662 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m18:51:11.766672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0b443858-2db2-4449-9c83-7dc7771617ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AB7BAF650>]}
[0m18:51:11.767680 [info ] [MainThread]: 
[0m18:51:11.767680 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:51:11.768687 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m18:51:11.782796 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m18:51:11.792878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0b443858-2db2-4449-9c83-7dc7771617ea', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AB82E8440>]}
[0m18:51:11.792878 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:51:11.793883 [info ] [MainThread]: 
[0m18:51:11.795899 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:51:11.795899 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:51:11.796904 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:51:11.800955 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:51:11.801964 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:51:11.796904 => 18:51:11.801964
[0m18:51:11.801964 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:51:11.801964 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:51:11.801964 => 18:51:11.801964
[0m18:51:11.802973 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:51:11.802973 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:51:11.803982 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:51:11.803982 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:51:11.804992 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:51:11.803982 => 18:51:11.804992
[0m18:51:11.806001 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:51:11.806001 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:51:11.806001 => 18:51:11.806001
[0m18:51:11.806001 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:51:11.807010 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:51:11.807010 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now model.testproj.my_second_dbt_model)
[0m18:51:11.807010 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:51:11.809028 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:51:11.810038 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:51:11.808019 => 18:51:11.810038
[0m18:51:11.810038 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:51:11.810038 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:51:11.810038 => 18:51:11.810038
[0m18:51:11.811048 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:51:11.811048 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:51:11.812057 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m18:51:11.812057 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:51:11.819148 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:51:11.820157 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 18:51:11.812057 => 18:51:11.820157
[0m18:51:11.820157 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:51:11.821168 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 18:51:11.820157 => 18:51:11.820157
[0m18:51:11.821168 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:51:11.821168 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:51:11.822178 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m18:51:11.822178 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:51:11.826216 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:51:11.826216 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 18:51:11.822178 => 18:51:11.826216
[0m18:51:11.827226 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:51:11.827226 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 18:51:11.827226 => 18:51:11.827226
[0m18:51:11.828236 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:51:11.828236 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:51:11.828236 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m18:51:11.829244 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:51:11.831277 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:51:11.832283 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 18:51:11.829244 => 18:51:11.832283
[0m18:51:11.832283 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:51:11.833291 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 18:51:11.832283 => 18:51:11.832283
[0m18:51:11.833291 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:51:11.833291 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:51:11.834298 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m18:51:11.834298 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:51:11.836308 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:51:11.837312 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 18:51:11.834298 => 18:51:11.837312
[0m18:51:11.837312 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:51:11.838317 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 18:51:11.838317 => 18:51:11.838317
[0m18:51:11.838317 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:51:11.840329 [debug] [MainThread]: Command end result
[0m18:51:11.997508 [debug] [MainThread]: Re-using an available connection from the pool (formerly master, now generate_catalog)
[0m18:51:11.998514 [info ] [MainThread]: Building catalog
[0m18:51:12.000530 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'lakesales'
[0m18:51:12.000530 [debug] [ThreadPool]: fabricsparknb adapter: database name is 
[0m18:51:12.000530 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.customers
[0m18:51:12.005572 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:51:12.006581 [debug] [ThreadPool]: Using fabricsparknb connection "lakesales"
[0m18:51:12.006581 [debug] [ThreadPool]: On lakesales: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/*{"project_root": "testproj"}*/
describe extended lakesales.customers
  
[0m18:51:12.006581 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:51:12.007592 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/*{"project_root": "testproj"}*/
describe extended lakesales.customers
  
[0m18:51:12.007592 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: cannot access local variable 'node_id' where it is not associated with a value
[0m18:51:12.007592 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro describe_table_extended_without_caching
[0m18:51:12.007592 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:51:12.008601 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about lakesales.customers: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:51:12.008601 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly lakesales, now lakesales2)
[0m18:51:12.008601 [warn ] [MainThread]: Encountered an error while generating catalog: Runtime Error
  Runtime Error
    cannot access local variable 'node_id' where it is not associated with a value
[0m18:51:12.009606 [debug] [ThreadPool]: fabricsparknb adapter: database name is 
[0m18:51:12.009606 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales2.sales_invoicelines2
[0m18:51:12.010612 [debug] [ThreadPool]: Using fabricsparknb connection "lakesales2"
[0m18:51:12.011622 [debug] [ThreadPool]: On lakesales2: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/*{"project_root": "testproj"}*/
describe extended lakesales2.sales_invoicelines2
  
[0m18:51:12.011622 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/*{"project_root": "testproj"}*/
describe extended lakesales2.sales_invoicelines2
  
[0m18:51:12.011622 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: cannot access local variable 'node_id' where it is not associated with a value
[0m18:51:12.011622 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro describe_table_extended_without_caching
[0m18:51:12.012632 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:51:12.012632 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about lakesales2.sales_invoicelines2: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:51:12.012632 [warn ] [MainThread]: Encountered an error while generating catalog: Runtime Error
  Runtime Error
    cannot access local variable 'node_id' where it is not associated with a value
[0m18:51:12.017673 [error] [MainThread]: dbt encountered 2 failures while writing the catalog
[0m18:51:12.018679 [info ] [MainThread]: Catalog written to C:\Users\jramp\source\gitjr\dbt-fabricsparknb\testproj\target\catalog.json
[0m18:51:12.019705 [debug] [MainThread]: Command `dbt docs generate` failed at 18:51:12.019705 after 1.25 seconds
[0m18:51:12.019705 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AB79FA480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AB79F8260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022AB9732180>]}
[0m18:51:12.019705 [debug] [MainThread]: Flushing usage events
[0m18:55:07.621198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200AB60E4B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200AB60E570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200AB60CE30>]}


============================== 18:55:07.624214 | 6e25be4e-ef30-421b-aeea-e516564141cb ==============================
[0m18:55:07.624214 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:55:07.625219 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt docs generate', 'static_parser': 'True', 'log_format': 'default', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:55:07.764286 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricspark.connector to DEBUG
[0m18:55:07.764286 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m18:55:07.765291 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m18:55:07.765291 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m18:55:07.817599 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricsparknb.connector to DEBUG
[0m18:55:07.818604 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m18:55:07.818604 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m18:55:07.818604 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m18:55:07.903163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6e25be4e-ef30-421b-aeea-e516564141cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200ABDBEB40>]}
[0m18:55:07.936353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6e25be4e-ef30-421b-aeea-e516564141cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200ABDBEB40>]}
[0m18:55:07.938373 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:55:07.944432 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:55:07.984705 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:55:07.985721 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:55:07.989750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6e25be4e-ef30-421b-aeea-e516564141cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200ABF29DF0>]}
[0m18:55:07.990761 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6e25be4e-ef30-421b-aeea-e516564141cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200AD00B830>]}
[0m18:55:07.991771 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m18:55:07.991771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e25be4e-ef30-421b-aeea-e516564141cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200ABDBEAB0>]}
[0m18:55:07.992779 [info ] [MainThread]: 
[0m18:55:07.993785 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:55:07.994791 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales2'
[0m18:55:08.002851 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales2, now list_None_lakesales)
[0m18:55:08.012902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e25be4e-ef30-421b-aeea-e516564141cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200AB07E780>]}
[0m18:55:08.012902 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:55:08.013907 [info ] [MainThread]: 
[0m18:55:08.015920 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:55:08.015920 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:55:08.015920 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:55:08.019961 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:55:08.020966 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:55:08.016927 => 18:55:08.020966
[0m18:55:08.020966 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:55:08.021971 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:55:08.021971 => 18:55:08.021971
[0m18:55:08.021971 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:55:08.022978 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:55:08.022978 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:55:08.022978 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:55:08.024998 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:55:08.023988 => 18:55:08.023988
[0m18:55:08.024998 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:55:08.024998 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:55:08.024998 => 18:55:08.024998
[0m18:55:08.026008 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:55:08.026008 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:55:08.026008 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now model.testproj.my_second_dbt_model)
[0m18:55:08.027017 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:55:08.028536 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:55:08.029045 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:55:08.027017 => 18:55:08.029045
[0m18:55:08.029045 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:55:08.030055 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:55:08.030055 => 18:55:08.030055
[0m18:55:08.030055 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:55:08.031065 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:55:08.031065 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m18:55:08.031065 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:55:08.039152 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:55:08.039152 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 18:55:08.032075 => 18:55:08.039152
[0m18:55:08.040167 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:55:08.040167 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 18:55:08.040167 => 18:55:08.040167
[0m18:55:08.040167 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:55:08.041177 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:55:08.041177 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m18:55:08.041177 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:55:08.045216 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:55:08.046227 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 18:55:08.042187 => 18:55:08.046227
[0m18:55:08.046227 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:55:08.046227 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 18:55:08.046227 => 18:55:08.046227
[0m18:55:08.047237 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:55:08.047237 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:55:08.048255 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m18:55:08.048255 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:55:08.050270 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:55:08.051278 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 18:55:08.048255 => 18:55:08.051278
[0m18:55:08.051278 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:55:08.052287 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 18:55:08.052287 => 18:55:08.052287
[0m18:55:08.052287 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:55:08.053298 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:55:08.053298 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m18:55:08.054309 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:55:08.056838 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:55:08.057351 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 18:55:08.054309 => 18:55:08.057351
[0m18:55:08.057351 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:55:08.058360 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 18:55:08.058360 => 18:55:08.058360
[0m18:55:08.058360 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:55:08.060379 [debug] [MainThread]: Command end result
[0m18:55:08.068481 [debug] [MainThread]: Re-using an available connection from the pool (formerly master, now generate_catalog)
[0m18:55:08.068481 [info ] [MainThread]: Building catalog
[0m18:55:08.069487 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'lakesales2'
[0m18:55:08.070495 [debug] [ThreadPool]: fabricsparknb adapter: database name is 
[0m18:55:08.070495 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales2.sales_invoicelines2
[0m18:55:08.076532 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:55:08.076532 [debug] [ThreadPool]: Using fabricsparknb connection "lakesales2"
[0m18:55:08.076532 [debug] [ThreadPool]: On lakesales2: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/*{"project_root": "testproj"}*/
describe extended lakesales2.sales_invoicelines2
  
[0m18:55:08.077539 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:55:08.077539 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/*{"project_root": "testproj"}*/
describe extended lakesales2.sales_invoicelines2
  
[0m18:55:08.078561 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: cannot access local variable 'node_id' where it is not associated with a value
[0m18:55:08.078561 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro describe_table_extended_without_caching
[0m18:55:08.078561 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:55:08.078561 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about lakesales2.sales_invoicelines2: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:55:08.079568 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly lakesales2, now lakesales)
[0m18:55:08.079568 [warn ] [MainThread]: Encountered an error while generating catalog: Runtime Error
  Runtime Error
    cannot access local variable 'node_id' where it is not associated with a value
[0m18:55:08.079568 [debug] [ThreadPool]: fabricsparknb adapter: database name is 
[0m18:55:08.080574 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.customers
[0m18:55:08.081580 [debug] [ThreadPool]: Using fabricsparknb connection "lakesales"
[0m18:55:08.081580 [debug] [ThreadPool]: On lakesales: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/*{"project_root": "testproj"}*/
describe extended lakesales.customers
  
[0m18:55:08.082587 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/*{"project_root": "testproj"}*/
describe extended lakesales.customers
  
[0m18:55:08.082587 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: cannot access local variable 'node_id' where it is not associated with a value
[0m18:55:08.082587 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro describe_table_extended_without_caching
[0m18:55:08.083598 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:55:08.083598 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about lakesales.customers: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:55:08.085636 [warn ] [MainThread]: Encountered an error while generating catalog: Runtime Error
  Runtime Error
    cannot access local variable 'node_id' where it is not associated with a value
[0m18:55:08.093779 [error] [MainThread]: dbt encountered 2 failures while writing the catalog
[0m18:55:08.094799 [info ] [MainThread]: Catalog written to C:\Users\jramp\source\gitjr\dbt-fabricsparknb\testproj\target\catalog.json
[0m18:55:08.096826 [debug] [MainThread]: Command `dbt docs generate` failed at 18:55:08.096826 after 0.55 seconds
[0m18:55:08.096826 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200A86B2750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200ABEFD790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000200AB5E2B10>]}
[0m18:55:08.097833 [debug] [MainThread]: Flushing usage events
[0m18:56:57.277261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225A8E3DBE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225A8E3CE60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225A8E3D6A0>]}


============================== 18:56:57.281282 | 1e7faa84-908f-4c40-82e8-e99832554fdf ==============================
[0m18:56:57.281282 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:56:57.281282 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt docs generate', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m18:56:57.412183 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricspark.connector to DEBUG
[0m18:56:57.412183 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m18:56:57.413187 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m18:56:57.413187 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m18:56:57.469616 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricsparknb.connector to DEBUG
[0m18:56:57.470621 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m18:56:57.470621 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m18:56:57.470621 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m18:56:57.555105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1e7faa84-908f-4c40-82e8-e99832554fdf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225A95E8800>]}
[0m18:56:57.588286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1e7faa84-908f-4c40-82e8-e99832554fdf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225A958E600>]}
[0m18:56:57.590296 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:56:57.596322 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:56:57.635578 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:56:57.636582 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:56:57.641605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1e7faa84-908f-4c40-82e8-e99832554fdf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225A975F080>]}
[0m18:56:57.643116 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1e7faa84-908f-4c40-82e8-e99832554fdf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225AA86BD40>]}
[0m18:56:57.643623 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m18:56:57.643623 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1e7faa84-908f-4c40-82e8-e99832554fdf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225AA8514F0>]}
[0m18:56:57.644631 [info ] [MainThread]: 
[0m18:56:57.645639 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:56:57.646644 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m18:56:57.655734 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m18:56:57.664779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1e7faa84-908f-4c40-82e8-e99832554fdf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225A88AE540>]}
[0m18:56:57.665784 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:56:57.665784 [info ] [MainThread]: 
[0m18:56:57.667805 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:56:57.668813 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:56:57.668813 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:56:57.673839 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:56:57.674847 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:56:57.668813 => 18:56:57.674847
[0m18:56:57.674847 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:56:57.675853 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:56:57.674847 => 18:56:57.674847
[0m18:56:57.675853 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:56:57.676859 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:56:57.676859 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:56:57.677867 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:56:57.678876 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:56:57.677867 => 18:56:57.678876
[0m18:56:57.679886 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:56:57.679886 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:56:57.679886 => 18:56:57.679886
[0m18:56:57.680897 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:56:57.680897 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:56:57.680897 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now model.testproj.my_second_dbt_model)
[0m18:56:57.681909 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:56:57.683937 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:56:57.684947 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:56:57.681909 => 18:56:57.683937
[0m18:56:57.684947 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:56:57.684947 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:56:57.684947 => 18:56:57.684947
[0m18:56:57.685957 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:56:57.685957 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:56:57.685957 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m18:56:57.686966 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:56:57.694016 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:56:57.695020 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 18:56:57.686966 => 18:56:57.695020
[0m18:56:57.695020 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:56:57.696025 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 18:56:57.696025 => 18:56:57.696025
[0m18:56:57.696025 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:56:57.697033 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:56:57.697033 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m18:56:57.697033 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:56:57.701067 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:56:57.702076 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 18:56:57.698042 => 18:56:57.702076
[0m18:56:57.702076 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:56:57.702076 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 18:56:57.702076 => 18:56:57.702076
[0m18:56:57.703086 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:56:57.703086 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:56:57.704097 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m18:56:57.704097 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:56:57.706110 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:56:57.707116 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 18:56:57.704097 => 18:56:57.707116
[0m18:56:57.707116 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:56:57.708121 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 18:56:57.708121 => 18:56:57.708121
[0m18:56:57.708121 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:56:57.708121 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:56:57.709127 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m18:56:57.709127 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:56:57.711138 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:56:57.712143 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 18:56:57.709127 => 18:56:57.712143
[0m18:56:57.712143 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:56:57.713148 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 18:56:57.712143 => 18:56:57.712143
[0m18:56:57.713148 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:56:57.714157 [debug] [MainThread]: Command end result
[0m18:56:57.724231 [debug] [MainThread]: Re-using an available connection from the pool (formerly master, now generate_catalog)
[0m18:56:57.725239 [info ] [MainThread]: Building catalog
[0m18:56:57.726246 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'lakesales'
[0m18:56:57.726246 [debug] [ThreadPool]: fabricsparknb adapter: database name is 
[0m18:56:57.727253 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.customers
[0m18:56:57.732320 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:56:57.732320 [debug] [ThreadPool]: Using fabricsparknb connection "lakesales"
[0m18:56:57.733326 [debug] [ThreadPool]: On lakesales: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/*{"project_root": "testproj"}*/
describe extended lakesales.customers
  
[0m18:56:57.733326 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:56:57.733326 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/*{"project_root": "testproj"}*/
describe extended lakesales.customers
  
[0m18:56:57.734333 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: cannot access local variable 'node_id' where it is not associated with a value
[0m18:56:57.734333 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro describe_table_extended_without_caching
[0m18:56:57.734333 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:56:57.735340 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about lakesales.customers: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:56:57.735340 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly lakesales, now lakesales2)
[0m18:56:57.735340 [warn ] [MainThread]: Encountered an error while generating catalog: Runtime Error
  Runtime Error
    cannot access local variable 'node_id' where it is not associated with a value
[0m18:56:57.736346 [debug] [ThreadPool]: fabricsparknb adapter: database name is 
[0m18:56:57.736346 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales2.sales_invoicelines2
[0m18:56:57.737352 [debug] [ThreadPool]: Using fabricsparknb connection "lakesales2"
[0m18:56:57.738359 [debug] [ThreadPool]: On lakesales2: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/*{"project_root": "testproj"}*/
describe extended lakesales2.sales_invoicelines2
  
[0m18:56:57.738865 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/*{"project_root": "testproj"}*/
describe extended lakesales2.sales_invoicelines2
  
[0m18:56:57.738865 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: cannot access local variable 'node_id' where it is not associated with a value
[0m18:56:57.739371 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro describe_table_extended_without_caching
[0m18:56:57.739371 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:56:57.739371 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about lakesales2.sales_invoicelines2: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:56:57.740379 [warn ] [MainThread]: Encountered an error while generating catalog: Runtime Error
  Runtime Error
    cannot access local variable 'node_id' where it is not associated with a value
[0m18:56:57.745419 [error] [MainThread]: dbt encountered 2 failures while writing the catalog
[0m18:56:57.746450 [info ] [MainThread]: Catalog written to C:\Users\jramp\source\gitjr\dbt-fabricsparknb\testproj\target\catalog.json
[0m18:56:57.747455 [debug] [MainThread]: Command `dbt docs generate` failed at 18:56:57.746450 after 0.56 seconds
[0m18:56:57.747455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225A8C3C9E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225A8E16C60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225A8E17620>]}
[0m18:56:57.747455 [debug] [MainThread]: Flushing usage events
[0m18:57:09.775355 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B596CE4E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B596CE720>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B596CE360>]}


============================== 18:57:09.779380 | 6dc6464c-073c-4f04-988c-33bf86866702 ==============================
[0m18:57:09.779380 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:57:09.779380 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'debug': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt docs generate', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m18:57:09.917403 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricspark.connector to DEBUG
[0m18:57:09.918407 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m18:57:09.918407 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m18:57:09.919413 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m18:57:09.985802 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricsparknb.connector to DEBUG
[0m18:57:09.985802 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m18:57:09.985802 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m18:57:09.986810 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m18:57:10.066282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6dc6464c-073c-4f04-988c-33bf86866702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B59E48EC0>]}
[0m18:57:10.098425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6dc6464c-073c-4f04-988c-33bf86866702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B594B02C0>]}
[0m18:57:10.099429 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:57:10.106493 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:57:10.148889 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:57:10.149895 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:57:10.153922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6dc6464c-073c-4f04-988c-33bf86866702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B596F6E40>]}
[0m18:57:10.155935 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6dc6464c-073c-4f04-988c-33bf86866702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B5B0C7A40>]}
[0m18:57:10.155935 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m18:57:10.156942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6dc6464c-073c-4f04-988c-33bf86866702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B5A05EEA0>]}
[0m18:57:10.157949 [info ] [MainThread]: 
[0m18:57:10.158956 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:57:10.159980 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales2'
[0m18:57:10.169299 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales2, now list_None_lakesales)
[0m18:57:10.179597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6dc6464c-073c-4f04-988c-33bf86866702', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B59DE9FD0>]}
[0m18:57:10.180603 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:57:10.180603 [info ] [MainThread]: 
[0m18:57:10.182616 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:57:10.183623 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:57:10.183623 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:57:10.187646 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:57:10.188653 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:57:10.183623 => 18:57:10.188653
[0m18:57:10.188653 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:57:10.189660 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:57:10.189660 => 18:57:10.189660
[0m18:57:10.189660 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:57:10.190670 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:57:10.190670 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:57:10.191679 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:57:10.192688 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:57:10.191679 => 18:57:10.192688
[0m18:57:10.192688 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:57:10.193696 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:57:10.193696 => 18:57:10.193696
[0m18:57:10.194203 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:57:10.194706 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:57:10.194706 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now model.testproj.my_second_dbt_model)
[0m18:57:10.195713 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:57:10.197726 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:57:10.197726 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:57:10.195713 => 18:57:10.197726
[0m18:57:10.198731 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:57:10.198731 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:57:10.198731 => 18:57:10.198731
[0m18:57:10.198731 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:57:10.199736 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:57:10.199736 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m18:57:10.200742 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:57:10.207785 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:57:10.208792 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 18:57:10.200742 => 18:57:10.208792
[0m18:57:10.208792 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:57:10.208792 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 18:57:10.208792 => 18:57:10.208792
[0m18:57:10.209801 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:57:10.210842 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:57:10.210842 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m18:57:10.210842 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:57:10.215899 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:57:10.216906 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 18:57:10.211849 => 18:57:10.215899
[0m18:57:10.216906 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:57:10.216906 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 18:57:10.216906 => 18:57:10.216906
[0m18:57:10.217912 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:57:10.217912 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:57:10.218918 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m18:57:10.218918 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:57:10.220931 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:57:10.221937 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 18:57:10.218918 => 18:57:10.221937
[0m18:57:10.221937 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:57:10.222944 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 18:57:10.222944 => 18:57:10.222944
[0m18:57:10.222944 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:57:10.223950 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:57:10.223950 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m18:57:10.223950 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:57:10.227003 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:57:10.228009 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 18:57:10.224973 => 18:57:10.227003
[0m18:57:10.228009 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:57:10.228009 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 18:57:10.228009 => 18:57:10.228009
[0m18:57:10.229029 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:57:10.231047 [debug] [MainThread]: Command end result
[0m18:57:10.245226 [debug] [MainThread]: Re-using an available connection from the pool (formerly master, now generate_catalog)
[0m18:57:10.245226 [info ] [MainThread]: Building catalog
[0m18:57:10.247241 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'lakesales2'
[0m18:57:10.247241 [debug] [ThreadPool]: fabricsparknb adapter: database name is 
[0m18:57:10.248249 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales2.sales_invoicelines2
[0m18:57:10.253792 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:57:10.254305 [debug] [ThreadPool]: Using fabricsparknb connection "lakesales2"
[0m18:57:10.254305 [debug] [ThreadPool]: On lakesales2: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/*{"project_root": "testproj"}*/
describe extended lakesales2.sales_invoicelines2
  
[0m18:57:10.254305 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:57:10.255316 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/*{"project_root": "testproj"}*/
describe extended lakesales2.sales_invoicelines2
  
[0m18:57:10.255316 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: cannot access local variable 'node_id' where it is not associated with a value
[0m18:57:10.256371 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro describe_table_extended_without_caching
[0m18:57:10.256371 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:57:10.256371 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about lakesales2.sales_invoicelines2: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:57:10.257378 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly lakesales2, now lakesales)
[0m18:57:10.258385 [debug] [ThreadPool]: fabricsparknb adapter: database name is 
[0m18:57:10.257378 [warn ] [MainThread]: Encountered an error while generating catalog: Runtime Error
  Runtime Error
    cannot access local variable 'node_id' where it is not associated with a value
[0m18:57:10.258385 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.customers
[0m18:57:10.260401 [debug] [ThreadPool]: Using fabricsparknb connection "lakesales"
[0m18:57:10.260401 [debug] [ThreadPool]: On lakesales: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/*{"project_root": "testproj"}*/
describe extended lakesales.customers
  
[0m18:57:10.261409 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/*{"project_root": "testproj"}*/
describe extended lakesales.customers
  
[0m18:57:10.261409 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: cannot access local variable 'node_id' where it is not associated with a value
[0m18:57:10.261409 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro describe_table_extended_without_caching
[0m18:57:10.261409 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:57:10.262431 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about lakesales.customers: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:57:10.262431 [warn ] [MainThread]: Encountered an error while generating catalog: Runtime Error
  Runtime Error
    cannot access local variable 'node_id' where it is not associated with a value
[0m18:57:10.269513 [error] [MainThread]: dbt encountered 2 failures while writing the catalog
[0m18:57:10.270523 [info ] [MainThread]: Catalog written to C:\Users\jramp\source\gitjr\dbt-fabricsparknb\testproj\target\catalog.json
[0m18:57:10.273600 [debug] [MainThread]: Command `dbt docs generate` failed at 18:57:10.272583 after 0.57 seconds
[0m18:57:10.274634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B59290050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B59293E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021B59D49E50>]}
[0m18:57:10.275655 [debug] [MainThread]: Flushing usage events
[0m18:59:29.352977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F1AFCCFB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F17F2E2A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F1AFCC770>]}


============================== 18:59:29.355992 | 6364c058-88d7-4c4c-a252-8b9c3a017deb ==============================
[0m18:59:29.355992 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:59:29.356997 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt docs generate', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:59:29.501974 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricspark.connector to DEBUG
[0m18:59:29.501974 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m18:59:29.501974 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m18:59:29.502977 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m18:59:29.553314 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricsparknb.connector to DEBUG
[0m18:59:29.553314 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m18:59:29.553314 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m18:59:29.554319 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m18:59:29.639018 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6364c058-88d7-4c4c-a252-8b9c3a017deb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F1B871040>]}
[0m18:59:29.672428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6364c058-88d7-4c4c-a252-8b9c3a017deb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F1C89B3E0>]}
[0m18:59:29.674535 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:59:29.681116 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:59:29.723560 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:59:29.723560 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:59:29.727581 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6364c058-88d7-4c4c-a252-8b9c3a017deb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F1B118E60>]}
[0m18:59:29.729592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6364c058-88d7-4c4c-a252-8b9c3a017deb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F1C9ABA10>]}
[0m18:59:29.729592 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m18:59:29.729592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6364c058-88d7-4c4c-a252-8b9c3a017deb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F1B2E6AB0>]}
[0m18:59:29.731611 [info ] [MainThread]: 
[0m18:59:29.731611 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:59:29.732617 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales2'
[0m18:59:29.741669 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales2, now list_None_lakesales)
[0m18:59:29.751736 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6364c058-88d7-4c4c-a252-8b9c3a017deb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F1B793AA0>]}
[0m18:59:29.751736 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m18:59:29.751736 [info ] [MainThread]: 
[0m18:59:29.753746 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m18:59:29.754751 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m18:59:29.754751 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m18:59:29.758772 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m18:59:29.759778 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 18:59:29.754751 => 18:59:29.759778
[0m18:59:29.759778 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m18:59:29.760783 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 18:59:29.759778 => 18:59:29.759778
[0m18:59:29.760783 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m18:59:29.760783 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m18:59:29.761789 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m18:59:29.761789 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m18:59:29.762794 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 18:59:29.761789 => 18:59:29.762794
[0m18:59:29.763814 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m18:59:29.763814 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 18:59:29.763814 => 18:59:29.763814
[0m18:59:29.764819 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m18:59:29.764819 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m18:59:29.764819 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now model.testproj.my_second_dbt_model)
[0m18:59:29.765824 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m18:59:29.766829 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m18:59:29.767835 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 18:59:29.765824 => 18:59:29.767835
[0m18:59:29.767835 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m18:59:29.768840 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 18:59:29.768840 => 18:59:29.768840
[0m18:59:29.768840 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m18:59:29.769845 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:59:29.769845 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m18:59:29.769845 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:59:29.776890 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m18:59:29.777895 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 18:59:29.770850 => 18:59:29.777895
[0m18:59:29.777895 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:59:29.778900 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 18:59:29.778900 => 18:59:29.778900
[0m18:59:29.779939 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m18:59:29.779939 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:59:29.780946 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m18:59:29.780946 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:59:29.784969 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m18:59:29.784969 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 18:59:29.780946 => 18:59:29.784969
[0m18:59:29.785974 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:59:29.785974 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 18:59:29.785974 => 18:59:29.785974
[0m18:59:29.786980 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m18:59:29.786980 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:59:29.786980 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m18:59:29.787986 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:59:29.789999 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m18:59:29.791011 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 18:59:29.787986 => 18:59:29.791011
[0m18:59:29.791011 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:59:29.791011 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 18:59:29.791011 => 18:59:29.791011
[0m18:59:29.792019 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m18:59:29.792019 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:59:29.793025 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m18:59:29.793025 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:59:29.796091 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m18:59:29.797096 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 18:59:29.793025 => 18:59:29.797096
[0m18:59:29.797096 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:59:29.798103 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 18:59:29.798103 => 18:59:29.798103
[0m18:59:29.798103 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m18:59:29.800114 [debug] [MainThread]: Command end result
[0m18:59:29.808165 [debug] [MainThread]: Re-using an available connection from the pool (formerly master, now generate_catalog)
[0m18:59:29.808165 [info ] [MainThread]: Building catalog
[0m18:59:29.809171 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'lakesales2'
[0m18:59:29.810187 [debug] [ThreadPool]: fabricsparknb adapter: database name is 
[0m18:59:29.810187 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales2.sales_invoicelines2
[0m18:59:29.815217 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m18:59:29.816222 [debug] [ThreadPool]: Using fabricsparknb connection "lakesales2"
[0m18:59:29.816222 [debug] [ThreadPool]: On lakesales2: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/*{"project_root": "testproj"}*/
describe extended lakesales2.sales_invoicelines2
  
[0m18:59:29.816222 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:59:29.818234 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/*{"project_root": "testproj"}*/
describe extended lakesales2.sales_invoicelines2
  
[0m18:59:29.818234 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: cannot access local variable 'node_id' where it is not associated with a value
[0m18:59:29.818234 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro describe_table_extended_without_caching
[0m18:59:29.819239 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:59:29.819239 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about lakesales2.sales_invoicelines2: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:59:29.820244 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly lakesales2, now lakesales)
[0m18:59:29.820244 [warn ] [MainThread]: Encountered an error while generating catalog: Runtime Error
  Runtime Error
    cannot access local variable 'node_id' where it is not associated with a value
[0m18:59:29.820244 [debug] [ThreadPool]: fabricsparknb adapter: database name is 
[0m18:59:29.820244 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.customers
[0m18:59:29.822254 [debug] [ThreadPool]: Using fabricsparknb connection "lakesales"
[0m18:59:29.822254 [debug] [ThreadPool]: On lakesales: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/*{"project_root": "testproj"}*/
describe extended lakesales.customers
  
[0m18:59:29.823259 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/*{"project_root": "testproj"}*/
describe extended lakesales.customers
  
[0m18:59:29.823259 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: cannot access local variable 'node_id' where it is not associated with a value
[0m18:59:29.823259 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro describe_table_extended_without_caching
[0m18:59:29.824272 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:59:29.824272 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about lakesales.customers: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m18:59:29.826355 [warn ] [MainThread]: Encountered an error while generating catalog: Runtime Error
  Runtime Error
    cannot access local variable 'node_id' where it is not associated with a value
[0m18:59:29.835475 [error] [MainThread]: dbt encountered 2 failures while writing the catalog
[0m18:59:29.836482 [info ] [MainThread]: Catalog written to C:\Users\jramp\source\gitjr\dbt-fabricsparknb\testproj\target\catalog.json
[0m18:59:29.837490 [debug] [MainThread]: Command `dbt docs generate` failed at 18:59:29.837490 after 0.56 seconds
[0m18:59:29.838495 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F1AB5FEC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F1B015790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022F1AE78BC0>]}
[0m18:59:29.838495 [debug] [MainThread]: Flushing usage events
[0m19:06:56.651359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203279D2D20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203279D2D80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203279D2F00>]}


============================== 19:06:56.655407 | e4953b6b-5e92-4343-88ba-6e35f8f3b532 ==============================
[0m19:06:56.655407 [info ] [MainThread]: Running with dbt=1.7.14
[0m19:06:56.656413 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt --log-level debug docs generate', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:06:56.854898 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricspark.connector to DEBUG
[0m19:06:56.855904 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m19:06:56.855904 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m19:06:56.855904 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m19:06:56.920313 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricsparknb.connector to DEBUG
[0m19:06:56.921321 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m19:06:56.922349 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m19:06:56.922349 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m19:06:57.008890 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e4953b6b-5e92-4343-88ba-6e35f8f3b532', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020328050560>]}
[0m19:06:57.046711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e4953b6b-5e92-4343-88ba-6e35f8f3b532', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203281F30E0>]}
[0m19:06:57.049780 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:06:57.073044 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m19:06:57.604151 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:06:57.604151 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:06:57.609191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e4953b6b-5e92-4343-88ba-6e35f8f3b532', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020328301910>]}
[0m19:06:57.610202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e4953b6b-5e92-4343-88ba-6e35f8f3b532', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020329437B30>]}
[0m19:06:57.611209 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m19:06:57.611209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e4953b6b-5e92-4343-88ba-6e35f8f3b532', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203282559D0>]}
[0m19:06:57.612215 [info ] [MainThread]: 
[0m19:06:57.613221 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:06:57.614244 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m19:06:57.628347 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m19:06:57.639443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e4953b6b-5e92-4343-88ba-6e35f8f3b532', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002032947F8F0>]}
[0m19:06:57.640450 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:06:57.641458 [info ] [MainThread]: 
[0m19:06:57.646515 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m19:06:57.647531 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:06:57.648540 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:06:57.656646 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:06:57.658658 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:06:57.649547 => 19:06:57.658658
[0m19:06:57.658658 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:06:57.659718 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:06:57.659718 => 19:06:57.659718
[0m19:06:57.660720 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:06:57.661727 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m19:06:57.662732 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m19:06:57.662732 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m19:06:57.664743 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 19:06:57.662732 => 19:06:57.663737
[0m19:06:57.664743 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m19:06:57.665751 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 19:06:57.664743 => 19:06:57.664743
[0m19:06:57.665751 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m19:06:57.666757 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m19:06:57.666757 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now model.testproj.my_second_dbt_model)
[0m19:06:57.667763 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:06:57.669774 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:06:57.670782 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:06:57.667763 => 19:06:57.670782
[0m19:06:57.671787 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:06:57.671787 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:06:57.671787 => 19:06:57.671787
[0m19:06:57.672794 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:06:57.673303 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:06:57.673812 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m19:06:57.673812 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:06:57.682399 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:06:57.683410 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 19:06:57.674817 => 19:06:57.682903
[0m19:06:57.683916 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:06:57.683916 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 19:06:57.683916 => 19:06:57.683916
[0m19:06:57.684923 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:06:57.684923 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:06:57.684923 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m19:06:57.685930 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:06:57.689957 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m19:06:57.691005 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 19:06:57.685930 => 19:06:57.691005
[0m19:06:57.692012 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:06:57.692012 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 19:06:57.692012 => 19:06:57.692012
[0m19:06:57.693020 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:06:57.693020 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:06:57.694026 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m19:06:57.694026 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:06:57.697046 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m19:06:57.698052 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 19:06:57.694026 => 19:06:57.698052
[0m19:06:57.698052 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:06:57.699059 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 19:06:57.699059 => 19:06:57.699059
[0m19:06:57.699059 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:06:57.700069 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:06:57.700069 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m19:06:57.701077 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:06:57.704098 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:06:57.705103 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 19:06:57.701077 => 19:06:57.705103
[0m19:06:57.706156 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:06:57.706156 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 19:06:57.706156 => 19:06:57.706156
[0m19:06:57.707164 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:06:57.709181 [debug] [MainThread]: Command end result
[0m19:06:57.906662 [debug] [MainThread]: Re-using an available connection from the pool (formerly master, now generate_catalog)
[0m19:06:57.907670 [info ] [MainThread]: Building catalog
[0m19:06:57.908675 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'lakesales'
[0m19:06:57.909682 [debug] [ThreadPool]: fabricsparknb adapter: database name is 
[0m19:06:57.909682 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.customers
[0m19:06:57.915725 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:06:57.915725 [debug] [ThreadPool]: Using fabricsparknb connection "lakesales"
[0m19:06:57.916733 [debug] [ThreadPool]: On lakesales: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/*{"project_root": "testproj"}*/
describe extended lakesales.customers
  
[0m19:06:57.916733 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:06:57.935881 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales"} */
/*{"project_root": "testproj"}*/
describe extended lakesales.customers
  
[0m19:06:57.935881 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: cannot access local variable 'node_id' where it is not associated with a value
[0m19:06:57.936888 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro describe_table_extended_without_caching
[0m19:06:57.936888 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m19:06:57.937894 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about lakesales.customers: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m19:06:57.938902 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly lakesales, now lakesales2)
[0m19:06:57.938902 [warn ] [MainThread]: Encountered an error while generating catalog: Runtime Error
  Runtime Error
    cannot access local variable 'node_id' where it is not associated with a value
[0m19:06:57.939910 [debug] [ThreadPool]: fabricsparknb adapter: database name is 
[0m19:06:57.940916 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales2.sales_invoicelines2
[0m19:06:57.942930 [debug] [ThreadPool]: Using fabricsparknb connection "lakesales2"
[0m19:06:57.942930 [debug] [ThreadPool]: On lakesales2: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/*{"project_root": "testproj"}*/
describe extended lakesales2.sales_invoicelines2
  
[0m19:06:57.943935 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
/*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "connection_name": "lakesales2"} */
/*{"project_root": "testproj"}*/
describe extended lakesales2.sales_invoicelines2
  
[0m19:06:57.944941 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: cannot access local variable 'node_id' where it is not associated with a value
[0m19:06:57.944941 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Error while running:
macro describe_table_extended_without_caching
[0m19:06:57.945946 [debug] [ThreadPool]: Microsoft Fabric-Spark adapter: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m19:06:57.945946 [debug] [ThreadPool]: fabricsparknb adapter: Error while retrieving information about lakesales2.sales_invoicelines2: Runtime Error
  cannot access local variable 'node_id' where it is not associated with a value
[0m19:06:57.946952 [warn ] [MainThread]: Encountered an error while generating catalog: Runtime Error
  Runtime Error
    cannot access local variable 'node_id' where it is not associated with a value
[0m19:06:57.954003 [error] [MainThread]: dbt encountered 2 failures while writing the catalog
[0m19:06:57.954003 [info ] [MainThread]: Catalog written to C:\Users\jramp\source\gitjr\dbt-fabricsparknb\testproj\target\catalog.json
[0m19:06:57.955009 [debug] [MainThread]: Command `dbt docs generate` failed at 19:06:57.955009 after 1.39 seconds
[0m19:06:57.956016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020327422120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203293CAD50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203278A9070>]}
[0m19:06:57.956016 [debug] [MainThread]: Flushing usage events
[0m19:15:29.058990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001538265FC50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001538265EF00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001538265F740>]}


============================== 19:15:29.063015 | 475f31b0-076b-4df1-9fa6-7cb800dd8bad ==============================
[0m19:15:29.063015 [info ] [MainThread]: Running with dbt=1.7.14
[0m19:15:29.063015 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt --log-level debug docs generate', 'send_anonymous_usage_stats': 'True'}
[0m19:15:29.203060 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricspark.connector to DEBUG
[0m19:15:29.204064 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m19:15:29.205068 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m19:15:29.205068 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m19:15:29.272603 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricsparknb.connector to DEBUG
[0m19:15:29.273610 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m19:15:29.274616 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m19:15:29.274616 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m19:15:29.376110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '475f31b0-076b-4df1-9fa6-7cb800dd8bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015382DFA450>]}
[0m19:15:29.418630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '475f31b0-076b-4df1-9fa6-7cb800dd8bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015381505BB0>]}
[0m19:15:29.421647 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:15:29.429714 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m19:15:29.483266 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:15:29.484273 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:15:29.491339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '475f31b0-076b-4df1-9fa6-7cb800dd8bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015383F93DA0>]}
[0m19:15:29.494356 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '475f31b0-076b-4df1-9fa6-7cb800dd8bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001538407B5C0>]}
[0m19:15:29.495361 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m19:15:29.495361 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '475f31b0-076b-4df1-9fa6-7cb800dd8bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001538407FCB0>]}
[0m19:15:29.497372 [info ] [MainThread]: 
[0m19:15:29.498377 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:15:29.500401 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m19:15:29.512691 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m19:15:29.527915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '475f31b0-076b-4df1-9fa6-7cb800dd8bad', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153820CE750>]}
[0m19:15:29.528927 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:15:29.528927 [info ] [MainThread]: 
[0m19:15:29.534998 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m19:15:29.536004 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:15:29.537011 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:15:29.543045 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:15:29.544051 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:15:29.537011 => 19:15:29.544051
[0m19:15:29.547918 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:15:29.548424 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:15:29.548424 => 19:15:29.548424
[0m19:15:29.549435 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:15:29.550444 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m19:15:29.550949 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m19:15:29.551453 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m19:15:29.553975 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 19:15:29.551957 => 19:15:29.553975
[0m19:15:29.554479 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m19:15:29.554986 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 19:15:29.554986 => 19:15:29.554986
[0m19:15:29.555490 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m19:15:29.555996 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m19:15:29.556500 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now model.testproj.my_second_dbt_model)
[0m19:15:29.557005 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:15:29.561055 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:15:29.562064 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:15:29.557510 => 19:15:29.562064
[0m19:15:29.562569 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:15:29.563115 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:15:29.563115 => 19:15:29.563115
[0m19:15:29.564129 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:15:29.564636 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:15:29.565646 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m19:15:29.566156 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:15:29.574784 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:15:29.577318 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 19:15:29.566662 => 19:15:29.577318
[0m19:15:29.578400 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:15:29.578905 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 19:15:29.578905 => 19:15:29.578905
[0m19:15:29.579923 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:15:29.580429 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:15:29.580935 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m19:15:29.581443 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:15:29.585996 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m19:15:29.588027 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 19:15:29.581443 => 19:15:29.587516
[0m19:15:29.589054 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:15:29.590074 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 19:15:29.589564 => 19:15:29.589564
[0m19:15:29.591100 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:15:29.592123 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:15:29.593197 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m19:15:29.593700 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:15:29.598827 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m19:15:29.601355 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 19:15:29.594207 => 19:15:29.601355
[0m19:15:29.601863 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:15:29.602368 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 19:15:29.602368 => 19:15:29.602368
[0m19:15:29.603378 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:15:29.603883 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:15:29.604893 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m19:15:29.605398 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:15:29.608980 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:15:29.609996 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 19:15:29.605398 => 19:15:29.609996
[0m19:15:29.610503 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:15:29.611011 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 19:15:29.610503 => 19:15:29.610503
[0m19:15:29.611518 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:15:29.613545 [debug] [MainThread]: Command end result
[0m19:15:29.626293 [debug] [MainThread]: Re-using an available connection from the pool (formerly master, now generate_catalog)
[0m19:15:29.626798 [info ] [MainThread]: Building catalog
[0m19:15:29.629325 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'lakesales'
[0m19:15:29.629830 [debug] [ThreadPool]: fabricsparknb adapter: database name is 
[0m19:15:29.630336 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.customers
[0m19:15:29.635931 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.dimstore
[0m19:15:29.641041 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.dimcustomer
[0m19:15:29.645586 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.products
[0m19:15:29.649120 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.my_first_dbt_model
[0m19:15:29.654883 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.sample
[0m19:15:29.661501 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.my_second_dbt_model
[0m19:15:29.667056 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.factsales
[0m19:15:29.671120 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.dimdate
[0m19:15:29.675160 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.application_cities
[0m19:15:29.679197 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.application_countries
[0m19:15:29.682727 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.application_deliverymethods
[0m19:15:29.688996 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.application_people
[0m19:15:29.693016 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.application_stateprovinces
[0m19:15:29.697033 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.sales_buyinggroups
[0m19:15:29.701085 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.sales_customercategories
[0m19:15:29.705115 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.sales_customers
[0m19:15:29.709136 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.sales_invoicelines
[0m19:15:29.713158 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.sales_invoices
[0m19:15:29.718243 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales.sales_invoicelines2
[0m19:15:29.724276 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly lakesales, now lakesales2)
[0m19:15:29.724276 [debug] [ThreadPool]: fabricsparknb adapter: database name is 
[0m19:15:29.725281 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales2.sales_invoicelines2
[0m19:15:29.729302 [debug] [ThreadPool]: fabricsparknb adapter: Getting table schema for relation lakesales2.my_second_dbt_model
[0m19:15:29.740378 [info ] [MainThread]: Catalog written to C:\Users\jramp\source\gitjr\dbt-fabricsparknb\testproj\target\catalog.json
[0m19:15:29.742389 [debug] [MainThread]: Command `dbt docs generate` succeeded at 19:15:29.742389 after 0.76 seconds
[0m19:15:29.742389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153822B1DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015382E327B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001538245CB30>]}
[0m19:15:29.743395 [debug] [MainThread]: Flushing usage events
[0m19:16:39.326510 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1388BA9F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1388B90D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1388B95B0>]}


============================== 19:16:39.330540 | 3cffbe3a-0e98-4af0-b5a9-597b66797f51 ==============================
[0m19:16:39.330540 [info ] [MainThread]: Running with dbt=1.7.14
[0m19:16:39.330540 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt docs serve', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:16:39.469441 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricspark.connector to DEBUG
[0m19:16:39.470447 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m19:16:39.470447 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m19:16:39.470447 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m19:16:39.523845 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricsparknb.connector to DEBUG
[0m19:16:39.524850 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m19:16:39.524850 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m19:16:39.524850 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m19:16:39.619651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3cffbe3a-0e98-4af0-b5a9-597b66797f51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C13910ACF0>]}
[0m19:16:39.651943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3cffbe3a-0e98-4af0-b5a9-597b66797f51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C1390B4800>]}
[0m15:43:48.710254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4D9A1D880>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4D9A1D9A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4D9A1CF50>]}


============================== 15:43:48.713279 | 49319c65-f64a-49ce-9a7b-d888caf2e197 ==============================
[0m15:43:48.713279 [info ] [MainThread]: Running with dbt=1.7.14
[0m15:43:48.714288 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'C:\\Users\\jramp\\source\\gitjr\\dbt-fabricsparknb\\testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt build', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m15:43:48.999198 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricspark.connector to DEBUG
[0m15:43:48.999722 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m15:43:49.000234 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m15:43:49.000234 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m15:43:49.097782 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting fabricsparknb.connector to DEBUG
[0m15:43:49.098801 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting botocore to DEBUG
[0m15:43:49.098801 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting boto3 to DEBUG
[0m15:43:49.098801 [debug] [MainThread]: Microsoft Fabric-Spark adapter: Setting Microsoft Fabric-Spark.connector to DEBUG
[0m15:43:49.188000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '49319c65-f64a-49ce-9a7b-d888caf2e197', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4D9A46900>]}
[0m15:43:49.223464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '49319c65-f64a-49ce-9a7b-d888caf2e197', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4D9DE1400>]}
[0m15:43:49.225490 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:43:49.242260 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m15:43:49.685856 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:43:49.685856 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:43:49.689918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '49319c65-f64a-49ce-9a7b-d888caf2e197', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4DA3B7980>]}
[0m15:43:49.704241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '49319c65-f64a-49ce-9a7b-d888caf2e197', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4DB458A40>]}
[0m15:43:49.704241 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m15:43:49.705255 [info ] [MainThread]: 
[0m15:43:49.706268 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:43:49.707287 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:43:49.712351 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m15:43:49.719451 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m15:43:49.730088 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '49319c65-f64a-49ce-9a7b-d888caf2e197', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4DB4A00E0>]}
[0m15:43:49.731101 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:43:49.731101 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:43:49.731101 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:43:49.732111 [info ] [MainThread]: 
[0m15:43:49.734152 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:43:49.735168 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m15:43:49.735168 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:43:49.735168 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:43:49.740225 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:43:49.740225 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:43:49.736182 => 15:43:49.740225
[0m15:43:49.741234 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:43:49.770604 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:43:49.770604 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:43:49.771613 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:43:49.771613 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:43:49.771613 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:43:49.771613 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'project_root'
[0m15:43:49.772622 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:43:49.741234 => 15:43:49.772622
[0m15:43:49.840068 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'project_root'
[0m15:43:49.841080 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '49319c65-f64a-49ce-9a7b-d888caf2e197', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4DB55FDD0>]}
[0m15:43:49.841080 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model lakesales.my_first_dbt_model ....... [[31mERROR[0m in 0.11s]
[0m15:43:49.842089 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:43:49.842089 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m15:43:49.843099 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m15:43:49.844114 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m15:43:49.844114 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m15:43:49.844114 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 15:43:49.844114 => 15:43:49.844114
[0m15:43:49.845127 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m15:43:49.859327 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m15:43:49.859327 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\testproj"}*/
drop table if exists lakesales.sample
[0m15:43:49.860345 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\testproj"}*/
drop table if exists lakesales.sample
[0m15:43:49.860345 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'project_root'
[0m15:43:49.861366 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
macro drop_relation
[0m15:43:49.861366 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Runtime Error
  'project_root'
[0m15:43:49.862404 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 15:43:49.845127 => 15:43:49.861366
[0m15:43:49.880638 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'project_root'
[0m15:43:49.881654 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '49319c65-f64a-49ce-9a7b-d888caf2e197', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4DA2E0AA0>]}
[0m15:43:49.881654 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file lakesales.sample ................................ [[31mERROR[0m in 0.04s]
[0m15:43:49.882668 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m15:43:49.883689 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:43:49.883689 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m15:43:49.884704 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:43:49.884704 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m15:43:49.884704 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m15:43:49.885734 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m15:43:49.885734 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:43:49.886751 [info ] [Thread-1 (]: 5 of 7 SKIP relation lakesales2.my_second_dbt_model ............................ [[33mSKIP[0m]
[0m15:43:49.886751 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:43:49.887763 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m15:43:49.887763 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m15:43:49.888773 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m15:43:49.888773 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m15:43:49.888773 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m15:43:49.889784 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m15:43:49.890796 [debug] [MainThread]: On master: ROLLBACK
[0m15:43:49.891815 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:43:49.891815 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:43:49.891815 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:43:49.892827 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:43:49.892827 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:43:49.893841 [info ] [MainThread]: 
[0m15:43:49.894866 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.19 seconds (0.19s).
[0m15:43:49.894924 [debug] [MainThread]: Command end result
[0m15:43:49.902052 [info ] [MainThread]: 
[0m15:43:49.902052 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m15:43:49.903066 [info ] [MainThread]: 
[0m15:43:49.903066 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'project_root'
[0m15:43:49.904085 [info ] [MainThread]: 
[0m15:43:49.904085 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'project_root'
[0m15:43:49.904085 [info ] [MainThread]: 
[0m15:43:49.905102 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m15:43:49.906120 [debug] [MainThread]: Command `dbt build` failed at 15:43:49.906120 after 1.28 seconds
[0m15:43:49.906120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4D958D520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4D6AC2690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B4D9A1E6F0>]}
[0m15:43:49.906120 [debug] [MainThread]: Flushing usage events
[0m15:44:12.630193 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0D6E7140>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0D6E71A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0D6E5FD0>]}


============================== 15:44:12.630193 | cef34758-79d6-4488-8b5e-fcee99bc7d70 ==============================
[0m15:44:12.630193 [info ] [MainThread]: Running with dbt=1.7.14
[0m15:44:12.631207 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m15:44:12.714685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cef34758-79d6-4488-8b5e-fcee99bc7d70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0C62F1A0>]}
[0m15:44:12.750141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cef34758-79d6-4488-8b5e-fcee99bc7d70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0D7515B0>]}
[0m15:44:12.751158 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m15:44:12.760271 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m15:44:12.762323 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m15:44:12.762323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cef34758-79d6-4488-8b5e-fcee99bc7d70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0D7515B0>]}
[0m15:44:13.494448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cef34758-79d6-4488-8b5e-fcee99bc7d70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0DA97BC0>]}
[0m15:44:13.503597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cef34758-79d6-4488-8b5e-fcee99bc7d70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0DB6D6D0>]}
[0m15:44:13.503597 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m15:44:13.505615 [info ] [MainThread]: 
[0m15:44:13.506629 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m15:44:13.507641 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m15:44:13.512681 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m15:44:13.519751 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m15:44:13.531957 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cef34758-79d6-4488-8b5e-fcee99bc7d70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0D8D68D0>]}
[0m15:44:13.531957 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:44:13.532966 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:44:13.532966 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m15:44:13.533973 [info ] [MainThread]: 
[0m15:44:13.535988 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m15:44:13.536996 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m15:44:13.536996 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m15:44:13.538004 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m15:44:13.548130 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m15:44:13.550149 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 15:44:13.544083 => 15:44:13.549139
[0m15:44:13.550149 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m15:44:13.581563 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:44:13.582576 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:44:13.582576 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m15:44:13.582576 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:44:13.652845 [debug] [Thread-1 (]: SQL status: OK in 0.07000000029802322 seconds
[0m15:44:13.664990 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m15:44:13.667005 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m15:44:13.667005 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m15:44:13.681141 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:44:13.689226 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 15:44:13.550149 => 15:44:13.689226
[0m15:44:13.690237 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cef34758-79d6-4488-8b5e-fcee99bc7d70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0DA978F0>]}
[0m15:44:13.691253 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model lakesales.my_first_dbt_model ........... [[32mOK[0m in 0.15s]
[0m15:44:13.691253 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m15:44:13.692264 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m15:44:13.692264 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m15:44:13.693274 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m15:44:13.694289 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m15:44:13.694289 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 15:44:13.694289 => 15:44:13.694289
[0m15:44:13.694289 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m15:44:13.708439 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m15:44:13.709447 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m15:44:13.713480 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:44:13.732825 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m15:44:13.732825 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table lakesales.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m15:44:13.748029 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m15:44:13.757135 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m15:44:13.760205 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m15:44:13.760205 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into lakesales.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m15:44:13.774348 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:44:13.777385 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m15:44:13.780416 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:44:13.780416 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 15:44:13.695302 => 15:44:13.780416
[0m15:44:13.781425 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cef34758-79d6-4488-8b5e-fcee99bc7d70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0DA92930>]}
[0m15:44:13.781425 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file lakesales.sample .................................... [[32mINSERT 2[0m in 0.09s]
[0m15:44:13.782434 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m15:44:13.783443 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:44:13.783443 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m15:44:13.784460 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m15:44:13.784460 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:44:13.792582 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:44:13.793604 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 15:44:13.784460 => 15:44:13.793604
[0m15:44:13.794614 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:44:13.802701 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:44:13.803711 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m15:44:13.804775 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m15:44:13.809826 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:44:13.811856 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 15:44:13.794614 => 15:44:13.810838
[0m15:44:13.812870 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m15:44:13.812870 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m15:44:13.813887 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m15:44:13.813887 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m15:44:13.814897 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m15:44:13.814897 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m15:44:13.821005 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m15:44:13.822016 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 15:44:13.815908 => 15:44:13.822016
[0m15:44:13.822526 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m15:44:13.825071 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m15:44:13.826084 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m15:44:13.826593 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:44:13.832191 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:44:13.833199 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 15:44:13.823035 => 15:44:13.833199
[0m15:44:13.833199 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m15:44:13.834206 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m15:44:13.835216 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m15:44:13.835216 [info ] [Thread-1 (]: 5 of 7 START sql incremental model lakesales2.my_second_dbt_model .............. [RUN]
[0m15:44:13.836256 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m15:44:13.836256 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m15:44:13.838275 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m15:44:13.839286 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 15:44:13.836256 => 15:44:13.839286
[0m15:44:13.839286 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m15:44:13.841797 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:44:13.841797 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_second_dbt_model__dbt_tmp as
      -- Use the `ref` function to select from other models

select *
from lakesales.my_first_dbt_model
where id = 1
  
[0m15:44:13.846848 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m15:44:13.850918 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m15:44:13.851926 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m15:44:13.852934 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales2.my_second_dbt_model
    select `id` from my_second_dbt_model__dbt_tmp


[0m15:44:13.868106 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:44:13.869114 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 15:44:13.840291 => 15:44:13.869114
[0m15:44:13.870121 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cef34758-79d6-4488-8b5e-fcee99bc7d70', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0E067590>]}
[0m15:44:13.870121 [info ] [Thread-1 (]: 5 of 7 OK created sql incremental model lakesales2.my_second_dbt_model ......... [[32mOK[0m in 0.03s]
[0m15:44:13.870121 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m15:44:13.871130 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m15:44:13.871130 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m15:44:13.872138 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m15:44:13.872138 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m15:44:13.875171 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m15:44:13.876179 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 15:44:13.873153 => 15:44:13.876179
[0m15:44:13.877188 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m15:44:13.878196 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m15:44:13.879208 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m15:44:13.879208 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales2.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m15:44:13.884263 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:44:13.885270 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 15:44:13.877188 => 15:44:13.884263
[0m15:44:13.885270 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m15:44:13.886279 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m15:44:13.886279 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m15:44:13.886279 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m15:44:13.887286 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m15:44:13.887286 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m15:44:13.890317 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m15:44:13.891339 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 15:44:13.887286 => 15:44:13.891339
[0m15:44:13.892356 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m15:44:13.894379 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m15:44:13.895393 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m15:44:13.895393 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales2.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m15:44:13.900451 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m15:44:13.901461 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 15:44:13.892356 => 15:44:13.900451
[0m15:44:13.901461 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.02s]
[0m15:44:13.902469 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m15:44:13.903480 [debug] [MainThread]: On master: ROLLBACK
[0m15:44:13.903480 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:44:13.903480 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m15:44:13.903480 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m15:44:13.904494 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m15:44:13.904494 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m15:44:13.904494 [info ] [MainThread]: 
[0m15:44:13.905502 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.40 seconds (0.40s).
[0m15:44:13.906513 [debug] [MainThread]: Command end result
[0m15:44:13.911585 [info ] [MainThread]: 
[0m15:44:13.912595 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:44:13.912595 [info ] [MainThread]: 
[0m15:44:13.913603 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m15:44:13.913603 [debug] [MainThread]: Command `cli build` succeeded at 15:44:13.913603 after 1.31 seconds
[0m15:44:13.914614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0C05FE60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0D92D1C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018C0D92F290>]}
[0m15:44:13.914614 [debug] [MainThread]: Flushing usage events
[0m16:04:27.479617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002843AC20E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002843AC21790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002843AC21B20>]}


============================== 16:04:27.480626 | 205ff847-1527-4ff9-8aa2-76641974e5eb ==============================
[0m16:04:27.480626 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:04:27.480626 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:04:27.579423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '205ff847-1527-4ff9-8aa2-76641974e5eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002843ACF00E0>]}
[0m16:04:27.614421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '205ff847-1527-4ff9-8aa2-76641974e5eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002843A99ADE0>]}
[0m16:04:27.616441 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:04:27.636754 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:04:27.638772 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:04:27.639274 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '205ff847-1527-4ff9-8aa2-76641974e5eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002843ACC9A90>]}
[0m16:04:28.996753 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '205ff847-1527-4ff9-8aa2-76641974e5eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002843C0BFD70>]}
[0m16:04:29.016040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '205ff847-1527-4ff9-8aa2-76641974e5eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002843BFA1700>]}
[0m16:04:29.017066 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:04:29.018079 [info ] [MainThread]: 
[0m16:04:29.019586 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:04:29.020596 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:04:29.037341 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales2'
[0m16:04:29.052583 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales2, now list_None_lakesales)
[0m16:04:29.066254 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '205ff847-1527-4ff9-8aa2-76641974e5eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002843BEE85C0>]}
[0m16:04:29.066254 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:04:29.067262 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:04:29.067262 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:04:29.068272 [info ] [MainThread]: 
[0m16:04:29.071316 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:04:29.071316 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:04:29.072331 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:04:29.072331 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:04:29.077410 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:04:29.079432 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:04:29.073339 => 16:04:29.078926
[0m16:04:29.079432 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:04:29.110372 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:04:29.110372 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:04:29.111382 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:04:29.111382 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:04:29.141252 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:04:29.141252 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:04:29.142260 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:04:29.079432 => 16:04:29.142260
[0m16:04:29.206579 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:04:29.207593 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '205ff847-1527-4ff9-8aa2-76641974e5eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002843C0165A0>]}
[0m16:04:29.207593 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model lakesales.my_first_dbt_model ....... [[31mERROR[0m in 0.14s]
[0m16:04:29.209111 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:04:29.209622 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:04:29.209622 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:04:29.210631 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:04:29.211669 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:04:29.211669 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:04:29.211669 => 16:04:29.211669
[0m16:04:29.211669 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:04:29.227880 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:04:29.228884 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:04:29.229386 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:04:29.230395 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:04:29.230395 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
macro drop_relation
[0m16:04:29.230395 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Runtime Error
  'NoneType' object has no attribute 'cells'
[0m16:04:29.231404 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:04:29.212680 => 16:04:29.231404
[0m16:04:29.252699 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'NoneType' object has no attribute 'cells'
[0m16:04:29.252699 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '205ff847-1527-4ff9-8aa2-76641974e5eb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002843AD1EC00>]}
[0m16:04:29.253707 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file lakesales.sample ................................ [[31mERROR[0m in 0.04s]
[0m16:04:29.253707 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:04:29.254717 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:04:29.254717 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m16:04:29.255729 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:04:29.256753 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:04:29.257775 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m16:04:29.257775 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:04:29.258778 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:04:29.259281 [info ] [Thread-1 (]: 5 of 7 SKIP relation lakesales2.my_second_dbt_model ............................ [[33mSKIP[0m]
[0m16:04:29.259281 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:04:29.260296 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:04:29.260296 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m16:04:29.261311 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:04:29.261311 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:04:29.261311 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m16:04:29.261311 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:04:29.262318 [debug] [MainThread]: On master: ROLLBACK
[0m16:04:29.263328 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:04:29.263328 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:04:29.263328 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:04:29.264337 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:04:29.264337 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:04:29.265351 [info ] [MainThread]: 
[0m16:04:29.265351 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.25 seconds (0.25s).
[0m16:04:29.266360 [debug] [MainThread]: Command end result
[0m16:04:29.271424 [info ] [MainThread]: 
[0m16:04:29.272463 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:04:29.272463 [info ] [MainThread]: 
[0m16:04:29.272463 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:04:29.273473 [info ] [MainThread]: 
[0m16:04:29.273473 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'NoneType' object has no attribute 'cells'
[0m16:04:29.273473 [info ] [MainThread]: 
[0m16:04:29.274489 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m16:04:29.275499 [debug] [MainThread]: Command `cli build` failed at 16:04:29.274489 after 1.82 seconds
[0m16:04:29.275499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002843C0A5C40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002843BFB2180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002843BFA9D90>]}
[0m16:04:29.275499 [debug] [MainThread]: Flushing usage events
[0m16:05:13.321897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBD784D100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBD784DB20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBD784DA90>]}


============================== 16:05:13.322401 | 971a1a88-032f-4a38-9d71-8a5634f35fe8 ==============================
[0m16:05:13.322401 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:05:13.323458 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:05:13.416224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '971a1a88-032f-4a38-9d71-8a5634f35fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBD781B830>]}
[0m16:05:13.450333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '971a1a88-032f-4a38-9d71-8a5634f35fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBD7217A10>]}
[0m16:05:13.451343 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:05:13.459525 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:05:13.460540 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:05:13.461549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '971a1a88-032f-4a38-9d71-8a5634f35fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBD725F980>]}
[0m16:05:14.175669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '971a1a88-032f-4a38-9d71-8a5634f35fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBD8A8F740>]}
[0m16:05:14.195055 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '971a1a88-032f-4a38-9d71-8a5634f35fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBD8A93A70>]}
[0m16:05:14.195055 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:05:14.197081 [info ] [MainThread]: 
[0m16:05:14.198096 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:05:14.199100 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:05:14.204659 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales2'
[0m16:05:14.212261 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales2, now list_None_lakesales)
[0m16:05:14.224450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '971a1a88-032f-4a38-9d71-8a5634f35fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBD8BD7920>]}
[0m16:05:14.224450 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:05:14.225460 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:05:14.226473 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:05:14.226473 [info ] [MainThread]: 
[0m16:05:14.229525 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:05:14.230533 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:05:14.231545 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:05:14.231545 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:05:14.237636 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:05:14.239665 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:05:14.232559 => 16:05:14.239665
[0m16:05:14.239665 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:05:14.271699 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:05:14.272712 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:05:14.272712 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:05:14.273721 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:05:14.297552 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:05:14.298567 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:05:14.299082 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:05:14.240677 => 16:05:14.299082
[0m16:05:14.309203 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:05:14.309710 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '971a1a88-032f-4a38-9d71-8a5634f35fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBD8A310D0>]}
[0m16:05:14.309710 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model lakesales.my_first_dbt_model ....... [[31mERROR[0m in 0.08s]
[0m16:05:14.310719 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:05:14.310719 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:05:14.311728 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:05:14.312777 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:05:14.312777 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:05:14.312777 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:05:14.312777 => 16:05:14.312777
[0m16:05:14.313792 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:05:14.329017 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:05:14.329527 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:05:14.330534 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:05:14.330534 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:05:14.331548 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
macro drop_relation
[0m16:05:14.331548 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Runtime Error
  'NoneType' object has no attribute 'cells'
[0m16:05:14.331548 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:05:14.313792 => 16:05:14.331548
[0m16:05:14.336612 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'NoneType' object has no attribute 'cells'
[0m16:05:14.336612 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '971a1a88-032f-4a38-9d71-8a5634f35fe8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBD8E243B0>]}
[0m16:05:14.337622 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file lakesales.sample ................................ [[31mERROR[0m in 0.02s]
[0m16:05:14.337622 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:05:14.338632 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:05:14.339152 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m16:05:14.339660 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:05:14.339660 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:05:14.339660 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m16:05:14.340671 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:05:14.340671 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:05:14.341688 [info ] [Thread-1 (]: 5 of 7 SKIP relation lakesales2.my_second_dbt_model ............................ [[33mSKIP[0m]
[0m16:05:14.341688 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:05:14.342729 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:05:14.342729 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m16:05:14.342729 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:05:14.343742 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:05:14.343742 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m16:05:14.343742 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:05:14.344761 [debug] [MainThread]: On master: ROLLBACK
[0m16:05:14.344761 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:05:14.345771 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:05:14.345771 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:05:14.345771 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:05:14.345771 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:05:14.346782 [info ] [MainThread]: 
[0m16:05:14.347299 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.15 seconds (0.15s).
[0m16:05:14.347808 [debug] [MainThread]: Command end result
[0m16:05:14.354376 [info ] [MainThread]: 
[0m16:05:14.355387 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:05:14.355387 [info ] [MainThread]: 
[0m16:05:14.355387 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:05:14.356397 [info ] [MainThread]: 
[0m16:05:14.356397 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'NoneType' object has no attribute 'cells'
[0m16:05:14.356397 [info ] [MainThread]: 
[0m16:05:14.357466 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m16:05:14.358990 [debug] [MainThread]: Command `cli build` failed at 16:05:14.358469 after 1.07 seconds
[0m16:05:14.358990 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBD723B680>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBD8CE4950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EBD784D970>]}
[0m16:05:14.359499 [debug] [MainThread]: Flushing usage events
[0m16:05:40.786824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C3708D850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C3708EB40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C3708DCA0>]}


============================== 16:05:40.787835 | b9701340-9f66-4465-9d25-0ecc417e9768 ==============================
[0m16:05:40.787835 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:05:40.788839 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:05:40.874453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b9701340-9f66-4465-9d25-0ecc417e9768', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C36A692E0>]}
[0m16:05:40.909456 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b9701340-9f66-4465-9d25-0ecc417e9768', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C3713BB00>]}
[0m16:05:40.912483 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:05:40.919573 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:05:40.920589 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:05:40.920589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b9701340-9f66-4465-9d25-0ecc417e9768', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C36D9F980>]}
[0m16:05:41.676504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b9701340-9f66-4465-9d25-0ecc417e9768', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C38414830>]}
[0m16:05:41.684656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b9701340-9f66-4465-9d25-0ecc417e9768', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C384D3A70>]}
[0m16:05:41.684656 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:05:41.685670 [info ] [MainThread]: 
[0m16:05:41.686682 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:05:41.687693 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:05:41.696889 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m16:05:41.704448 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m16:05:41.715598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b9701340-9f66-4465-9d25-0ecc417e9768', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C3629B140>]}
[0m16:05:41.715598 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:05:41.716608 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:05:41.716608 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:05:41.717623 [info ] [MainThread]: 
[0m16:05:41.719659 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:05:41.720669 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:05:41.721684 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:05:41.721684 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:05:41.727936 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:05:41.729441 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:05:41.721684 => 16:05:41.728939
[0m16:05:41.729441 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:05:41.759830 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:05:41.760863 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:05:41.760863 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:05:41.761880 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:05:41.782598 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:05:41.783634 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:05:41.783634 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:05:41.729441 => 16:05:41.783634
[0m16:05:41.791834 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:05:41.791834 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9701340-9f66-4465-9d25-0ecc417e9768', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C3721CB00>]}
[0m16:05:41.792846 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model lakesales.my_first_dbt_model ....... [[31mERROR[0m in 0.07s]
[0m16:05:41.793857 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:05:41.794878 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:05:41.794878 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:05:41.795889 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:05:41.795889 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:05:41.796904 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:05:41.795889 => 16:05:41.795889
[0m16:05:41.796904 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:05:41.810559 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:05:41.810559 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:05:41.811568 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:05:41.811568 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:05:41.812577 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
macro drop_relation
[0m16:05:41.812577 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Runtime Error
  'NoneType' object has no attribute 'cells'
[0m16:05:41.812577 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:05:41.796904 => 16:05:41.812577
[0m16:05:41.816628 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'NoneType' object has no attribute 'cells'
[0m16:05:41.817647 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b9701340-9f66-4465-9d25-0ecc417e9768', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C3708DB80>]}
[0m16:05:41.817647 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file lakesales.sample ................................ [[31mERROR[0m in 0.02s]
[0m16:05:41.818660 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:05:41.818660 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:05:41.819171 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m16:05:41.819679 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:05:41.819679 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:05:41.820699 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m16:05:41.820699 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:05:41.821716 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:05:41.821716 [info ] [Thread-1 (]: 5 of 7 SKIP relation lakesales2.my_second_dbt_model ............................ [[33mSKIP[0m]
[0m16:05:41.822786 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:05:41.823789 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:05:41.823789 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m16:05:41.823789 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:05:41.824831 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:05:41.824831 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m16:05:41.824831 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:05:41.825843 [debug] [MainThread]: On master: ROLLBACK
[0m16:05:41.826852 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:05:41.826852 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:05:41.826852 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:05:41.826852 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:05:41.827863 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:05:41.828868 [info ] [MainThread]: 
[0m16:05:41.828868 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m16:05:41.829371 [debug] [MainThread]: Command end result
[0m16:05:41.835454 [info ] [MainThread]: 
[0m16:05:41.835454 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:05:41.835454 [info ] [MainThread]: 
[0m16:05:41.836467 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:05:41.836467 [info ] [MainThread]: 
[0m16:05:41.836467 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'NoneType' object has no attribute 'cells'
[0m16:05:41.837474 [info ] [MainThread]: 
[0m16:05:41.837474 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m16:05:41.838485 [debug] [MainThread]: Command `cli build` failed at 16:05:41.838485 after 1.07 seconds
[0m16:05:41.838993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C36F55220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C36E4A150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000024C36E49AC0>]}
[0m16:05:41.839501 [debug] [MainThread]: Flushing usage events
[0m16:07:42.199741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D22861040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D228607D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D22861640>]}


============================== 16:07:42.199741 | 178c6d18-c9b0-4e9d-a70e-14a6a49a4cd6 ==============================
[0m16:07:42.199741 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:07:42.200752 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'version_check': 'True', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:07:42.296588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '178c6d18-c9b0-4e9d-a70e-14a6a49a4cd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D228600E0>]}
[0m16:07:42.332595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '178c6d18-c9b0-4e9d-a70e-14a6a49a4cd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D21FC32C0>]}
[0m16:07:42.333605 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:07:42.340736 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:07:42.341750 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:07:42.341750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '178c6d18-c9b0-4e9d-a70e-14a6a49a4cd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D224D4680>]}
[0m16:07:43.073574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '178c6d18-c9b0-4e9d-a70e-14a6a49a4cd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D23C52810>]}
[0m16:07:43.080689 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '178c6d18-c9b0-4e9d-a70e-14a6a49a4cd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D23BE0FE0>]}
[0m16:07:43.081711 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:07:43.082724 [info ] [MainThread]: 
[0m16:07:43.083732 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:07:43.084742 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:07:43.090379 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m16:07:43.099546 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m16:07:43.109695 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '178c6d18-c9b0-4e9d-a70e-14a6a49a4cd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D23BE2480>]}
[0m16:07:43.109695 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:07:43.109695 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:07:43.110705 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:07:43.110705 [info ] [MainThread]: 
[0m16:07:43.113221 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:07:43.113221 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:07:43.114233 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:07:43.114233 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:07:43.118797 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:07:43.120314 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:07:43.114233 => 16:07:43.119304
[0m16:07:43.120314 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:07:43.149730 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:07:43.149730 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:07:43.150741 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:07:43.150741 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:07:43.173584 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:07:43.173584 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:07:43.173584 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:07:43.120314 => 16:07:43.173584
[0m16:07:43.179646 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:07:43.179646 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '178c6d18-c9b0-4e9d-a70e-14a6a49a4cd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D239FEBA0>]}
[0m16:07:43.180656 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model lakesales.my_first_dbt_model ....... [[31mERROR[0m in 0.07s]
[0m16:07:43.180656 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:07:43.181675 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:07:43.181675 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:07:43.182696 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:07:43.182696 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:07:43.183734 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:07:43.182696 => 16:07:43.182696
[0m16:07:43.183734 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:07:43.198916 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:07:43.199420 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:07:43.200435 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:07:43.200435 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:07:43.200435 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
macro drop_relation
[0m16:07:43.201450 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Runtime Error
  'NoneType' object has no attribute 'cells'
[0m16:07:43.201450 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:07:43.183734 => 16:07:43.201450
[0m16:07:43.205504 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'NoneType' object has no attribute 'cells'
[0m16:07:43.205504 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '178c6d18-c9b0-4e9d-a70e-14a6a49a4cd6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D23E31F40>]}
[0m16:07:43.205504 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file lakesales.sample ................................ [[31mERROR[0m in 0.02s]
[0m16:07:43.206518 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:07:43.206518 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:07:43.207529 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m16:07:43.207529 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:07:43.207529 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:07:43.208540 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m16:07:43.208540 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:07:43.209047 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:07:43.209556 [info ] [Thread-1 (]: 5 of 7 SKIP relation lakesales2.my_second_dbt_model ............................ [[33mSKIP[0m]
[0m16:07:43.209556 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:07:43.210566 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:07:43.210566 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m16:07:43.210566 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:07:43.211581 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:07:43.211581 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m16:07:43.211581 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:07:43.213086 [debug] [MainThread]: On master: ROLLBACK
[0m16:07:43.213086 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:07:43.213086 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:07:43.213086 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:07:43.214113 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:07:43.214113 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:07:43.214113 [info ] [MainThread]: 
[0m16:07:43.215132 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.13 seconds (0.13s).
[0m16:07:43.215132 [debug] [MainThread]: Command end result
[0m16:07:43.222740 [info ] [MainThread]: 
[0m16:07:43.223755 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:07:43.223755 [info ] [MainThread]: 
[0m16:07:43.223755 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:07:43.224776 [info ] [MainThread]: 
[0m16:07:43.224776 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'NoneType' object has no attribute 'cells'
[0m16:07:43.224776 [info ] [MainThread]: 
[0m16:07:43.225784 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m16:07:43.226796 [debug] [MainThread]: Command `cli build` failed at 16:07:43.226796 after 1.05 seconds
[0m16:07:43.226796 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D21B86D80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D21C19820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021D23933E30>]}
[0m16:07:43.226796 [debug] [MainThread]: Flushing usage events
[0m16:09:07.947881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B6C94D08C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B6C94D0950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B6C94D2FC0>]}


============================== 16:09:07.947881 | 33fa5f9f-f7ed-483a-9940-643fb3d1f5a6 ==============================
[0m16:09:07.947881 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:09:07.949387 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:09:08.043739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '33fa5f9f-f7ed-483a-9940-643fb3d1f5a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B6C93A9DC0>]}
[0m16:09:08.078679 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '33fa5f9f-f7ed-483a-9940-643fb3d1f5a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B6C9407DA0>]}
[0m16:09:08.079697 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:09:08.087824 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:09:08.089331 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:09:08.090347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '33fa5f9f-f7ed-483a-9940-643fb3d1f5a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B6CA59E240>]}
[0m16:09:08.857939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '33fa5f9f-f7ed-483a-9940-643fb3d1f5a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B6CA727740>]}
[0m16:09:08.869093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '33fa5f9f-f7ed-483a-9940-643fb3d1f5a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B6CA6AEE70>]}
[0m16:09:08.869602 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:09:08.871625 [info ] [MainThread]: 
[0m16:09:08.872651 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:09:08.873661 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:09:08.879231 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales2'
[0m16:09:08.886026 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales2, now list_None_lakesales)
[0m16:09:08.897706 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '33fa5f9f-f7ed-483a-9940-643fb3d1f5a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B6CA833530>]}
[0m16:09:08.897706 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:09:08.898716 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:09:08.899742 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:09:08.899742 [info ] [MainThread]: 
[0m16:09:08.902784 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:09:08.903795 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:09:08.903795 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:09:08.904804 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:09:08.908832 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:09:08.910343 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:09:08.904804 => 16:09:08.910343
[0m16:09:08.911351 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:09:08.943529 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:09:08.944542 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:09:08.944542 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:09:08.944542 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:09:08.967905 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:09:08.968910 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:09:08.969412 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:09:08.911351 => 16:09:08.969412
[0m16:09:08.979053 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:09:08.979564 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '33fa5f9f-f7ed-483a-9940-643fb3d1f5a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B6CA778890>]}
[0m16:09:08.980575 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model lakesales.my_first_dbt_model ....... [[31mERROR[0m in 0.08s]
[0m16:09:08.980575 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:09:08.981583 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:09:08.981583 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:09:08.982595 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:09:08.983606 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:09:08.983606 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:09:08.983606 => 16:09:08.983606
[0m16:09:08.983606 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:09:08.999335 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:09:08.999843 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:09:09.000855 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:09:09.000855 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:09:09.001870 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
macro drop_relation
[0m16:09:09.001870 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Runtime Error
  'NoneType' object has no attribute 'cells'
[0m16:09:09.001870 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:09:08.984621 => 16:09:09.001870
[0m16:09:09.005946 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'NoneType' object has no attribute 'cells'
[0m16:09:09.006953 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '33fa5f9f-f7ed-483a-9940-643fb3d1f5a6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B6CABB87D0>]}
[0m16:09:09.006953 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file lakesales.sample ................................ [[31mERROR[0m in 0.02s]
[0m16:09:09.007964 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:09:09.007964 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:09:09.008968 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m16:09:09.008968 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:09:09.009471 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:09:09.009471 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m16:09:09.009471 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:09:09.010483 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:09:09.010483 [info ] [Thread-1 (]: 5 of 7 SKIP relation lakesales2.my_second_dbt_model ............................ [[33mSKIP[0m]
[0m16:09:09.011501 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:09:09.011501 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:09:09.012514 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m16:09:09.012514 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:09:09.012514 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:09:09.013529 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m16:09:09.013529 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:09:09.014543 [debug] [MainThread]: On master: ROLLBACK
[0m16:09:09.014543 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:09:09.014543 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:09:09.015555 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:09:09.015555 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:09:09.015555 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:09:09.016564 [info ] [MainThread]: 
[0m16:09:09.016564 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m16:09:09.017580 [debug] [MainThread]: Command end result
[0m16:09:09.024726 [info ] [MainThread]: 
[0m16:09:09.025748 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:09:09.025748 [info ] [MainThread]: 
[0m16:09:09.026759 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:09:09.026759 [info ] [MainThread]: 
[0m16:09:09.026759 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'NoneType' object has no attribute 'cells'
[0m16:09:09.026759 [info ] [MainThread]: 
[0m16:09:09.027767 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m16:09:09.027767 [debug] [MainThread]: Command `cli build` failed at 16:09:09.027767 after 1.11 seconds
[0m16:09:09.028775 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B6C94D35C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B6C9368B60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B6C936A150>]}
[0m16:09:09.028775 [debug] [MainThread]: Flushing usage events
[0m16:19:08.459043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F266DA58B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F266DA6B10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F266DA56A0>]}


============================== 16:19:08.460077 | 1d322c23-fac9-452b-8231-8cff8a529154 ==============================
[0m16:19:08.460077 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:19:08.461089 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:19:08.550959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1d322c23-fac9-452b-8231-8cff8a529154', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F266E11970>]}
[0m16:19:08.594164 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1d322c23-fac9-452b-8231-8cff8a529154', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F266533E00>]}
[0m16:19:08.595176 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:19:08.607820 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:19:08.608828 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:19:08.609336 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1d322c23-fac9-452b-8231-8cff8a529154', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F266DBDA30>]}
[0m16:19:10.176297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1d322c23-fac9-452b-8231-8cff8a529154', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F26812BB00>]}
[0m16:19:10.186005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1d322c23-fac9-452b-8231-8cff8a529154', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F267E86DE0>]}
[0m16:19:10.187023 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:19:10.188039 [info ] [MainThread]: 
[0m16:19:10.189565 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:19:10.191111 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:19:10.207325 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m16:19:10.226114 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m16:19:10.237786 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1d322c23-fac9-452b-8231-8cff8a529154', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F267FE2480>]}
[0m16:19:10.238799 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:19:10.239312 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:19:10.239824 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:19:10.239824 [info ] [MainThread]: 
[0m16:19:10.243880 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:19:10.243880 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:19:10.244892 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:19:10.244892 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:19:10.251009 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:19:10.253033 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:19:10.245905 => 16:19:10.253033
[0m16:19:10.253033 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:19:10.283971 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:19:10.283971 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:19:10.284980 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:19:10.284980 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:19:10.285989 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:19:10.285989 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: unsupported operand type(s) for /: 'staticmethod' and 'WindowsPath'
[0m16:19:10.286998 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:19:10.253033 => 16:19:10.286998
[0m16:19:10.396688 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  unsupported operand type(s) for /: 'staticmethod' and 'WindowsPath'
[0m16:19:10.397697 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d322c23-fac9-452b-8231-8cff8a529154', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2681B15E0>]}
[0m16:19:10.399757 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model lakesales.my_first_dbt_model ....... [[31mERROR[0m in 0.15s]
[0m16:19:10.400767 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:19:10.400767 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:19:10.401780 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:19:10.402792 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:19:10.402792 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:19:10.403805 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:19:10.402792 => 16:19:10.402792
[0m16:19:10.403805 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:19:10.419488 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:19:10.419996 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:19:10.419996 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:19:10.421004 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: unsupported operand type(s) for /: 'staticmethod' and 'WindowsPath'
[0m16:19:10.421004 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
macro drop_relation
[0m16:19:10.422015 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Runtime Error
  unsupported operand type(s) for /: 'staticmethod' and 'WindowsPath'
[0m16:19:10.422015 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:19:10.403805 => 16:19:10.422015
[0m16:19:10.454958 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    unsupported operand type(s) for /: 'staticmethod' and 'WindowsPath'
[0m16:19:10.454958 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d322c23-fac9-452b-8231-8cff8a529154', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F26838E060>]}
[0m16:19:10.455969 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file lakesales.sample ................................ [[31mERROR[0m in 0.05s]
[0m16:19:10.456987 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:19:10.458000 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:19:10.458000 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m16:19:10.459013 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:19:10.459527 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:19:10.460043 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m16:19:10.460043 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:19:10.461055 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:19:10.461055 [info ] [Thread-1 (]: 5 of 7 SKIP relation lakesales2.my_second_dbt_model ............................ [[33mSKIP[0m]
[0m16:19:10.462090 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:19:10.462090 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:19:10.463101 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m16:19:10.463101 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:19:10.463101 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:19:10.464112 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m16:19:10.464112 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:19:10.465126 [debug] [MainThread]: On master: ROLLBACK
[0m16:19:10.465126 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:19:10.466146 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:19:10.466146 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:19:10.466146 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:19:10.467160 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:19:10.467160 [info ] [MainThread]: 
[0m16:19:10.468178 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.28 seconds (0.28s).
[0m16:19:10.468178 [debug] [MainThread]: Command end result
[0m16:19:10.473717 [info ] [MainThread]: 
[0m16:19:10.474727 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:19:10.474727 [info ] [MainThread]: 
[0m16:19:10.475736 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  unsupported operand type(s) for /: 'staticmethod' and 'WindowsPath'
[0m16:19:10.475736 [info ] [MainThread]: 
[0m16:19:10.475736 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    unsupported operand type(s) for /: 'staticmethod' and 'WindowsPath'
[0m16:19:10.476777 [info ] [MainThread]: 
[0m16:19:10.476777 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m16:19:10.477786 [debug] [MainThread]: Command `cli build` failed at 16:19:10.477786 after 2.05 seconds
[0m16:19:10.477786 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2669DE960>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2669DE120>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2669DF680>]}
[0m16:19:10.477786 [debug] [MainThread]: Flushing usage events
[0m16:19:28.182659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000171B10CD8B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000171B10CC440>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000171B10CD430>]}


============================== 16:19:28.182659 | 23c8262a-8a4e-4e14-9ab3-7f787853fb95 ==============================
[0m16:19:28.182659 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:19:28.183668 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:19:28.285816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '23c8262a-8a4e-4e14-9ab3-7f787853fb95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000171B2171100>]}
[0m16:19:28.325993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '23c8262a-8a4e-4e14-9ab3-7f787853fb95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000171B105CBF0>]}
[0m16:19:28.329034 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:19:28.338197 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:19:28.339705 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:19:28.340716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '23c8262a-8a4e-4e14-9ab3-7f787853fb95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000171B2148BC0>]}
[0m16:19:29.081746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '23c8262a-8a4e-4e14-9ab3-7f787853fb95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000171B22FEFF0>]}
[0m16:19:29.090864 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '23c8262a-8a4e-4e14-9ab3-7f787853fb95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000171B230C1A0>]}
[0m16:19:29.091881 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:19:29.093915 [info ] [MainThread]: 
[0m16:19:29.094930 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:19:29.095944 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:19:29.101032 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m16:19:29.109112 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m16:19:29.119728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '23c8262a-8a4e-4e14-9ab3-7f787853fb95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000171B24CB740>]}
[0m16:19:29.120737 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:19:29.120737 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:19:29.121755 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:19:29.121755 [info ] [MainThread]: 
[0m16:19:29.125809 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:19:29.126837 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:19:29.127863 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:19:29.127863 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:19:29.132930 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:19:29.134947 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:19:29.128877 => 16:19:29.134947
[0m16:19:29.134947 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:19:29.165888 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:19:29.166897 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:19:29.166897 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:19:29.167904 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:19:29.199807 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:19:29.200816 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:19:29.200816 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:19:29.135958 => 16:19:29.200816
[0m16:19:29.210957 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:19:29.211971 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23c8262a-8a4e-4e14-9ab3-7f787853fb95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000171B230F530>]}
[0m16:19:29.211971 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model lakesales.my_first_dbt_model ....... [[31mERROR[0m in 0.08s]
[0m16:19:29.212982 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:19:29.212982 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:19:29.213996 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:19:29.215000 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:19:29.215000 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:19:29.215502 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:19:29.215502 => 16:19:29.215502
[0m16:19:29.215502 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:19:29.230667 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:19:29.230667 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:19:29.232689 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:19:29.232689 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:19:29.232689 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
macro drop_relation
[0m16:19:29.233696 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Runtime Error
  'NoneType' object has no attribute 'cells'
[0m16:19:29.233696 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:19:29.215502 => 16:19:29.233696
[0m16:19:29.239791 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'NoneType' object has no attribute 'cells'
[0m16:19:29.240804 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '23c8262a-8a4e-4e14-9ab3-7f787853fb95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000171B2525DC0>]}
[0m16:19:29.241820 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file lakesales.sample ................................ [[31mERROR[0m in 0.03s]
[0m16:19:29.241820 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:19:29.242829 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:19:29.242829 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m16:19:29.243838 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:19:29.243838 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:19:29.243838 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m16:19:29.244845 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:19:29.244845 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:19:29.245856 [info ] [Thread-1 (]: 5 of 7 SKIP relation lakesales2.my_second_dbt_model ............................ [[33mSKIP[0m]
[0m16:19:29.245856 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:19:29.245856 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:19:29.246865 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m16:19:29.246865 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:19:29.247875 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:19:29.247875 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m16:19:29.247875 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:19:29.249417 [debug] [MainThread]: On master: ROLLBACK
[0m16:19:29.249925 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:19:29.249925 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:19:29.249925 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:19:29.250934 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:19:29.250934 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:19:29.251945 [info ] [MainThread]: 
[0m16:19:29.251945 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.16 seconds (0.16s).
[0m16:19:29.252955 [debug] [MainThread]: Command end result
[0m16:19:29.263122 [info ] [MainThread]: 
[0m16:19:29.264140 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:19:29.265191 [info ] [MainThread]: 
[0m16:19:29.265191 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:19:29.266203 [info ] [MainThread]: 
[0m16:19:29.266203 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'NoneType' object has no attribute 'cells'
[0m16:19:29.266203 [info ] [MainThread]: 
[0m16:19:29.267219 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m16:19:29.268231 [debug] [MainThread]: Command `cli build` failed at 16:19:29.268231 after 1.11 seconds
[0m16:19:29.269738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000171B04B1BE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000171B0D6B380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000171B0D6AFC0>]}
[0m16:19:29.270750 [debug] [MainThread]: Flushing usage events
[0m16:20:14.856851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2ACA6D040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2ACA6C320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2ACA6D4C0>]}


============================== 16:20:14.857870 | 08e10f0a-57c5-4910-a653-7f2804e9a7df ==============================
[0m16:20:14.857870 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:20:14.858892 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:20:14.948213 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '08e10f0a-57c5-4910-a653-7f2804e9a7df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2AC6D7140>]}
[0m16:20:14.983913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '08e10f0a-57c5-4910-a653-7f2804e9a7df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2AC7F87A0>]}
[0m16:20:14.985458 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:20:14.994328 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:20:14.994842 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:20:14.995369 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '08e10f0a-57c5-4910-a653-7f2804e9a7df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2ACA36090>]}
[0m16:20:15.737303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '08e10f0a-57c5-4910-a653-7f2804e9a7df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2ADE6A060>]}
[0m16:20:15.744875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '08e10f0a-57c5-4910-a653-7f2804e9a7df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2ADCC1AC0>]}
[0m16:20:15.745888 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:20:15.747918 [info ] [MainThread]: 
[0m16:20:15.748929 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:20:15.749954 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:20:15.757066 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m16:20:15.765195 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m16:20:15.775797 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '08e10f0a-57c5-4910-a653-7f2804e9a7df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2ADC4CCE0>]}
[0m16:20:15.775797 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:20:15.776810 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:20:15.776810 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:20:15.777822 [info ] [MainThread]: 
[0m16:20:15.780868 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:20:15.780868 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:20:15.781883 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:20:15.782895 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:20:15.786954 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:20:15.790017 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:20:15.782895 => 16:20:15.790017
[0m16:20:15.791034 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:20:15.821940 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:20:15.822950 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:20:15.822950 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:20:15.822950 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:20:15.847844 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:20:15.847844 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:20:15.848855 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:20:15.791034 => 16:20:15.848855
[0m16:20:15.854923 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:20:15.855935 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '08e10f0a-57c5-4910-a653-7f2804e9a7df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2ADC2E5D0>]}
[0m16:20:15.856947 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model lakesales.my_first_dbt_model ....... [[31mERROR[0m in 0.07s]
[0m16:20:15.856947 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:20:15.857959 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:20:15.858971 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:20:15.859494 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:20:15.860109 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:20:15.860109 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:20:15.860109 => 16:20:15.860109
[0m16:20:15.860109 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:20:15.873732 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:20:15.873732 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:20:15.874742 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:20:15.874742 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:20:15.875750 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
macro drop_relation
[0m16:20:15.875750 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Runtime Error
  'NoneType' object has no attribute 'cells'
[0m16:20:15.876795 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:20:15.861112 => 16:20:15.875750
[0m16:20:15.880848 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'NoneType' object has no attribute 'cells'
[0m16:20:15.880848 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '08e10f0a-57c5-4910-a653-7f2804e9a7df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2AE158620>]}
[0m16:20:15.880848 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file lakesales.sample ................................ [[31mERROR[0m in 0.02s]
[0m16:20:15.881860 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:20:15.881860 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:20:15.882873 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m16:20:15.882873 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:20:15.882873 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:20:15.883883 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m16:20:15.883883 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:20:15.884891 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:20:15.884891 [info ] [Thread-1 (]: 5 of 7 SKIP relation lakesales2.my_second_dbt_model ............................ [[33mSKIP[0m]
[0m16:20:15.884891 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:20:15.885900 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:20:15.885900 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m16:20:15.885900 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:20:15.886912 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:20:15.886912 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m16:20:15.886912 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:20:15.887923 [debug] [MainThread]: On master: ROLLBACK
[0m16:20:15.888933 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:20:15.888933 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:20:15.889446 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:20:15.889957 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:20:15.889957 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:20:15.890973 [info ] [MainThread]: 
[0m16:20:15.890973 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.14 seconds (0.14s).
[0m16:20:15.892014 [debug] [MainThread]: Command end result
[0m16:20:15.899109 [info ] [MainThread]: 
[0m16:20:15.899617 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:20:15.899617 [info ] [MainThread]: 
[0m16:20:15.900125 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:20:15.900125 [info ] [MainThread]: 
[0m16:20:15.900125 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    'NoneType' object has no attribute 'cells'
[0m16:20:15.901133 [info ] [MainThread]: 
[0m16:20:15.901133 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m16:20:15.902142 [debug] [MainThread]: Command `cli build` failed at 16:20:15.902142 after 1.07 seconds
[0m16:20:15.902142 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2AC847C80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2ABFF7860>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F2AC6834A0>]}
[0m16:20:15.902142 [debug] [MainThread]: Flushing usage events
[0m16:22:50.661234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002678C51E930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002678C51D1F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002678C51C320>]}


============================== 16:22:50.665363 | cb98ce72-d6a5-47e3-b604-19272786e257 ==============================
[0m16:22:50.665363 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:22:50.668402 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:22:50.971945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cb98ce72-d6a5-47e3-b604-19272786e257', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002678C2A6A20>]}
[0m16:22:51.099951 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cb98ce72-d6a5-47e3-b604-19272786e257', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002678C42AE40>]}
[0m16:22:51.105039 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:22:51.137078 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:22:51.140138 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:22:51.142171 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'cb98ce72-d6a5-47e3-b604-19272786e257', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002678BDF3380>]}
[0m16:22:54.101911 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cb98ce72-d6a5-47e3-b604-19272786e257', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002678CA38CB0>]}
[0m16:22:54.134996 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cb98ce72-d6a5-47e3-b604-19272786e257', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002678CA63E00>]}
[0m16:22:54.137023 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:22:54.144127 [info ] [MainThread]: 
[0m16:22:54.148184 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:22:54.155358 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:22:54.239708 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m16:22:54.269695 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m16:22:54.319446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cb98ce72-d6a5-47e3-b604-19272786e257', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002678CA5BFB0>]}
[0m16:22:54.321611 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:22:54.323695 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:22:54.326821 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:22:54.328917 [info ] [MainThread]: 
[0m16:22:54.341481 [debug] [Thread-6 (]: Began running node model.testproj.my_first_dbt_model
[0m16:22:54.344066 [info ] [Thread-6 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:22:54.347173 [debug] [Thread-6 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:22:54.348777 [debug] [Thread-6 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:22:54.385406 [debug] [Thread-6 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:22:54.390985 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:22:54.350329 => 16:22:54.389460
[0m16:22:54.392003 [debug] [Thread-6 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:22:54.525906 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:22:54.527927 [debug] [Thread-6 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:22:54.529450 [debug] [Thread-6 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:22:54.530969 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m16:23:39.165237 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:23:39.166250 [debug] [Thread-6 (]: Microsoft Fabric-Spark adapter: 'NoneType' object has no attribute 'cells'
[0m16:23:39.170410 [debug] [Thread-6 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:22:54.394105 => 16:23:39.170410
[0m16:23:39.189179 [debug] [Thread-6 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  'NoneType' object has no attribute 'cells'
[0m16:23:39.192254 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cb98ce72-d6a5-47e3-b604-19272786e257', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002678C7A50A0>]}
[0m16:23:39.196410 [error] [Thread-6 (]: 1 of 7 ERROR creating sql incremental model lakesales.my_first_dbt_model ....... [[31mERROR[0m in 44.85s]
[0m16:23:39.199441 [debug] [Thread-6 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:23:39.200954 [debug] [Thread-6 (]: Began running node seed.testproj.sample
[0m16:23:39.205048 [info ] [Thread-6 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:23:39.208093 [debug] [Thread-6 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:23:39.211226 [debug] [Thread-6 (]: Began compiling node seed.testproj.sample
[0m16:23:39.213262 [debug] [Thread-6 (]: Timing info for seed.testproj.sample (compile): 16:23:39.212249 => 16:23:39.212249
[0m16:23:39.214276 [debug] [Thread-6 (]: Began executing node seed.testproj.sample
[0m16:23:39.284052 [debug] [Thread-6 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:23:39.286091 [debug] [Thread-6 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:24:33.383961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002061A2F5490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002061A1C5910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002061A2F65D0>]}


============================== 16:24:33.383961 | d1c4152f-c932-4d70-9efb-f973d425c461 ==============================
[0m16:24:33.383961 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:24:33.384970 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'invocation_command': 'dbt ', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m16:24:33.471162 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd1c4152f-c932-4d70-9efb-f973d425c461', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002061A2F65A0>]}
[0m16:24:33.506109 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd1c4152f-c932-4d70-9efb-f973d425c461', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002061B392990>]}
[0m16:24:33.507121 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:24:33.514207 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:24:33.515217 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:24:33.515217 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd1c4152f-c932-4d70-9efb-f973d425c461', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002061B369760>]}
[0m16:24:34.207101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd1c4152f-c932-4d70-9efb-f973d425c461', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002061B401DC0>]}
[0m16:24:34.223834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd1c4152f-c932-4d70-9efb-f973d425c461', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002061B745D90>]}
[0m16:24:34.223834 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:24:34.225857 [info ] [MainThread]: 
[0m16:24:34.226869 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:24:34.227880 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:24:34.232975 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m16:24:34.241084 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m16:24:34.250194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd1c4152f-c932-4d70-9efb-f973d425c461', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002061B3C3FB0>]}
[0m16:24:34.251204 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:24:34.251204 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:24:34.252213 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:24:34.252213 [info ] [MainThread]: 
[0m16:24:34.255250 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:24:34.255250 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:24:34.256261 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:24:34.257271 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:24:34.260791 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:24:34.262843 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:24:34.257271 => 16:24:34.262843
[0m16:24:34.262843 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:24:34.292352 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:24:34.293370 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:24:34.293370 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:24:34.294386 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:24:34.295403 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:24:34.296421 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook file \dbt\include\fabricsparknb\notebooks\model_notebook.ipynb does not exist
[0m16:24:34.296421 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:24:34.263852 => 16:24:34.296421
[0m16:24:34.308050 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook file \dbt\include\fabricsparknb\notebooks\model_notebook.ipynb does not exist
[0m16:24:34.308050 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd1c4152f-c932-4d70-9efb-f973d425c461', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002061B73B9E0>]}
[0m16:24:34.309059 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model lakesales.my_first_dbt_model ....... [[31mERROR[0m in 0.05s]
[0m16:24:34.309567 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:24:34.310075 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:24:34.310075 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:24:34.311085 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:24:34.311085 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:24:34.312097 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:24:34.312097 => 16:24:34.312097
[0m16:24:34.312097 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:24:34.326305 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:24:34.326305 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:24:34.327320 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:24:34.327320 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook file \dbt\include\fabricsparknb\notebooks\model_notebook.ipynb does not exist
[0m16:24:34.328341 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
macro drop_relation
[0m16:24:34.328341 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Runtime Error
  Notebook file \dbt\include\fabricsparknb\notebooks\model_notebook.ipynb does not exist
[0m16:24:34.329345 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:24:34.312097 => 16:24:34.328341
[0m16:24:34.333892 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    Notebook file \dbt\include\fabricsparknb\notebooks\model_notebook.ipynb does not exist
[0m16:24:34.334901 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd1c4152f-c932-4d70-9efb-f973d425c461', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002061B795A90>]}
[0m16:24:34.334901 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file lakesales.sample ................................ [[31mERROR[0m in 0.02s]
[0m16:24:34.335912 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:24:34.336925 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:24:34.336925 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m16:24:34.337935 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:24:34.337935 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:24:34.338979 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m16:24:34.339495 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:24:34.340005 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:24:34.340005 [info ] [Thread-1 (]: 5 of 7 SKIP relation lakesales2.my_second_dbt_model ............................ [[33mSKIP[0m]
[0m16:24:34.340005 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:24:34.341019 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:24:34.341019 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m16:24:34.342033 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:24:34.342033 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:24:34.342033 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m16:24:34.342033 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:24:34.343042 [debug] [MainThread]: On master: ROLLBACK
[0m16:24:34.344052 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:24:34.344052 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:24:34.344052 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:24:34.344052 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:24:34.345064 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:24:34.345064 [info ] [MainThread]: 
[0m16:24:34.345064 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.12 seconds (0.12s).
[0m16:24:34.346072 [debug] [MainThread]: Command end result
[0m16:24:34.351122 [info ] [MainThread]: 
[0m16:24:34.352133 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:24:34.352133 [info ] [MainThread]: 
[0m16:24:34.352133 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook file \dbt\include\fabricsparknb\notebooks\model_notebook.ipynb does not exist
[0m16:24:34.353206 [info ] [MainThread]: 
[0m16:24:34.353206 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    Notebook file \dbt\include\fabricsparknb\notebooks\model_notebook.ipynb does not exist
[0m16:24:34.353206 [info ] [MainThread]: 
[0m16:24:34.354208 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m16:24:34.355221 [debug] [MainThread]: Command `cli build` failed at 16:24:34.355221 after 0.99 seconds
[0m16:24:34.355221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020619EB34A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002061A1C5D00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002061A1C5910>]}
[0m16:24:34.355221 [debug] [MainThread]: Flushing usage events
[0m16:26:28.038219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBC9116A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBC910E30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBC9107D0>]}


============================== 16:26:28.038219 | 27306e20-7615-4943-a630-461f38336428 ==============================
[0m16:26:28.038219 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:26:28.039227 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'static_parser': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:26:28.134407 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '27306e20-7615-4943-a630-461f38336428', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBC6CA9F0>]}
[0m16:26:28.169878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '27306e20-7615-4943-a630-461f38336428', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBC217C20>]}
[0m16:26:28.170891 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:26:28.177961 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:26:28.179489 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:26:28.180017 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '27306e20-7615-4943-a630-461f38336428', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBDA4F0B0>]}
[0m16:26:28.886968 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '27306e20-7615-4943-a630-461f38336428', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBDC81A60>]}
[0m16:26:28.897094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '27306e20-7615-4943-a630-461f38336428', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBDB41FD0>]}
[0m16:26:28.897094 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:26:28.899119 [info ] [MainThread]: 
[0m16:26:28.900134 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:26:28.901162 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:26:28.906202 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m16:26:28.913281 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m16:26:28.923917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '27306e20-7615-4943-a630-461f38336428', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBC911AC0>]}
[0m16:26:28.924934 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:26:28.924934 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:26:28.925942 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:26:28.925942 [info ] [MainThread]: 
[0m16:26:28.929479 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:26:28.929989 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:26:28.931023 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:26:28.931023 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:26:28.936089 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:26:28.938105 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:26:28.931023 => 16:26:28.938105
[0m16:26:28.938105 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:26:28.972071 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:26:28.972071 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:26:28.973082 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:26:28.973082 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:26:29.055910 [debug] [Thread-1 (]: SQL status: OK in 0.07999999821186066 seconds
[0m16:26:29.075572 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m16:26:29.077123 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:26:29.077123 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m16:26:29.092622 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:26:29.101142 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:26:28.938105 => 16:26:29.101142
[0m16:26:29.101142 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27306e20-7615-4943-a630-461f38336428', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBDB40440>]}
[0m16:26:29.102166 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model lakesales.my_first_dbt_model ........... [[32mOK[0m in 0.17s]
[0m16:26:29.103177 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:26:29.103177 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:26:29.103177 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:26:29.104189 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:26:29.104189 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:26:29.105198 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:26:29.104189 => 16:26:29.104189
[0m16:26:29.105198 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:26:29.118348 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:26:29.118348 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:26:29.122892 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:26:29.141166 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:26:29.141166 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table lakesales.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m16:26:29.155342 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:26:29.164989 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m16:26:29.167017 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:26:29.168032 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into lakesales.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m16:26:29.182221 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:26:29.185247 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m16:26:29.187264 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:26:29.188275 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:26:29.105198 => 16:26:29.188275
[0m16:26:29.189296 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27306e20-7615-4943-a630-461f38336428', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBDEEB290>]}
[0m16:26:29.189296 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file lakesales.sample .................................... [[32mINSERT 2[0m in 0.09s]
[0m16:26:29.190313 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:26:29.190313 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:26:29.191324 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m16:26:29.192374 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m16:26:29.192374 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:26:29.199965 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:26:29.201999 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 16:26:29.192374 => 16:26:29.201999
[0m16:26:29.201999 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:26:29.210117 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:26:29.211125 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:26:29.212137 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m16:26:29.225342 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:26:29.226364 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 16:26:29.201999 => 16:26:29.226364
[0m16:26:29.227375 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.04s]
[0m16:26:29.228392 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:26:29.228392 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:26:29.228392 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m16:26:29.229900 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m16:26:29.229900 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:26:29.234970 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:26:29.235984 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 16:26:29.229900 => 16:26:29.235984
[0m16:26:29.235984 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:26:29.238026 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:26:29.239041 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:26:29.239557 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m16:26:29.244116 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:26:29.245127 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 16:26:29.237001 => 16:26:29.245127
[0m16:26:29.246143 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m16:26:29.246143 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:26:29.247155 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:26:29.247155 [info ] [Thread-1 (]: 5 of 7 START sql incremental model lakesales2.my_second_dbt_model .............. [RUN]
[0m16:26:29.248162 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m16:26:29.248162 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m16:26:29.250185 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m16:26:29.251193 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 16:26:29.249170 => 16:26:29.251193
[0m16:26:29.252206 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m16:26:29.254254 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:26:29.254254 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_second_dbt_model__dbt_tmp as
      -- Use the `ref` function to select from other models

select *
from lakesales.my_first_dbt_model
where id = 1
  
[0m16:26:29.259847 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:26:29.264908 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m16:26:29.265920 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:26:29.266931 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales2.my_second_dbt_model
    select `id` from my_second_dbt_model__dbt_tmp


[0m16:26:29.281107 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:26:29.282118 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 16:26:29.252206 => 16:26:29.282118
[0m16:26:29.283128 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '27306e20-7615-4943-a630-461f38336428', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBE23CC20>]}
[0m16:26:29.283128 [info ] [Thread-1 (]: 5 of 7 OK created sql incremental model lakesales2.my_second_dbt_model ......... [[32mOK[0m in 0.03s]
[0m16:26:29.284162 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:26:29.284162 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:26:29.284162 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m16:26:29.285177 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m16:26:29.285177 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:26:29.288209 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m16:26:29.290246 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 16:26:29.286189 => 16:26:29.290246
[0m16:26:29.290246 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:26:29.292272 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m16:26:29.293290 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m16:26:29.293290 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales2.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m16:26:29.299366 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:26:29.299869 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 16:26:29.290246 => 16:26:29.299869
[0m16:26:29.300879 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.02s]
[0m16:26:29.300879 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:26:29.301889 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:26:29.301889 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m16:26:29.301889 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m16:26:29.302898 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:26:29.304915 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:26:29.305923 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 16:26:29.302898 => 16:26:29.305923
[0m16:26:29.305923 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:26:29.306932 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:26:29.307942 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:26:29.307942 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales2.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m16:26:29.312998 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:26:29.314008 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 16:26:29.305923 => 16:26:29.314008
[0m16:26:29.315054 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m16:26:29.315054 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:26:29.316066 [debug] [MainThread]: On master: ROLLBACK
[0m16:26:29.316066 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:26:29.317077 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:26:29.317077 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:26:29.317077 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:26:29.317077 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:26:29.318088 [info ] [MainThread]: 
[0m16:26:29.318088 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.42 seconds (0.42s).
[0m16:26:29.319609 [debug] [MainThread]: Command end result
[0m16:26:29.326207 [info ] [MainThread]: 
[0m16:26:29.327216 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:26:29.327216 [info ] [MainThread]: 
[0m16:26:29.327216 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m16:26:29.328226 [debug] [MainThread]: Command `cli build` succeeded at 16:26:29.328226 after 1.31 seconds
[0m16:26:29.329244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBC910710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBC9107D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002DEBC910E30>]}
[0m16:26:29.329781 [debug] [MainThread]: Flushing usage events
[0m16:28:30.653038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A94BFE480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A94BFDC40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A94BFF350>]}


============================== 16:28:30.654049 | 13adab95-f031-4326-b885-481e47ba2511 ==============================
[0m16:28:30.654049 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:28:30.655064 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'log_format': 'default', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m16:28:30.738014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '13adab95-f031-4326-b885-481e47ba2511', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A94B08E00>]}
[0m16:28:30.773919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '13adab95-f031-4326-b885-481e47ba2511', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A937E4380>]}
[0m16:28:30.774929 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:28:30.783016 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:28:30.784025 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:28:30.784025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '13adab95-f031-4326-b885-481e47ba2511', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A9450F980>]}
[0m16:28:31.503030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '13adab95-f031-4326-b885-481e47ba2511', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A95D86FF0>]}
[0m16:28:31.511133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '13adab95-f031-4326-b885-481e47ba2511', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A95E0F470>]}
[0m16:28:31.511133 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:28:31.513161 [info ] [MainThread]: 
[0m16:28:31.514173 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:28:31.515186 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:28:31.520244 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m16:28:31.528402 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m16:28:31.540044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '13adab95-f031-4326-b885-481e47ba2511', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A95E4F680>]}
[0m16:28:31.540044 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:28:31.541056 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:28:31.541056 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:28:31.542066 [info ] [MainThread]: 
[0m16:28:31.544082 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:28:31.545089 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:28:31.545089 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:28:31.546101 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:28:31.550145 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:28:31.552163 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:28:31.546101 => 16:28:31.552163
[0m16:28:31.552163 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:28:31.584117 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:28:31.585156 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:28:31.585156 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:28:31.585156 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:28:31.640921 [debug] [Thread-1 (]: SQL status: OK in 0.05999999865889549 seconds
[0m16:28:31.653057 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m16:28:31.656106 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:28:31.657122 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m16:28:31.670875 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:28:31.679491 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:28:31.552163 => 16:28:31.679491
[0m16:28:31.679999 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '13adab95-f031-4326-b885-481e47ba2511', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A95FFB1A0>]}
[0m16:28:31.681007 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model lakesales.my_first_dbt_model ........... [[32mOK[0m in 0.13s]
[0m16:28:31.681007 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:28:31.682019 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:28:31.682019 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:28:31.683032 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:28:31.683032 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:28:31.683032 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:28:31.683032 => 16:28:31.683032
[0m16:28:31.683032 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:28:31.698260 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:28:31.698260 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:28:31.703323 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:28:31.719504 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:28:31.720013 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table lakesales.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m16:28:31.735212 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m16:28:31.744325 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m16:28:31.747377 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:28:31.748389 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into lakesales.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m16:28:31.765126 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m16:28:31.768179 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m16:28:31.770206 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:28:31.770206 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:28:31.684044 => 16:28:31.770206
[0m16:28:31.771215 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '13adab95-f031-4326-b885-481e47ba2511', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A963A2EA0>]}
[0m16:28:31.772229 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file lakesales.sample .................................... [[32mINSERT 2[0m in 0.09s]
[0m16:28:31.772229 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:28:31.772229 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:28:31.773238 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m16:28:31.773238 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m16:28:31.774254 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:28:31.781359 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:28:31.783411 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 16:28:31.774254 => 16:28:31.783411
[0m16:28:31.783411 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:28:31.794087 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:28:31.796120 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:28:31.796120 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m16:28:31.802194 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:28:31.803204 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 16:28:31.784420 => 16:28:31.803204
[0m16:28:31.804212 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m16:28:31.804212 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:28:31.805223 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:28:31.805223 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m16:28:31.806236 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m16:28:31.806236 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:28:31.811311 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:28:31.813331 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 16:28:31.806236 => 16:28:31.813331
[0m16:28:31.813331 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:28:31.815383 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:28:31.816391 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:28:31.817402 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m16:28:31.822978 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:28:31.823993 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 16:28:31.814370 => 16:28:31.823993
[0m16:28:31.825004 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m16:28:31.826018 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:28:31.827031 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:28:31.827031 [info ] [Thread-1 (]: 5 of 7 START sql incremental model lakesales2.my_second_dbt_model .............. [RUN]
[0m16:28:31.828046 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m16:28:31.828046 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m16:28:31.831160 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m16:28:31.832163 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 16:28:31.828046 => 16:28:31.832163
[0m16:28:31.832163 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m16:28:31.835188 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:28:31.835188 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_second_dbt_model__dbt_tmp as
      -- Use the `ref` function to select from other models

select *
from lakesales.my_first_dbt_model
where id = 1
  
[0m16:28:31.839733 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:28:31.844277 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m16:28:31.845296 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:28:31.846306 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales2.my_second_dbt_model
    select `id` from my_second_dbt_model__dbt_tmp


[0m16:28:31.859496 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:28:31.861013 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 16:28:31.833172 => 16:28:31.861013
[0m16:28:31.861013 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '13adab95-f031-4326-b885-481e47ba2511', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A9653B7D0>]}
[0m16:28:31.862021 [info ] [Thread-1 (]: 5 of 7 OK created sql incremental model lakesales2.my_second_dbt_model ......... [[32mOK[0m in 0.03s]
[0m16:28:31.863035 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:28:31.863035 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:28:31.864047 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m16:28:31.865059 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m16:28:31.865059 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:28:31.868084 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m16:28:31.869094 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 16:28:31.865059 => 16:28:31.869094
[0m16:28:31.869602 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:28:31.871124 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m16:28:31.872140 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m16:28:31.873152 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales2.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m16:28:31.878239 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:28:31.879249 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 16:28:31.870113 => 16:28:31.879249
[0m16:28:31.879759 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.02s]
[0m16:28:31.880266 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:28:31.880266 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:28:31.880266 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m16:28:31.881275 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m16:28:31.881275 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:28:31.884314 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:28:31.885328 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 16:28:31.882284 => 16:28:31.884314
[0m16:28:31.885328 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:28:31.886348 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:28:31.887363 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:28:31.887363 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales2.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m16:28:31.893979 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:28:31.894990 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 16:28:31.885328 => 16:28:31.894990
[0m16:28:31.896010 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m16:28:31.897020 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:28:31.898033 [debug] [MainThread]: On master: ROLLBACK
[0m16:28:31.898033 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:28:31.898033 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:28:31.899046 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:28:31.899046 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:28:31.899558 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:28:31.900067 [info ] [MainThread]: 
[0m16:28:31.900067 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.39 seconds (0.39s).
[0m16:28:31.901078 [debug] [MainThread]: Command end result
[0m16:28:31.907188 [info ] [MainThread]: 
[0m16:28:31.908201 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:28:31.908201 [info ] [MainThread]: 
[0m16:28:31.909212 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m16:28:31.909728 [debug] [MainThread]: Command `cli build` succeeded at 16:28:31.909728 after 1.28 seconds
[0m16:28:31.910236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A96053BF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A94C2DFA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A95DDE8D0>]}
[0m16:28:31.910236 [debug] [MainThread]: Flushing usage events
[0m16:37:24.399973 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187AC0F0FE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187AC0F01D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187AC0F2030>]}


============================== 16:37:24.400481 | e6b3cc94-2369-4e2e-b1c0-63b53d969a15 ==============================
[0m16:37:24.400481 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:37:24.400481 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'introspect': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'log_format': 'default', 'send_anonymous_usage_stats': 'True'}
[0m16:37:24.495347 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e6b3cc94-2369-4e2e-b1c0-63b53d969a15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187AD17BB30>]}
[0m16:37:24.530281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e6b3cc94-2369-4e2e-b1c0-63b53d969a15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187AC0F1A60>]}
[0m16:37:24.532301 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:37:24.544483 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:37:24.544483 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:37:24.545497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'e6b3cc94-2369-4e2e-b1c0-63b53d969a15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187AC0F1520>]}
[0m16:37:25.950342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e6b3cc94-2369-4e2e-b1c0-63b53d969a15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187AD287290>]}
[0m16:37:25.959993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e6b3cc94-2369-4e2e-b1c0-63b53d969a15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187AD577B60>]}
[0m16:37:25.960501 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:37:25.962527 [info ] [MainThread]: 
[0m16:37:25.963535 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:37:25.963535 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:37:25.975149 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m16:37:25.988273 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m16:37:25.999947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e6b3cc94-2369-4e2e-b1c0-63b53d969a15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187AD48E6C0>]}
[0m16:37:26.000455 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:37:26.000455 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:37:26.000455 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:37:26.001463 [info ] [MainThread]: 
[0m16:37:26.003480 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:37:26.003480 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:37:26.004488 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:37:26.004488 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:37:26.008545 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:37:26.010587 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:37:26.004488 => 16:37:26.010072
[0m16:37:26.010587 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:37:26.040465 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:37:26.041471 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:37:26.041471 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:37:26.041471 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:37:26.120022 [debug] [Thread-1 (]: SQL status: OK in 0.07999999821186066 seconds
[0m16:37:26.138217 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m16:37:26.139739 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:37:26.139739 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m16:37:26.151376 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:37:26.160505 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:37:26.010587 => 16:37:26.160505
[0m16:37:26.161514 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e6b3cc94-2369-4e2e-b1c0-63b53d969a15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187AD48F2F0>]}
[0m16:37:26.161514 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model lakesales.my_first_dbt_model ........... [[32mOK[0m in 0.16s]
[0m16:37:26.162524 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:37:26.162524 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:37:26.163566 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:37:26.163566 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:37:26.164574 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:37:26.164574 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:37:26.164574 => 16:37:26.164574
[0m16:37:26.164574 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:37:26.178748 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:37:26.179751 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:37:26.184319 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:37:26.203587 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:37:26.203587 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table lakesales.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m16:37:26.219251 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m16:37:26.229992 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m16:37:26.231516 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:37:26.232532 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into lakesales.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m16:37:26.247208 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:37:26.250260 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m16:37:26.252335 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:37:26.252335 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:37:26.164574 => 16:37:26.252335
[0m16:37:26.253354 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e6b3cc94-2369-4e2e-b1c0-63b53d969a15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187AD19E5D0>]}
[0m16:37:26.253354 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file lakesales.sample .................................... [[32mINSERT 2[0m in 0.09s]
[0m16:37:26.254368 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:37:26.254368 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:37:26.255390 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m16:37:26.256408 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m16:37:26.256408 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:37:26.264545 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:37:26.265556 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 16:37:26.256408 => 16:37:26.265556
[0m16:37:26.265556 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:37:26.274136 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:37:26.275172 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:37:26.275172 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m16:37:26.285241 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:37:26.286250 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 16:37:26.265556 => 16:37:26.286250
[0m16:37:26.287260 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.03s]
[0m16:37:26.288270 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:37:26.289284 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:37:26.289827 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m16:37:26.290336 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m16:37:26.291350 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:37:26.296407 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:37:26.297424 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 16:37:26.291350 => 16:37:26.297424
[0m16:37:26.298435 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:37:26.299965 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:37:26.300476 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:37:26.301487 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m16:37:26.306555 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:37:26.307563 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 16:37:26.298435 => 16:37:26.307563
[0m16:37:26.308573 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m16:37:26.308573 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:37:26.309578 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:37:26.309578 [info ] [Thread-1 (]: 5 of 7 START sql incremental model lakesales2.my_second_dbt_model .............. [RUN]
[0m16:37:26.310858 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m16:37:26.310858 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m16:37:26.312887 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m16:37:26.313898 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 16:37:26.310858 => 16:37:26.313898
[0m16:37:26.313898 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m16:37:26.315914 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:37:26.315914 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_second_dbt_model__dbt_tmp as
      -- Use the `ref` function to select from other models

select *
from lakesales.my_first_dbt_model
where id = 1
  
[0m16:37:26.320446 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:37:26.326541 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m16:37:26.327551 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:37:26.327551 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales2.my_second_dbt_model
    select `id` from my_second_dbt_model__dbt_tmp


[0m16:37:26.342220 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:37:26.343234 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 16:37:26.313898 => 16:37:26.343234
[0m16:37:26.344248 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e6b3cc94-2369-4e2e-b1c0-63b53d969a15', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187AD6D2300>]}
[0m16:37:26.344248 [info ] [Thread-1 (]: 5 of 7 OK created sql incremental model lakesales2.my_second_dbt_model ......... [[32mOK[0m in 0.03s]
[0m16:37:26.345264 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:37:26.345264 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:37:26.346279 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m16:37:26.346279 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m16:37:26.346279 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:37:26.349308 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m16:37:26.350323 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 16:37:26.347291 => 16:37:26.350323
[0m16:37:26.350323 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:37:26.352386 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m16:37:26.353396 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m16:37:26.353396 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales2.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m16:37:26.360007 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:37:26.360519 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 16:37:26.351334 => 16:37:26.360519
[0m16:37:26.361531 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.02s]
[0m16:37:26.362545 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:37:26.362545 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:37:26.362545 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m16:37:26.363558 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m16:37:26.363558 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:37:26.366588 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:37:26.366588 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 16:37:26.363558 => 16:37:26.366588
[0m16:37:26.367616 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:37:26.368627 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:37:26.369630 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:37:26.369630 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales2.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m16:37:26.374172 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:37:26.375182 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 16:37:26.367616 => 16:37:26.375182
[0m16:37:26.375182 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m16:37:26.376190 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:37:26.377200 [debug] [MainThread]: On master: ROLLBACK
[0m16:37:26.377200 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:37:26.377200 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:37:26.377200 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:37:26.378203 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:37:26.378203 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:37:26.378705 [info ] [MainThread]: 
[0m16:37:26.378705 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.42 seconds (0.42s).
[0m16:37:26.379708 [debug] [MainThread]: Command end result
[0m16:37:26.385802 [info ] [MainThread]: 
[0m16:37:26.385802 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:37:26.386816 [info ] [MainThread]: 
[0m16:37:26.386816 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m16:37:26.387830 [debug] [MainThread]: Command `cli build` succeeded at 16:37:26.387830 after 2.02 seconds
[0m16:37:26.387830 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187A89594F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187AC0F0FE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000187AD583290>]}
[0m16:37:26.388849 [debug] [MainThread]: Flushing usage events
[0m16:58:09.068715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013FFD487A40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013FFD487D70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013FFD487890>]}


============================== 16:58:09.069723 | 3aeab241-576d-47ea-9edf-3dbd706fffe4 ==============================
[0m16:58:09.069723 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:58:09.069723 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'fail_fast': 'False', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'version_check': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'target_path': 'None', 'introspect': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:58:09.156715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3aeab241-576d-47ea-9edf-3dbd706fffe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013FFD4CEDB0>]}
[0m16:58:09.192102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3aeab241-576d-47ea-9edf-3dbd706fffe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013FFD3FCE30>]}
[0m16:58:09.194131 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:58:09.205242 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:58:09.206250 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:58:09.206250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3aeab241-576d-47ea-9edf-3dbd706fffe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013FFE5921E0>]}
[0m16:58:10.582328 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3aeab241-576d-47ea-9edf-3dbd706fffe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013FFD425160>]}
[0m16:58:10.600572 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3aeab241-576d-47ea-9edf-3dbd706fffe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013FFE6761E0>]}
[0m16:58:10.600572 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:58:10.602595 [info ] [MainThread]: 
[0m16:58:10.602595 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:58:10.603604 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:58:10.618753 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales2'
[0m16:58:10.636961 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales2, now list_None_lakesales)
[0m16:58:10.646046 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3aeab241-576d-47ea-9edf-3dbd706fffe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013FFE7E18B0>]}
[0m16:58:10.647054 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:58:10.647054 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:58:10.647054 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:58:10.648062 [info ] [MainThread]: 
[0m16:58:10.650079 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:58:10.651087 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:58:10.652100 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:58:10.652100 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:58:10.657200 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:58:10.659243 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:58:10.652100 => 16:58:10.659243
[0m16:58:10.660255 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:58:10.689553 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:58:10.690586 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:58:10.690586 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:58:10.690586 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:58:10.691600 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:58:10.692616 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook file C:\Users\jramp\source\gitjr\dbt-fabricsparknb\dbt\include\fabricsparknb\dbt\include\fabricsparknb\notebooks\model_notebook.ipynb does not exist
[0m16:58:10.692616 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:58:10.660255 => 16:58:10.692616
[0m16:58:10.770470 [debug] [Thread-1 (]: Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook file C:\Users\jramp\source\gitjr\dbt-fabricsparknb\dbt\include\fabricsparknb\dbt\include\fabricsparknb\notebooks\model_notebook.ipynb does not exist
[0m16:58:10.771478 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3aeab241-576d-47ea-9edf-3dbd706fffe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013FFE8DFBC0>]}
[0m16:58:10.771478 [error] [Thread-1 (]: 1 of 7 ERROR creating sql incremental model lakesales.my_first_dbt_model ....... [[31mERROR[0m in 0.12s]
[0m16:58:10.772486 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:58:10.773498 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:58:10.773498 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:58:10.774514 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:58:10.774514 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:58:10.775527 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:58:10.775527 => 16:58:10.775527
[0m16:58:10.775527 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:58:10.790744 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:58:10.790744 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:58:10.791752 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:58:10.792797 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Notebook file C:\Users\jramp\source\gitjr\dbt-fabricsparknb\dbt\include\fabricsparknb\dbt\include\fabricsparknb\notebooks\model_notebook.ipynb does not exist
[0m16:58:10.792797 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Error while running:
macro drop_relation
[0m16:58:10.792797 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: Runtime Error
  Notebook file C:\Users\jramp\source\gitjr\dbt-fabricsparknb\dbt\include\fabricsparknb\dbt\include\fabricsparknb\notebooks\model_notebook.ipynb does not exist
[0m16:58:10.793807 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:58:10.775527 => 16:58:10.793807
[0m16:58:10.816068 [debug] [Thread-1 (]: Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    Notebook file C:\Users\jramp\source\gitjr\dbt-fabricsparknb\dbt\include\fabricsparknb\dbt\include\fabricsparknb\notebooks\model_notebook.ipynb does not exist
[0m16:58:10.817080 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3aeab241-576d-47ea-9edf-3dbd706fffe4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013FFEA91D60>]}
[0m16:58:10.818092 [error] [Thread-1 (]: 2 of 7 ERROR loading seed file lakesales.sample ................................ [[31mERROR[0m in 0.04s]
[0m16:58:10.818092 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:58:10.819102 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:58:10.819102 [info ] [Thread-1 (]: 3 of 7 SKIP test not_null_my_first_dbt_model_id ................................ [[33mSKIP[0m]
[0m16:58:10.819102 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:58:10.820111 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:58:10.820111 [info ] [Thread-1 (]: 4 of 7 SKIP test unique_my_first_dbt_model_id .................................. [[33mSKIP[0m]
[0m16:58:10.820111 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:58:10.821122 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:58:10.821122 [info ] [Thread-1 (]: 5 of 7 SKIP relation lakesales2.my_second_dbt_model ............................ [[33mSKIP[0m]
[0m16:58:10.822188 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:58:10.822188 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:58:10.823192 [info ] [Thread-1 (]: 6 of 7 SKIP test not_null_my_second_dbt_model_id ............................... [[33mSKIP[0m]
[0m16:58:10.823192 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:58:10.824210 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:58:10.824210 [info ] [Thread-1 (]: 7 of 7 SKIP test unique_my_second_dbt_model_id ................................. [[33mSKIP[0m]
[0m16:58:10.824210 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:58:10.825222 [debug] [MainThread]: On master: ROLLBACK
[0m16:58:10.826232 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:58:10.826232 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:58:10.826232 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:58:10.827243 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:58:10.827243 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:58:10.827243 [info ] [MainThread]: 
[0m16:58:10.828254 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.22 seconds (0.22s).
[0m16:58:10.829267 [debug] [MainThread]: Command end result
[0m16:58:10.835344 [info ] [MainThread]: 
[0m16:58:10.836354 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m16:58:10.836354 [info ] [MainThread]: 
[0m16:58:10.837362 [error] [MainThread]:   Runtime Error in model my_first_dbt_model (models\example\my_first_dbt_model.sql)
  Notebook file C:\Users\jramp\source\gitjr\dbt-fabricsparknb\dbt\include\fabricsparknb\dbt\include\fabricsparknb\notebooks\model_notebook.ipynb does not exist
[0m16:58:10.837362 [info ] [MainThread]: 
[0m16:58:10.837362 [error] [MainThread]:   Runtime Error in seed sample (seeds\sample.csv)
  Runtime Error
    Notebook file C:\Users\jramp\source\gitjr\dbt-fabricsparknb\dbt\include\fabricsparknb\dbt\include\fabricsparknb\notebooks\model_notebook.ipynb does not exist
[0m16:58:10.838394 [info ] [MainThread]: 
[0m16:58:10.838394 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=2 SKIP=5 TOTAL=7
[0m16:58:10.839406 [debug] [MainThread]: Command `cli build` failed at 16:58:10.839406 after 1.80 seconds
[0m16:58:10.840433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013FFD44BD70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013FF9C791F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013FFE55F680>]}
[0m16:58:10.841458 [debug] [MainThread]: Flushing usage events
[0m16:59:13.136858 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028411EABAA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028411EAB380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028411EA8DD0>]}


============================== 16:59:13.136858 | 54ae3701-e18d-4299-83b6-7ed201a805ef ==============================
[0m16:59:13.136858 [info ] [MainThread]: Running with dbt=1.7.14
[0m16:59:13.137865 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'debug': 'False', 'log_path': 'testproj\\logs', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m16:59:13.222856 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '54ae3701-e18d-4299-83b6-7ed201a805ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028411DB9430>]}
[0m16:59:13.258252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '54ae3701-e18d-4299-83b6-7ed201a805ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028411D7D070>]}
[0m16:59:13.259262 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m16:59:13.266357 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m16:59:13.267363 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m16:59:13.267363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '54ae3701-e18d-4299-83b6-7ed201a805ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002840E343050>]}
[0m16:59:13.990216 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '54ae3701-e18d-4299-83b6-7ed201a805ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028413117500>]}
[0m16:59:13.999359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '54ae3701-e18d-4299-83b6-7ed201a805ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028413096B10>]}
[0m16:59:13.999359 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m16:59:14.000366 [info ] [MainThread]: 
[0m16:59:14.001373 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m16:59:14.002382 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m16:59:14.007439 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m16:59:14.015030 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m16:59:14.025236 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '54ae3701-e18d-4299-83b6-7ed201a805ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000284132303E0>]}
[0m16:59:14.026244 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:59:14.027254 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:59:14.027254 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m16:59:14.028265 [info ] [MainThread]: 
[0m16:59:14.030284 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m16:59:14.031294 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m16:59:14.031294 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m16:59:14.032304 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m16:59:14.036357 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m16:59:14.038385 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 16:59:14.032304 => 16:59:14.038385
[0m16:59:14.038385 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m16:59:14.067756 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:59:14.067756 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:59:14.068764 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m16:59:14.068764 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m16:59:14.140600 [debug] [Thread-1 (]: SQL status: OK in 0.07000000029802322 seconds
[0m16:59:14.159827 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m16:59:14.161854 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m16:59:14.162862 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m16:59:14.173953 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:59:14.182035 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 16:59:14.039407 => 16:59:14.182035
[0m16:59:14.183043 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '54ae3701-e18d-4299-83b6-7ed201a805ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000284130E4290>]}
[0m16:59:14.183043 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model lakesales.my_first_dbt_model ........... [[32mOK[0m in 0.15s]
[0m16:59:14.184050 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m16:59:14.184050 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m16:59:14.185061 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m16:59:14.185061 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m16:59:14.185061 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m16:59:14.186074 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 16:59:14.186074 => 16:59:14.186074
[0m16:59:14.186074 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m16:59:14.200289 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:59:14.201296 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m16:59:14.205342 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:59:14.222570 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:59:14.222570 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table lakesales.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m16:59:14.237740 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:59:14.245826 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m16:59:14.247839 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m16:59:14.248847 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into lakesales.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m16:59:14.262028 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:59:14.265055 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m16:59:14.267083 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:59:14.268095 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 16:59:14.186074 => 16:59:14.267083
[0m16:59:14.268095 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '54ae3701-e18d-4299-83b6-7ed201a805ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000284137C2DE0>]}
[0m16:59:14.268095 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file lakesales.sample .................................... [[32mINSERT 2[0m in 0.08s]
[0m16:59:14.269102 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m16:59:14.269102 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:59:14.270111 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m16:59:14.270111 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m16:59:14.271123 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:59:14.277178 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:59:14.278186 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 16:59:14.271123 => 16:59:14.278186
[0m16:59:14.279193 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:59:14.287290 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:59:14.288301 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m16:59:14.288301 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m16:59:14.302478 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:59:14.304500 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 16:59:14.279193 => 16:59:14.304500
[0m16:59:14.305512 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.04s]
[0m16:59:14.305512 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m16:59:14.306522 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:59:14.306522 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m16:59:14.307541 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m16:59:14.307541 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:59:14.312638 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:59:14.313647 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 16:59:14.307541 => 16:59:14.313647
[0m16:59:14.313647 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:59:14.314664 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:59:14.315672 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m16:59:14.315672 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m16:59:14.320725 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:59:14.322759 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 16:59:14.313647 => 16:59:14.321738
[0m16:59:14.323790 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m16:59:14.323790 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m16:59:14.324804 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m16:59:14.324804 [info ] [Thread-1 (]: 5 of 7 START sql incremental model lakesales2.my_second_dbt_model .............. [RUN]
[0m16:59:14.325819 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m16:59:14.325819 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m16:59:14.328889 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m16:59:14.329899 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 16:59:14.325819 => 16:59:14.328889
[0m16:59:14.329899 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m16:59:14.331918 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:59:14.331918 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_second_dbt_model__dbt_tmp as
      -- Use the `ref` function to select from other models

select *
from lakesales.my_first_dbt_model
where id = 1
  
[0m16:59:14.335952 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:59:14.340993 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m16:59:14.340993 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m16:59:14.340993 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales2.my_second_dbt_model
    select `id` from my_second_dbt_model__dbt_tmp


[0m16:59:14.353141 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:59:14.355172 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 16:59:14.329899 => 16:59:14.355172
[0m16:59:14.356191 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '54ae3701-e18d-4299-83b6-7ed201a805ef', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000284137C1BE0>]}
[0m16:59:14.356191 [info ] [Thread-1 (]: 5 of 7 OK created sql incremental model lakesales2.my_second_dbt_model ......... [[32mOK[0m in 0.03s]
[0m16:59:14.357203 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m16:59:14.357203 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:59:14.358251 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m16:59:14.358251 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m16:59:14.359265 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:59:14.361291 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m16:59:14.363316 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 16:59:14.359265 => 16:59:14.363316
[0m16:59:14.363316 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:59:14.364325 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m16:59:14.365334 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m16:59:14.365334 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales2.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m16:59:14.370375 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m16:59:14.371383 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 16:59:14.363316 => 16:59:14.371383
[0m16:59:14.372391 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m16:59:14.372391 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m16:59:14.372391 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:59:14.373414 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m16:59:14.373414 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m16:59:14.373414 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:59:14.376452 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:59:14.377462 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 16:59:14.374431 => 16:59:14.377462
[0m16:59:14.377462 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:59:14.378475 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:59:14.379483 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m16:59:14.379483 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales2.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m16:59:14.384524 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m16:59:14.385534 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 16:59:14.377462 => 16:59:14.385534
[0m16:59:14.386546 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m16:59:14.386546 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m16:59:14.387558 [debug] [MainThread]: On master: ROLLBACK
[0m16:59:14.388581 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:59:14.388581 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m16:59:14.388581 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m16:59:14.389592 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m16:59:14.389592 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m16:59:14.390603 [info ] [MainThread]: 
[0m16:59:14.390603 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.39 seconds (0.39s).
[0m16:59:14.391615 [debug] [MainThread]: Command end result
[0m16:59:14.397671 [info ] [MainThread]: 
[0m16:59:14.397671 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:59:14.398681 [info ] [MainThread]: 
[0m16:59:14.398681 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m16:59:14.399694 [debug] [MainThread]: Command `cli build` succeeded at 16:59:14.398681 after 1.29 seconds
[0m16:59:14.399694 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002841153AB70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028411EAB380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028411E17F50>]}
[0m16:59:14.399694 [debug] [MainThread]: Flushing usage events
[0m17:01:07.723121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022608943920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022608942930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226089412E0>]}


============================== 17:01:07.724129 | 26302bcd-82a8-4691-b511-35e508fb2539 ==============================
[0m17:01:07.724129 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:01:07.724129 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'static_parser': 'True', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'invocation_command': 'dbt ', 'send_anonymous_usage_stats': 'True'}
[0m17:01:07.808089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '26302bcd-82a8-4691-b511-35e508fb2539', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002260895C470>]}
[0m17:01:07.842474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '26302bcd-82a8-4691-b511-35e508fb2539', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022608917260>]}
[0m17:01:07.844490 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m17:01:07.859692 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m17:01:07.860701 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m17:01:07.861708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '26302bcd-82a8-4691-b511-35e508fb2539', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002260885B560>]}
[0m17:01:09.170396 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '26302bcd-82a8-4691-b511-35e508fb2539', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022609BA9C70>]}
[0m17:01:09.188709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '26302bcd-82a8-4691-b511-35e508fb2539', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022609B7A450>]}
[0m17:01:09.189725 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 447 macros, 0 groups, 0 semantic models
[0m17:01:09.191763 [info ] [MainThread]: 
[0m17:01:09.193791 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m17:01:09.194802 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m17:01:09.206998 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m17:01:09.220178 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m17:01:09.231338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '26302bcd-82a8-4691-b511-35e508fb2539', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022609CD0EC0>]}
[0m17:01:09.232347 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:01:09.232347 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:01:09.233373 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m17:01:09.233373 [info ] [MainThread]: 
[0m17:01:09.237416 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m17:01:09.238426 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m17:01:09.238426 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m17:01:09.239435 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m17:01:09.243467 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m17:01:09.245482 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 17:01:09.239435 => 17:01:09.244475
[0m17:01:09.245482 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m17:01:09.273817 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:01:09.273817 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:01:09.274825 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m17:01:09.274825 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m17:01:09.345634 [debug] [Thread-1 (]: SQL status: OK in 0.07000000029802322 seconds
[0m17:01:09.363919 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m17:01:09.365944 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m17:01:09.366958 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m17:01:09.380113 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:01:09.389239 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 17:01:09.245482 => 17:01:09.389239
[0m17:01:09.390274 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '26302bcd-82a8-4691-b511-35e508fb2539', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022609A5C3B0>]}
[0m17:01:09.391288 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model lakesales.my_first_dbt_model ........... [[32mOK[0m in 0.15s]
[0m17:01:09.391288 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m17:01:09.392297 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m17:01:09.392297 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m17:01:09.393312 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m17:01:09.394326 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m17:01:09.394326 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 17:01:09.394326 => 17:01:09.394326
[0m17:01:09.394326 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m17:01:09.408496 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:01:09.408496 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m17:01:09.412528 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:01:09.428731 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:01:09.429745 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table lakesales.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m17:01:09.442891 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:01:09.450970 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m17:01:09.454018 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m17:01:09.455028 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into lakesales.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m17:01:09.468192 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:01:09.472230 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m17:01:09.473237 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:01:09.474246 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 17:01:09.395334 => 17:01:09.474246
[0m17:01:09.475254 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '26302bcd-82a8-4691-b511-35e508fb2539', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002260A24DA90>]}
[0m17:01:09.475254 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file lakesales.sample .................................... [[32mINSERT 2[0m in 0.08s]
[0m17:01:09.476275 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m17:01:09.476275 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:01:09.477286 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m17:01:09.478299 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m17:01:09.478299 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:01:09.486394 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:01:09.487407 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 17:01:09.478299 => 17:01:09.487407
[0m17:01:09.488417 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:01:09.497580 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:01:09.498642 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m17:01:09.499702 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:01:09.512227 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:01:09.513282 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 17:01:09.488417 => 17:01:09.513282
[0m17:01:09.514336 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.04s]
[0m17:01:09.514336 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m17:01:09.515352 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:01:09.515352 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m17:01:09.516361 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m17:01:09.516361 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:01:09.521465 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:01:09.522471 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 17:01:09.516361 => 17:01:09.522471
[0m17:01:09.523488 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:01:09.525517 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:01:09.526527 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m17:01:09.526527 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:01:09.533622 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:01:09.534641 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 17:01:09.523488 => 17:01:09.534641
[0m17:01:09.535651 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m17:01:09.535651 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m17:01:09.537191 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m17:01:09.537704 [info ] [Thread-1 (]: 5 of 7 START sql incremental model lakesales2.my_second_dbt_model .............. [RUN]
[0m17:01:09.538214 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m17:01:09.538725 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m17:01:09.541286 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m17:01:09.542830 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 17:01:09.538725 => 17:01:09.542314
[0m17:01:09.542830 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m17:01:09.545391 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:01:09.545900 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_second_dbt_model__dbt_tmp as
      -- Use the `ref` function to select from other models

select *
from lakesales.my_first_dbt_model
where id = 1
  
[0m17:01:09.549968 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:01:09.558138 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m17:01:09.559149 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m17:01:09.560159 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales2.my_second_dbt_model
    select `id` from my_second_dbt_model__dbt_tmp


[0m17:01:09.577472 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m17:01:09.578489 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 17:01:09.543348 => 17:01:09.578489
[0m17:01:09.579503 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '26302bcd-82a8-4691-b511-35e508fb2539', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002260A24C3B0>]}
[0m17:01:09.579503 [info ] [Thread-1 (]: 5 of 7 OK created sql incremental model lakesales2.my_second_dbt_model ......... [[32mOK[0m in 0.04s]
[0m17:01:09.580517 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m17:01:09.580517 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:01:09.580517 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m17:01:09.581530 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m17:01:09.581530 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:01:09.584584 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:01:09.585597 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 17:01:09.581530 => 17:01:09.585597
[0m17:01:09.585597 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:01:09.587635 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:01:09.589683 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m17:01:09.589683 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales2.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m17:01:09.594745 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m17:01:09.595752 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 17:01:09.586614 => 17:01:09.595752
[0m17:01:09.596761 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m17:01:09.596761 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m17:01:09.596761 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:01:09.597770 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m17:01:09.597770 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m17:01:09.598800 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:01:09.600824 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:01:09.601836 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 17:01:09.598800 => 17:01:09.601836
[0m17:01:09.601836 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:01:09.603857 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:01:09.603857 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m17:01:09.604866 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales2.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m17:01:09.609396 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m17:01:09.610404 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 17:01:09.602846 => 17:01:09.610404
[0m17:01:09.610404 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.01s]
[0m17:01:09.611413 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m17:01:09.612423 [debug] [MainThread]: On master: ROLLBACK
[0m17:01:09.612423 [debug] [MainThread]: Opening a new connection, currently in state init
[0m17:01:09.612423 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m17:01:09.612423 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m17:01:09.613433 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m17:01:09.613433 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m17:01:09.613433 [info ] [MainThread]: 
[0m17:01:09.614459 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.42 seconds (0.42s).
[0m17:01:09.615472 [debug] [MainThread]: Command end result
[0m17:01:09.620522 [info ] [MainThread]: 
[0m17:01:09.621605 [info ] [MainThread]: [32mCompleted successfully[0m
[0m17:01:09.621605 [info ] [MainThread]: 
[0m17:01:09.622608 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m17:01:09.623621 [debug] [MainThread]: Command `cli build` succeeded at 17:01:09.623621 after 1.93 seconds
[0m17:01:09.623621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226088D8110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022609CC8650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022609B29970>]}
[0m17:01:09.623621 [debug] [MainThread]: Flushing usage events
[0m19:06:18.027493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C906DEB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C906D580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C906D130>]}


============================== 19:06:18.030519 | b93b8410-996b-462f-81f7-80587fcbba13 ==============================
[0m19:06:18.030519 [info ] [MainThread]: Running with dbt=1.7.14
[0m19:06:18.031534 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'version_check': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt seed', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:06:18.035586 [info ] [MainThread]: Error importing adapter: No module named 'dbt.adapters.fabricsparknb'
[0m19:06:18.035586 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Could not find adapter type fabricsparknb!
[0m19:06:18.036597 [debug] [MainThread]: Command `dbt seed` failed at 19:06:18.036597 after 0.10 seconds
[0m19:06:18.036597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C8DDFB30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C9053D40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F0C849CB60>]}
[0m19:06:18.037610 [debug] [MainThread]: Flushing usage events
[0m19:06:35.525158 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290EBD18620>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290EBD18EC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290EBD184D0>]}


============================== 19:06:35.526165 | 818a1a64-525a-47e2-8494-d7456859bb4f ==============================
[0m19:06:35.526165 [info ] [MainThread]: Running with dbt=1.7.14
[0m19:06:35.526165 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'warn_error': 'None', 'log_path': 'testproj\\logs', 'debug': 'False', 'fail_fast': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'introspect': 'True', 'static_parser': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m19:06:35.622406 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '818a1a64-525a-47e2-8494-d7456859bb4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290EBD32BA0>]}
[0m19:06:35.656778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '818a1a64-525a-47e2-8494-d7456859bb4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290EBADA7B0>]}
[0m19:06:35.658794 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:06:35.670955 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m19:06:35.671970 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m19:06:35.672980 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '818a1a64-525a-47e2-8494-d7456859bb4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290EBD8A8A0>]}
[0m19:06:37.044412 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '818a1a64-525a-47e2-8494-d7456859bb4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290ED0DDDF0>]}
[0m19:06:37.070327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '818a1a64-525a-47e2-8494-d7456859bb4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290ECEBB5C0>]}
[0m19:06:37.071344 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 453 macros, 0 groups, 0 semantic models
[0m19:06:37.072355 [info ] [MainThread]: 
[0m19:06:37.073369 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:06:37.074380 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:06:37.084502 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m19:06:37.097661 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m19:06:37.107770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '818a1a64-525a-47e2-8494-d7456859bb4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290ED015D30>]}
[0m19:06:37.107770 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:06:37.108782 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:06:37.108782 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:06:37.109790 [info ] [MainThread]: 
[0m19:06:37.112819 [debug] [Thread-1 (]: Began running node model.testproj.my_first_dbt_model
[0m19:06:37.113830 [info ] [Thread-1 (]: 1 of 7 START sql incremental model lakesales.my_first_dbt_model ................ [RUN]
[0m19:06:37.113830 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'model.testproj.my_first_dbt_model'
[0m19:06:37.114921 [debug] [Thread-1 (]: Began compiling node model.testproj.my_first_dbt_model
[0m19:06:37.119970 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_first_dbt_model"
[0m19:06:37.121993 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (compile): 19:06:37.114921 => 19:06:37.121993
[0m19:06:37.123001 [debug] [Thread-1 (]: Began executing node model.testproj.my_first_dbt_model
[0m19:06:37.154388 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:06:37.154897 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:06:37.154897 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_first_dbt_model__dbt_tmp as
      /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  
[0m19:06:37.155405 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:06:37.233382 [debug] [Thread-1 (]: SQL status: OK in 0.07999999821186066 seconds
[0m19:06:37.250615 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_first_dbt_model"
[0m19:06:37.253693 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_first_dbt_model"
[0m19:06:37.253693 [debug] [Thread-1 (]: On model.testproj.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_first_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales.my_first_dbt_model
    select `id` from my_first_dbt_model__dbt_tmp


[0m19:06:37.265788 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m19:06:37.273890 [debug] [Thread-1 (]: Timing info for model.testproj.my_first_dbt_model (execute): 19:06:37.123001 => 19:06:37.273890
[0m19:06:37.274900 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '818a1a64-525a-47e2-8494-d7456859bb4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290ECF77980>]}
[0m19:06:37.275908 [info ] [Thread-1 (]: 1 of 7 OK created sql incremental model lakesales.my_first_dbt_model ........... [[32mOK[0m in 0.16s]
[0m19:06:37.275908 [debug] [Thread-1 (]: Finished running node model.testproj.my_first_dbt_model
[0m19:06:37.276916 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m19:06:37.276916 [info ] [Thread-1 (]: 2 of 7 START seed file lakesales.sample ........................................ [RUN]
[0m19:06:37.277926 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_first_dbt_model, now seed.testproj.sample)
[0m19:06:37.277926 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m19:06:37.278937 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 19:06:37.277926 => 19:06:37.277926
[0m19:06:37.278937 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m19:06:37.294182 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m19:06:37.294182 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m19:06:37.299240 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:06:37.316481 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m19:06:37.317503 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table lakesales.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m19:06:37.332662 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m19:06:37.341760 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m19:06:37.343802 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m19:06:37.343802 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into lakesales.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m19:06:37.359051 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m19:06:37.362076 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m19:06:37.363085 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:06:37.364095 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 19:06:37.278937 => 19:06:37.364095
[0m19:06:37.365108 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '818a1a64-525a-47e2-8494-d7456859bb4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290ED356F30>]}
[0m19:06:37.365108 [info ] [Thread-1 (]: 2 of 7 OK loaded seed file lakesales.sample .................................... [[32mINSERT 2[0m in 0.09s]
[0m19:06:37.365108 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m19:06:37.366123 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:06:37.366123 [info ] [Thread-1 (]: 3 of 7 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m19:06:37.367138 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly seed.testproj.sample, now test.testproj.not_null_my_first_dbt_model_id.5fb22c2710)
[0m19:06:37.367138 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:06:37.374246 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:06:37.375254 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (compile): 19:06:37.367138 => 19:06:37.375254
[0m19:06:37.376262 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:06:37.386391 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:06:37.387400 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"
[0m19:06:37.387400 [debug] [Thread-1 (]: On test.testproj.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_first_dbt_model_id.5fb22c2710"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales.my_first_dbt_model
where id is null



      
    ) dbt_internal_test
[0m19:06:37.399549 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m19:06:37.400556 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_first_dbt_model_id.5fb22c2710 (execute): 19:06:37.376262 => 19:06:37.400556
[0m19:06:37.401568 [info ] [Thread-1 (]: 3 of 7 PASS not_null_my_first_dbt_model_id ..................................... [[32mPASS[0m in 0.04s]
[0m19:06:37.402584 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_first_dbt_model_id.5fb22c2710
[0m19:06:37.402584 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:06:37.403609 [info ] [Thread-1 (]: 4 of 7 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m19:06:37.403609 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_first_dbt_model_id.5fb22c2710, now test.testproj.unique_my_first_dbt_model_id.16e066b321)
[0m19:06:37.403609 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:06:37.407667 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m19:06:37.408674 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (compile): 19:06:37.404637 => 19:06:37.408674
[0m19:06:37.408674 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:06:37.410694 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m19:06:37.411703 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_first_dbt_model_id.16e066b321"
[0m19:06:37.411703 [debug] [Thread-1 (]: On test.testproj.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_first_dbt_model_id.16e066b321"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales.my_first_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m19:06:37.416763 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m19:06:37.419869 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_first_dbt_model_id.16e066b321 (execute): 19:06:37.409683 => 19:06:37.418865
[0m19:06:37.420881 [info ] [Thread-1 (]: 4 of 7 PASS unique_my_first_dbt_model_id ....................................... [[32mPASS[0m in 0.02s]
[0m19:06:37.420881 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_first_dbt_model_id.16e066b321
[0m19:06:37.421895 [debug] [Thread-1 (]: Began running node model.testproj.my_second_dbt_model
[0m19:06:37.421895 [info ] [Thread-1 (]: 5 of 7 START sql incremental model lakesales2.my_second_dbt_model .............. [RUN]
[0m19:06:37.422905 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.unique_my_first_dbt_model_id.16e066b321, now model.testproj.my_second_dbt_model)
[0m19:06:37.423915 [debug] [Thread-1 (]: Began compiling node model.testproj.my_second_dbt_model
[0m19:06:37.425944 [debug] [Thread-1 (]: Writing injected SQL for node "model.testproj.my_second_dbt_model"
[0m19:06:37.426964 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (compile): 19:06:37.423915 => 19:06:37.426964
[0m19:06:37.427974 [debug] [Thread-1 (]: Began executing node model.testproj.my_second_dbt_model
[0m19:06:37.429997 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:06:37.429997 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

  
    create or replace temporary view my_second_dbt_model__dbt_tmp as
      -- Use the `ref` function to select from other models

select *
from lakesales.my_first_dbt_model
where id = 1
  
[0m19:06:37.435088 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:06:37.440145 [debug] [Thread-1 (]: Writing runtime sql for node "model.testproj.my_second_dbt_model"
[0m19:06:37.441155 [debug] [Thread-1 (]: Using fabricsparknb connection "model.testproj.my_second_dbt_model"
[0m19:06:37.441155 [debug] [Thread-1 (]: On model.testproj.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "model.testproj.my_second_dbt_model"} */
/*{"project_root": "testproj"}*/

    insert into table lakesales2.my_second_dbt_model
    select `id` from my_second_dbt_model__dbt_tmp


[0m19:06:37.456871 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m19:06:37.457883 [debug] [Thread-1 (]: Timing info for model.testproj.my_second_dbt_model (execute): 19:06:37.427974 => 19:06:37.457883
[0m19:06:37.458894 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '818a1a64-525a-47e2-8494-d7456859bb4f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290ED409CD0>]}
[0m19:06:37.458894 [info ] [Thread-1 (]: 5 of 7 OK created sql incremental model lakesales2.my_second_dbt_model ......... [[32mOK[0m in 0.04s]
[0m19:06:37.459903 [debug] [Thread-1 (]: Finished running node model.testproj.my_second_dbt_model
[0m19:06:37.459903 [debug] [Thread-1 (]: Began running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:06:37.460911 [info ] [Thread-1 (]: 6 of 7 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m19:06:37.460911 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly model.testproj.my_second_dbt_model, now test.testproj.not_null_my_second_dbt_model_id.151b76d778)
[0m19:06:37.461921 [debug] [Thread-1 (]: Began compiling node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:06:37.463953 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m19:06:37.464969 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (compile): 19:06:37.461921 => 19:06:37.464969
[0m19:06:37.466008 [debug] [Thread-1 (]: Began executing node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:06:37.467021 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m19:06:37.469040 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.not_null_my_second_dbt_model_id.151b76d778"
[0m19:06:37.469040 [debug] [Thread-1 (]: On test.testproj.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.not_null_my_second_dbt_model_id.151b76d778"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from lakesales2.my_second_dbt_model
where id is null



      
    ) dbt_internal_test
[0m19:06:37.474097 [debug] [Thread-1 (]: SQL status: OK in 0.009999999776482582 seconds
[0m19:06:37.475105 [debug] [Thread-1 (]: Timing info for test.testproj.not_null_my_second_dbt_model_id.151b76d778 (execute): 19:06:37.466008 => 19:06:37.475105
[0m19:06:37.475105 [info ] [Thread-1 (]: 6 of 7 PASS not_null_my_second_dbt_model_id .................................... [[32mPASS[0m in 0.01s]
[0m19:06:37.476117 [debug] [Thread-1 (]: Finished running node test.testproj.not_null_my_second_dbt_model_id.151b76d778
[0m19:06:37.476117 [debug] [Thread-1 (]: Began running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:06:37.477125 [info ] [Thread-1 (]: 7 of 7 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m19:06:37.477125 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.testproj.not_null_my_second_dbt_model_id.151b76d778, now test.testproj.unique_my_second_dbt_model_id.57a0f8c493)
[0m19:06:37.477125 [debug] [Thread-1 (]: Began compiling node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:06:37.481210 [debug] [Thread-1 (]: Writing injected SQL for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:06:37.482228 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (compile): 19:06:37.478139 => 19:06:37.482228
[0m19:06:37.483239 [debug] [Thread-1 (]: Began executing node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:06:37.484251 [debug] [Thread-1 (]: Writing runtime sql for node "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:06:37.485274 [debug] [Thread-1 (]: Using fabricsparknb connection "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"
[0m19:06:37.486290 [debug] [Thread-1 (]: On test.testproj.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "test.testproj.unique_my_second_dbt_model_id.57a0f8c493"} */
/*{"project_root": "testproj"}*/
select
      INT(count(*)) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

select
    id as unique_field,
    count(*) as n_records

from lakesales2.my_second_dbt_model
where id is not null
group by id
having count(*) > 1



      
    ) dbt_internal_test
[0m19:06:37.490345 [debug] [Thread-1 (]: SQL status: OK in 0.0 seconds
[0m19:06:37.491357 [debug] [Thread-1 (]: Timing info for test.testproj.unique_my_second_dbt_model_id.57a0f8c493 (execute): 19:06:37.483239 => 19:06:37.491357
[0m19:06:37.492371 [info ] [Thread-1 (]: 7 of 7 PASS unique_my_second_dbt_model_id ...................................... [[32mPASS[0m in 0.02s]
[0m19:06:37.492371 [debug] [Thread-1 (]: Finished running node test.testproj.unique_my_second_dbt_model_id.57a0f8c493
[0m19:06:37.493386 [debug] [MainThread]: On master: ROLLBACK
[0m19:06:37.494399 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:06:37.494399 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:06:37.494399 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:06:37.494399 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:06:37.495409 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:06:37.495409 [info ] [MainThread]: 
[0m19:06:37.495409 [info ] [MainThread]: Finished running 2 incremental models, 1 seed, 4 tests in 0 hours 0 minutes and 0.42 seconds (0.42s).
[0m19:06:37.496454 [debug] [MainThread]: Command end result
[0m19:06:37.501523 [info ] [MainThread]: 
[0m19:06:37.502539 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:06:37.502539 [info ] [MainThread]: 
[0m19:06:37.502539 [info ] [MainThread]: Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
[0m19:06:37.503549 [debug] [MainThread]: Command `cli build` succeeded at 19:06:37.503549 after 2.00 seconds
[0m19:06:37.503549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290EBAFD970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290EB677380>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000290ED0B2660>]}
[0m19:06:37.504557 [debug] [MainThread]: Flushing usage events
[0m19:18:16.441956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D2A0F7590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D2A0F7CB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D2A0F7440>]}


============================== 19:18:16.442992 | 0ee545a6-b4c2-48b2-8380-38e01ca7081b ==============================
[0m19:18:16.442992 [info ] [MainThread]: Running with dbt=1.7.14
[0m19:18:16.444012 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt ', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m19:18:16.529428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0ee545a6-b4c2-48b2-8380-38e01ca7081b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D296B9430>]}
[0m19:18:16.562833 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0ee545a6-b4c2-48b2-8380-38e01ca7081b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D2B1CE600>]}
[0m19:18:16.564869 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m19:18:16.580035 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m19:18:16.582083 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m19:18:16.582083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '0ee545a6-b4c2-48b2-8380-38e01ca7081b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D2B1A5700>]}
[0m19:18:18.006191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0ee545a6-b4c2-48b2-8380-38e01ca7081b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D2B5A4950>]}
[0m19:18:18.021388 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0ee545a6-b4c2-48b2-8380-38e01ca7081b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D2B5238C0>]}
[0m19:18:18.022399 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 453 macros, 0 groups, 0 semantic models
[0m19:18:18.022399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ee545a6-b4c2-48b2-8380-38e01ca7081b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D2B39A3C0>]}
[0m19:18:18.024424 [info ] [MainThread]: 
[0m19:18:18.024424 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m19:18:18.025438 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m19:18:18.033524 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales'
[0m19:18:18.047739 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales, now list_None_lakesales2)
[0m19:18:18.058929 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0ee545a6-b4c2-48b2-8380-38e01ca7081b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D2B44AC30>]}
[0m19:18:18.058929 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:18:18.059940 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:18:18.059940 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m19:18:18.060949 [info ] [MainThread]: 
[0m19:18:18.062974 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m19:18:18.062974 [info ] [Thread-1 (]: 1 of 1 START seed file lakesales.sample ........................................ [RUN]
[0m19:18:18.063983 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m19:18:18.063983 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m19:18:18.064997 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 19:18:18.064997 => 19:18:18.064997
[0m19:18:18.064997 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m19:18:18.086253 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m19:18:18.086253 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m19:18:18.087277 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:18:18.162806 [debug] [Thread-1 (]: SQL status: OK in 0.07999999821186066 seconds
[0m19:18:18.184170 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:18:18.184170 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m19:18:18.185179 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table lakesales.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m19:18:18.201456 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m19:18:18.211629 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m19:18:18.215738 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m19:18:18.216760 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into lakesales.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m19:18:18.233184 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m19:18:18.237231 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m19:18:18.247399 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:18:18.249451 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 19:18:18.066010 => 19:18:18.249451
[0m19:18:18.249451 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ee545a6-b4c2-48b2-8380-38e01ca7081b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D2B290D10>]}
[0m19:18:18.250459 [info ] [Thread-1 (]: 1 of 1 OK loaded seed file lakesales.sample .................................... [[32mINSERT 2[0m in 0.19s]
[0m19:18:18.251471 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m19:18:18.252485 [debug] [MainThread]: On master: ROLLBACK
[0m19:18:18.252485 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:18:18.252485 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m19:18:18.253497 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m19:18:18.253497 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m19:18:18.253497 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m19:18:18.254505 [info ] [MainThread]: 
[0m19:18:18.254505 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.23 seconds (0.23s).
[0m19:18:18.255531 [debug] [MainThread]: Command end result
[0m19:18:18.260572 [info ] [MainThread]: 
[0m19:18:18.261575 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:18:18.262078 [info ] [MainThread]: 
[0m19:18:18.262078 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m19:18:18.263089 [debug] [MainThread]: Command `cli seed` succeeded at 19:18:18.263089 after 1.85 seconds
[0m19:18:18.263089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D2B44AF60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D2B494A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012D2B39A3C0>]}
[0m19:18:18.263089 [debug] [MainThread]: Flushing usage events
[0m21:10:56.166278 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DCF839DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DCFCE03E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DCFCE0D10>]}


============================== 21:10:56.167286 | 2460a3d5-b431-4f08-802e-ebcb59398b34 ==============================
[0m21:10:56.167286 [info ] [MainThread]: Running with dbt=1.7.14
[0m21:10:56.167286 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'log_path': 'testproj\\logs', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'log_format': 'default', 'invocation_command': 'dbt ', 'static_parser': 'True', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'send_anonymous_usage_stats': 'True'}
[0m21:10:56.255338 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2460a3d5-b431-4f08-802e-ebcb59398b34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DCFD5E7E0>]}
[0m21:10:56.287897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2460a3d5-b431-4f08-802e-ebcb59398b34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DCFCB7200>]}
[0m21:10:56.288928 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m21:10:56.297711 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m21:10:56.298740 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m21:10:56.299265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2460a3d5-b431-4f08-802e-ebcb59398b34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DD0DC1160>]}
[0m21:10:57.373913 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2460a3d5-b431-4f08-802e-ebcb59398b34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DD10D7F80>]}
[0m21:10:57.391095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2460a3d5-b431-4f08-802e-ebcb59398b34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DD0F38F20>]}
[0m21:10:57.391095 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 453 macros, 0 groups, 0 semantic models
[0m21:10:57.392105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2460a3d5-b431-4f08-802e-ebcb59398b34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DD10D7F80>]}
[0m21:10:57.393111 [info ] [MainThread]: 
[0m21:10:57.393111 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m21:10:57.394121 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m21:10:57.401186 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_None_lakesales2'
[0m21:10:57.422375 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_None_lakesales2, now list_None_lakesales)
[0m21:10:57.432476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2460a3d5-b431-4f08-802e-ebcb59398b34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DD113D550>]}
[0m21:10:57.432476 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:10:57.433482 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m21:10:57.433482 [info ] [MainThread]: Concurrency: 1 threads (target='fabricspark-devnb')
[0m21:10:57.433482 [info ] [MainThread]: 
[0m21:10:57.435496 [debug] [Thread-1 (]: Began running node seed.testproj.sample
[0m21:10:57.436503 [info ] [Thread-1 (]: 1 of 1 START seed file lakesales.sample ........................................ [RUN]
[0m21:10:57.436503 [debug] [Thread-1 (]: Acquiring new fabricsparknb connection 'seed.testproj.sample'
[0m21:10:57.437511 [debug] [Thread-1 (]: Began compiling node seed.testproj.sample
[0m21:10:57.437511 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (compile): 21:10:57.437511 => 21:10:57.437511
[0m21:10:57.437511 [debug] [Thread-1 (]: Began executing node seed.testproj.sample
[0m21:10:57.456670 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m21:10:57.456670 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/
drop table if exists lakesales.sample
[0m21:10:57.456670 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m21:10:57.518327 [debug] [Thread-1 (]: SQL status: OK in 0.05999999865889549 seconds
[0m21:10:57.535457 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:10:57.535457 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m21:10:57.535457 [debug] [Thread-1 (]: On seed.testproj.sample: /* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */
/*{"project_root": "testproj"}*/

    create table lakesales.sample (`name` string,`id` bigint)
    
    
    
    
    
  
[0m21:10:57.551612 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m21:10:57.560701 [debug] [Thread-1 (]: Inserting batches of 500 records
[0m21:10:57.563720 [debug] [Thread-1 (]: Using fabricsparknb connection "seed.testproj.sample"
[0m21:10:57.563720 [debug] [Thread-1 (]: On seed.testproj.sample: /*{"project_root": "testproj"}*/
/* {"app": "dbt", "dbt_version": "1.7.14", "profile_name": "fabric-spark-testnb", "target_name": "fabricspark-devnb", "node_id": "seed.testproj.sample"} */

          insert into lakesales.sample values
          (cast(%s as string),cast(%s as bigint)),(cast(%s as string),cast(%s as bigint))
      ...
[0m21:10:57.579871 [debug] [Thread-1 (]: SQL status: OK in 0.019999999552965164 seconds
[0m21:10:57.582897 [debug] [Thread-1 (]: Writing runtime SQL for node "seed.testproj.sample"
[0m21:10:57.591957 [debug] [Thread-1 (]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m21:10:57.591957 [debug] [Thread-1 (]: Timing info for seed.testproj.sample (execute): 21:10:57.437511 => 21:10:57.591957
[0m21:10:57.592967 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2460a3d5-b431-4f08-802e-ebcb59398b34', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DD0F479B0>]}
[0m21:10:57.592967 [info ] [Thread-1 (]: 1 of 1 OK loaded seed file lakesales.sample .................................... [[32mINSERT 2[0m in 0.16s]
[0m21:10:57.593979 [debug] [Thread-1 (]: Finished running node seed.testproj.sample
[0m21:10:57.595003 [debug] [MainThread]: On master: ROLLBACK
[0m21:10:57.595003 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:10:57.595003 [debug] [MainThread]: fabricsparknb adapter: Reusing session: 0
[0m21:10:57.596015 [debug] [MainThread]: fabricsparknb adapter: NotImplemented: rollback
[0m21:10:57.596015 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: add_begin_query
[0m21:10:57.596015 [debug] [MainThread]: Microsoft Fabric-Spark adapter: NotImplemented: commit
[0m21:10:57.597022 [info ] [MainThread]: 
[0m21:10:57.597022 [info ] [MainThread]: Finished running 1 seed in 0 hours 0 minutes and 0.20 seconds (0.20s).
[0m21:10:57.598030 [debug] [MainThread]: Command end result
[0m21:10:57.602062 [info ] [MainThread]: 
[0m21:10:57.603071 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:10:57.603071 [info ] [MainThread]: 
[0m21:10:57.604078 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m21:10:57.604078 [debug] [MainThread]: Command `cli seed` succeeded at 21:10:57.604078 after 1.46 seconds
[0m21:10:57.605084 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DCFCE0320>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DCFCE0F20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023DCFCE08F0>]}
[0m21:10:57.605084 [debug] [MainThread]: Flushing usage events
[0m17:42:29.863391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000266DD7706E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000266DD7708C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000266DD7704A0>]}


============================== 17:42:29.867469 | 191e187f-2a82-4bdb-8f41-ce69bf5cde45 ==============================
[0m17:42:29.867469 [info ] [MainThread]: Running with dbt=1.7.14
[0m17:42:29.867469 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'warn_error': 'None', 'version_check': 'True', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'debug': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'invocation_command': 'dbt build', 'log_format': 'default', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m17:42:29.871524 [info ] [MainThread]: Error importing adapter: No module named 'dbt.adapters.fabricsparknb'
[0m17:42:29.872538 [error] [MainThread]: Encountered an error:
Runtime Error
  Credentials in profile "fabric-spark-testnb", target "fabricspark-devnb" invalid: Runtime Error
    Could not find adapter type fabricsparknb!
[0m17:42:29.873548 [debug] [MainThread]: Command `dbt build` failed at 17:42:29.873548 after 0.09 seconds
[0m17:42:29.873548 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000266DD79FD40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000266DCA2EFF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000266DD5CD070>]}
[0m17:42:29.873548 [debug] [MainThread]: Flushing usage events
[0m18:14:25.194420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013319331FA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133193329C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013319333800>]}


============================== 18:14:25.195438 | a92cc3c2-e824-4fba-a400-2e6d003c5190 ==============================
[0m18:14:25.195438 [info ] [MainThread]: Running with dbt=1.7.14
[0m18:14:25.195438 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': 'C:\\Users\\jramp\\.dbt', 'debug': 'False', 'fail_fast': 'False', 'log_path': 'testproj\\logs', 'version_check': 'True', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptions(include=[], exclude=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m18:14:25.295330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a92cc3c2-e824-4fba-a400-2e6d003c5190', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133191AF770>]}
[0m18:14:25.330834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a92cc3c2-e824-4fba-a400-2e6d003c5190', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001331898F440>]}
[0m18:14:25.332851 [info ] [MainThread]: Registered adapter: fabricsparknb=1.7.0
[0m18:14:25.346997 [debug] [MainThread]: checksum: ccbe0bccecb23e0b0bcca76d525b7d642e28acd474046360f7f33e61c6a2b781, vars: {}, profile: , target: , version: 1.7.14
[0m18:14:25.348008 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m18:14:25.348008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a92cc3c2-e824-4fba-a400-2e6d003c5190', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013318F1C9E0>]}
[0m18:14:26.737306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a92cc3c2-e824-4fba-a400-2e6d003c5190', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001331A582870>]}
[0m18:14:26.760629 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a92cc3c2-e824-4fba-a400-2e6d003c5190', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001331A713B30>]}
[0m18:14:26.760629 [info ] [MainThread]: Found 2 models, 1 seed, 4 tests, 0 sources, 0 exposures, 0 metrics, 454 macros, 0 groups, 0 semantic models
[0m18:14:26.761638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a92cc3c2-e824-4fba-a400-2e6d003c5190', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001331A6B2450>]}
[0m18:14:26.762646 [info ] [MainThread]: 
[0m18:14:26.763655 [debug] [MainThread]: Acquiring new fabricsparknb connection 'master'
[0m18:14:26.764662 [debug] [ThreadPool]: Acquiring new fabricsparknb connection 'list_schemas'
[0m18:14:26.765672 [info ] [MainThread]: 
[0m18:14:26.765672 [info ] [MainThread]: Finished running  in 0 hours 0 minutes and 0.00 seconds (0.00s).
[0m18:14:26.766679 [error] [MainThread]: Encountered an error:
[Errno 2] No such file or directory: 'testproj/metaextracts/ListSchemas.json'
[0m18:14:26.866542 [error] [MainThread]: Traceback (most recent call last):
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\.env\Lib\site-packages\dbt\cli\requires.py", line 91, in wrapper
    result, success = func(*args, **kwargs)
                      ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\.env\Lib\site-packages\dbt\cli\requires.py", line 76, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\.env\Lib\site-packages\dbt\cli\requires.py", line 169, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\.env\Lib\site-packages\dbt\cli\requires.py", line 198, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\.env\Lib\site-packages\dbt\cli\requires.py", line 245, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\.env\Lib\site-packages\dbt\cli\requires.py", line 278, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\.env\Lib\site-packages\dbt\cli\main.py", line 762, in seed
    results = task.run()
              ^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\.env\Lib\site-packages\dbt\task\runnable.py", line 502, in run
    result = self.execute_with_hooks(selected_uids)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\.env\Lib\site-packages\dbt\task\runnable.py", line 462, in execute_with_hooks
    self.before_run(adapter, selected_uids)
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\.env\Lib\site-packages\dbt\task\run.py", line 450, in before_run
    self.create_schemas(adapter, required_schemas)
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\.env\Lib\site-packages\dbt\task\runnable.py", line 601, in create_schemas
    existing_schemas_lowered.update(ls_future.result())
                                    ^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\jramp\AppData\Local\Programs\Python\Python312\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\.env\Lib\site-packages\dbt\utils.py", line 471, in connected
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\.env\Lib\site-packages\dbt\task\runnable.py", line 578, in list_schemas
    for s in adapter.list_schemas(database_quoted)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\dbt\adapters\fabricsparknb\impl.py", line 509, in list_schemas
    results = catalog.ListSchemas(profile=self.config)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jramp\source\gitjr\dbt-fabricsparknb\dbt\adapters\fabricsparknb\catalog.py", line 76, in ListSchemas
    with io.open(profile.project_root + '/metaextracts/ListSchemas.json', 'r') as file:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'testproj/metaextracts/ListSchemas.json'

[0m18:14:26.869584 [debug] [MainThread]: Command `cli seed` failed at 18:14:26.869584 after 1.71 seconds
[0m18:14:26.869584 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133181DC350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000133192BBEF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001331A768B00>]}
[0m18:14:26.870595 [debug] [MainThread]: Flushing usage events
