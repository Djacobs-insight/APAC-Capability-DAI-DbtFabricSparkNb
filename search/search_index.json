{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Dbt Fabric Spark Notebook Generator (Dbt-FabricSparkNb)","text":"<p><p>\"The first and only dbt adapter for a true, modern, software-as-a-service (SAAS) Lakehouse.\"</p></p>"},{"location":"#what-is-it","title":"What is it","text":"<p>The <code>dbt-fabricsparknb</code> package contains all of the code enabling dbt to work with Microsoft Fabric WITHOUT a connection endpoint like livy or the Datawarehouse SQL endpoint. </p>"},{"location":"#why-did-we-build-it","title":"Why did we build it?","text":"<p>As a team of data specialists we have been working with dbt for a number of years. We have found that dbt is a powerful tool for data transformation, but it has some limitations. We have built this adapter to address some of these limitations and to make it easier to work with dbt in a modern, software-as-a-service (SAAS) lakehouse environment. </p>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>Dbt-FabricSparkNb works by leverging the power of the dbt-core, and the dbt-fabrickspark apater to create a new adapter. As such, it can be described as a \"child apater\" of dbt-fabrickspark. </p> <p>The adapter inherits all of the functionality of the dbt-fabrickspark adapter and simply extends it to meet the unique requirements of our project.</p> <p>Consequently, to use this adapter, you will need to install the dbt-fabrickspark adapter and then install the dbt-fabricksparknb adapter.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> Support for SAAS only lakehouse architecture (No PAAS components requried)</li> <li> Support for lightweight, disconnected local development workflow</li> <li> Fully featured with modern data transformation capabilities such as data lineage, data catalog, data quality checks and templated data transformation activities</li> <li> Opensource and free to use</li> <li> Extensible and customisable</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p> User Guide</p> <p>For data engineers looking to use the adapter to create data transformation projects</p> <p> User Guide</p> <p>Note</p> <p>This is the guide appropriate for MOST users.</p> </li> <li> <p> Developer Guide</p> <p>For advanced users looking to extend or customise the adapter</p> <p> Developer Guide</p> <p>Danger</p> <p>This is the guide for advanced users only.</p> </li> <li> <p> Documentation Guide</p> <p>For users who are looking to contribute to the adapter documentation</p> <p> Documentation Guide</p> <p>Warning</p> <p>This guide is still under construction</p> </li> </ul>"},{"location":"#contributing","title":"Contributing","text":""},{"location":"#branching","title":"Branching","text":"<p>When creating a branch to work on from please use the branch name of <code>feature/YourBranchName</code>. The case on <code>feature/</code> matters so please make sure to keep it lower case. Pull requests are to be made into the \"dev\" branch only. Any pull requests made into \"Main\" will be removed and not merged.</p>"},{"location":"#community","title":"Community","text":""},{"location":"#logging-to-delta","title":"Logging to Delta","text":"<p>Logging was previously done to a log file saved in the lakehouse and in json format. This has been changed to now log to a delta table in the lakehouse.</p> <p>It works using 2 tables \"batch\" and \"execution_log\". At the start of the ETL the Prepare step will check if the tables exist and if they don't they will be created. This is followed by a check for an \"open\" batch and where the batch is still open it will fail. </p> <p>If you need to close the batch manually, this code is available at the end of the master notebook. </p> <p>If this check passes, a batch will be opened. There are steps in each master numbered notebook to check for failures in previousn notebook runs and this is done using the open batch so previous ETL executions with failures are not picked up and return false stops on the current execution.</p>"},{"location":"developer_guide/","title":"Developer Guide","text":""},{"location":"developer_guide/applications_setup/","title":"Applications Required for Development (Windows)","text":"<p>This section covers the applications required for developing using the dbt framework. The applications are:</p> <ul> <li>python (latest version available)</li> <li>Visual Studio Code</li> <li>GIT for Windows</li> <li>OneLake Explorer (Preview)</li> </ul>"},{"location":"developer_guide/applications_setup/#install-python","title":"Install python","text":"<p>First you need to install python. This can be done from this link Download Python | Python.org download the latest version and install it. </p> <p>NOTE: Make sure to tick the box on the first window of the installation to Add Python.exe to PATH</p> <p>Use Install now for standard installation.</p> <p>Once installed, confirm installation by opening the command line. WindowsKey + R then enter \u201ccmd\u201d click OK. </p> <p>Type the following and you should get a reply similar to the screenshot below. <code>command prompt  \u201cpython --version\u201d</code></p> <p></p>"},{"location":"developer_guide/applications_setup/#install-visual-studio-code","title":"Install Visual Studio Code","text":"<p>Browse to website Visual Studio Code - Code Editing. Redefined and download Visual Studio Code and then open the file.</p> <p>Select the standard options and install.</p>"},{"location":"developer_guide/applications_setup/#install-git-for-windows","title":"Install GIT for Windows","text":"<p>Browse to website Git - Downloading Package (git-scm.com) and download the latest by clicking \u201cClick here to download\u201d option. Open the file and install following the step examples.</p> <p></p> <p>Select the standard options until you get these next steps.</p> <p>This step will ask you about your default git application please change this to Use Visual Studio Code as Git default. </p> <p></p> <p>This step will ask you about your default git console, please select Use Windows default console window.</p> <p></p> <p>The rest of the installation options should be standard unless you need to change them for other reasons.</p>"},{"location":"developer_guide/applications_setup/#onelake-explorer-preview","title":"OneLake Explorer (Preview)","text":"<p>Browse to website OneLake Explorer and browse down the page to Installation instructions and download OneLake file explorer and then open the file.</p> <p>Click install and follow standard install options. </p> <p>Opening up for the first time will require you to login using your Fabric tenant details. You will then be able to access the Lakehouses from your Windows Explorer.</p> <p></p> <p>This concludes the required applications.</p>"},{"location":"developer_guide/dbt_setup/","title":"Setting Up dbt","text":"<p>The following sections are covered in this document:</p> <ul> <li>Repo clone</li> <li>python environment setup</li> <li>dbt installation</li> </ul>"},{"location":"developer_guide/dbt_setup/#repo-clone","title":"Repo Clone","text":"<p>First you need to clone this repo locally using Visual studio code. For these instructions the feature/dev or dev branch will be the branches to work with.</p> <p>If you do not get the Clone Repository option when selecting Source Control from the menu, then you have not installed GIT and will need to complete that first.</p> <p></p>"},{"location":"developer_guide/dbt_setup/#python-environment-setup","title":"Python Environment Setup","text":"<p>Once the repo has been cloned you can open a terminal window in VS Code and open a bash console.</p> <p>In new terminal window there is a plus symbol with a drop down. Select this drop down and click Git Bash. This will open a bash console. </p> <p></p> <p>To create a virtual python environment execute the code below, \"dbt-env\" being the name of your virtual environment: <pre><code># Python Virtual Environment\npython -m venv dbt-env\n</code></pre></p> <p>To activate the environment execute this: <pre><code># Python Virtual Environment\nsource dbt-env/Scripts/activate\n</code></pre></p> <p></p> <p>The virtual environment would have created a folder structure in your repo. This can be excluded in your gitignore file. If you used the default above it is already in the gitignore file.</p>"},{"location":"developer_guide/dbt_setup/#dbt-installation","title":"dbt Installation","text":"<p>Still in the bash console and having your virtual environment active, you can execute the following command to install all the components required for this dbt framework. The requirements.txt file is in the root of the repo. <pre><code># dbt installation\npip install -r requirements.txt\n</code></pre> NOTE: The installation can take sometime to complete. It may look like it's hanging but it is busy executing. If you close the installation you can restart it using the same command above. It will skip any components already installed. </p> <p>This concludes the dbt installation.</p>"},{"location":"developer_guide/framework_setup/","title":"Framework Setup","text":""},{"location":"developer_guide/framework_setup/#fabric-workspace-setup","title":"Fabric Workspace Setup","text":"<p>Here we will create a new dbt project and configure it to use the dbt-fabricsparknb adapter. But, before we do this we need to gather some information from the Power BI / Fabric Portal. To do this, follow the steps below:</p> <ol> <li>Open the Power BI Portal and navigate to the workspace you want to use for development. If necessary, create a new workspace.</li> <li>Ensure that the workspace is Fabric enabled. If not, enable it.</li> <li>Make sure that there is at least one Datalake in the workspace.</li> <li>Get the connection details for the workspace. <ol> <li>Get the lakehouse name, the workspace id, and the lakehouse id. </li> <li>The lakehouse name and workspace name are easily viewed from the fabric / power bi portal.</li> <li>The easiest way to get this information is to <ol> <li>Navigate to a file or folder in the lakehouse, </li> <li>click on the three dots to the right of the file or folder name, and select \"Properties\". Details will be displayed in the properties window. </li> <li>From these properties select copy url and paste it into a text editor. The workspace id is the first GUID in the URL, the lakehouse id is the second GUID in the URL. </li> <li>In the example below, the workspace id is <code>4f0cb887-047a-48a1-98c3-ebdb38c784c2</code> and the lakehouse id is <code>aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9</code>.</li> </ol> </li> </ol> </li> </ol> <p>https://onelake.dfs.fabric.microsoft.com/4f0cb887-047a-48a1-98c3-ebdb38c784c2/aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9/Files/notebooks</p>"},{"location":"developer_guide/framework_setup/#create-dbt-project","title":"Create Dbt Project","text":"<p>Once you have taken note of the workspace id, lakehouse id, and lakehouse name, you can create a new dbt project and configure it to use the dbt-fabricsparknb adapter. To do this, run the code shown below:</p> <p>!&gt; Important Note when asked to select the adapter choose <code>dbt-fabricksparknb</code>. If you can't see the adapter, first install the dbt-fabricsparknb package from repository. During this process you will also be asked for the <code>workspace id</code>, <code>lakehouse id</code>, and <code>lakehouse name</code>. Use the values you gathered from the Power BI Portal. </p> <pre><code># Create your dbt project directories and profiles.yml file\ndbt init my_project # Note that the name of the project is arbitrary... call it whatever you like\n</code></pre> <p>The command above will create a new directory called <code>my_project</code>. Within this directory you will find a <code>dbt_project.yml</code> file. Open this file in your favourite text editor and note that it should look like the example below except that in your case my_project will be replaced with the name of the project you created above.:</p> <pre><code># Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'my_project'\nversion: '1.0.0'\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: 'my_project'\n\n# These configurations specify where dbt should look for different types of files.\n# The `model-paths` config, for example, states that models in this project can be\n# found in the \"models/\" directory. You probably won't need to change these!\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analyses\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"seeds\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\n\nclean-targets:         # directories to be removed by `dbt clean`\n  - \"target\"\n  - \"dbt_packages\"\n\n\n# Configuring models\n# Full documentation: https://docs.getdbt.com/docs/configuring-models\n\n# In this example config, we tell dbt to build all models in the example/\n# directory as views. These settings can be overridden in the individual model\n# files using the ` config(...) ` macro.\nmodels:\n  test4:\n    # Config indicated by + and applies to all files under models/example/\n    example:\n      +materialized: view\n</code></pre> <p>The dbt init command will also update your <code>profiles.yml</code> file with a profile matching your dbt project name. Open this file in VS Code. This file can be found in \"./%USER_DIRECTORY%/.dbt/\"</p> <p>When run this will display a file similar to the one below. Check that your details are correct. </p> <pre><code>my_project:\n  outputs:\n    dev:\n      auth: cli #remove\n      client_id: dlkdjl #remove\n      client_scrent: dlkdjl #remove\n      connect_retries: 0 #remove\n      connect_timeout: 0 #remove\n      endpoint: dkld #remove\n      lakehouse: 'lakehouse' #the name of your lakehouse\n      lakehouseid: 'aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9' #the guid of your lakehouse\n      method: livy\n      schema: dbo #the schema you want to use\n      tenant_id: '72f988bf-86f1-41af-91ab-2d7cd011db47' #your power bi tenant id\n      threads: 1 #the number of threads to use\n      type: fabricsparknb #the type of adapter to use.. always use fabricsparknb\n      workspaceid: '4f0cb887-047a-48a1-98c3-ebdb38c784c2' #the guid of your workspace\n  target: dev\n</code></pre> <p>To complete the newly created project you will need to copy some directories from the project called \"ptfproj\" dbt project. Copy ptfproj/macros/ and ptfproj/metaextracts/ directories with their files into your new dbt project. Overwrite any directories or files if they exist. Now in metaextracts the file ListSchemas.json contains the lakehouses in your workspace. You can manually update this file.</p> <pre><code>[{\"namespace\":\"lh_raw\"},{\"namespace\":\"lh_conformed\"},{\"namespace\":\"lh_consolidated\"}]\n</code></pre>"},{"location":"developer_guide/framework_setup/#review-the-build-python-script","title":"Review the build python Script","text":"<p>To ensure a successful dbt build process, please verify that the Python script test_pre_install.py exists in the root directory of your repository</p> <pre><code>from dbt.adapters.fabricsparknb import utils as utils\nimport os \nimport sys\n\nutils.RunDbtProjectArg(PreInstall=True,argv = sys.argv)\n</code></pre>"},{"location":"developer_guide/framework_setup/#execute-the-build-python-script","title":"Execute the build python Script","text":"<p>You can execute this file by passing your project name as the parameter <pre><code>python test_pre_install.py my_project\n</code></pre></p> <p>If you get an error with Azure CLI connection issues or type errors. This is because the Profile.yaml file has the incorrect adaptor set. It should be \"fabricsparknb\" not \"fabricspark\".</p> <p>After successful execution and number of notebooks have been created in your project/target folder under notebooks. </p> <p>import_notebook.ipynb this will be used to import notebook files into your lakehouse.</p> <p>metadata_extract.ipynb is used to update the metadata json files in your project. </p> <p>The above two notebooks can be imported using the standard import notebooks function in fabric. The rest of the notebooks can be copied into your lakehouse Files/notebooks folder by running the following script in pwsh. </p> <pre><code>#Run upload.ps1\nInvoke-Expression -Command $env:DBT_PROJECT_DIR/target/pwsh/upload.ps1\n</code></pre> <p>You then open the import_notebook.ipynb in fabric and Run All to import the notebooks from the Files/Notebooks directory in fabric. </p> <p>Similar to  upload, using the following pwsh script will help you to download the metaextract files to the metaextrcats folder in repo.</p> <pre><code>#Run upload.ps1\nInvoke-Expression -Command $env:DBT_PROJECT_DIR/target/pwsh/download.ps1\n</code></pre> <p>Executing the master_notebook.ipynb notebook will execute all notebooks created in your project.</p> <p>This concludes the Framework setup.</p>"},{"location":"developer_guide/initial_setup/","title":"Initial setup","text":"<p>\ud83d\udce6dbt  \u2523 \ud83d\udcc2adapters  \u2503 \u2523 \ud83d\udcc2fabricsparknb  \u2503 \u2503 \u2523 \ud83d\udcdcinit.py  \u2503 \u2503 \u2523 \ud83d\udcdcversion.py  \u2503 \u2503 \u2523 \ud83d\udcdccatalog.py  \u2503 \u2503 \u2523 \ud83d\udcdcconnections.py  \u2503 \u2503 \u2523 \ud83d\udcdcfabric_spark_credentials.py  \u2503 \u2503 \u2523 \ud83d\udcdcimpl.py  \u2503 \u2503 \u2523 \ud83d\udcdclivysession.py  \u2503 \u2503 \u2523 \ud83d\udcdcmanifest.py  \u2503 \u2503 \u2523 \ud83d\udcdcmock.py  \u2503 \u2503 \u2523 \ud83d\udcdcnotebook.py  \u2503 \u2503 \u2517 \ud83d\udcdcutils.py  \u2503 \u2517 \ud83d\udcdcinit.py   \u2523 \ud83d\udcc2include  \u2503 \u2523 \ud83d\udcc2fabricsparknb  \u2503 \u2503 \u2523 \ud83d\udcc2macros  \u2503 \u2503 \u2503 \u2517 \ud83d\udcc2adapters  \u2503 \u2503 \u2503 \u2503 \u2517 \ud83d\udcdcschema.sql  \u2503 \u2503 \u2523 \ud83d\udcc2notebooks  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcimport_notebook.ipynb  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmaster_notebook.ipynb  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmaster_notebook_x.ipynb  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmetadata_extract.ipynb  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmodel_notebook.ipynb  \u2503 \u2503 \u2503 \u2517 \ud83d\udcdctest_notebook.ipynb  \u2503 \u2503 \u2523 \ud83d\udcc2pwsh  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcdownload.ps1  \u2503 \u2503 \u2503 \u2517 \ud83d\udcdcupload.ps1  \u2503 \u2503 \u2523 \ud83d\udcdcinit.py  \u2503 \u2503 \u2523 \ud83d\udcdcdbt_project.yml  \u2503 \u2503 \u2523 \ud83d\udcdcmetadata_extract.ipynb  \u2503 \u2503 \u2517 \ud83d\udcdcprofile_template.yml  \u2503 \u2517 \ud83d\udcdcinit.py  \u2517 \ud83d\udcdcinit.py</p>"},{"location":"documentation_guide/","title":"Documentation Guide","text":""},{"location":"documentation_guide/#building-you-environment","title":"Building you environment","text":"<p>Documentation for this project is built using mkdocs-material. To contribute to the documentation you will need to create a separate python environment. I suggest that you call this <code>.env_mkdocs</code> to avoid confusion with the dbt environment. </p> <p>Important</p> <p>The commands below assume that you have already performed the <code>Core Tools Installation</code> steps in the User Guide. If you have not done this yet, please do so before proceeding. Note you ONLY have to install <code>core tools</code> it is not necessary to move on to the <code>other tools</code> section. </p> <p>Before creating the environment you will need to clone the repository. You can do this by running the command below:</p> <p>clone the repository<pre><code>git clone https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb.git MyDocsProject\n</code></pre> This will clone the repository into a directory called MyDocsProject. You can rename this directory to whatever you like. Navigate into this new directory and then run the commands below.</p> Create and activate the Python environment<pre><code># Create and activate the Python environment\npython -m venv .env_mkdocs\n.\\.env_mkdocs\\Scripts\\activate.ps1\npip install -r ./requirements_mkdocs.txt\n</code></pre> <p>These commands will create a new python environment and install the required packages for building the documentation. To launch the new environment you will need to run the command below:</p> Activate the Python environment<pre><code>.\\.env_mkdocs\\Scripts\\activate.ps1\n</code></pre>"},{"location":"documentation_guide/#updating-the-documentation","title":"Updating the documentation","text":"<p>The documentation source is held in the <code>docs</code> directory. To update the documentation you will need to edit the markdown files in this directory. In order to understand the syntax used for the markdown be sure to review the reference section for mkdocs-material. Once you have made your changes you can build the documentation using the command below:</p> Build the documentation<pre><code>mkdocs build\n</code></pre> <p>To view the documentation locally you can use the command below:</p> View the documentation locally<pre><code>mkdocs serve\n</code></pre> <p>Tip</p> <p>The <code>mkdocs serve</code> command will start a local web server that will allow you to view the documentation in your browser. The server will also automatically rebuild the documentation when you make changes to the source files.</p> <p>Before publishing the documentation you should ensure that the documentation is up to date and that the changes are correct. You should also pull the latest from the repository to ensure that you are not overwriting someone else's changes. Do this by running the command below:</p> Pull the latest changes from the repository<pre><code>git pull\n</code></pre> <p>You can now publish the documentation to the repository by running the command below:</p> Publish the documentation<pre><code>mkdocs gh-deploy\n</code></pre>"},{"location":"user_guide/","title":"User Guide","text":""},{"location":"user_guide/dbt_build_process/","title":"Dbt Build Process","text":""},{"location":"user_guide/dbt_build_process/#dbt-build-process-dbt_wrapper","title":"Dbt Build Process &amp; Dbt_Wrapper","text":"<p>The dbt-fabricksparknb package includes a console application that will allow you to build your dbt project and generate a series of notebooks that can be run in a Fabric workspace. This application is called <code>dbt_wrapper</code> and is a python script that is run from the command line. You can invoke the application and view information about it by running the following command in a terminal:</p> <p>Note</p> <p>Make sure that you have activated your python virtual environment before running this code. Also be sure to replace my_project with the name of your dbt project folder</p> <pre><code>dbt_wrapper --help\n</code></pre> <p>To build your dbt project and publish your notebook to your Fabric workspace you can run the command below:</p> <p>Note</p> <p>Be sure to replace my_project with the name of your dbt project folder</p> <pre><code>dbt_wrapper run-all my_project\n</code></pre> <p>The command above will carry out all of the necessary \"stages\" required to fully build your dbt project and generate the notebooks that can be run in a Fabric workspace. When run successfully your should see output similar to the image below.</p> <p></p>"},{"location":"user_guide/dbt_build_process/#toggling-build-stages-off-and-on","title":"Toggling Build Stages Off and On","text":"<p>There are times when you may not wish to run ALL of the build steps. In such circumstances you can toggle off specific stages by using the options built in to the <code>dbt_wrapper</code> application. To view all of the options available to you run the command below:</p> <pre><code>dbt_wrapper build --help\n</code></pre> <p>For example, should you wish to run all stages except for the upload of the generated notebooks to your Fabric workspace you can run the command below:</p> <p><pre><code>dbt_wrapper build my_project --no-upload-notebooks-via-api  \n</code></pre> Alternatively, you might want to make use of some additional \"helper\" commands that we have included in the application. For example, you can run the run-all-local command to run all stages except for those that require a live Fabric connection. This is useful when you are testing the build process locally. To run this command you can use the command below:</p> <pre><code>dbt_wrapper run-all-local my_project\n</code></pre> <p>Review all of the commands available to you by running using the help option as shown below:</p> <pre><code>dbt_wrapper --help\n</code></pre> <p>Info</p> <p>You are now ready to move to the next step in which you gain an understanding of the various kinds of notebooks generated by the adapter. Follow the Understanding the Generated Notebooks guide.</p>"},{"location":"user_guide/dbt_project_setup/","title":"DBT Project Setup","text":""},{"location":"user_guide/dbt_project_setup/#fabric-workspace-setup","title":"Fabric Workspace Setup","text":"<p>Next we will create a new dbt project and configure it to use the dbt-fabricsparknb adapter. But, before we do this we need to gather some information from the Power BI / Fabric Portal. To do this, follow the steps below:</p> <ol> <li>Open the Power BI Portal and navigate to the workspace you want to use for development. If necessary, create a new workspace.</li> <li>Ensure that the workspace is Fabric enabled. If not, enable it.</li> <li>Make sure that there is at least one Datalake in the workspace.</li> <li> <p>Get the connection details for the workspace.</p> <ol> <li>You will need to get the workspace name, workspace id, lakehouse id, and lakehouse name. </li> <li>The lakehouse name and workspace name are easily viewed from the fabric / power bi portal. </li> <li>The easiest way to get the id information is to:<ol> <li>Navigate to a file or folder in your target lakehouse.</li> <li>Click on the three dots to the right of the file or folder name, and select \"Properties\". Details will be displayed in the properties window.</li> <li>From these properties select copy url and paste it into a text editor. The workspace id is the first GUID in the URL, the lakehouse id is the second GUID in the URL.</li> <li>In the example below, the workspace id is <code>4f0cb887-047a-48a1-98c3-ebdb38c784c2</code> and the lakehouse id is <code>aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9</code>.</li> </ol> </li> </ol> </li> </ol> Example URL<pre><code>https://onelake.dfs.fabric.microsoft.com/4f0cb887-047a-48a1-98c3-ebdb38c784c2/aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9/Files/notebooks\n</code></pre>"},{"location":"user_guide/dbt_project_setup/#create-dbt-project","title":"Create Dbt Project","text":"<p>Once you have taken note of the workspace id, lakehouse id, workspace name and lakehouse name you can create a new dbt project and configure it to use the dbt-fabricsparknb adapter. To do this, run the code shown below:</p> <pre><code># Create your dbt project directories and profiles.yml file\ndbt init my_project # Note that the name of the project is arbitrary... call it whatever you like\n</code></pre> <p>When asked the questions below, provide the answers in bold below:</p> <ol> <li><code>Which data base would you like to use?</code> select <code>dbt-fabricksparknb</code></li> <li><code>Desired authentication method option (enter a number):</code> select <code>livy</code></li> <li><code>workspaceid (GUID of the workspace. Open the workspace from fabric.microsoft.com and copy the workspace url):</code> Enter the workspace id</li> <li><code>lakehouse (Name of the Lakehouse in the workspace that you want to connect to):</code> Enter the lakehouse name</li> <li><code>lakehouseid (GUID of the lakehouse, which can be extracted from url when you open lakehouse artifact from fabric.microsoft.com):</code> Enter the lakehouse id</li> <li><code>endpoint [https://api.fabric.microsoft.com/v1]:</code> Press enter to accept the default</li> <li><code>auth (Use CLI (az login) for interactive execution or SPN for automation) [CLI]:</code> select <code>cli</code></li> <li><code>client_id (Use when SPN auth is used.):</code> Enter a single space and press enter</li> <li><code>client_scrent (Use when SPN auth is used.):</code> Enter a single space and press enter</li> <li><code>tenant_id (Use when SPN auth is used.):</code> Enter a single space or Enter your PowerBI tenant id</li> <li><code>connect_retries [0]:</code> Enter 0</li> <li><code>connect_timeout [10]:</code> Enter 10</li> <li><code>schema (default schema that dbt will build objects in):</code> Enter <code>dbo</code></li> <li>threads (1 or more) [1]: Enter 1</li> </ol> <p>The command above will create a new directory called <code>my_project</code>. Within this directory you will find a <code>dbt_project.yml</code> file. Open this file in your favourite text editor and note that it should look like the example below except that in your case my_project will be replaced with the name of the project you created above.:</p> dbt_project.yml<pre><code># Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'my_project'\nversion: '1.0.0'\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: 'my_project'\n\n# These configurations specify where dbt should look for different types of files.\n# The `model-paths` config, for example, states that models in this project can be\n# found in the \"models/\" directory. You probably won't need to change these!\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analyses\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"seeds\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\n\nclean-targets:         # directories to be removed by `dbt clean`\n  - \"target\"\n  - \"dbt_packages\"\n\n\n# Configuring models\n# Full documentation: https://docs.getdbt.com/docs/configuring-models\n\nmodels:\n  test4:\n    # Config indicated by + and applies to all files under models/example/\n    example:\n      +materialized: view\n</code></pre> <p>The dbt init command will also update your <code>profiles.yml</code> file with a profile matching your dbt project name. Open this file in your favourite text editor using the command below:</p> WindowsMacOSLinux <pre><code>code  $home/.dbt/profiles.yml\n</code></pre> <pre><code>code  ~/.dbt/profiles.yml\n</code></pre> <pre><code>code  ~/.dbt/profiles.yml\n</code></pre> <p>When run this will display a file similar to the one below. Check that your details are correct.</p> <p>Note</p> <p>The <code>profiles.yml</code> file should look like the example below except that in your case the highlighted lines may contain different values.</p> profiles.yml<pre><code>my_project:\n  target: my_project_target\n  outputs:\n    my_project_target:\n      authentication: CLI\n      method: livy\n      connect_retries: 0\n      connect_timeout: 10\n      endpoint: https://api.fabric.microsoft.com/v1\n      workspaceid: 4f0cb887-047a-48a1-98c3-ebdb38c784c2\n      workspacename: test\n      lakehousedatapath: /lakehouse\n      lakehouseid: 031feff6-071d-42df-818a-984771c083c4\n      lakehouse: datalake\n      schema: dbo\n      threads: 1\n      type: fabricsparknb\n      retry_all: true\n</code></pre> <p>Info</p> <p>You are now ready to move to the next step in which you will build your dbt project. Follow the Dbt Build Process guide.</p>"},{"location":"user_guide/development_workflow/","title":"Development Workflow","text":""},{"location":"user_guide/development_workflow/#development-and-deployment-flow-using-fabric-spark-notebook-adapter","title":"Development and Deployment Flow Using Fabric Spark Notebook Adapter","text":"<p>Advantages</p> <ul> <li> Available today</li> <li> Native Fabric Notebooks Generated and Deployed<ol> <li>Non dbt users able to view notebooks and business logic</li> <li>Monitoring and debugging of loads directly in Fabric without the need for a separate tool</li> </ol> </li> <li> Re-occurring loads achieved using native Fabric scheduling </li> <li> Simplified code promotion process using native Fabric Git integration</li> <li> No need for dbt hosted in a virtual machine <ol> <li>No need for service account</li> <li>No need for Azure Landing Zone</li> <li>No need for secure network connectivity between Azure VM and Fabric   </li> </ol> </li> <li> Allows for disconnected development environment providing<ol> <li>Faster DBT build times</li> <li>Greater developer flexibility</li> </ol> </li> <li> Simplified code promotion Process using native Fabric Git integration<ol> <li>Single, native promotion process for all Fabric artifacts including non-dbt ones</li> </ol> </li> </ul> <p>Disadvantages</p> <ul> <li>Requires Additional Steps<ol> <li>Meta Data Extract</li> <li>Notebook Upload </li> <li>Notebook Import</li> </ol> </li> </ul>"},{"location":"user_guide/development_workflow/#development-and-deployment-flow-using-original-fabric-spark-adapter","title":"Development and Deployment Flow Using Original Fabric Spark Adapter","text":""},{"location":"user_guide/development_workflow/#detailed-workflow","title":"Detailed Workflow","text":"<p>Inital Setup 1. Provision Workspace    - Development Environment: Fabric Portal    - Re-occurence: Do once per development environment set-up    - Instructions: Create a new workspace in the Power BI Portal, or use an existing workspace.</p> <ol> <li>Get Workspace Connection Details</li> <li>Development Environment: Fabric Portal</li> <li>Re-occurence: Do once per development environment set-up</li> <li> <p>Instructions: Get the workspace connection details from the Power BI Portal.</p> </li> <li> <p>Create or Update <code>profiles.yml</code></p> </li> <li>Development Environment: VS Code on local, developemnt machine</li> <li></li> <li> <p>Create or Update <code>dbt_project.yml</code> </p> </li> <li>Build Project</li> <li>Manually Upload Notebooks </li> <li>Run Meta Data Extract</li> </ol> <p>Ongoing Development Cycle</p> <ol> <li> <p>Download Metadata: </p> </li> <li> <p>Update Dbt Project </p> </li> <li>Build Dbt Project </li> <li>Verify Outputs </li> <li>Update Notebooks     <ol> <li>Upload to Onelake</li> <li>Update to GIT repo</li> </ol> </li> <li>Promote to Workspace     <ol> <li>Run Import Notebook</li> <li>Promote GIT branch</li> </ol> </li> <li>Run Master Notebook </li> <li>Validate Results </li> <li>Run Metadata Extract</li> </ol>"},{"location":"user_guide/fabric_ci_cd_process/","title":"Fabric CI/CD with Git Deployment","text":""},{"location":"user_guide/fabric_ci_cd_process/#git-based-deployment","title":"Git Based Deployment","text":"<p>The initial setup is based on a Git branch that is linked to all workspaces. As illustrated in the given example, we have described three stages: Development, Test, and Production. It also employs feature branches for individual developments within isolated workspaces using branch out functionality.</p> <p>The successful operation of this scenario depends on branching, merging, and pull requests.</p> <ol> <li><code>Each workspace is assigned its own branch.</code></li> <li><code>The introduction of new features is facilitated by raising pull requests.</code></li> <li><code>All deployments are initiated from the repository.</code></li> <li><code>To transition from Development to Test, and subsequently from Test to Production, a pull request must be initiated from the originating stage.</code></li> </ol> <p>The synchronization between the Git branch and the workspace can be automated. This is achieved by invoking the Git Sync API as part of a build pipelines, which is automatically triggered following the approval of a pull request.</p> <p></p>"},{"location":"user_guide/fabric_ci_cd_process/#git-and-build-environment-based-deployment","title":"Git and Build Environment Based Deployment","text":"<p>Git is exclusively linked to the Development workspace. The deployment to other stages is executed based on Build environments. This implies that the Fabric Item APIs are utilized to perform Create, Read, Update or Delete operations.</p> <p>Key points of this setup are:</p> <ol> <li><code>The Git repository serves as the foundation for creating, updating, or deleting items in the workspace.</code></li> <li><code>Git is solely connected to the Development workspace.</code></li> <li><code>Following a pull request, a Build pipeline is activated.</code></li> <li><code>The Build pipeline executes operations to the workspace.</code></li> </ol> <p>Note</p> <pre><code>This approach is code-intensive and for each future item to be supported, modifications may be required in the Build pipelines.\n</code></pre> <p></p>"},{"location":"user_guide/fabric_ci_cd_process/#git-and-fabric-deployment-pipeline-based-deployment","title":"Git and Fabric Deployment Pipeline Based Deployment","text":"<p>This is based on Fabric Deployment pipelines. This user-friendly interface simplifies the deployment process from one stage to another and is less code-intensive.</p> <p>Git is solely connected to the Development workspace, and feature branches continue to exist in separate workspaces. However, the Test, Production, and any additional workspaces are not linked to Git.</p> <p>Key aspects of this setup include:</p> <ol> <li><code>The release process to other stages, such as Test and Production, is managed via Deployment Pipelines in the Fabric.</code></li> <li><code>The Development workspace is the only one connected to Git.</code></li> <li><code>Triggers for the Fabric deployment pipeline can be automated. This is achieved by using Build Pipelines, which are automatically activated following the approval of a pull request.</code></li> </ol> <p>These pipelines can call the Fabric REST API and can also be integrated with Git Sync API for synchronizing the development workspace.</p> <p></p>"},{"location":"user_guide/initial_setup/","title":"Environment Setup","text":"<p>This section outlines the steps required to setup the development environment to use this dbt-adapter as part of a dbt data transformation project.</p> <p>To provide a common, cross-platform set of instructions we will first install Powershell. To facilitate the installation process we will use package managers such as winget for Windows, brew for MacOS and <code>apt</code> for Linux.</p>"},{"location":"user_guide/initial_setup/#core-tools-installation","title":"Core Tools Installation","text":"WindowsMacOSLinux <pre><code># Winget Installs \nwinget install Microsoft.PowerShell\n</code></pre> <pre><code>brew install powershell/tap/powershell\n</code></pre> <pre><code># TBA\n</code></pre> <p>Next we will install Python and development tools such as vscode.</p> WindowsMacOSLinux <pre><code># Winget Installs \nwinget install -e --id Python.Python -v 3.12\nwinget install -e --id Microsoft.VisualStudioCode\nwinget install --id Git.Git -e --source winget\n\n# Python Environment Manager\nPython -m pip install --user virtualenv\n</code></pre> <pre><code># Brew Installs\nbrew install python@3.12\nbrew install --cask visual-studio-code\nbrew install git\n\n# Python Environment Manager\nPython -m pip install --user virtualenv\n\n# TODO \n# Add OSX AZ Copy Instructions\n</code></pre> <pre><code># TBA\n</code></pre>"},{"location":"user_guide/initial_setup/#other-tools","title":"Other tools","text":"<p>Now that we have pwsh installed, we can use it as a cross platform shell to install the additional required tools. </p> WindowsMacOSLinux <pre><code># Az Copy Install - No Winget Package Available\nInvoke-WebRequest -Uri https://aka.ms/downloadazcopy-v10-windows -OutFile AzCopy.zip -UseBasicParsing\nExpand-Archive ./AzCopy.zip ./AzCopy -Force\nNew-Item -ItemType \"directory\" -Path \"$home/AzCopy\"  -Force  \nGet-ChildItem ./AzCopy/*/azcopy.exe | Move-Item -Destination \"$home\\AzCopy\\AzCopy.exe\" -Force  \n$userenv = [System.Environment]::GetEnvironmentVariable(\"Path\", \"User\") \n[System.Environment]::SetEnvironmentVariable(\"PATH\", $userenv + \";$home\\AzCopy\", \"User\")\nRemove-Item .\\AzCopy\\ -Force\nRemove-Item AzCopy.zip -Force\n</code></pre> <pre><code># TODO \n# Add OSX AZ Copy Instructions\n</code></pre> <pre><code># TBA\n</code></pre>"},{"location":"user_guide/initial_setup/#source-directory-python-env","title":"Source Directory &amp; Python Env","text":"<p>Now lets create and activate our Python environment and install the required packages.</p> <p>Tip</p> <p>When doing pip install dbt-fabricspark below it can take a few minutes to complete on some machines. Occasionally pip may get stuck and in such cases break the execution using ctrl-c and run the same pip again. </p> WindowsMacOSLinux <pre><code># Ensure that you are in the pwsh shell\npwsh\n\n# Create a new source code directory\nmkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like\n# Navigate to the new directory\ncd dbt-fabricsparknb-test\n\n# Create and activate the Python environment\npython -m venv .env\n./.env/Scripts/Activate.ps1\n\n# Install the dbt-fabricsparknb package from the repository\npip install --upgrade git+https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb\n</code></pre> <pre><code># Ensure that you are in the pwsh shell\npwsh\n\n# Create a new source code directory\nmkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like\n# Navigate to the new directory\ncd dbt-fabricsparknb-test\n\n# Create and activate the Python environment\npython -m venv .env\n./.env/Scripts/Activate.ps1  \n\n# Install the dbt-fabricsparknb package from the repository\npip install --upgrade git+https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb\n</code></pre> <pre><code># TBA\n</code></pre> <p>Info</p> <p>You are now ready to move to the next step in which you will set up your dbt project. Follow the Dbt Project Setup guide.</p>"},{"location":"user_guide/notebooks/","title":"Understanding the Generated Notebooks","text":""},{"location":"user_guide/notebooks/#understanding-the-notebooks-generated","title":"Understanding the Notebooks Generated","text":"<p>When you run this build script successfully, you will see a series of notebooks generated in your my_project/target/notebooks directory. This is the <code>\"special sauce\"</code> of this dbt-adapter that allows your to run your dbt project natively as notebooks in a Fabric workspace. The image below shows a sample listing of generated notebooks. Your specific notebooks will be contain the name of your dbt project and may be different depending on the models and tests that you have defined in your dbt project. </p>"},{"location":"user_guide/notebooks/#sample-listing-of-generated-notebooks","title":"Sample listing of Generated Notebooks","text":"<p>If you study the files shown above you will notice that there is a naming convention and that the notebooks are prefixed with a specific string. The following table explains at a high level the naming convention and the purpose of each type of notebook.</p> Notebook Prefix Description model. These are dbt model notebooks. A notebook will be generated for each dbt model that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. test. These are dbt test notebooks. A notebook will be generated for each dbt test that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. seed. These are dbt seed notebooks. A notebook will be generated for each dbt seed that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. master_ These are execution orchestration notebooks. They allow the running of your models, tests and seeds in parallel and in the correct order. They are what allow you to run your transformation pipelines independently of dbt as an orchestrator. In order to run your project simply schedule master.{project_name}.notebook.iypnb using Fabric's native scheduling functionality import_ This is a helper notebook that facilitate import of generated notebooks into workspace. metadata_ This is a helper notebook to facilitate generation of workspace metadata json files. <p>Important</p> <p>The green panels below provide a more detailed discussion of each type of notebook. Take a moment to expand each panel by clicking on it and read the detailed explanation of each type of notebook.</p> Notebooks with the Prefix <code>\"model.\"</code> <p>These are dbt model notebooks. A notebook will be generated for each dbt model that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt.</p> <p></p> <p></p> Notebooks with the Prefix <code>\"test.\"</code> <p>These are dbt test notebooks. A notebook will be generated for each dbt test that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt.</p> Notebooks with the Prefix <code>\"seed.\"</code> <p>These are dbt seed notebooks. A notebook will be generated for each dbt seed that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt.</p> Notebooks with the Prefix <code>\"master_\"</code> <p>These are execution orchestration notebooks. They allow the running of your models, tests and seeds in parallel and in the correct order. They are what allow you to run your transformation pipelines independently of dbt as an orchestrator. In order to run your project simply schedule master.{project_name}.notebook.iypnb using Fabric's native scheduling functionality.</p> Notebooks with the Prefix <code>\"import_\"</code> <p>This is a helper notebook that facilitates import of generated notebooks into workspace.</p> Notebooks with the Prefix <code>\"metadata_\"</code> <p>This is a helper notebook to facilitates the generation of workspace metadata json files.</p>"},{"location":"zzz_archive/","title":"Archive","text":""},{"location":"zzz_archive/1-Why/","title":"Objective of this project","text":"<p>This project is the result of a search to find a tool that would accelerate the development and ongoing maintenance processes relating to data transformation activities within a software-as-a-service (SAAS) alignd lakehouse architecture and technology stack.</p> <p>When designing this project the core, \"must have\" requirements were to:</p> <p>1) Minimise the effort required to <code>track lineage and dependencies</code> between data transformation activities</p> <p>2) Minimise the effort required to create, maintain and update <code>data transformation activities</code></p> <p>3) Minimise the effort required to <code>document data transformation activities</code></p> <p>4) Minimise the effort required to <code>create and maintain a data catalog</code> of the entities created by data transformation activities</p> <p>5) Be compatable with a <code>Software-As-A-Service (SAAS) technology stack</code> and avoid the neeed for platform as a Service (PaaS) components</p> <p>6) <code>Avoid locking-in transformation logic</code> into specific vendor proprietry language or technology stack.</p> <p>7) Minimise the effort required to <code>test data transformation activities</code></p> <p>Secondary \"nice to have\" requirements were:</p> <p>1) Minimise the effort required to create, maintain, update and execute <code>data quality checks</code></p> <p>2) <code>Automate the generation of data transformation activities</code> from a data lineage and dependencies</p>"},{"location":"zzz_archive/1-Why/#discovering-our-approach","title":"Discovering our approach","text":"<p>After reviewing the available tools and technologies and considering the requirements it soon became clear to us that Dbt-Core  appeared to be good fit for our needs. However, there were some limitations that we needed to address:</p> <p>1) Dbt-Core is a command line tool and all of its adapters appeared to require some <code>Platform as a Service (PaaS) comoponents</code> to be introduced into the technology stack in order to host a secure \"python\" runtime environement. This was not compatable with our requirement for a Software-As-A-Service (SAAS) technology stack.</p> <p>2) Dbt-Core <code>combined code build, test and deployment with re-occuring data pipeline execution</code>. This requires Dbt-core to be run everytime a data pipeline needs to be refreshed regardless of whether underlying changes to business logic have been made.</p> <p>3) Dbt-Core rose to popularity as a tool for data transformation at a time when data warehouses and relational dataabses were the predominant transformation engines. There seemed to be some uncertainty as to how it would  perform in the context of a SAAS, lakehouse architecture. Dbt-Core can be quite \"chatty\" and create multiple connections to the transformation engine. This could be a problem in a SAAS environment where the number of connections may be limited. In addition, code execution against a lakehouse architecture tend to incur higher latency than traditional data warehouses. It is plausible to expect that Dbt-Core's deafult behaviour of creating multiple connections and executing a large number of separate and distinct code blocks would result in relatively long project run times and a degraded developer experience.</p> <p>4) Dbt-core might be considered to be a \"heavy\" tool for some use cases. It is a large codebase with a lot of functionality. It is possible that some users may find it difficult to understand and use.</p>"},{"location":"zzz_archive/1-Why/#dbt-child-adapter","title":"Dbt Child Adapter","text":"<p>```mermaid classDiagram     BaseAdapter &lt;|-- SparkAdapter : Inheritance     SparkAdapter &lt;|-- FabricSparkAdapter : Inheritance     namespace Adapter {     class BaseAdapter{         +get_columns_in_relation(relation)         +get_missing_columns(from_relation, to_relation)         +expand_target_column_types(temp_table, to_relation)         +list_relations_without_caching(schema)         +drop_relation(relation)         +truncate_relation(relation)         +rename_relation(from_relation, to_relation)         +get_relation(database, schema, identifier)         +create_schema(schema)         +drop_schema(schema)         +quote(identifier)         +convert_text_type()         +convert_number_type()         +convert_boolean_type()         +convert_datetime_type()         +convert_date_type()         +convert_time_type()         +get_rows_different_sql()         +get_merge_sql()         +get_distinct_sql()         +date_function()     }</p> <pre><code>class SparkAdapter{\n    +get_columns_in_relation(relation)\n    +get_missing_columns(from_relation, to_relation)\n    +expand_target_column_types(temp_table, to_relation)\n    +list_relations_without_caching(schema)\n    +drop_relation(relation)\n    +truncate_relation(relation)\n    +rename_relation(from_relation, to_relation)\n    +get_relation(database, schema, identifier)\n    +create_schema(schema)\n    +drop_schema(schema)\n    +quote(identifier)\n    +convert_text_type()\n    +convert_number_type()\n    +convert_boolean_type()\n    +convert_datetime_type()\n    +convert_date_type()\n    +convert_time_type()\n    +get_rows_different_sql()\n    +get_merge_sql()\n    +get_distinct_sql()\n    +date_function()\n}\nclass FabricSparkAdapter {\n     -COLUMN_NAMES\n     -INFORMATION_COLUMNS_REGEX\n     -INFORMATION_OWNER_REGEX\n     -INFORMATION_STATISTICS_REGEX\n     -HUDI_METADATA_COLUMNS\n     -CONSTRAINT_SUPPORT\n     -Relation\n     -RelationInfo\n     -Column\n     -ConnectionManager\n     -AdapterSpecificConfigs\n\n     +_get_relation_information()\n     +_get_relation_information_using_describe()\n     +_build_spark_relation_list()\n     +get_relation()\n     +parse_describe_extended()\n     +find_table_information_separator()\n     +get_columns_in_relation()\n     +parse_columns_from_information()\n     +_get_columns_for_catalog()\n     +get_catalog()\n     +execute()\n     +list_schemas()\n     +check_schema_exists()\n     +get_rows_different_sql()\n     +standardize_grants_dict()\n     +debug_query()\n\n     +date_function()\n     +convert_text_type()\n     +convert_number_type()\n     +convert_integer_type()\n     +convert_date_type()\n     +convert_time_type()\n     +convert_datetime_type()\n     +quote()\n }\n}\n</code></pre>"},{"location":"zzz_archive/dbt-adapter-user-guide/","title":"Environment Setup","text":"<p>This section outlines the steps required to setup the development environment to use this dbt-adapter as part of a dbt data transformation project.</p> <p>To provide a common, cross-platform set of instructions we will first install Powershell. To facilitate the installation process we will use package managers such as winget for Windows, brew for MacOS and <code>apt</code> for Linux.</p>"},{"location":"zzz_archive/dbt-adapter-user-guide/#windows","title":"Windows","text":"<pre><code># Winget Installs \nwinget install Microsoft.PowerShell\n</code></pre>"},{"location":"zzz_archive/dbt-adapter-user-guide/#macos","title":"MacOS","text":"<pre><code>brew install powershell/tap/powershell\n</code></pre>"},{"location":"zzz_archive/dbt-adapter-user-guide/#linux","title":"Linux","text":"<pre><code># TBA\n</code></pre> <p>Next we will install Python and development tools such as vscode.</p>"},{"location":"zzz_archive/dbt-adapter-user-guide/#windows_1","title":"Windows","text":"<pre><code># Winget Installs \nwinget install -e --id Python.Python -v 3.12.0\nwinget install -e --id Microsoft.VisualStudioCode\nwinget install --id Git.Git -e --source winget\n\n# Python Environment Manager\nPython -m pip install --user virtualenv\n</code></pre>"},{"location":"zzz_archive/dbt-adapter-user-guide/#macos_1","title":"MacOS","text":"<pre><code># Brew Installs\nbrew install python@3.12\nbrew install --cask visual-studio-code\nbrew install git\n\n# Python Environment Manager\nPython -m pip install --user virtualenv\n</code></pre>"},{"location":"zzz_archive/dbt-adapter-user-guide/#linux_1","title":"Linux","text":"<pre><code># TBA\n</code></pre> <p>Now lets create and activate our Python environment and install the required packages.</p> <pre><code># Create a new source code directory\nmkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like\n# Navigate to the new directory\ncd dbt-fabricsparknb-test\n\n# Create and activate the Python environment\npython -m venv .env\n./.env/bin/Activate.ps1   \n\n# Install dbt-core \npip install dbt-core\n\n# Install dbt-fabricspark\npip install dbt-fabricspark\n\n# Install the dbt-fabricsparknb pre-requisites \npip install azure-storage-file-datalake\npip install nbformat\n\n# Install the dbt-fabricsparknb package from the repository\npip install --upgrade git+https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb\n</code></pre>"},{"location":"zzz_archive/dbt-adapter-user-guide/#new-dbt-project-creation-and-configuration","title":"New Dbt Project Creation and Configuration","text":"<p>Next we will create a new dbt project and configure it to use the dbt-fabricsparknb adapter. But, before we do this we need to gather some information from the Power BI / Fabric Portal. To do this, follow the steps below:</p> <ul> <li>Open the Power BI Portal and navigate to the workspace you want to use for development. If necessary, create a new workspace.</li> <li>Ensure that the workspace is Fabric enabled. If not, enable it.</li> <li>Make sure that there is at least one Datalake in the workspace.</li> <li>Get the connection details for the workspace. This will include the workspace name, the workspace id, and the lakehouse id. The easiest way to get this information is to navigate to a file or folder in the lakehouse, click on the three dots to the right of the file or folder name, and select \"Properties\". Details will be displayed in the properties window. From these properties select copy url and paste it into a text editor. The workspace id is the first GUID in the URL, the lakehouse id is the second GUID in the URL. In the example below, the workspace id is <code>4f0cb887-047a-48a1-98c3-ebdb38c784c2</code> and the lakehouse id is <code>aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9</code>.</li> </ul> <p>https://onelake.dfs.fabric.microsoft.com/4f0cb887-047a-48a1-98c3-ebdb38c784c2/aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9/Files/notebooks</p> <p>Once you have taken note of the workspace id, lakehouse id, and workspace name, you can create a new dbt project and configure it to use the dbt-fabricsparknb adapter. To do this, run the code shown below:</p> Note when asked to select the adapter choose dbt-fabricksparknb. During this process you will also be asked for the workspace id, lakehouse id, and workspace name. Use the values you gathered from the Power BI Portal. <pre><code># Create your dbt project directories and profiles.yml file\ndbt init my_project # Note that the name of the project is arbitrary... call it whatever you like\n</code></pre> <p>The command above will create a new directory called <code>my_project</code>. Within this directory you will find a <code>profiles.yml</code> file. Open this file in your favourite text editor and note that it should look like the example below except that in your case my_project will be replaced with the name of the project you created above.:</p> <pre><code># Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'my_project'\nversion: '1.0.0'\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: 'my_project'\n\n# These configurations specify where dbt should look for different types of files.\n# The `model-paths` config, for example, states that models in this project can be\n# found in the \"models/\" directory. You probably won't need to change these!\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analyses\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"seeds\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\n\nclean-targets:         # directories to be removed by `dbt clean`\n  - \"target\"\n  - \"dbt_packages\"\n\n\n# Configuring models\n# Full documentation: https://docs.getdbt.com/docs/configuring-models\n\n# In this example config, we tell dbt to build all models in the example/\n# directory as views. These settings can be overridden in the individual model\n# files using the ` config(...) ` macro.\nmodels:\n  test4:\n    # Config indicated by + and applies to all files under models/example/\n    example:\n      +materialized: view\n</code></pre> <p>The dbt init command will also update your <code>profiles.yml</code> file with a profile matching your dbt project name. Open this file in your favourite text editor using the command below:</p> <p><pre><code>code  ~/.dbt/profiles.yml\n</code></pre> When run this will display a file similar to the one below. Check that your details are correct. </p> <pre><code>test4:\n  outputs:\n    dev:\n      auth: cli #remove\n      client_id: dlkdjl #remove\n      client_scrent: dlkdjl #remove\n      connect_retries: 0 #remove\n      connect_timeout: 0 #remove\n      endpoint: dkld #remove\n      lakehouse: 'lakehouse' #the name of your lakehouse\n      lakehouseid: 'aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9' #the guid of your lakehouse\n      method: livy\n      schema: dbo #the schema you want to use\n      tenant_id: '72f988bf-86f1-41af-91ab-2d7cd011db47' #your power bi tenant id\n      threads: 1 #the number of threads to use\n      type: fabricsparknb #the type of adapter to use.. always use fabricsparknb\n      workspaceid: '4f0cb887-047a-48a1-98c3-ebdb38c784c2' #the guid of your workspace\n  target: dev\n</code></pre> <pre><code># Set the DBT_PROJECT_DIR environment variable\n$env:DBT_PROJECT_DIR = \"my_project\" #this is the name of the project you created above\n\n\n# Build your dbt project \ndbt build\n</code></pre> <p>filename</p> <p>Inital Setup 1. Provision Workspace    - Development Environment: Fabric Portal    - Re-occurence: Do once per development environment set-up    - Instructions: Create a new workspace in the Power BI Portal, or use an existing workspace.</p> <ol> <li>Get Workspace Connection Details</li> <li>Development Environment: Fabric Portal</li> <li>Re-occurence: Do once per development environment set-up</li> <li> <p>Instructions: Get the workspace connection details from the Power BI Portal.</p> </li> <li> <p>Create or Update <code>profiles.yml</code></p> </li> <li>Development Environment: VS Code on local, developemnt machine</li> <li></li> <li> <p>Create or Update <code>dbt_project.yml</code> </p> </li> <li>Build Project</li> <li>Manually Upload Notebooks </li> <li>Run Meta Data Extract</li> </ol> <p>Ongoing Development Cycle</p> <ol> <li> <p>Download Metadata: </p> </li> <li> <p>Update Dbt Project </p> </li> <li>Build Dbt Project </li> <li>Verify Outputs </li> <li>Update Notebooks     <ol> <li>Upload to Onelake</li> <li>Update to GIT repo</li> </ol> </li> <li>Promote to Workspace     <ol> <li>Run Import Notebook</li> <li>Promote GIT branch</li> </ol> </li> <li>Run Master Notebook </li> <li>Validate Results </li> <li>Run Metadata Extract</li> </ol>"}]}