{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Dbt Fabric Spark Notebook Generator (Dbt-FabricSparkNb)","text":"<p>The first and only dbt adapter for a true, modern, software-as-a-service (SAAS) Lakehouse.</p>"},{"location":"#what-is-it","title":"What is it","text":"<p>...</p>"},{"location":"#why-did-we-build-it","title":"Why did we build it?","text":"<p>As a team of data specialists we have been working with dbt for a number of years. We have found that dbt is a powerful tool for data transformation, but it has some limitations. We have built this adapter to address some of these limitations and to make it easier to work with dbt in a modern, software-as-a-service (SAAS) lakehouse environment. </p>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>Dbt-FabricSparkNb works by leverging the power of the dbt-core, and the dbt-fabrickspark apater to create a new adapter. As such, it can be described as a \"child apater\" of dbt-fabrickspark. </p> <p>The adapter inherits all of the functionality of the dbt-fabrickspark adapter and simply extends it to meet the unique requirements of our project.</p> <p>Consequently, to use this adapter, you will need to install the dbt-fabrickspark adapter and then install the dbt-fabricksparknb adapter.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Support for SAAS only lakehouse architecture (No PAAS components requried)</li> <li>Support for lightweight, disconnected local development workflow</li> <li>Fully featured with modern data transformation capabilities such as data lineage, data catalog, data quality checks and templated data transformation activities</li> <li>Opensource and free to use</li> <li>Extensible and customisable</li> </ul>"},{"location":"#adapter-user-guide","title":"Adapter User Guide","text":"<p>For data engineers looking to use the adapter, the user guide can be found here</p>"},{"location":"#adapter-developer-guides","title":"Adapter Developer Guides","text":"<p>For advanced users looking to extend or customise the adapter the adapter developer guide can be found here</p>"},{"location":"#branching","title":"Branching","text":"<p>When creating a branch to work on from please use the branch name of \"feature/YourBranchName\". The case on \"feature/\" matters so please make sure to keep it lower case. Pull requests are to be made into the \"dev\" branch only. Any pull requests made into \"Main\" will be removed and not merged.</p>"},{"location":"#community","title":"Community","text":""},{"location":"#logging-to-delta","title":"Logging to Delta","text":"<p>Logging was previously done to a log file saved in the lakehouse and in json format. This has been changed to now log to a delta table in the lakehouse.</p> <p>It works using 2 tables \"batch\" and \"execution_log\". At the start of the ETL the Prepare step will check if the tables exist and if they done they will be created. This is followed by a check for an \"open\" batch and where the batch is still open it will fail. </p> <p>If you need to close the batch manually, this code is available at the end of the master notebook. </p> <p>If this check passes, a batch will be opened. There are steps in each master numbered notebook to check for failures in previousn notebook runs and this is done using the open batch so previous ETL executions with failures are not picked up and return false stops on the current execution.</p>"},{"location":"1-Why/","title":"Objective of this project","text":"<p>This project is the result of a search to find a tool that would accelerate the development and ongoing maintenance processes relating to data transformation activities within a software-as-a-service (SAAS) alignd lakehouse architecture and technology stack.</p> <p>When designing this project the core, \"must have\" requirements were to:</p> <p>1) Minimise the effort required to <code>track lineage and dependencies</code> between data transformation activities</p> <p>2) Minimise the effort required to create, maintain and update <code>data transformation activities</code></p> <p>3) Minimise the effort required to <code>document data transformation activities</code></p> <p>4) Minimise the effort required to <code>create and maintain a data catalog</code> of the entities created by data transformation activities</p> <p>5) Be compatable with a <code>Software-As-A-Service (SAAS) technology stack</code> and avoid the neeed for platform as a Service (PaaS) components</p> <p>6) <code>Avoid locking-in transformation logic</code> into specific vendor proprietry language or technology stack.</p> <p>7) Minimise the effort required to <code>test data transformation activities</code></p> <p>Secondary \"nice to have\" requirements were:</p> <p>1) Minimise the effort required to create, maintain, update and execute <code>data quality checks</code></p> <p>2) <code>Automate the generation of data transformation activities</code> from a data lineage and dependencies</p>"},{"location":"1-Why/#discovering-our-approach","title":"Discovering our approach","text":"<p>After reviewing the available tools and technologies and considering the requirements it soon became clear to us that Dbt-Core  appeared to be good fit for our needs. However, there were some limitations that we needed to address:</p> <p>1) Dbt-Core is a command line tool and all of its adapters appeared to require some <code>Platform as a Service (PaaS) comoponents</code> to be introduced into the technology stack in order to host a secure \"python\" runtime environement. This was not compatable with our requirement for a Software-As-A-Service (SAAS) technology stack.</p> <p>2) Dbt-Core <code>combined code build, test and deployment with re-occuring data pipeline execution</code>. This requires Dbt-core to be run everytime a data pipeline needs to be refreshed regardless of whether underlying changes to business logic have been made.</p> <p>3) Dbt-Core rose to popularity as a tool for data transformation at a time when data warehouses and relational dataabses were the predominant transformation engines. There seemed to be some uncertainty as to how it would  perform in the context of a SAAS, lakehouse architecture. Dbt-Core can be quite \"chatty\" and create multiple connections to the transformation engine. This could be a problem in a SAAS environment where the number of connections may be limited. In addition, code execution against a lakehouse architecture tend to incur higher latency than traditional data warehouses. It is plausible to expect that Dbt-Core's deafult behaviour of creating multiple connections and executing a large number of separate and distinct code blocks would result in relatively long project run times and a degraded developer experience.</p> <p>4) Dbt-core might be considered to be a \"heavy\" tool for some use cases. It is a large codebase with a lot of functionality. It is possible that some users may find it difficult to understand and use.</p>"},{"location":"1-Why/#dbt-child-adapter","title":"Dbt Child Adapter","text":"<p>```mermaid classDiagram     BaseAdapter &lt;|-- SparkAdapter : Inheritance     SparkAdapter &lt;|-- FabricSparkAdapter : Inheritance     namespace Adapter {     class BaseAdapter{         +get_columns_in_relation(relation)         +get_missing_columns(from_relation, to_relation)         +expand_target_column_types(temp_table, to_relation)         +list_relations_without_caching(schema)         +drop_relation(relation)         +truncate_relation(relation)         +rename_relation(from_relation, to_relation)         +get_relation(database, schema, identifier)         +create_schema(schema)         +drop_schema(schema)         +quote(identifier)         +convert_text_type()         +convert_number_type()         +convert_boolean_type()         +convert_datetime_type()         +convert_date_type()         +convert_time_type()         +get_rows_different_sql()         +get_merge_sql()         +get_distinct_sql()         +date_function()     }</p> <pre><code>class SparkAdapter{\n    +get_columns_in_relation(relation)\n    +get_missing_columns(from_relation, to_relation)\n    +expand_target_column_types(temp_table, to_relation)\n    +list_relations_without_caching(schema)\n    +drop_relation(relation)\n    +truncate_relation(relation)\n    +rename_relation(from_relation, to_relation)\n    +get_relation(database, schema, identifier)\n    +create_schema(schema)\n    +drop_schema(schema)\n    +quote(identifier)\n    +convert_text_type()\n    +convert_number_type()\n    +convert_boolean_type()\n    +convert_datetime_type()\n    +convert_date_type()\n    +convert_time_type()\n    +get_rows_different_sql()\n    +get_merge_sql()\n    +get_distinct_sql()\n    +date_function()\n}\nclass FabricSparkAdapter {\n     -COLUMN_NAMES\n     -INFORMATION_COLUMNS_REGEX\n     -INFORMATION_OWNER_REGEX\n     -INFORMATION_STATISTICS_REGEX\n     -HUDI_METADATA_COLUMNS\n     -CONSTRAINT_SUPPORT\n     -Relation\n     -RelationInfo\n     -Column\n     -ConnectionManager\n     -AdapterSpecificConfigs\n\n     +_get_relation_information()\n     +_get_relation_information_using_describe()\n     +_build_spark_relation_list()\n     +get_relation()\n     +parse_describe_extended()\n     +find_table_information_separator()\n     +get_columns_in_relation()\n     +parse_columns_from_information()\n     +_get_columns_for_catalog()\n     +get_catalog()\n     +execute()\n     +list_schemas()\n     +check_schema_exists()\n     +get_rows_different_sql()\n     +standardize_grants_dict()\n     +debug_query()\n\n     +date_function()\n     +convert_text_type()\n     +convert_number_type()\n     +convert_integer_type()\n     +convert_date_type()\n     +convert_time_type()\n     +convert_datetime_type()\n     +quote()\n }\n}\n</code></pre>"},{"location":"_coverpage/","title":"coverpage","text":""},{"location":"_coverpage/#dbt-fabric-spark-notebook-generator-dbt-fabricsparknb-10","title":"Dbt Fabric Spark Notebook Generator (Dbt-FabricSparkNb 1.0","text":"<p>A transformation accelerator for a true, modern, software-as-a-service (SAAS) Lakehouse using Microsoft Fabric</p> <p>GitHub Get Started</p>"},{"location":"_sidebar/","title":"sidebar","text":"<ul> <li>HOME</li> <li>What?</li> <li>Why?</li> <li>How?</li> <li>Who?</li> <li>User Guide for Data Engineers</li> <li>Environment Setup</li> <li>Dbt Project Setup</li> <li>Development Workflow</li> <li> <p>Adpater Developer Guide</p> </li> <li> <p>Contact</p> </li> </ul>"},{"location":"dbt-adapter-user-guide/","title":"Macro Syntax Error","text":"<p>File: <code>dbt-adapter-user-guide.md</code></p> <p>Line 144 in Markdown file: unexpected '.' <pre><code># files using the `{{ config(...) }}` macro.\n</code></pre></p>"},{"location":"test/","title":"Test","text":"<p>dbt enables data analysts and engineers to transform their data using the same practices that software engineers use to build applications.</p> <p>dbt is the T in ELT. Organize, cleanse, denormalize, filter, rename, and pre-aggregate the raw data in your warehouse so that it's ready for analysis.</p>"},{"location":"test/#getting-started","title":"Getting started","text":"<ul> <li>Install dbt</li> <li>Read the introduction and viewpoint</li> </ul>"},{"location":"test/#running-locally","title":"Running locally","text":""},{"location":"test/#installs","title":"Installs","text":""},{"location":"test/#windows","title":"Windows","text":"<pre><code>winget install microsoft.azd\n</code></pre>"},{"location":"test/#osx","title":"OSX","text":"<pre><code>brew tap azure/azd &amp;&amp; brew install azd\n</code></pre>"},{"location":"test/#profile","title":"Profile","text":"<p>Create a profile like this one:</p> <pre><code>fabric-spark-test:\n  target: fabricspark-dev\n    fabricspark-dev:\n        authentication: CLI\n        method: livy\n        connect_retries: 0\n        connect_timeout: 10\n        endpoint: https://api.fabric.microsoft.com/v1\n        workspaceid: bab084ca-748d-438e-94ad-405428bd5694\n        workspacename: myworkspace\n        lakehouseid: ccb45a7d-60fc-447b-b1d3-713e05f55e9a\n        lakehouse: test\n        schema: test\n        threads: 1\n        type: fabricspark\n        retry_all: true\n</code></pre>"},{"location":"developer_guide/applications_setup/","title":"Applications Required for Development (Windows)","text":"<p>This section covers the applications required for developing using the dbt framework. The applications are:</p> <ul> <li>python (latest version available)</li> <li>Visual Studio Code</li> <li>GIT for Windows</li> <li>OneLake Explorer (Preview)</li> </ul>"},{"location":"developer_guide/applications_setup/#install-python","title":"Install python","text":"<p>First you need to install python. This can be done from this link Download Python | Python.org download the latest version and install it. </p> <p>NOTE: Make sure to tick the box on the first window of the installation to Add Python.exe to PATH</p> <p>Use Install now for standard installation.</p> <p>Once installed, confirm installation by opening the command line. WindowsKey + R then enter \u201ccmd\u201d click OK. </p> <p>Type the following and you should get a reply similar to the screenshot below. <code>command prompt  \u201cpython --version\u201d</code></p> <p></p>"},{"location":"developer_guide/applications_setup/#install-visual-studio-code","title":"Install Visual Studio Code","text":"<p>Browse to website Visual Studio Code - Code Editing. Redefined and download Visual Studio Code and then open the file.</p> <p>Select the standard options and install.</p>"},{"location":"developer_guide/applications_setup/#install-git-for-windows","title":"Install GIT for Windows","text":"<p>Browse to website Git - Downloading Package (git-scm.com) and download the latest by clicking \u201cClick here to download\u201d option. Open the file and install following the step examples.</p> <p></p> <p>Select the standard options until you get these next steps.</p> <p>This step will ask you about your default git application please change this to Use Visual Studio Code as Git default. </p> <p></p> <p>This step will ask you about your default git console, please select Use Windows default console window.</p> <p></p> <p>The rest of the installation options should be standard unless you need to change them for other reasons.</p>"},{"location":"developer_guide/applications_setup/#onelake-explorer-preview","title":"OneLake Explorer (Preview)","text":"<p>Browse to website OneLake Explorer and browse down the page to Installation instructions and download OneLake file explorer and then open the file.</p> <p>Click install and follow standard install options. </p> <p>Opening up for the first time will require you to login using your Fabric tenant details. You will then be able to access the Lakehouses from your Windows Explorer.</p> <p></p> <p>This concludes the required applications.</p>"},{"location":"developer_guide/dbt_setup/","title":"Setting Up dbt","text":"<p>The following sections are covered in this document:</p> <ul> <li>Repo clone</li> <li>python evironment setup</li> <li>dbt installation</li> </ul>"},{"location":"developer_guide/dbt_setup/#repo-clone","title":"Repo Clone","text":"<p>First you need to clone this repo locally using Visual studio code. For these instructions the feature/dev or dev branch will be the branches to work with.</p> <p>If you do not get the Clone Repository option when selecting Source Control from the menu, then you have not installed GIT and will need to complete that first.</p> <p></p>"},{"location":"developer_guide/dbt_setup/#python-eevironment-setup","title":"Python Eevironment Setup","text":"<p>Once the repo has been cloned you can open a terminal window in VS Code and open a bash console.</p> <p>In new terminal window there is a plus symbol with a drop down. Select this drop down and click Git Bash. This will open a bash console. </p> <p></p> <p>To create a virtual python environment execute the code below, \"dbt-env\" being the name of your virtual environment: <pre><code># Python Virtual Environment\npython -m venv dbt-env\n</code></pre></p> <p>To activate the environment execute this: <pre><code># Python Virtual Environment\nsource dbt-env/Scripts/activate\n</code></pre></p> <p></p> <p>The virtual environment would have created a folder structure in your repo. This can be excluded in your gitignore file. If you used the default above it is already in the gitignore file.</p>"},{"location":"developer_guide/dbt_setup/#dbt-installation","title":"dbt Installation","text":"<p>Still in the bash console and having your virtual environment active, you can execute the following command to install all the components required for this dbt framework. The requirements.txt file is in the root of the repo. <pre><code># dbt installation\npip install -r requirements.txt\n</code></pre> NOTE: The installation can take sometime to complete. It may look like it's hanging but it is busy executing. If you close the installation you can restart it using the same command above. It will skip any components already installed. </p> <p>This concludes the dbt installation.</p>"},{"location":"developer_guide/framework_setup/","title":"Macro Syntax Error","text":"<p>File: <code>developer_guide\\framework_setup.md</code></p> <p>Line 58 in Markdown file: unexpected '.' <pre><code># files using the `{{ config(...) }}` macro.\n</code></pre></p>"},{"location":"developer_guide/initial_setup/","title":"Initial setup","text":"<p>\ud83d\udce6dbt  \u2523 \ud83d\udcc2adapters  \u2503 \u2523 \ud83d\udcc2fabricsparknb  \u2503 \u2503 \u2523 \ud83d\udcdcinit.py  \u2503 \u2503 \u2523 \ud83d\udcdcversion.py  \u2503 \u2503 \u2523 \ud83d\udcdccatalog.py  \u2503 \u2503 \u2523 \ud83d\udcdcconnections.py  \u2503 \u2503 \u2523 \ud83d\udcdcfabric_spark_credentials.py  \u2503 \u2503 \u2523 \ud83d\udcdcimpl.py  \u2503 \u2503 \u2523 \ud83d\udcdclivysession.py  \u2503 \u2503 \u2523 \ud83d\udcdcmanifest.py  \u2503 \u2503 \u2523 \ud83d\udcdcmock.py  \u2503 \u2503 \u2523 \ud83d\udcdcnotebook.py  \u2503 \u2503 \u2517 \ud83d\udcdcutils.py  \u2503 \u2517 \ud83d\udcdcinit.py   \u2523 \ud83d\udcc2include  \u2503 \u2523 \ud83d\udcc2fabricsparknb  \u2503 \u2503 \u2523 \ud83d\udcc2macros  \u2503 \u2503 \u2503 \u2517 \ud83d\udcc2adapters  \u2503 \u2503 \u2503 \u2503 \u2517 \ud83d\udcdcschema.sql  \u2503 \u2503 \u2523 \ud83d\udcc2notebooks  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcimport_notebook.ipynb  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmaster_notebook.ipynb  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmaster_notebook_x.ipynb  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmetadata_extract.ipynb  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcmodel_notebook.ipynb  \u2503 \u2503 \u2503 \u2517 \ud83d\udcdctest_notebook.ipynb  \u2503 \u2503 \u2523 \ud83d\udcc2pwsh  \u2503 \u2503 \u2503 \u2523 \ud83d\udcdcdownload.ps1  \u2503 \u2503 \u2503 \u2517 \ud83d\udcdcupload.ps1  \u2503 \u2503 \u2523 \ud83d\udcdcinit.py  \u2503 \u2503 \u2523 \ud83d\udcdcdbt_project.yml  \u2503 \u2503 \u2523 \ud83d\udcdcmetadata_extract.ipynb  \u2503 \u2503 \u2517 \ud83d\udcdcprofile_template.yml  \u2503 \u2517 \ud83d\udcdcinit.py  \u2517 \ud83d\udcdcinit.py</p>"},{"location":"user_guide/dbt_project_setup/","title":"Macro Syntax Error","text":"<p>File: <code>user_guide\\dbt_project_setup.md</code></p> <p>Line 58 in Markdown file: unexpected '.' <pre><code># files using the `{{ config(...) }}` macro.\n</code></pre></p>"},{"location":"user_guide/development_workflow/","title":"Development Workflow","text":"<p>The following diagram illustrates the development workflow for a dbt project that uses the Dbt-FabricSparkNb adapter.</p> <p>filename</p> <p>Inital Setup 1. Provision Workspace    - Development Environment: Fabric Portal    - Re-occurence: Do once per development environment set-up    - Instructions: Create a new workspace in the Power BI Portal, or use an existing workspace.</p> <ol> <li>Get Workspace Connection Details</li> <li>Development Environment: Fabric Portal</li> <li>Re-occurence: Do once per development environment set-up</li> <li> <p>Instructions: Get the workspace connection details from the Power BI Portal.</p> </li> <li> <p>Create or Update <code>profiles.yml</code></p> </li> <li>Development Environment: VS Code on local, developemnt machine</li> <li></li> <li> <p>Create or Update <code>dbt_project.yml</code> </p> </li> <li>Build Project</li> <li>Manually Upload Notebooks </li> <li>Run Meta Data Extract</li> </ol> <p>Ongoing Development Cycle</p> <ol> <li> <p>Download Metadata: </p> </li> <li> <p>Update Dbt Project </p> </li> <li>Build Dbt Project </li> <li>Verify Outputs </li> <li>Update Notebooks     <ol> <li>Upload to Onelake</li> <li>Update to GIT repo</li> </ol> </li> <li>Promote to Workspace     <ol> <li>Run Import Notebook</li> <li>Promote GIT branch</li> </ol> </li> <li>Run Master Notebook </li> <li>Validate Results </li> <li>Run Metadata Extract</li> </ol>"},{"location":"user_guide/initial_setup/","title":"Environment Setup","text":"<p>This section outlines the steps required to setup the development environment to use this dbt-adapter as part of a dbt data transformation project.</p> <p>To provide a common, cross-platform set of instructions we will first install Powershell. To facilitate the installation process we will use package managers such as winget for Windows, brew for MacOS and <code>apt</code> for Linux.</p>"},{"location":"user_guide/initial_setup/#core-tools-installation","title":"Core Tools Installation","text":""},{"location":"user_guide/initial_setup/#windows","title":"Windows","text":"<pre><code># Winget Installs \nwinget install Microsoft.PowerShell\n</code></pre>"},{"location":"user_guide/initial_setup/#macos","title":"MacOS","text":"<pre><code>brew install powershell/tap/powershell\n</code></pre>"},{"location":"user_guide/initial_setup/#linux","title":"Linux","text":"<pre><code># TBA\n</code></pre> <p>Next we will install Python and development tools such as vscode.</p>"},{"location":"user_guide/initial_setup/#windows_1","title":"Windows","text":"<pre><code># Winget Installs \nwinget install -e --id Python.Python -v 3.12.0\nwinget install -e --id Microsoft.VisualStudioCode\nwinget install --id Git.Git -e --source winget\n\n# Python Environment Manager\nPython3 -m pip install --user virtualenv\n</code></pre>"},{"location":"user_guide/initial_setup/#macos_1","title":"MacOS","text":"<pre><code># Brew Installs\nbrew install python@3.12\nbrew install --cask visual-studio-code\nbrew install git\n\n# Python Environment Manager\nPython3 -m pip install --user virtualenv\n\n# TODO \n# Add OSX AZ Copy Instructions\n</code></pre>"},{"location":"user_guide/initial_setup/#linux_1","title":"Linux","text":"<pre><code># TBA\n</code></pre>"},{"location":"user_guide/initial_setup/#other-tools","title":"Other tools","text":"<p>Now that we have pwsh installed, we can use it as a cross platform shell to install the additional required tools. </p>"},{"location":"user_guide/initial_setup/#windows_2","title":"Windows","text":"<pre><code># Az Copy Install - No Winget Package Available\nInvoke-WebRequest -Uri https://aka.ms/downloadazcopy-v10-windows -OutFile AzCopy.zip -UseBasicParsing\nExpand-Archive ./AzCopy.zip ./AzCopy -Force\nNew-Item -ItemType \"directory\" -Path \"$home/AzCopy\"  -Force  \nGet-ChildItem ./AzCopy/*/azcopy.exe | Move-Item -Destination \"$home\\AzCopy\\AzCopy.exe\" -Force  \n$userenv = [System.Environment]::GetEnvironmentVariable(\"Path\", \"User\") \n[System.Environment]::SetEnvironmentVariable(\"PATH\", $userenv + \";$home\\AzCopy\", \"User\")\nRemove-Item .\\AzCopy\\ -Force\nRemove-Item AzCopy.zip -Force\n</code></pre>"},{"location":"user_guide/initial_setup/#macos_2","title":"MacOS","text":"<pre><code># TODO \n# Add OSX AZ Copy Instructions\n</code></pre>"},{"location":"user_guide/initial_setup/#linux_2","title":"Linux","text":"<pre><code># TBA\n</code></pre>"},{"location":"user_guide/initial_setup/#source-directory-python-env","title":"Source Directory &amp; Python Env","text":"<p>Now lets create and activate our Python environment and install the required packages.</p>"},{"location":"user_guide/initial_setup/#windows_3","title":"Windows","text":"<pre><code># Ensure that you are in the pwsh shell\npwsh\n\n# Create a new source code directory\nmkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like\n# Navigate to the new directory\ncd dbt-fabricsparknb-test\n\n# Create and activate the Python environment\npython3 -m venv .env\n./.env/Scripts/Activate.ps1   \n\n# Install dbt-core \npip install dbt-core\n\n# Install dbt-fabricspark\npip install dbt-fabricspark\n\n# Install the dbt-fabricsparknb pre-requisites \npip install azure-storage-file-datalake\npip install nbformat\n\n# Install the dbt-fabricsparknb package from the repository\npip install --upgrade git+https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb\n</code></pre>"},{"location":"user_guide/initial_setup/#macos_3","title":"MacOS","text":"<pre><code># Ensure that you are in the pwsh shell\npwsh\n\n# Create a new source code directory\nmkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like\n# Navigate to the new directory\ncd dbt-fabricsparknb-test\n\n# Create and activate the Python environment\npython3 -m venv .env\n./.env/bin/Activate.ps1   \n\n# Install dbt-core \npip install dbt-core\n\n# Install dbt-fabricspark\npip install dbt-fabricspark\n\n# Install the dbt-fabricsparknb pre-requisites \npip install azure-storage-file-datalake\npip install nbformat\n\n# Install the dbt-fabricsparknb package from the repository\npip install --upgrade git+https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb\n</code></pre>"},{"location":"user_guide/initial_setup/#linux_3","title":"Linux","text":"<pre><code># Ensure that you are in the pwsh shell\npwsh\n\n# Create a new source code directory\nmkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like\n# Navigate to the new directory\ncd dbt-fabricsparknb-test\n\n# Create and activate the Python environment\npython3 -m venv .env\n./.env/bin/Activate.ps1   \n\n# Install dbt-core \npip install dbt-core\n\n# Install dbt-fabricspark\npip install dbt-fabricspark\n\n# Install the dbt-fabricsparknb pre-requisites \npip install azure-storage-file-datalake\npip install nbformat\n\n# Install the dbt-fabricsparknb package from the repository\npip install --upgrade git+https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb\n</code></pre> <p>!&gt; Important You are now ready to move to the next step in which you will set up your dbt project. Follow the Dbt Project Setup guide.</p>"}]}