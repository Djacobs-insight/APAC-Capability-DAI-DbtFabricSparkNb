{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 Attach Default Lakehouse\n",
    "❗**Note the code in the cell that follows is required to programatically attach the lakehouse and enable the running of spark.sql(). If this cell fails simply restart your session as this cell MUST be the first command executed on session start.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"defaultLakehouse\": {  \n",
    "        \"name\": \"{{lakehouse_name}}\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📦 Pip\n",
    "Pip installs reqired specifically for this template should occur here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jsonpickle\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔗 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils # type: ignore\n",
    "from dataclasses import dataclass\n",
    "import jsonpickle # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "from tabulate import tabulate # type: ignore\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #️⃣ Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NotebookResult:    \n",
    "    notebook: str\n",
    "    start_time: float\n",
    "    status: str\n",
    "    error: str\n",
    "    execution_time: float\n",
    "    run_order: int\n",
    "    \n",
    "@dataclass\n",
    "class FileListing:\n",
    "    \"\"\"Class for Files - Attributes: name, directory\"\"\"\n",
    "    name: str\n",
    "    directory: str\n",
    "\n",
    "def get_file_content_using_notebookutils(file):\n",
    "    \"\"\"Get the content of a file using notebookutils.\"\"\"\n",
    "    #return self.mssparkutils.fs.head(file, 1000000000)\n",
    "    data = spark.sparkContext.wholeTextFiles(file).collect() # type: ignore\n",
    "\n",
    "    # data is a list of tuples, where the first element is the file path and the second element is the content of the file\n",
    "    file_content = data[0][1]\n",
    "\n",
    "    return file_content\n",
    "\n",
    "def remove_file_using_notebookutils(file):\n",
    "    \"\"\"Remove a file using notebookutils.\"\"\"\n",
    "    try:\n",
    "        mssparkutils.fs.rm(file, True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def create_path_using_notebookutils(path):\n",
    "    \"\"\"Create a path using notebookutils.\"\"\"\n",
    "    mssparkutils.fs.mkdirs(path)\n",
    "\n",
    "def walk_directory_using_notebookutils(path):\n",
    "    \"\"\"Walk a directory using notebookutils.\"\"\"\n",
    "    # List the files in the directory\n",
    "    files = mssparkutils.fs.ls(path)\n",
    "\n",
    "    # Initialize the list of all files\n",
    "    all_files = []\n",
    "\n",
    "    # Iterate over the files\n",
    "    for file in files:\n",
    "        # If the file is a directory, recursively walk the directory\n",
    "        if file.isDir:\n",
    "            all_files.extend(\n",
    "                walk_directory_using_notebookutils(file.path))\n",
    "        else:\n",
    "            # If the file is not a directory, add it to the list of all files\n",
    "            directory = os.path.dirname(file.path)\n",
    "            name = file.name\n",
    "            all_files.append(FileListing(\n",
    "                name=name, directory=directory))\n",
    "\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔒 Embed HASH information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_hashes = {{ hashes }}\n",
    "RelativePathForMetaData = \"Files/MetaExtracts/\"\n",
    "current_hashes = json.loads(get_file_content_using_notebookutils(RelativePathForMetaData + 'MetaHashes.json'))\n",
    "\n",
    "def get_hash(file, hashes):\n",
    "    ret = \"\"\n",
    "    for h in hashes:\n",
    "        if(h['file'] == file):\n",
    "            return h['hash']\n",
    "    return ret\n",
    "        \n",
    "if current_hashes != embedded_hashes:\n",
    "    for h in embedded_hashes:\n",
    "        print(\n",
    "                h['file'] + '\\n \\t Emb Hash: ' + get_hash(h['file'], embedded_hashes) + '\\n \\t Env Hash: ' + get_hash(h['file'], current_hashes)\n",
    "        )\n",
    "    raise Exception('Hashes do not match. Please re-generate the dbt project using the latest extract of the target environment metadata.')\n",
    "else:\n",
    "    print('Metadata Hashes Match 😏')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_location = 'Files/Control/'\n",
    "create_path_using_notebookutils(log_location)\n",
    "remove_file_using_notebookutils(log_location+'dbt_execution.log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executions for Each Run Order Below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if file exists\n",
    "log_location = 'Files/Control/'\n",
    "create_path_using_notebookutils(log_location)\n",
    "files = walk_directory_using_notebookutils(log_location)\n",
    "failed_results = []\n",
    "\n",
    "if len([f for f in files if f.name == 'dbt_execution.log']) > 0:\n",
    "    # Read the file\n",
    "    contents = get_file_content_using_notebookutils(log_location + 'dbt_execution.log')\n",
    "    \n",
    "    # Deserialize the JSON string into a list of dictionaries\n",
    "    results_dict = jsonpickle.decode(contents)\n",
    "\n",
    "    # Convert each dictionary to a NotebookResult object\n",
    "    results = [NotebookResult(**result) for result in results_dict]\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame([(result.notebook, result.status) for result in results], columns=['Notebook', 'Status'])\n",
    "\n",
    "    # Convert the DataFrame to a pretty text-based table\n",
    "    table = tabulate(df, headers='keys', tablefmt='psql')\n",
    "\n",
    "    # Print the table\n",
    "    print(table)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
