{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 Attach Default Lakehouse\n",
    "❗**Note the code in the cell that follows is required to programatically attach the lakehouse and enable the running of spark.sql(). If this cell fails simply restart your session as this cell MUST be the first command executed on session start.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"defaultLakehouse\": {  \n",
    "        \"name\": \"{{lakehouse_name}}\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📦 Pip\n",
    "Pip installs reqired specifically for this template should occur here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jsonpickle\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔗 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils # type: ignore\n",
    "from dataclasses import dataclass\n",
    "import jsonpickle # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "from tabulate import tabulate # type: ignore\n",
    "from pyspark.sql.functions import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #️⃣ Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NotebookResult:    \n",
    "    notebook: str\n",
    "    start_time: float\n",
    "    status: str\n",
    "    error: str\n",
    "    execution_time: float\n",
    "    run_order: int\n",
    "    \n",
    "@dataclass\n",
    "class FileListing:\n",
    "    \"\"\"Class for Files - Attributes: name, directory\"\"\"\n",
    "    name: str\n",
    "    directory: str\n",
    "\n",
    "def get_file_content_using_notebookutils(file):\n",
    "    \"\"\"Get the content of a file using notebookutils.\"\"\"\n",
    "    #return self.mssparkutils.fs.head(file, 1000000000)\n",
    "    data = spark.sparkContext.wholeTextFiles(file).collect() # type: ignore\n",
    "\n",
    "    # data is a list of tuples, where the first element is the file path and the second element is the content of the file\n",
    "    file_content = data[0][1]\n",
    "\n",
    "    return file_content\n",
    "\n",
    "def remove_file_using_notebookutils(file):\n",
    "    \"\"\"Remove a file using notebookutils.\"\"\"\n",
    "    try:\n",
    "        mssparkutils.fs.rm(file, True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def create_path_using_notebookutils(path):\n",
    "    \"\"\"Create a path using notebookutils.\"\"\"\n",
    "    mssparkutils.fs.mkdirs(path)\n",
    "\n",
    "def walk_directory_using_notebookutils(path):\n",
    "    \"\"\"Walk a directory using notebookutils.\"\"\"\n",
    "    # List the files in the directory\n",
    "    files = mssparkutils.fs.ls(path)\n",
    "\n",
    "    # Initialize the list of all files\n",
    "    all_files = []\n",
    "\n",
    "    # Iterate over the files\n",
    "    for file in files:\n",
    "        # If the file is a directory, recursively walk the directory\n",
    "        if file.isDir:\n",
    "            all_files.extend(\n",
    "                walk_directory_using_notebookutils(file.path))\n",
    "        else:\n",
    "            # If the file is not a directory, add it to the list of all files\n",
    "            directory = os.path.dirname(file.path)\n",
    "            name = file.name\n",
    "            all_files.append(FileListing(\n",
    "                name=name, directory=directory))\n",
    "\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS execution_log (\n",
    "  notebook STRING,\n",
    "  start_time DOUBLE,\n",
    "  status STRING,\n",
    "  error STRING,\n",
    "  execution_time DOUBLE,\n",
    "  run_order INT,\n",
    "  batch_id INT\n",
    ")\n",
    "USING DELTA\n",
    "'''\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "CREATE TABLE IF NOT EXISTS batch (\n",
    "  batch_id INT,\n",
    "  start_time LONG,\n",
    "  status STRING\n",
    ")\n",
    "USING DELTA\n",
    "'''\n",
    "spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the SQL query to find the latest open batch\n",
    "latest_batch_id = spark.sql(\"SELECT MAX(batch_id) AS LatestBatchID FROM batch WHERE status = 'open'\").collect()[0]['LatestBatchID']\n",
    "\n",
    "# Check if there is an open batch and raise an error if there is\n",
    "if latest_batch_id is not None:\n",
    "    raise ValueError(f\"There is an open batch with BatchID {latest_batch_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SQL query\n",
    "sql_query = \"SELECT COALESCE(MAX(batch_id), 0) + 1 AS batch_id, UNIX_TIMESTAMP() AS start_time, 'open' AS status FROM batch\"\n",
    "\n",
    "# Execute the SQL query and store the output in a DataFrame\n",
    "df = spark.sql(sql_query)\n",
    "\n",
    "# Append the DataFrame to the existing table 'batch'\n",
    "df.write.format(\"delta\").mode(\"append\").saveAsTable(\"batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executions for Each Run Order Below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current Open Batch\n",
    "latest_batch_id = spark.sql(\"SELECT MAX(batch_id) AS LatestBatchID FROM batch WHERE status = 'open'\").collect()[0]['LatestBatchID']\n",
    "# Create a DataFrame with the updated status\n",
    "df = spark.table(\"batch\").where((col(\"batch_id\") == latest_batch_id) & (col(\"status\") == \"open\")).withColumn(\"status\", lit(\"closed\"))\n",
    "# Update the Delta Lake table with the new status\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"replaceWhere\", \"batch_id = {}\".format(latest_batch_id)).save(\"Tables/batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if batch exists\n",
    "failed_results = []\n",
    "# Get latest Batch\n",
    "latest_batch_id = spark.sql(\"SELECT MAX(batch_id) AS LatestBatchID FROM batch\").collect()[0]['LatestBatchID']\n",
    "# Read the log for this batch execution\n",
    "df_execution_log = spark.table(\"execution_log\").where((col(\"batch_id\") == latest_batch_id))\n",
    "if df_execution_log.count() > 0:\n",
    "\n",
    "    # Check if have succeeded\n",
    "    all_results = df_execution_log\n",
    "\n",
    "    # Print the succeeded results\n",
    "    for row in all_results.select(\"notebook\", \"status\").collect():\n",
    "        print(f\"Notebook {row['notebook']} execution status: {row['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🛑 Execution Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exit to prevent spark sql debug cell running \n",
    "mssparkutils.notebook.exit(\"value string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close The Batch Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure that the config, pip install and import tasks have been executed before running this code\n",
    "\n",
    "# Get current Open Batch\n",
    "latest_batch_id = spark.sql(\"SELECT MAX(batch_id) AS LatestBatchID FROM batch WHERE status = 'open'\").collect()[0]['LatestBatchID']\n",
    "# Create a DataFrame with the updated status\n",
    "df = spark.table(\"batch\").where((col(\"batch_id\") == latest_batch_id) & (col(\"status\") == \"open\")).withColumn(\"status\", lit(\"closed\"))\n",
    "# Update the Delta Lake table with the new status\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"replaceWhere\", \"batch_id = {}\".format(latest_batch_id)).save(\"Tables/batch\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
