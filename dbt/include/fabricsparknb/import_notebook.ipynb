{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "from notebookutils import mssparkutils\n",
    "\n",
    "class Utils:\n",
    "\n",
    "    # Synapse utils\n",
    "\n",
    "    def get_access_token(azure_client_id, azure_tenant_id, azure_client_secret):\n",
    "        url = f\"https://login.microsoftonline.com/{azure_tenant_id}/oauth2/token\"\n",
    "\n",
    "        payload = {\n",
    "            \"grant_type\": \"client_credentials\",\n",
    "            \"client_id\": {azure_client_id},\n",
    "            \"client_secret\": {azure_client_secret},\n",
    "            \"resource\": f\"https://dev.azuresynapse.net/\"\n",
    "        }\n",
    "\n",
    "        headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "\n",
    "        response = requests.post(url, data=payload, headers=headers)\n",
    "        response_json = json.loads(response.text)\n",
    "        synapse_dev_token = response_json[\"access_token\"]\n",
    "\n",
    "        return synapse_dev_token\n",
    "\n",
    "    # Notebook utils\n",
    "\n",
    "    def clean_notebook_cells(ntbk_json, tags_to_clean):\n",
    "        for cell in ntbk_json['cells']:\n",
    "            for tag in tags_to_clean:\n",
    "                if tag in cell:\n",
    "                    cell[tag] = []\n",
    "\n",
    "        return ntbk_json\n",
    "\n",
    "    def export_notebooks(azure_client_id, azure_tenant_id, azure_client_secret, synapse_workspace_name, output_folder):\n",
    "        resource_type = \"notebooks\"\n",
    "        Utils.export_resources(resource_type, azure_client_id, azure_tenant_id, azure_client_secret, synapse_workspace_name, output_folder)\n",
    "\n",
    "    def import_notebooks(output_folder, workspace_id, prefix, notebook_names=None):\n",
    "        date = datetime.now().strftime('%Y_%m_%dT%H_%M_%S')\n",
    "        resource_type = \"notebooks\"\n",
    "        res_imported = 0\n",
    "        resources_imported = {}\n",
    "\n",
    "        artifact_path = f\"{output_folder}/{resource_type}\"\n",
    "\n",
    "        if not os.path.exists(artifact_path):\n",
    "            print(f\"Path where the import artifacts from Synapse are located {artifact_path} does not exist. Exiting ...\")\n",
    "            return\n",
    "\n",
    "        print(f\"Importing individual resources of type '{resource_type}' into Fabric workspace '{workspace_id}'...\")\n",
    "        if notebook_names is None:\n",
    "            notebook_names = [name.split('.')[0] for name in os.listdir(artifact_path) if name.endswith(\".ipynb\")]\n",
    "\n",
    "        for notebook_name in notebook_names:\n",
    "            file_path = os.path.join(artifact_path, f\"{notebook_name}.ipynb\")\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, \"r\", encoding='utf-8') as read_file:\n",
    "                    ntbk_json = json.load(read_file)\n",
    "                ntbk_name = f\"{prefix}_{notebook_name}\"\n",
    "                Utils.import_notebook(ntbk_name, ntbk_json, workspace_id, False)\n",
    "                res_imported += 1\n",
    "\n",
    "        resources_imported[resource_type] = res_imported\n",
    "        print(f\"Finish importing {res_imported} items of type: {resource_type}\")\n",
    "        \n",
    "    def import_notebook(ntbk_name, ntbk_json, workspace_id, overwrite=False):\n",
    "\n",
    "        api_endpoint = \"api.fabric.microsoft.com\"\n",
    "        pbi_token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api') \n",
    "\n",
    "        print(f\"Importing '{ntbk_name}'...\")\n",
    "        url = f\"https://{api_endpoint}/v1/workspaces/{workspace_id}/items\"\n",
    "\n",
    "        json_str = json.dumps(ntbk_json)\n",
    "        json_bytes = json_str.encode('utf-8')\n",
    "        base64_encoded_json = base64.b64encode(json_bytes)\n",
    "        base64_str = base64_encoded_json.decode('utf-8')\n",
    "\n",
    "        payload = json.dumps({\n",
    "            \"type\": \"Notebook\",\n",
    "            \"description\": \"Imported from Synapse\",\n",
    "            \"displayName\": ntbk_name,\n",
    "            \"definition\" : {\n",
    "                \"format\": \"ipynb\",\n",
    "                \"parts\" : [\n",
    "                    {\n",
    "                        \"path\": \"notebook-content.ipynb\",\n",
    "                        \"payload\": base64_str,\n",
    "                        \"payloadType\": \"InlineBase64\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        })\n",
    "\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {pbi_token}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "        if response.ok:\n",
    "            print(f\">> Notebook '{ntbk_name}' created.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Notebook '{ntbk_name}' creation failed: {response.status_code}: {response.text}\")\n",
    "\n",
    "    # SJD utils\n",
    "\n",
    "    def export_sjd(azure_client_id, azure_tenant_id, azure_client_secret, synapse_workspace_name, output_folder):\n",
    "        resource_type = \"sparkJobDefinitions\"\n",
    "        Utils.export_resources(resource_type, azure_client_id, azure_tenant_id, azure_client_secret, synapse_workspace_name, output_folder)\n",
    "\n",
    "    def import_sjd(sjd_name, sjd_json, workspace_id, overwrite=False):\n",
    "\n",
    "        api_endpoint = \"api.fabric.microsoft.com\"\n",
    "        pbi_token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api') \n",
    "\n",
    "        print(f\"Importing '{sjd_name}'...\")\n",
    "        url = f\"https://{api_endpoint}/v1/workspaces/{workspace_id}/items\"\n",
    "\n",
    "        json_str = json.dumps(sjd_json)\n",
    "        json_bytes = json_str.encode('utf-8')\n",
    "        base64_encoded_json = base64.b64encode(json_bytes)\n",
    "        base64_str = base64_encoded_json.decode('utf-8')\n",
    "\n",
    "        payload = json.dumps({\n",
    "            \"type\": \"SparkJobDefinition\",\n",
    "            \"description\": \"Imported from Synapse\",\n",
    "            \"displayName\": sjd_name,\n",
    "            \"definition\" : {\n",
    "                \"format\": \"SparkJobDefinitionV1\",\n",
    "                \"parts\" : [\n",
    "                    {\n",
    "                        \"path\": \"SparkJobDefinitionV1.json\",\n",
    "                        \"payload\": base64_str,\n",
    "                        \"payloadType\": \"InlineBase64\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        })\n",
    "\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {pbi_token}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "        if response.ok:\n",
    "            print(f\">> SJD '{sjd_name}' created.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"SJD '{sjd_name}' creation failed: {response.status_code}: {response.text}\")\n",
    "        \n",
    "    def import_sjd_from_json(sjd_name, sjd_json, workspace_id, lakehouse_id, overwrite=False):\n",
    "\n",
    "        executable_file_path = sjd_json[\"properties\"][\"jobProperties\"][\"file\"]\n",
    "        language = sjd_json[\"properties\"][\"language\"]\n",
    "        if language == \"scala\":\n",
    "            mainclass = sjd_json[\"properties\"][\"jobProperties\"][\"className\"]\n",
    "            language = \"Scala/Java\"\n",
    "        else:\n",
    "            mainclass = None\n",
    "        libs = sjd_json[\"properties\"][\"jobProperties\"][\"jars\"]\n",
    "        libs = \" \".join(libs)\n",
    "        args = sjd_json[\"properties\"][\"jobProperties\"][\"args\"]\n",
    "        args = \" \".join(args)\n",
    "        \n",
    "        workload_json = {\n",
    "            \"executableFile\":executable_file_path,\n",
    "            \"defaultLakehouseArtifactId\":lakehouse_id,\n",
    "            \"mainClass\":mainclass,\n",
    "            \"additionalLakehouseIds\":[],\n",
    "            \"retryPolicy\":None,\n",
    "            \"commandLineArguments\":args,\n",
    "            \"additionalLibraryUris\":libs,\n",
    "            \"language\":language,\n",
    "            \"environmentArtifactId\":None\n",
    "        }\n",
    "    \n",
    "        Utils.import_sjd(sjd_name, workload_json, workspace_id, False)\n",
    "\n",
    "    def import_sjds(output_folder, workspace_id, lakehouse_id, prefix, sjd_names=None):\n",
    "        resource_type = \"sparkJobDefinitions\"\n",
    "        res_imported = 0\n",
    "        resources_imported = {}\n",
    "\n",
    "        artifact_path = f\"{output_folder}/{resource_type}\"\n",
    "\n",
    "        if not os.path.exists(artifact_path):\n",
    "            print(f\"Path where the import artifacts from Synapse are located {artifact_path} does not exist. Exiting ...\")\n",
    "            return\n",
    "\n",
    "        print(f\"Importing individual resources of type '{resource_type}' into Fabric workspace '{workspace_id}'...\")\n",
    "        if sjd_names is None:\n",
    "            sjd_names = [name.split('.')[0] for name in os.listdir(artifact_path) if name.endswith(\".json\")]\n",
    "\n",
    "        for sjd_name in sjd_names:\n",
    "            file_path = os.path.join(artifact_path, f\"{sjd_name}.json\")\n",
    "            if os.path.exists(file_path):\n",
    "                with open(file_path, \"r\", encoding='utf-8') as read_file:\n",
    "                    sjd_json = json.load(read_file)\n",
    "                full_sjd_name = f\"{prefix}_{sjd_name}\"\n",
    "                Utils.import_sjd_from_json(full_sjd_name, sjd_json, workspace_id, lakehouse_id, False)\n",
    "                res_imported += 1\n",
    "\n",
    "        resources_imported[resource_type] = res_imported\n",
    "        print(f\"Finish importing {res_imported} items of type: {resource_type}\")\n",
    "\n",
    "    # Generic\n",
    "\n",
    "    def export_resources(resource_type, azure_client_id, azure_tenant_id, azure_client_secret, synapse_workspace_name, output_folder):\n",
    "\n",
    "        base_uri = f\"{synapse_workspace_name}.dev.azuresynapse.net\"\n",
    "        api_version = \"2020-12-01\"\n",
    "        synapse_dev_token = Utils.get_access_token(azure_client_id, azure_tenant_id, azure_client_secret)\n",
    "        res_exported = 0\n",
    "        resources_exported = {}\n",
    "\n",
    "        url = f\"https://{base_uri}/{resource_type}?api-version={api_version}\"\n",
    "\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {synapse_dev_token}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "        if response != None and response.ok:\n",
    "            response_json = response.json()\n",
    "            print(f\"Exporting individual resources of type '{resource_type}' from '{synapse_workspace_name}' Azure Synapse workspace...\")\n",
    "            if \"value\" in response_json:\n",
    "                response_json = response_json['value']\n",
    "            elif \"items\" in response_json:\n",
    "                response_json = response_json['items']\n",
    "            for artifact in response_json:\n",
    "                if \"name\" in artifact:\n",
    "                    resource_name = artifact[\"name\"]\n",
    "                elif \"Name\" in artifact:\n",
    "                    resource_name = artifact[\"Name\"]\n",
    "                print(f\"Exporting '{resource_name}' ...\")\n",
    "                resource_url = f\"https://{base_uri}/{resource_type}/{resource_name}?api-version={api_version}\"\n",
    "                resource_response = requests.request(\"GET\", resource_url, headers=headers)\n",
    "\n",
    "                if resource_response != None and response.ok:\n",
    "                    resource_response_json = resource_response.json()\n",
    "\n",
    "                    if (resource_type == \"sparkJobDefinitions\"):\n",
    "                        sjd_json = resource_response_json\n",
    "                        file_name = f\"{resource_name}.json\"\n",
    "                        data = json.dumps(sjd_json, indent=4)\n",
    "                    elif (resource_type == \"notebooks\"):\n",
    "                        notebook_json = resource_response_json['properties']\n",
    "                        tags_to_clean = ['outputs']\n",
    "                        updated_ntbk_json = Utils.clean_notebook_cells(notebook_json, tags_to_clean)\n",
    "                        file_name = f\"{resource_name}.ipynb\"\n",
    "                        data = json.dumps(updated_ntbk_json, indent=4)\n",
    "                    \n",
    "                    mssparkutils.fs.put(f\"{output_folder}/{resource_type}/{file_name}\", data, False)\n",
    "                    res_exported += 1\n",
    "                    resources_exported[resource_type] = res_exported\n",
    "                    \n",
    "        else:\n",
    "            raise RuntimeError(f\"Exporting items of type '{resource_type}' failed: {response.status_code}: {response}\")\n",
    "\n",
    "        print(f\"Finish exporting {resources_exported[resource_type]} items of type: {resource_type}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Utils.import_notebooks(f\"/lakehouse/default/Files/notebooks/\", {{workspace_id}}, \"\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
